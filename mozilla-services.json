[
{
  "name": "mozilla-pipeline-schemas",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CMakeLists.txt",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "GRAVEYARD.md",
      "LICENSE.txt",
      "README.md",
      "README.pioneer.md",
      "README.shield.md",
      "mozilla_pipeline_schemas",
      "pom.xml",
      "requirements-dev.in",
      "requirements-dev.txt",
      "requirements.txt",
      "schemas",
      "scripts",
      "setup.py",
      "templates",
      "tests",
      "validation"
    ],
    "/.github": [
      "auto_assign.yml",
      "pull_request_template.md",
      "push-to-trigger-integration"
    ],
    "/.circleci": [
      "config.yml",
      "post-artifact.js"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Pipeline Schemas\n\nThis repository contains schemas for Mozilla's data ingestion pipeline and data\nlake outputs.\n\nThe JSON schemas are used to validate incoming submissions at ingestion time.\nThey also are used as the source of truth for defining metadata about how each\ndocument type should be handled in the pipeline (see [the metaschema](templates/metadata/metaschema/metaschema.1.schema.json)).\n\nThe [`jsonschema` [Python]](https://python-jsonschema.readthedocs.io/en/stable/)\nand [`everit-org/json-schema` [Java]](https://github.com/everit-org/json-schema)\nlibrary (using draft 4) are used for JSON Schema Validation in this repository's\ntests.\nThis has implications for what kinds of string patterns are supported,\nsee the `Conformance` section in the linked document for further details.\nNote that as of 2019, the data pipeline uses the\n[everit-org/json-schema](https://github.com/everit-org/json-schema) library\nfor validation in production (see\n[#302](https://github.com/mozilla-services/mozilla-pipeline-schemas/issues/332)).\n\nTo learn more about writing JSON Schemas,\n[Understanding JSON Schema](https://spacetelescope.github.io/understanding-json-schema/index.html)\nis a great resource.\n\n## Adding a new schema\n\n- Create the JSON Schema in the `templates` directory first. Make use of common schema components from the `templates/include` directory where possible, including things like the telemetry `environment`, `clientId`, `application` block, or UUID patterns. The filename should be `templates/<namespace>/<doctype>/<doctype>.<version>.schema.json`.\n- Build the rendered schemas using the instructions below, and check those artifacts (in the `schemas` directory) in to the git repo as well. See the rationale for this in the \"Notes\" section below.\n- Add one or more example JSON documents to the `validation` directory.\n- Run the tests (either via Docker or directly) using the instructions below.\n- Once all tests pass, submit a PR to the github repository against the `main` branch. See also the notes on [contributions](#contributions).\n\nNote that Pioneer studies have a [slightly amended](README.pioneer.md) process.\n\n## Build\n\n### Prerequisites\n\n- [`CMake` (3.0+)](http://cmake.org/cmake/resources/software.html)\n- [`jq` (1.5+)](https://github.com/stedolan/jq)\n- `python` (3.6+)\n- Optional: `java 11`, `maven`\n- Optional: [Docker](https://www.docker.com/get-started)\n\nOn MacOS, these prerequisites can be installed using [homebrew](https://brew.sh/):\n\n```bash\nbrew install cmake\nbrew install jq\nbrew install python\nbrew cask install docker\n```\n\n### CMake Build Instructions\n\n```bash\ngit clone https://github.com/mozilla-services/mozilla-pipeline-schemas.git\ncd mozilla-pipeline-schemas\nmkdir release\ncd release\n\ncmake ..  # this is the build process (the schemas are built with cmake templates)\n```\n\n### Running Tests via Docker (optional)\n\nYou can generally skip this step if you're just make a small change to an existing schema: tests are\nautomatically run via continuous integration.\n\nThe tests expect example pings to be in the `validation/<namespace>/` subdirectory, with files named\nin the form `<ping type>.<version>.<test name>.pass.json` for documents expected to be valid, or\n`<ping type>.<version>.<test name>.fail.json` for documents expected to fail validation.\nThe `test name` should match the pattern `[0-9a-zA-Z_]+`\n\nTo run the tests, make use of the wrapper scripts:\n\n```bash\n./scripts/mps-build\n./scripts/mps-test\n```\n\n### Packaging and integration tests (optional)\n\nFollow the CMake Build Instructions above to update the `schemas` directory.\nTo run the unit-tests, run the following commands:\n\n```bash\n# optional: activate a virtual environment with python3.6+\npython3 -m venv venv\nsource venv/bin/activate\n\n# install python dependencies, if they haven't already\npip install -r requirements-dev.txt\npip install .\n\n# run the tests, with 8 parallel processes\npytest -n 8\n\n# run tests for a specific namespace and doctype\npytest -k telemetry/main.4\n\n# run java tests only (if Java is configured)\npytest -k java\n```\n\nTo generate a diff of BigQuery schemas, use the `mps` command-line tool.\n\n```bash\n# optionally, enter the mozilla-pipeline-schemas environment\n# for jsonschema-transpiler and python3 dependencies\n./script/mps-shell\n\n# generate an integration folder, the options will default to HEAD and main\n# respectively\nmps bigquery diff --base-ref main --head-ref HEAD\n```\n\nThis generates an `integration` folder:\n\n```bash\nintegration\n\u251c\u2500\u2500 bq_schema_f59ca95-d502688.diff\n\u251c\u2500\u2500 d502688\n\u2502   \u251c\u2500\u2500 activity-stream.events.1.bq\n\u2502   \u251c\u2500\u2500 activity-stream.impression-stats.1.bq\n...\n\u2502   \u2514\u2500\u2500 webpagetest.webpagetest-run.1.bq\n\u2514\u2500\u2500 f59ca95\n    \u251c\u2500\u2500 activity-stream.events.1.bq\n    \u251c\u2500\u2500 activity-stream.impression-stats.1.bq\n    ...\n    \u2514\u2500\u2500 webpagetest.webpagetest-run.1.bq\n```\n\nPushes to the main repo will trigger integration tests in CircleCI that directly\ncompare the revision to the `main` branch. These tests do not run for forked PRs\nin order to protect data and credentials, but reviewers can trigger tests to run\nby pushing the PR's revisions to a branch of the main repo. We provide a script for this:\n\n```bash\n# Before running, double check that the PR doesn't make any changes to\n# .circleci/config.yml that could spill sensitive environment variables\n# or data contents to the public CircleCI logs.\n./.github/push-to-trigger-integration <username>:<branchname>\n```\n\nFor details on how to compare two arbitrary revisions, refer to the `integration` job in `.circleci/config.yml`. For more documentation, see [mozilla-services/edge-validator](https://github.com/mozilla-services/edge-validator).\n\n### `mps` command-line tool\n\nThe repository has an `mps` command-line tool for checking on the output of\nschema transformations used for BigQuery. Enter the shell using\n`scripts/mps-shell`.\n\nTo transpile a schema for Bigquery:\n\n```bash\nschema=schemas/glean/glean/glean.1.schema.json\nmps bigquery transpile $schema\n```\n\nIt may be useful to look at a compact version of the output:\n\n```bash\nschema=schemas/glean/glean/glean.1.schema.json\nmps bigquery transpile $schema | mps bigquery columns /dev/stdin\n```\n\nThe output of the ingestion sink can be viewed for validation documents.\n\n```bash\nvalidation=validation/glean/glean.1.baseline.pass.json\nmps bigquery transform $validation | jq\n```\n\nAny value that is not captured in the schema is put into `additional_properties`.\n\n```bash\nvalidation=validation/glean/glean.1.baseline.pass.json\nmps bigquery transform $validation | jq '.additional_properties'\n\"{\\\"$schema\\\":\\\"moz://mozilla.org/schemas/glean/ping/1\\\"}\"\n```\n\n\n## Releases\n\nThere is a daily series of tasks run by Airflow (see the\n[`probe_scraper` DAG](https://github.com/mozilla/telemetry-airflow/blob/main/dags/probe_scraper.py))\nthat uses the `main` branch of this repository as input and ends up pushing\nfinal JSONSchema and BigQuery schema files to the `generated-schemas` branch.\nAs of January 2020, deploying schema changes still requires manual intervention\nby a member of the Data Ops team, but you can generally expect schemas to be\ndeployed to production BigQuery tables several times a week.\n\n## Contributions\n\n- All non trivial contributions should start with a bug or issue being filed (if it is\n  a new feature please propose your design/approach before doing any work as not\n  all feature requests are accepted).\n- If updating the glean schemas, be sure to update the changelog in\n  `include/glean/CHANGELOG.md`.\n- This repository is configured to auto-assign a reviewer on PR submission. If you\n  do not receive a response within a few business days (or your request is\n  urgent), please followup in the\n  [#fx-metrics slack channel](https://mozilla.slack.com/messages/fx-metrics/).\n- If updating schemas associated with certain restricted-access datasets\n  (specified in [`CODEOWNERS`](/CODEOWNERS)), a CODEOWNER (usually\n  SRE) will automatically be assigned to review the PR. Please follow\n  additional [change control procedures](https://docs.google.com/document/d/1TTJi4ht7NuzX6BPG_KTr6omaZg70cEpxe9xlpfnHj9k/edit#heading=h.ttegrcfy18ck) \n  for PRs referencing these schemas. The CODEOWNER will be responsible for\n  merging the PR once it has been approved.\n- If your PR is associated with a bugzilla bug, please title it `Bug XXX - Description of change`, that way the [Bugzilla PR Linker](https://github.com/mozilla/github-bugzilla-pr-linker) will automatically add an attachment with your PR to bugzilla, for future reference.\n\n### Notes\n\nAll schemas are generated from the 'templates' directory and written into the\n'schemas' directory (i.e., the artifacts are generated/saved back into the\nrepository) and validated against the [draft 4 schema](http://json-schema.org/draft-04/schema)\na [copy](https://github.com/mozilla-services/mozilla-pipeline-schemas/blob/main/tests/hindsight/jsonschema.4.json)\nof which resides in the 'tests' directory. The reason for this is twofold:\n\n1. It lets us easily see and refer to complete schemas as they are actually used.\nThis means that the schemas can be referenced directly in bugs and such,\nas well as being fetched directly from the repo for testing other schema\nconsumers (test being important here, as any production use should be using the\ninstallable packages).\n1. It gives us a changelog for each schema, rather than having to reason about\nchanges to templated external pieces and when/how that impacted a given\ndoctype's schema over time. This means that it should be easy to look back in\ntime for the provenance of different parts of the schema for each doctype.\n\nWe have a number of scripts to keep the schemas in sync with various in-tree\ndefinitions. See the contents of the `scripts` subdirectory.\n"
},
{
  "name": "remote-settings-lambdas",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Makefile",
      "README.md",
      "aws_lambda.py",
      "commands",
      "pytest.ini",
      "requirements-dev.txt",
      "requirements.in",
      "requirements.txt",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "VENV := $(shell echo $${VIRTUAL_ENV-.venv})\nINSTALL_STAMP := $(VENV)/.install.stamp\n\nclean:\n\trm -fr .venv lambda.zip\n\n$(INSTALL_STAMP): requirements.txt requirements-dev.txt\n\tvirtualenv $(VENV) --python=python3\n\t$(VENV)/bin/python -m pip install --upgrade pip\n\t$(VENV)/bin/pip install -r requirements.txt\n\t$(VENV)/bin/pip install -r requirements-dev.txt\n\ttouch $(INSTALL_STAMP)\n\nformat: $(INSTALL_STAMP)\n\t$(VENV)/bin/isort --profile=black --lines-after-imports=2 commands tests --virtual-env=$(VENV)\n\t$(VENV)/bin/black commands tests\n\nlint: $(INSTALL_STAMP)\n\t$(VENV)/bin/isort --profile=black --lines-after-imports=2 --check-only commands tests --virtual-env=$(VENV)\n\t$(VENV)/bin/black --check commands tests --diff\n\t$(VENV)/bin/flake8 --ignore=W503,E501 commands tests\n\ntest: $(INSTALL_STAMP)\n\tPYTHONPATH=. $(VENV)/bin/pytest\n\nbuild:\n\tdocker build -t remote-settings-lambdas .\n\nzip: build\n\tdocker cp `docker create remote-settings-lambdas /bin/true`:/lambda.zip remote-settings-lambdas-`git describe --tags --abbrev=0`.zip\n",
  "readme": "# Remote Settings Lambdas\n\nA collection of scripts related to the Remote Settings service.\n\n## Sentry\n\nAll commands use Sentry to report any unexpected errors. Sentry can be configured with these environment variables, which are recommended, but not required:\n\n- `SENTRY_DSN`: The DSN from the \"Client Keys\" section in the project settings in Sentry.\n- `SENTRY_ENV`: The environment to use for Sentry, e.g. dev, stage or prod.\n\n## Commands\n\nEach command can be run, either with Python:\n\n```\n$ python aws_lambda.py validate_signature\n```\n\nor via the Docker container:\n\n```\n$ docker run remote-settings-lambdas validate_signature\n```\n\n\n### refresh_signature\n\nEnvironment config:\n\n- ``SERVER``: server URL (default: ``http://localhost:8888/v1``)\n- ``REFRESH_SIGNATURE_AUTH``: credentials, either ``user:pass`` or ``{access-token}`` (default: ``None``)\n- ``REQUESTS_TIMEOUT_SECONDS``: Connection/Read timeout in seconds (default: ``2``)\n- ``REQUESTS_NB_RETRIES``: Number of retries before failing (default: ``4``)\n- ``MAX_SIGNATURE_AGE``: Refresh signatures that are older that this age in days (default: ``7``)\n\n> **Note**:\n> In order to force refresh of all signatures, set ``MAX_SIGNATURE_AGE=0``\n\nExample:\n\n```\n$ REFRESH_SIGNATURE_AUTH=reviewer:pass  python aws_lambda.py refresh_signature\n\nLooking at /buckets/monitor/collections/changes:\nLooking at /buckets/source/collections/source: to-review at 2018-03-05 13:56:08 UTC ( 1520258168885 )\nLooking at /buckets/staging/collections/addons: Trigger new signature: signed at 2018-03-05 13:57:31 UTC ( 1520258251343 )\nLooking at /buckets/staging/collections/certificates: Trigger new signature: signed at 2018-03-05 13:57:31 UTC ( 1520258251441 )\nLooking at /buckets/staging/collections/plugins: Trigger new signature: signed at 2018-03-05 13:57:31 UTC ( 1520258251547 )\nLooking at /buckets/staging/collections/gfx: Trigger new signature: signed at 2018-03-05 13:57:31 UTC ( 1520258251640 )\n\n```\n\n\n### backport_records\n\nBackport the changes from one collection to another. This is useful if the new collection (*source*) has become the source of truth,\nbut there are still clients pulling data from the old collection (*destination*).\n\n> Note: This lambda is not safe if other users can interact with the destination collection.\n\nEnvironment config:\n\n- ``SERVER``: server URL (default: ``http://localhost:8888/v1``)\n- ``BACKPORT_RECORDS_SOURCE_AUTH``: authentication for source collection\n- ``BACKPORT_RECORDS_DEST_AUTH``: authentication for destination collection (default: same as source)\n- ``BACKPORT_RECORDS_SOURCE_BUCKET``: bucket id to read records from\n- ``BACKPORT_RECORDS_SOURCE_COLLECTION``: collection id to read records from\n- ``BACKPORT_RECORDS_SOURCE_FILTERS``: optional filters when backporting records as JSON format (default: none, eg. ``\"{\"min_age\": 42}\"``)\n- ``BACKPORT_RECORDS_DEST_BUCKET``: bucket id to copy records to (default: same as source bucket)\n- ``BACKPORT_RECORDS_DEST_COLLECTION``:collection id to copy records to (default: same as source collection)\n- ``REQUESTS_TIMEOUT_SECONDS``: Connection/Read timeout in seconds (default: ``2``)\n- ``REQUESTS_NB_RETRIES``: Number of retries before failing (default: ``4``)\n- ``SAFE_HEADERS``: Add concurrency control headers to update requests (default: ``false``)\n\nExample:\n\n```\n$ BACKPORT_RECORDS_SOURCE_AUTH=user:pass BACKPORT_RECORDS_SOURCE_BUCKET=blocklists BACKPORT_RECORDS_SOURCE_COLLECTION=certificates BACKPORT_RECORDS_DEST_BUCKET=security-state BACKPORT_RECORDS_DEST_COLLECTION=onecrl  python3 aws_lambda.py backport_records\n\nBatch #0: PUT /buckets/security-state/collections/onecrl/records/003234b2-f425-eae6-9596-040747dab2b9 - 201\nBatch #1: PUT /buckets/security-state/collections/onecrl/records/00ac492e-04f7-ee6d-5fd2-bb12b97a4b7f - 201\nBatch #2: DELETE /buckets/security-state/collections/onecrl/records/23 - 200\nDone. 3 changes applied.\n\n```\n\n```\n$ BACKPORT_RECORDS_SOURCE_AUTH=user:pass BACKPORT_RECORDS_SOURCE_BUCKET=blocklists BACKPORT_RECORDS_SOURCE_COLLECTION=certificates BACKPORT_RECORDS_DEST_BUCKET=security-state BACKPORT_RECORDS_DEST_COLLECTION=onecrl  python3 aws_lambda.py backport_records\n\nRecords are in sync. Nothing to do.\n\n```\n\n### publish_dafsa\n\nEnvironment config:\n\n- ``SERVER``: server URL (default: ``http://localhost:8888/v1``)\n- ``AUTH``: authentication to edit the ``public-suffix-list`` collection\n\n\n### sync_megaphone\n\nSend the current version of Remote Settings data to the Push server.\n\nDoes nothing if versions are in sync.\n\nEnvironment config:\n\n- ``SERVER``: Remote Settings server URL (default: ``http://localhost:8888/v1``)\n- ``MEGAPHONE_URL``: Megaphone service URL\n- ``MEGAPHONE_READER_AUTH``: Bearer token for Megaphone read access\n- ``MEGAPHONE_BROADCASTER_AUTH``: Bearer token for Megaphone broadcaster access\n- ``BROADCASTER_ID``: Push broadcaster ID (default: ``remote-settings``)\n- ``CHANNEL_ID``: Push channel ID (default: ``monitor_changes``)\n\nExample:\n\n```\n$ SERVER=https://settings.prod.mozaws.net/v1 MEGAPHONE_URL=\"https://push.services.mozilla.com/v1\" MEGAPHONE_READER_AUTH=\"a-b-c\" MEGAPHONE_BROADCASTER_AUTH=\"d-e-f\" python aws_lambda.py sync_megaphone\n```\n\n\n## Test locally\n\n```\n$ make virtualenv\n$ source .venv/bin/activate\n\n$ SERVER=https://firefox.settings.services.mozilla.com/v1/  python aws_lambda.py validate_signature\n```\n\n### Local Kinto server\n\nBest way to obtain a local setup that looks like a writable Remote Settings instance is to follow [this tutorial](https://remote-settings.readthedocs.io/en/latest/tutorial-local-server.html)\n\nIt is possible to initialize the server with some fake data, like for the Kinto Dist smoke tests:\n\n```\n$ bash /path/to/kinto-dist/tests/smoke-test.sh\n```\n\n## Releasing\n\n- `git tag vX.Y.Z`\n- `git push origin master; git push --tags origin`\n- `make zip`\n- Go to releases page on Github and create a release for x.y.z\n- Attach the `remote-settings-lambdas-x.y.z.zip` file\n- [Click here][bugzilla-stage-link] to open a ticket to get it deployed to stage and [here][bugzilla-prod-link] to prod\n\n\n[bugzilla-stage-link]: https://bugzilla.mozilla.org/enter_bug.cgi?comment=Please%20upgrade%20the%20lambda%20functions%20to%20use%20the%20last%20release%20of%20remote-settings-lambdas.%0D%0A%0D%0A%5BInsert%20a%20short%20description%20of%20the%20changes%20here.%5D%0D%0A%0D%0Ahttps%3A%2F%2Fgithub.com%2Fmozilla-services%2Fremote-settings-lambdas%2Freleases%2Ftag%2FX.Y.Z%0D%0A%0D%0AThanks%21&component=Operations%3A%20Storage&product=Cloud%20Services&qa_contact=chartjes%40mozilla.com&short_desc=Please%20deploy%20remote-settings-lambdas-X.Y.Z%20lambda%20function%20to%20STAGE\n\n[bugzilla-prod-link]: https://bugzilla.mozilla.org/enter_bug.cgi?comment=Please%20upgrade%20the%20lambda%20functions%20to%20use%20the%20last%20release%20of%20remote-settings-lambdas.%0D%0A%0D%0A%5BInsert%20a%20short%20description%20of%20the%20changes%20here.%5D%0D%0A%0D%0Ahttps%3A%2F%2Fgithub.com%2Fmozilla-services%2Fremote-settings-lambdas%2Freleases%2Ftag%2FX.Y.Z%0D%0A%0D%0AThanks%21&component=Operations%3A%20Storage&product=Cloud%20Services&qa_contact=chartjes%40mozilla.com&short_desc=Please%20deploy%20remote-settings-lambdas-X.Y.Z%20lambda%20function%20to%20PROD\n"
},
{
  "name": "telescope",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "checks",
      "config.toml.sample",
      "docs",
      "poetry.lock",
      "pyproject.toml",
      "scripts",
      "telescope",
      "tests"
    ],
    "/docs": [
      "adrs"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "NAME := telescope\nCOMMIT_HOOK := .git/hooks/pre-commit\nINSTALL_STAMP := .install.stamp\nPOETRY := $(shell command -v poetry 2> /dev/null)\n\nCONFIG_FILE := $(shell echo $${CONFIG_FILE-config.toml})\nVERSION_FILE := $(shell echo $${VERSION_FILE-version.json})\nSOURCE := $(shell git config remote.origin.url | sed -e 's|git@|https://|g' | sed -e 's|github.com:|github.com/|g')\nVERSION := $(shell git describe --always --tag)\nCOMMIT := $(shell git log --pretty=format:'%H' -n 1)\n\n.PHONY: clean lint format tests check\n\ninstall: $(INSTALL_STAMP) $(COMMIT_HOOK)\n$(INSTALL_STAMP): pyproject.toml poetry.lock\n\t@if [ -z $(POETRY) ]; then echo \"Poetry could not be found. See https://python-poetry.org/docs/\"; exit 2; fi\n\t$(POETRY) install --extras=remotesettings --extras=taskcluster\n\ttouch $(INSTALL_STAMP)\n\n$(COMMIT_HOOK):\n\techo \"make format\" > $(COMMIT_HOOK)\n\tchmod +x $(COMMIT_HOOK)\n\nclean:\n\tfind . -type d -name \"__pycache__\" | xargs rm -rf {};\n\trm -rf .install.stamp .coverage .mypy_cache $(VERSION_FILE)\n\nlint: $(INSTALL_STAMP)\n\t$(POETRY) run isort --profile=black --lines-after-imports=2 --check-only checks tests $(NAME)\n\t$(POETRY) run black --check checks tests $(NAME) --diff\n\t$(POETRY) run flake8 --ignore=W503,E501 checks tests $(NAME)\n\t$(POETRY) run mypy checks tests $(NAME) --ignore-missing-imports\n\t$(POETRY) run bandit -r $(NAME) -s B608\n\nformat: $(INSTALL_STAMP)\n\t$(POETRY) run isort --profile=black --lines-after-imports=2 checks tests $(NAME)\n\t$(POETRY) run black checks tests $(NAME)\n\ntest: tests\ntests: $(INSTALL_STAMP) $(VERSION_FILE)\n\t$(POETRY) run pytest tests --cov-report term-missing --cov-fail-under 100 --cov $(NAME) --cov checks\n\n$(CONFIG_FILE):\n\tcp config.toml.sample $(CONFIG_FILE)\n\n$(VERSION_FILE):\n\techo '{\"name\":\"$(NAME)\",\"version\":\"$(VERSION)\",\"source\":\"$(SOURCE)\",\"commit\":\"$(COMMIT)\"}' > $(VERSION_FILE)\n\nserve: $(INSTALL_STAMP) $(VERSION_FILE) $(CONFIG_FILE)\n\tLOG_LEVEL=DEBUG LOG_FORMAT=text $(POETRY) run python -m $(NAME)\n\ncheck: $(INSTALL_STAMP) $(CONFIG_FILE)\n\tLOG_LEVEL=DEBUG LOG_FORMAT=text $(POETRY) run python -m $(NAME) check $(project) $(check)\n",
  "readme": "# Telescope\n\n[![CircleCI](https://circleci.com/gh/mozilla-services/telescope.svg?style=svg)](https://circleci.com/gh/mozilla-services/telescope)\n\n*Telescope* is a small Web app that will act as a proxy between a monitoring service \u2014 like Pingdom or [Upptime](https://upptime.js.org/) \u2014 and a series of domain specific checks for your infrastructure.\n\n\n## Usage\n\nEvery check defined in your configuration file is exposed as an endpoint that returns `200` if successful or `5XX` otherwise:\n\n```http\nGET /checks/{a-project}/{a-check}\n\nHTTP/1.1 200 OK\nContent-Length: 260\nContent-Type: application/json; charset=utf-8\nDate: Fri, 16 Aug 2019 13:29:55 GMT\nServer: Python/3.7 aiohttp/3.5.4\n\n{\n    \"name\": \"a-check\",\n    \"project\": \"a-project\",\n    \"url\": \"/checks/a-project/a-check\",\n    \"module\": \"checks.core.heartbeat\",\n    \"documentation\": \"URL should return a 200 response.\",\n    \"description\": \"Some check description.\",\n    \"success\": true,\n    \"parameters\": {},\n    \"data\": {\n        \"ok\": true\n    }\n}\n\n```\n\nThe response has some additional `\"data\"`, specific to each type of check.\n\nCache can be forced to be refreshed with the ``?refresh={s3cr3t}`` querystring. See *Environment variables* section.\n\n### Other endpoints:\n\n* ``/checks``: list all checks, without executing them.\n* ``/checks/{a-project}``: execute all checks of project ``a-project``\n* ``/checks/tags/{a-tag}``: execute all checks with tag ``a-tag``\n\nOutput format:\n\n* Request header ``Accept: plain/text``: renders the check(s) as a human readable table.\n\n\n## Configure\n\nThe checks are defined in a `config.toml` file, and their module must be available in the current `PYTHONPATH`:\n\n```toml\n[checks.a-project.a-check]\ndescription = \"Heartbeat of the public read-only instance.\"\nmodule = \"checks.core.heartbeat\"\nparams.url = \"https://firefox.settings.services.mozilla.com/v1/__heartbeat__\"\n\n[checks.normandy.published-recipes]\ndescription = \"Normandy over Remote Settings.\"\nmodule = \"checks.normandy.remotesettings_recipes\"\nparams.normandy_server = \"https://normandy.cdn.mozilla.net\"\nparams.remotesettings_server = \"https://firefox.settings.services.mozilla.com/v1\"\nttl = 3600\ntags = [\"critical\"]\n```\n\n* `description`: Some details about this check\n* `module`: Path to Python module\n* `params`: (*optional*) Parameters specific to the check\n* `ttl`: (*optional*) Cache the check result for a number of seconds\n* `tags`: (*optional*) List of strings allowing grouping of checks at `/tags/{tag}`\n\n\n### Environment variables\n\nThe config file values can refer to environment variables (eg. secrets) using the ``${}`` syntax.\n\n```toml\n[checks.myproject.mycheck]\nmodule = \"checks.remotesettings.collections_consistency\"\nparams.url = \"http://${ENV_NAME}.service.org\"\nparams.auth = \"Bearer ${AUTH}\"\n```\n\n### Server configuration\n\nServer configuration is done via environment variables:\n\n* ``CONFIG_FILE``: Path to configuration file (default: ``\"config.toml\"``)\n* ``CONTACT_EMAIL``: Contact email for this instance (default: ``postmaster@localhost``)\n* ``DIAGRAM_FILE``: Path to SVG diagram file (default: ``\"diagram.svg\"``)\n* ``CORS_ORIGIN``: Allowed requests origins (default: ``*``)\n* ``ENV_NAME``: A string to identify the current environment name like ``\"prod\"`` or ``\"stage\"`` (default: None)\n* ``HOST``: Bind to host (default: ``\"localhost\"``)\n* ``PORT``: Listen on port (default: ``8000``)\n* ``DEFAULT_TTL``: Default TTL for endpoints in seconds (default: ``60``)\n* ``DEFAULT_REQUEST_HEADERS``: Default headers sent in every HTTP requests, as JSON dict format (example: ``{\"Allow-Access\": \"CDN\"}``, default: ``{}``)\n* ``LOG_LEVEL``: One of ``DEBUG``, ``INFO``, ``WARNING``, ``ERROR``, ``CRITICAL`` (default: ``INFO``)\n* ``LOG_FORMAT``: Set to ``text`` for human-readable logs (default: ``json``)\n* ``VERSION_FILE``: Path to version JSON file (default: ``\"version.json\"``)\n* ``REFRESH_SECRET``: Secret to allow forcing cache refresh via querystring (default: ``\"\"``)\n* ``REQUESTS_TIMEOUT_SECONDS``: Timeout in seconds for HTTP requests (default: ``5``)\n* ``REQUESTS_MAX_RETRIES``: Number of retries for HTTP requests (default: ``4``)\n* ``SENTRY_DSN``: Report errors to the specified Sentry ``\"https://<key>@sentry.io/<project>\"`` (default: disabled)\n* ``SERVICE_NAME``: Name of the running service, used to link known issues in bug tracker (default: ``telescope``)\n* ``SERVICE_TITLE``: Title shown in the UI (default: capitalized service name)\n\n* ``BUGTRACKER_URL``: Bug tracker URL. Set to empty string to disable. (default: ``https://bugzilla.mozilla.org``)\n* ``BUGTRACKER_API_KEY``: Bug tracker API key to fetch non-public bugs (default: none)\n* ``BUGTRACKER_TTL``: Default TTL for endpoints in seconds (default: ``3600``)\n\n* ``HISTORY_DAYS``: Number of days to cover whening fetch history of checks (default: 0, disabled)\n* ``HISTORY_TTL``: Default TTL for history refresh in seconds (default: ``3600``)\n\n* ``GITHUB_TOKEN``: Github [Personal Access Token value](https://github.com/settings/tokens) to avoid rate-limiting (default: disabled)\n* ``GOOGLE_APPLICATION_CREDENTIALS``: Absolute path to credentials file for BigQuery authentication (eg. `` `pwd`/key.json``, default: disabled)\n\nConfiguration can be stored in a ``.env`` file:\n\n```\nLOG_LEVEL=debug\n# Disable JSON logs\nLOG_FORMAT=text\n```\n\n## Run Web server\n\nUsing Docker, and a local config file:\n\n```\ndocker run -p 8000:8000 -v `pwd`/config.toml:/app/config.toml mozilla/telescope\n```\n\nOr from source (*requires Python 3.10+ and Poetry*):\n\n```\nmake serve\n```\n\n## Web UI\n\nA minimalist Web page is accessible at ``/html/index.html`` and shows every check status,\nalong with the returned data and documentation.\n\nA SVG diagram can be shown in the UI (see ``DIAGRAM_FILE``). Elements of the SVG diagram will be turned red or green based on check results.\nSet the ``id`` attribute of relevant diagram elements to ``${project}--${name}`` (eg. ``remotesettings-uptake--error-rate``) and the app will toggle the ``fill`` attribute.\n\n\n## Bug tracker integration\n\nThe list of known issues for each check can be obtained from the configured bug tracker (see configuration section to disable).\n\nThe default implementation is for Bugzilla, and retrieves the bugs which have a certain string in their ``whiteboard`` field.\nFor example, if the ``SERVICE_NAME`` is ``delivery-checks`` and ``ENV_NAME`` is ``prod``, the bugs for the check ``server/heartbeat`` should have ``delivery-checks prod server/heartbeat`` in their ``whiteboard`` field to be listed.\n\nIf the bugs are only readable by authenticated users, then set ``BUGTRACKER_API_KEY`` to retrieve them all.\nNote that for security bugs only bug IDs are shown, summaries will be empty.\n\n\n## CLI\n\nUsing Docker, and a local config file:\n\n```\ndocker run -v `pwd`/config.toml:/app/config.toml mozilla/telescope check\n\ndocker run -v `pwd`/config.toml:/app/config.toml mozilla/telescope check myproject\n\ndocker run -v `pwd`/config.toml:/app/config.toml mozilla/telescope check myproject mycheck\n```\n\nOr from source (*requires Python 3.8+ and Poetry*):\n\n```\nmake check\n\nmake check project=myproject\n\nmake check project=myproject check=mycheck\n```\n\nReturn codes:\n\n- `0`: all checks were successful\n- `1`: some check failed\n- `2`: some check crashed (ie. Python exception)\n\n\n## Tests\n\n```\nmake tests\n```\n\n## License\n\n*Poucave* is licensed under the MPLv2. See the `LICENSE` file for details.\n"
},
{
  "name": "python-dockerflow",
  "files": {
    "/": [
      ".coveragerc",
      ".github",
      ".gitignore",
      ".isort.cfg",
      "AUTHORS.rst",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "docs",
      "pyproject.toml",
      "pytest.ini",
      "setup.cfg",
      "setup.py",
      "src",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "api",
      "authors.rst",
      "changelog.rst",
      "conf.py",
      "django.rst",
      "flask.rst",
      "index.rst",
      "logging.rst",
      "requirements.txt",
      "sanic.rst"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "Python Dockerflow\n=================\n\nThis package implements a few helpers and tools for Mozilla's\n`Dockerflow pattern <https://github.com/mozilla-services/Dockerflow>`_.\nThe documentation can be found on `python-dockerflow.readthedocs.io`_\n\n.. _`python-dockerflow.readthedocs.io`: https://python-dockerflow.readthedocs.io/\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n    :target: https://github.com/ambv/black\n\n.. image:: https://github.com/mozilla-services/python-dockerflow/workflows/Test/badge.svg\n   :target: https://github.com/mozilla-services/python-dockerflow/actions\n   :alt: GitHub Actions\n\n.. image:: https://codecov.io/github/mozilla-services/python-dockerflow/coverage.svg?branch=main\n   :alt: Codecov\n   :target: https://codecov.io/github/mozilla-services/python-dockerflow?branch=main\n\n.. image:: https://readthedocs.org/projects/python-dockerflow/badge/?version=latest\n   :alt: Documentation Status\n   :target: https://python-dockerflow.readthedocs.io/en/latest/?badge=latest\n\n.. image:: https://img.shields.io/badge/calver-YYYY.M.PATCH-22bfda.svg\n   :target: https://calver.org/\n   :alt: CalVer - Timely Software Versioning\n\nInstallation\n------------\n\nPlease install the package using your favorite package installer::\n\n    pip install dockerflow\n\nIssues & questions\n------------------\n\nSee the `issue tracker on GitHub <https://github.com/mozilla-services/python-dockerflow/issues>`_\nto open tickets if you have issues or questions about python-dockerflow.\n"
},
{
  "name": "contile",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".clog.toml",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "adm_settings_test.json",
      "docs",
      "entrypoint.sh",
      "imgs",
      "mmdb",
      "smoke-tests",
      "src",
      "test-engineering",
      "tools",
      "version.json"
    ],
    "/docs": [
      "API.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "![Contile graphic](imgs/Contile_title.svg)\n# Contile Tile Server\n\nThis is the back-end server for the Mozilla Tile Service (MTS).\n\nThe goal of this service is to pass tiles from partners along to Firefox for display while ensuring customer privacy and choice as discussed in the [support article \"Sponsored tiles on the New Tab page\"](https://support.mozilla.org/en-US/kb/sponsor-privacy).\n\nSupports the TopSites feature within Firefox.\n\nSee also:\n\n- [In-repo documentation](docs/)\n- [Monitoring dashboard](https://earthangel-b40313e5.influxcloud.net/d/oak1zw6Gz/contile-infrastructure) (Mozilla internal)\n\n## Requirements\n\nThis system uses [Actix](https://actix.rs/) web, and Google Cloud APIs (currently vendored).\n\n## Setting Up\n\nContile uses Rust, and requires the latest stable iteration. See\n[rustup.rs](https://rustup.rs/) for how to install this application.\n\nOnce Rust is installed you can compile using `cargo build`. This will\ncreate a development release.\n\n### Running\n\nContile is configured via environment variables. To see the complete list of available settings in `contile::settings::Settings` (note, you can use `cargo doc --open` to generate documentation.) In general, we have tried to provide sensible default values for most of these,\nhowever you may need to specify the following:\n\n```\nCONTILE_ADM_ENDPOINT_URL={Your ADM endpoint} \\\n    cargo run\n```\nPlease note that the `{}` indicate a variable replacement and should not be included, for example, a real environmet variable would look like: `CONTILE_ADM_ENDPOINT_URL=https://example.com/`\n\n### Testing\n#### Unit tests\n\nTo run Contile's unit tests, run\n\n```cargo test```\n\nThis will test everything, except for Google Storage for images. In order to test that, you\nwill need to include the following:\n```\nGOOGLE_APPLICATION_CREDENTIALS={path to your credential.json file} \\\n    CONTILE_TEST_PROJECT={GCP Project name} \\\n    CONTILE_TEST_BUCKET={GCP Bucket name} \\\n    cargo test\n```\n\n#### Contract tests\n\nContract tests are currently run using Docker images. This is so that they can be run as\npart of our automated continuous integration (CI) testing. \nSee the dedicated [contract-tests README](test-engineering/contract-tests/README.md) for details.\n\n## Why \"Contile\"?\n\nIt's a portmanteau of \"Context\" and \"Tile\", which turns out to be the name of [a small village](https://www.google.com/maps/place/Contile/@44.6503701,9.9015688,3a,15y,40.52h,87.97t/data=!3m10!1e1!3m8!1shPkpksIO5_yiJpqYALgcNQ!2e0!6s%2F%2Fgeo3.ggpht.com%2Fcbk%3Fpanoid%3DhPkpksIO5_yiJpqYALgcNQ%26output%3Dthumbnail%26cb_client%3Dmaps_sv.tactile.gps%26thumb%3D2%26w%3D203%26h%3D100%26yaw%3D8.469731%26pitch%3D0%26thumbfov%3D100!7i13312!8i6656!9m2!1b1!2i22!4m5!3m4!1s0x47808736ea28b80d:0xd17ee6c4205c4451!8m2!3d44.650751!4d9.902755) in the Parma region of Italy. So it's pronounced \"[kon **t\u0113`** l\u0101](https://translate.google.com/?sl=it&tl=en&text=contile&op=translate)\"\n"
},
{
  "name": "bouncer-admin",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".pyup.yml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "app.sh",
      "bounceradmin_grants.sql",
      "bouncerscript_config.json",
      "bouncerscript_prod_config.json",
      "bouncerscript_prod_task.json",
      "cli.py",
      "conftest.py",
      "data.sql",
      "docker-compose.yml",
      "nazgul",
      "requirements.txt",
      "run",
      "setup.py",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# bouncer-admin\nThe admin interface for https://github.com/mozilla-services/go-bouncer/.\n\n---\n## Requirements\n|         |            |\n| ------------- |:-------------:|\n | docker | 18.09.2 |\n| mysql  | 5.6     |\n---\n\n### Updating Requirements\n\nTo update all requirements: `hashin -u`\n\n## How to get started\n\n```sh app.sh MYSQL_LOGIN```\n> replace ```MYSQL_LOGIN``` with whatever parameter you use to connect to mysql with read and write privileges (eg. ```sh app.sh -u root -p```)\n\nRun ```docker exec -it local-nazgul pytest``` in a separate terminal to test the code\n\nThat's it! Go to [0.0.0.0:8000](0.0.0.0:8000/)\n"
},
{
  "name": "absearch",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Makefile",
      "README.rst",
      "absearch",
      "config",
      "dev-requirements.txt",
      "docs",
      "example-data",
      "requirements.txt",
      "scripts",
      "setup.py",
      "tox.ini"
    ],
    "/docs": [
      "absearch.jpg"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "SERVER_CONFIG = config/absearch.ini\n\nVIRTUALENV = virtualenv\nSPHINX_BUILDDIR = docs/_build\nVENV := $(shell echo $${VIRTUAL_ENV-.venv})\nPYTHON = $(VENV)/bin/python\nDEV_STAMP = $(VENV)/.dev_env_installed.stamp\nINSTALL_STAMP = $(VENV)/.install.stamp\n\n.IGNORE: clean\n.PHONY: all install virtualenv tests\n\nOBJECTS = .venv .coverage\n\nall: install\ninstall: $(INSTALL_STAMP)\n$(INSTALL_STAMP): $(PYTHON)\n\t$(PYTHON) setup.py develop\n\ttouch $(INSTALL_STAMP)\n\ninstall-dev: $(INSTALL_STAMP) $(DEV_STAMP)\n$(DEV_STAMP): $(PYTHON)\n\t$(VENV)/bin/pip install --ignore-installed -r dev-requirements.txt\n\ttouch $(DEV_STAMP)\n\nvirtualenv: $(PYTHON)\n$(PYTHON):\n\t$(VIRTUALENV) $(VENV)\n\ndata:\n\tcp -R example-data/ data/\n\ntests:\tinstall-dev data\n\trm -f .coverage;$(VENV)/bin/tox\n\nclean:\n\tfind . -name '*.pyc' -delete\n\tfind . -name '__pycache__' -type d -exec rm -fr {} \\;\n",
  "readme": "absearch\n========\n\n\nThe AB Search service provides 2 commands:\n\n- **absearch-server**: runs the service\n- **absearch-check**: validates the config using the schema\n\n\nOverview\n========\n\n.. image:: https://github.com/mozilla-services/searchab/blob/master/docs/absearch.jpg?raw=true\n\n\n* config.json contains the search settings per locale and territory, and also cohorts for a/b testing\n* Editors change the config JSON file in Github\n* Firefox calls the service to get the search settings, providing a locale & territory\n\n"
},
{
  "name": "merino-py",
  "files": {
    "/": [
      ".gitignore",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# merino-py"
},
{
  "name": "shavar-list-creation",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "constants.py",
      "lists2safebrowsing.py",
      "publish2cloud.py",
      "requirements-test.txt",
      "requirements.txt",
      "sample_shavar_list_creation.ini",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "shavar-list-creation\n====================\n\n[![Build Status](https://circleci.com/gh/mozilla-services/shavar-list-creation/tree/main.svg?style=shield)](https://circleci.com/gh/mozilla-services/shavar-list-creation/tree/main)\n[![Coverage](https://circleci.com/api/v1.1/project/github/mozilla-services/shavar-list-creation/latest/artifacts/0/coverage.svg?branch=main)](https://circleci.com/api/v1.1/project/github/mozilla-services/shavar-list-creation/latest/artifacts/0/htmlcov/index.html?branch=main)\n\nThis script fetches blocklist `.json` from urls (such as\n[shavar-prod-lists](https://github.com/mozilla-services/shavar-prod-lists)) and\ngenerates safebrowsing-compatible digest list files to be served by\n[shavar](https://github.com/mozilla-services/shavar).\n\n# Requirements\n\n* python &geq; 3.6\n* (optional) virtualenv and/or virtualenvwrapper\n\n# Run\n\n1. (optional) Make a virtualenv for the project and activate it:\n\n    ```\n    virtualenv -p python3.8 shavar-list-creation\n    source shavar-list-creation/bin/activate\n    ```\n\n2. Install required libraries:\n\n    ```\n    pip install -r requirements-test.txt\n    ```\n\n3. Copy the `sample_shavar_list_creation.ini` file to\n   `shavar_list_creation.ini`:\n\n    ```\n    cp sample_shavar_list_creation.ini shavar_list_creation.ini\n    ```\n\n4. Run the unit tests (currently under development):\n\n    ```\n    python -m pytest -v --cov=. --cov-branch\n    ```\n\n5. Run the `lists2safebrowsing.py` script:\n\n    ```\n    ./lists2safebrowsing.py\n    ```\n\n# Usage\nThis is run by a Jenkins deployment job every 30 minutes that:\n\n1. Checks out this repository\n2. Checks out the [shavar-list-creation-config](https://github.com/mozilla-services/shavar-list-creation-config/) repository\n3. Copies `stage.ini` or `prod.ini` to `shavar_list_creation.ini`\n4. Runs `python lists2safebrowsing.py`, which uploads updated safebrowsing list files to S3 for [shavar](https://github.com/mozilla-services/shavar).\n"
},
{
  "name": "updatebot",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".gitignore",
      "CONTRIBUTING.md",
      "LICENSE.md",
      "README.md",
      "__init__.py",
      "apis",
      "automation.py",
      "components",
      "format.sh",
      "localconfig.py.example",
      "poetry.lock",
      "pyproject.toml",
      "setup_notes.md",
      "tasktypes",
      "test.py",
      "tests"
    ],
    "/.circleci": [
      "config.yml",
      "gecko-test",
      "pr_comment.sh",
      "run_cmd.sh"
    ]
  },
  "makefile": null,
  "readme": "# Updatebot\n\n[![<mozilla-services>](https://img.shields.io/circleci/build/gh/mozilla-services/updatebot?label=tests&style=flat-square)](https://circleci.com/gh/mozilla-services/updatebot)\n[![codecov](https://img.shields.io/codecov/c/gh/mozilla-services/updatebot?style=flat-square)](https://codecov.io/gh/mozilla-services/updatebot)\n\nAutomated detection, patching, testing, and bug filing for updates in third-party libraries for Firefox.\n\n## For Mozilla Developers\n\nUpdatebot is a bot that looks for upstream updates to a third party dependency (typically a library) and if it detects an update it will:\n\n - File a bug that an update is available\n - Attempt to automatically vendor it, reporting errors if they occur\n - Send in a try run with the update\n - Submit a phabricator patch of the update\n - Once the try run is complete, look for test failures and retrigger them\n - Once the retriggers are complete, report a summary of the results (what tests failed, how many times, and if they are known issues or not)\n - Flag the dependency's maintainer for review of the patch or send them a needinfo on the bug\n\nUpdatebot can be thought of as two halves: the bot that does the above and the in-tree `./mach vendor` component that makes it easy for a library to be updated locally. *We would be happy to help you set up your library for vendoring in Updatebot.*\n\nUpdatebot **doesn't have to vendor the update** - it can instead just alert you that there were new commits.  This is good for infrequently updated upstreams that are difficult to automatically vendor.  In the future we intend to add some intelligence to this to let us filter by suspected security issues.\n\nUpdatebot has several configurable options:\n\n1. It can look for updates:\n   - every run (6 hours) - good for infrequently updated upstreams\n   - every N weeks\n   - every N commits\n   - only upon a new Firefox release (good for frequently updated libraries we bump once-per-FF release)\n2. It can track a specific upstream branch, or only look for newly tagged releases\n3. It can use `./mach try auto` or `./mach try fuzzy` with a custom query string to send in the try run\n4. Through the [moz.yaml format](https://searchfox.org/mozilla-central/source/python/mozbuild/mozbuild/vendor/moz_yaml.py), it can handle more complicated vendoring steps using custom scripts, or more simple vendoring steps using a predefined language.\n\n\n## Updatebot Development\n\nThis project requires [Poetry](https://python-poetry.org/docs/) and a version of [Python](https://www.python.org/downloads/release/python-359/) at least greater than 3.5.\n\nWe talk to a database; currently [MySQL](https://www.mysql.com/downloads/) is supported. Copy the local config file with `cp localconfig.py.example localconfig.py` and configure the database connection parameters. Updatebot will automatically create and populate the database with its structure and required data.\n\nTo get started developing Updatebot, or to run it locally you'll need to run `poetry install` and then `poetry run ./automation.py`\n\nTesting is handled in a single step of `poetry run ./test.py`\n\nFor formatting code automatically please use `poetry run autopep8 --in-place --recursive --ignore E501,E402 .`\n\nFor linting the codebase run `poetry run flake8 --ignore=E501,E402 .`\n\nUpdatebot is currently in active development with a lot of churn. We welcome patches and bugfixes, but encourage you to reach out to June Wilde or Tom Ritter before spending too much time as we may be already addressing your issue.\n\n### How it works\n - Updatebot runs as a Linux-based cron job in mozilla-central every 6 hours (defined in [.cron.yml](https://searchfox.org/mozilla-central/source/.cron.yml)).  (There is a windows cron job in development, but ignore it for now.) This job:\n  1. Runs in the [Updatebot Docker Image](https://searchfox.org/mozilla-central/source/taskcluster/docker/updatebot)\n  - [Searches](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/components/libraryprovider.py#L122-L129) the tree for [moz.yaml files](https://searchfox.org/mozilla-central/search?q=moz.yaml&case=true&path=) that [define an enabled Updatebot task](https://searchfox.org/mozilla-central/rev/83e67336083df9f9a3d1e0c33f2ba19703d57161/media/libdav1d/moz.yaml#40-43)\n  - Figures out [which task type we are dealing with](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/automation.py#L134-L137).  For here on out we will assume a [vendoring task](https://github.com/mozilla-services/updatebot/blob/master/tasktypes/vendoring.py) but there is also a [commit alert task](https://github.com/mozilla-services/updatebot/blob/master/tasktypes/commitalert.py).\n  - [Checks](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/base.py#L14) if we should process the library according to its [requested frequency](https://searchfox.org/mozilla-central/rev/83e67336083df9f9a3d1e0c33f2ba19703d57161/python/mozbuild/mozbuild/vendor/moz_yaml.py#392).\n  - [Compares](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L51-L54) the current upstream revision with the current [in-tree revision](https://searchfox.org/mozilla-central/rev/83e67336083df9f9a3d1e0c33f2ba19703d57161/media/libdav1d/moz.yaml#27) to determine if an update is needed.\n  - [Checks](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L57) if this revision has already been processed.  We will assume it has not.\n  - [Files a bugzilla bug](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L154)\n  - [Vendors](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/components/mach_vendor.py#L42-L43) the latest version from upstream by calling `./mach vendor path/to/moz.yaml`\n  - [Checks the result](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L159-L171)\n  - [Commits](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/components/hg.py#L14-L19) the change locally\n  - [Submits it to try](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/apis/taskcluster.py#L51)\n  - [Comments on the bug](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L179)\n  - Finally, the new revision is [recorded in the Updatebot database](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L181).\n- That database lives in Google CloudSQL.  There is a dev and prod database, as well as [dev and prod credentials](https://searchfox.org/mozilla-central/rev/83e67336083df9f9a3d1e0c33f2ba19703d57161/taskcluster/docker/updatebot/run.py#79-84) for those databases, bugzilla, try server, phabricator, sentry, and sql-proxy (which is used to connect to the database).  You can find them in [grants.yml](https://hg.mozilla.org/ci/ci-configuration/file/tip/grants.yml#l644) searching for 'updatebot'.  The dev credentials are granted to holly, which is our reserved development instance because Updatebot can't tested on try safely.  The prod credentials are only available to mozilla-central.\n- The next time the Updatebot job runs it will get to step (6) and see that it has seen the (new) revision before.  It will [process the job from there](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L238).\n  1. Will check if all the jobs in the try run are done. If they are not, it will do nothing and check again on the next Updatebot run.\n  2. If they are done, it will look to see if there any test failures. If so it will [retrigger them](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L384) and wait until the next job.\n  3. Once we've [received the retrigger results](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L404) then we [look at failures](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L411-L431), summarize them, and [add a comment to the bug and update the database](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L420-L431).\n\n\n### Architecture\nUpdatebot's architecture is.... not great.\n\n - In an effort to make mockable classes for testing and stubbing functionality, nearly all the low-to-medium level logic about 'how to do something' is contained in either a [component class](https://github.com/mozilla-services/updatebot/tree/master/components) or an [api class](https://github.com/mozilla-services/updatebot/tree/master/apis) - both called 'Providers'.  The distinction between the two is not very significant, except the API classes were originally separated to indicate an external API we talk to.\n  - We describe two types of Providers: Functionality Providers, and Utility Providers.  Functionality Providers may require and use Utility Providers.  And Utility Providers can include Utility Providers.\n  - Concretely, there are two Utility Providers: a Logging Provider and a CommandProvider, the latter of which requires the former.\n - Updatebot takes a [configuration](https://github.com/mozilla-services/updatebot/blob/master/localconfig.py.example), which is a dictionary of dictionaries. A sub-dictionary for each Provider, plus a 'General' dictionary given to every Provider\n - Initialization of the providers is [complex](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/automation.py#L61-L97) because we want to allow tests to define alternate Providers (mocked Providers).  And because Functionality Providers need Utility Providers....\n - __The whole Provider thing is a giant mess and needs to be completely redone.__\n - For each of our two task types, vendoring and commit-alert, we have a [tasktype class](https://github.com/mozilla-services/updatebot/tree/master/tasktypes) that defines the higher-level logic.  This logic is tested in the `functionality_*` tests.  (Those tests themselves need a README explaining how they work.)\n - The entry point is [automation.py](https://github.com/mozilla-services/updatebot/blob/master/automation.py).\n - We have a [dbc layer](https://github.com/mozilla-services/updatebot/blob/master/components/dbc.py) that's intended to support abstracting away to a different database if we ever switch.\n - We have the [db layer](https://github.com/mozilla-services/updatebot/blob/master/components/db.py) which is the only thing that speaks MySQL.\n - Inside the db layer we define the database structure. It will create the database if one does not exist.  When we need to alter the database structure we bump the database revision and [write migration code](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/components/db.py#L217-L349).\n\nThere are a few bits of complexity elided in the overview and architecture details above:\n\n - We support (in theory) doing two try runs: one for linux64 and if that succeeds a follow-up run of everything else. This is to be more mindful of try resources, but presently this doesn't work as intended (on the try side) so we only do one try run.\n - We have the notion of Job States (done or not done) and outcomes (success, failed with known failures, failed with unknown).\n - Presently we only allow one prior job to be in the 'Running ' or 'Done' state, which is confusing because we try to close old bugs we filed as obsolete (dupe them to a newer bug) *but* we don't want to re-close bugs developers re-open because they're working on them...\n - This leads to a state of Job called 'Abandoned' in particular that is [a bit of tech debt we need to refactor](https://github.com/mozilla-services/updatebot/issues/201). And leads to confusing code [dealing with cleaning up older jobs](https://github.com/mozilla-services/updatebot/blob/c9133c4f2c15b30438fe6721ef7f490472851de4/tasktypes/vendoring.py#L108-L116).\n - We have a bit of complexity in how we compare our current in-tree revision with the upstream revision, and code that looks for the commits in between and adds them to bug comments.\n - We have logic to update bugs tracking flags when they are left open for a long period of time\n\n## Fine Print\n\nThis repo is subject to [our quality standards and practices](https://developer.mozilla.org/en-US/docs/Mozilla/Developer_guide/Committing_Rules_and_Responsibilities) and any interaction here is governed by the [Mozilla Community Participation Guidelines.](https://www.mozilla.org/en-US/about/governance/policies/participation/)\n"
},
{
  "name": "audit-filter",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "example",
      "output_usage.sh",
      "pkg",
      "src",
      "tests"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "### audit-filter\n\n[![crates.io version](https://img.shields.io/crates/v/audit-filter.svg)](https://img.shields.io/crates/v/audit-filter.svg)\n[![Build Status](https://travis-ci.org/mozilla-services/audit-filter.svg?branch=master)](https://travis-ci.org/mozilla-services/audit-filter)\n[![npm version](https://badge.fury.io/js/audit-filter.svg)](https://badge.fury.io/js/audit-filter)\n\n`audit-filter` takes the output of [`npm audit\n--json`](https://docs.npmjs.com/cli/audit) and an\n[nsp](https://github.com/nodesecurity/nsp) rc config file [*without\ncomments*](#fixing-comments-in-nsprc-files) and filters out advisories\naccording to the nsp offline exceptions format (see usage for an\nexample).\n\nThis provides a migration path from `nsp check` to `npm audit` and\nlets projects to use `npm audit` in CI pipelines without masking all\nadvisories (e.g. with `npm audit || true`).\n\n### Install\n\n#### Requirements\n\n* node 8.x or 10.x\n* npm@6 (for `--json` support and newer package-lock.json format)\n\n#### Local NPM package\n\n1. Run `npm install --save-dev audit-filter` to add it as a dev dependency\n\n1. Require an npm version with `npm audit` support in `package.json` e.g.\n\n```json\n{\n  ...\n  \"engines\": {\n    \"node\": \">=8\",\n    \"npm\": \">=6.4.1\"\n  },\n  ...\n}\n```\n\n1. Add an empty exceptions file named `.nsprc`:\n\n\n```json\n{\n  \"exceptions\": [\n  ]\n}\n```\n\n1. Optionally, add an npm script command:\n\n```json\n{\n  \"scripts\": {\n    \"lint:deps\": \"npm audit --json | audit-filter --nsp-config=.nsprc --audit=-\"\n\t...\n  }\n  ...\n  \"devDependencies\": {\n    \"audit-filter\": \"0.3.0\"\n  },\n  ...\n}\n```\n\nand test it with: `npm run lint:deps` or `npm run-script lint:deps`\n\n1. Optionally, set \"The minimum level of vulnerability for npm audit to exit with a non-zero exit with [`npm config audit level ('low', 'moderate', 'high', 'critical')`](https://docs.npmjs.com/misc/config#audit-level)\n\n#### Global NPM package\n\n```console\nnpm install -g audit-filter\n```\n\n#### Cargo\n\n```console\ncargo install audit-filter\n```\n\n### Usage\n\nNote: all commands run from the project root\n\n```console\n$ audit-filter -h\naudit-filter filters the output of \"npm audit --json\"\n\nUsage:\n  audit-filter [--json] [--audit=<->] [--nsp-config=<.nsprc>]\n  audit-filter (-h | --help | --version)\n\nOptions:\n  -h --help                       Show this screen.\n  --version                       Show version.\n  --json                          Output subset of JSON for the unfiltered advisories as an array.\n  --audit=<audit>                 NPM Audit JSON file [default: -].\n  --nsp-config=<config>           Default filter config [default: .nsprc].\n$ cd audit-filter/example/\n$ cat package.json\n{\n  \"dependencies\": {\n    \"moment\": \"2.19.2\",\n    \"restify\": \"7.0.0\"\n  },\n  \"devDependencies\": {\n    \"audit-filter\": \"0.3.0\",\n    \"lodash\": \"^4.17.15\"\n  },\n  \"engines\": {\n    \"node\": \">=8\",\n    \"npm\": \">=6.4.1\"\n  },\n  \"scripts\": {\n    \"lint:deps\": \"npm audit --json | audit-filter --nsp-config=.nsprc --audit=-\"\n  }\n}\n$ npm --version\n6.9.0\n$ npm audit\n\u001b[90m                                                                                \u001b[39m\n\u001b[90m \u001b[39m                      === npm audit security report ===                       \u001b[90m \u001b[39m\n\u001b[90m                                                                                \u001b[39m\n# Run  npm install moment@2.24.0  to resolve 1 vulnerability\n\u001b[90m\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u001b[39m\n\u001b[90m\u2502\u001b[39m Low           \u001b[90m\u2502\u001b[39m Regular Expression Denial of Service                         \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Package       \u001b[90m\u2502\u001b[39m moment                                                       \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Dependency of \u001b[90m\u2502\u001b[39m moment                                                       \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Path          \u001b[90m\u2502\u001b[39m moment                                                       \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m More info     \u001b[90m\u2502\u001b[39m https://npmjs.com/advisories/532                             \u001b[90m\u2502\u001b[39m\n\u001b[90m\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u001b[39m\n\n\n# Run  npm update moment --depth 3  to resolve 1 vulnerability\n\u001b[90m\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u001b[39m\n\u001b[90m\u2502\u001b[39m Low           \u001b[90m\u2502\u001b[39m Regular Expression Denial of Service                         \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Package       \u001b[90m\u2502\u001b[39m moment                                                       \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Dependency of \u001b[90m\u2502\u001b[39m restify                                                      \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Path          \u001b[90m\u2502\u001b[39m restify > bunyan > moment                                    \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m More info     \u001b[90m\u2502\u001b[39m https://npmjs.com/advisories/532                             \u001b[90m\u2502\u001b[39m\n\u001b[90m\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u001b[39m\n\n\n# Run  npm update lodash --depth 3  to resolve 2 vulnerabilities\n\u001b[90m\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u001b[39m\n\u001b[90m\u2502\u001b[39m High          \u001b[90m\u2502\u001b[39m Prototype Pollution                                          \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Package       \u001b[90m\u2502\u001b[39m lodash                                                       \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Dependency of \u001b[90m\u2502\u001b[39m restify                                                      \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Path          \u001b[90m\u2502\u001b[39m restify > lodash                                             \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m More info     \u001b[90m\u2502\u001b[39m https://npmjs.com/advisories/1065                            \u001b[90m\u2502\u001b[39m\n\u001b[90m\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u001b[39m\n\n\n\u001b[90m\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u001b[39m\n\u001b[90m\u2502\u001b[39m High          \u001b[90m\u2502\u001b[39m Prototype Pollution                                          \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Package       \u001b[90m\u2502\u001b[39m lodash                                                       \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Dependency of \u001b[90m\u2502\u001b[39m restify                                                      \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m Path          \u001b[90m\u2502\u001b[39m restify > restify-errors > lodash                            \u001b[90m\u2502\u001b[39m\n\u001b[90m\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\u001b[39m\n\u001b[90m\u2502\u001b[39m More info     \u001b[90m\u2502\u001b[39m https://npmjs.com/advisories/1065                            \u001b[90m\u2502\u001b[39m\n\u001b[90m\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u001b[39m\u001b[90m\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u001b[39m\n\n\nfound 4 vulnerabilities (2 low, 2 high) in 137 scanned packages\n  run `npm audit fix` to fix 4 of them.\n$ echo $?\n1\n$ cat .nsprc\n{\n  \"exceptions\": [\n    \"https://npmjs.com/advisories/532\",\n    \"https://npmjs.com/advisories/577\",\n    \"https://npmjs.com/advisories/782\",\n    \"https://npmjs.com/advisories/1065\"\n   ]\n}\n$ npm audit --json | audit-filter\nNo advisories found after filtering.\n$ echo $?\n0\n$ # Alternatively specify audit and config file paths (note: errors print to stderr)\n$ cd .. && audit-filter --nsp-config example/.nsprc --audit tests/fixtures/screenshots-e78ee92b9a76ed6796cbdf0a9f643e00efc8b8b1-npm-6.9.0-audit.json\nUnfiltered advisories:\n  https://npmjs.com/advisories/118\n  https://npmjs.com/advisories/534\n  https://npmjs.com/advisories/566\n  https://npmjs.com/advisories/598\n  https://npmjs.com/advisories/663\n  https://npmjs.com/advisories/755\n  https://npmjs.com/advisories/777\n  https://npmjs.com/advisories/786\n  https://npmjs.com/advisories/788\n  https://npmjs.com/advisories/803\n  https://npmjs.com/advisories/813\n  https://npmjs.com/advisories/886\n  https://npmjs.com/advisories/996\n  https://npmjs.com/advisories/1012\n  https://npmjs.com/advisories/1013\n  https://npmjs.com/advisories/1071\n$ echo $?\n1\n$ # use --json for JSON output\n$ audit-filter --json --nsp-config example/.nsprc --audit tests/fixtures/screenshots-e78ee92b9a76ed6796cbdf0a9f643e00efc8b8b1-npm-6.9.0-audit.json | head\n[\n  {\n    \"findings\": [\n      {\n        \"version\": \"2.0.10\",\n        \"paths\": [\n          \"istanbul-middleware>archiver>glob>minimatch\"\n        ],\n        \"dev\": null,\n        \"optional\": null,\n```\n\n### Fixing comments in .nsprc files\n\n```console\n$ cat tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-nsprc-comment.json\n{\n  // See https://github.com/mozilla-services/screenshots/issues/4397\n  \"exceptions\": [\n    \"https://nodesecurity.io/advisories/566\",\n    \"https://nodesecurity.io/advisories/577\",\n    \"https://nodesecurity.io/advisories/598\",\n    \"https://nodesecurity.io/advisories/663\",\n    \"https://nodesecurity.io/advisories/664\"\n   ]\n}\n$ audit-filter --nsp-config tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-nsprc-comment.json --audit tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-npm-6.4.1-audit.json\nError parsing nsp config JSON: key must be a string at line 2 column 3\n$ echo $?\n2\n$ cat tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-nsprc-comment.json | sed \"s|// .*||g\" | python -m json.tool\n{\n    \"exceptions\": [\n        \"https://nodesecurity.io/advisories/566\",\n        \"https://nodesecurity.io/advisories/577\",\n        \"https://nodesecurity.io/advisories/598\",\n        \"https://nodesecurity.io/advisories/663\",\n        \"https://nodesecurity.io/advisories/664\"\n    ]\n}\n$ # alternatively convert comments into valid JSON e.g.\n{\n  \"comment\": \"See https://github.com/mozilla-services/screenshots/issues/4397\",\n  \"exceptions\": [\n    \"https://nodesecurity.io/advisories/566\",\n    \"https://nodesecurity.io/advisories/577\",\n    \"https://nodesecurity.io/advisories/598\",\n    \"https://nodesecurity.io/advisories/663\",\n    \"https://nodesecurity.io/advisories/664\"\n   ]\n}\n```\n\n### Exit Codes\n\n* 0 - No advisories or all advisories acked from filters\n* 1 - New failures one or more unacked advisory. Rerun `npm audit` to see the errors.\n* 2 - Error finding or parsing config files or audit JSON.\n\n### Other errors\n\nNB: error messages will differ for audit-filter installed with NPM\n\nEnumerated here for completeness. These all exit with code 2.\n\n#### Error opening audit file\n\n```console\n$ audit-filter --nsp-config tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-nsprc.json --audit no-file\nError opening audit JSON no-file: No such file or directory (os error 2)\n```\n\n#### Error parsing audit from stdin\n\n```console\n$ echo \"this is not JSON\" | audit-filter --nsp-config tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-nsprc.json --audit -\nError parsing audit JSON from stdin: expected ident at line 1 column 2\n```\n\n#### Error parsing audit from file\n\n```console\n$ echo \"this is not JSON\" > not_json.txt\n$ audit-filter --nsp-config tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-nsprc.json --audit not_json.txt\nError parsing audit JSON: expected ident at line 1 column 2\n```\n\n#### Error opening nsp config file\n\n```console\n$ audit-filter --nsp-config no-file --audit tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-npm-6.4.1-audit.json\nError opening nsp config JSON no-file: No such file or directory (os error 2)\n```\n\n#### Error parsing nsp config from stdin\n\n```console\n$ echo \"this is not JSON\" | audit-filter --nsp-config - --audit tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-npm-6.4.1-audit.json\nError parsing nsp config JSON from stdin: expected ident at line 1 column 2\n```\n\n#### Error parsing nsp config from file\n\n```console\n$ echo \"this is not JSON\" > not_json.txt\n$ audit-filter --nsp-config not_json.txt --audit tests/fixtures/screenshots-0191b17d3bac5de51efa7acbaa0d52bb26c91573-npm-6.4.1-audit.json\nError parsing nsp config JSON: expected ident at line 1 column 2\n```\n\n### Building\n\nTo build a static executable:\n\n```console\n$ rustup target add x86_64-unknown-linux-musl\n...\n$ cargo build --release --target x86_64-unknown-linux-musl\n...\n$ ls -lh ./target/x86_64-unknown-linux-musl/release/audit-filter\n-rwxrwxr-x 2 gguthe gguthe 7.0M Sep 20 13:09 ./target/x86_64-unknown-linux-musl/release/audit-filter\n$ ldd ./target/x86_64-unknown-linux-musl/release/audit-filter\n        not a dynamic executable\n```\n\n### Contributors\n\n* @agwells\n"
},
{
  "name": "restify-safe-json-formatter",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "README.md",
      "example",
      "index.js",
      "package-lock.json",
      "package.json",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "A restify formatter that unicode escapes <, >, & as \\u003c, \\u003e,\n\\u0026 respectively to keep some browsers from misinterpreting it as\nHTML.\n\n\nUsage:\n\n```js\nvar restify = require('restify');\nvar safeJsonFormatter = require('restify-safe-json-formatter');\n\nvar server = restify.createServer({\n  formatters: {\n    'application/json; q=0.9': safeJsonFormatter\n  }\n});\n```\n\n\nSee also: [the example server](example/server.js)\n"
},
{
  "name": "syncstorage-rs",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".clog.toml",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".sentryclirc.example",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "config",
      "docker-compose.e2e.mysql.yaml",
      "docker-compose.e2e.spanner.yaml",
      "docker-compose.mysql.yaml",
      "docker-compose.spanner.yaml",
      "docs",
      "migrations",
      "requirements.txt",
      "scripts",
      "shell.nix",
      "spanner_config.ini",
      "syncstorage-common",
      "syncstorage-db-common",
      "syncstorage",
      "tests.ini",
      "tokenserver-common",
      "tools"
    ],
    "/docs": [
      "config.md"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "##\n# Collection of helper scripts used for local dev.\n##\n\nSYNC_DATABASE_URL = 'mysql://sample_user:sample_password@localhost/syncstorage_rs'\nSYNC_TOKENSERVER__DATABASE_URL = 'mysql://sample_user:sample_password@localhost/tokenserver_rs'\n\n# This key can live anywhere on your machine. Adjust path as needed.\nPATH_TO_SYNC_SPANNER_KEYS = `pwd`/service-account.json\n\n# TODO: replace with rust grpc alternative when ready\n# Assumes you've cloned the server-syncstorage repo locally into a peer dir.\n# https://github.com/mozilla-services/server-syncstorage\nPATH_TO_GRPC_CERT = ../server-syncstorage/local/lib/python2.7/site-packages/grpc/_cython/_credentials/roots.pem\n\nclippy:\n\t# Matches what's run in circleci\n\tcargo clippy --all --all-targets --all-features -- -D warnings\n\nclean:\n\tcargo clean\n\trm -r venv\n\ndocker_start_mysql:\n\tdocker-compose -f docker-compose.mysql.yaml up -d\n\ndocker_start_mysql_rebuild:\n\tdocker-compose -f docker-compose.mysql.yaml up --build -d\n\ndocker_stop_mysql:\n\tdocker-compose -f docker-compose.mysql.yaml down\n\ndocker_start_spanner:\n\tdocker-compose -f docker-compose.spanner.yaml up -d\n\ndocker_start_spanner_rebuild:\n\tdocker-compose -f docker-compose.spanner.yaml up --build -d\n\ndocker_stop_spanner:\n\tdocker-compose -f docker-compose.spanner.yaml down\n\npython:\n\tpython3 -m venv venv\n\tvenv/bin/python -m pip install -r requirements.txt\n\nrun: python\n\tPATH=\"./venv/bin:$(PATH)\" RUST_LOG=debug RUST_BACKTRACE=full cargo run -- --config config/local.toml\n\nrun_spanner:\n\tGOOGLE_APPLICATION_CREDENTIALS=$(PATH_TO_SYNC_SPANNER_KEYS) GRPC_DEFAULT_SSL_ROOTS_FILE_PATH=$(PATH_TO_GRPC_CERT) make run\n\ntest:\n\tSYNC_DATABASE_URL=$(SYNC_DATABASE_URL) SYNC_TOKENSERVER__DATABASE_URL=$(SYNC_TOKENSERVER__DATABASE_URL) RUST_TEST_THREADS=1 cargo test\n",
  "readme": "[![License: MPL 2.0][mpl-svg]][mpl] [![Build Status][circleci-badge]][circleci] [![Connect to Matrix via the Riot webapp][matrix-badge]][matrix]\n\n# Syncstorage-rs\n\nMozilla Sync Storage built with [Rust](https://rust-lang.org).\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n\n- [System Requirements](#system-requirements)\n- [Local Setup](#local-setup)\n  - [MySQL](#mysql)\n  - [Spanner](#spanner)\n  - [Running via Docker](#running-via-docker)\n  - [Connecting to Firefox](#connecting-to-firefox)\n- [Logging](#logging)\n  - [Sentry:](#sentry)\n  - [RUST_LOG](#rust_log)\n- [Tests](#tests)\n  - [Unit tests](#unit-tests)\n  - [End-to-End tests](#end-to-end-tests)\n- [Creating Releases](#creating-releases)\n- [Troubleshooting](#troubleshooting)\n- [Related Documentation](#related-documentation)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## System Requirements\n\n- cmake\n- gcc\n- [golang](https://golang.org/doc/install)\n- libcurl4-openssl-dev\n- libssl-dev\n- make\n- pkg-config\n- [Rust stable](https://rustup.rs)\n- MySQL 5.7 (or compatible)\n  * libmysqlclient (`brew install mysql` on macOS, `apt install libmysqlclient-dev` on Ubuntu, `apt install libmariadb-dev-compat` on Debian)\n\nDepending on your OS, you may also need to install `libgrpcdev`,\nand `protobuf-compiler-grpc`. *Note*: if the code complies cleanly,\nbut generates a Segmentation Fault within Sentry init, you probably\nare missing `libcurl4-openssl-dev`.\n\n## Local Setup\n\n1. Follow the instructions below to use either MySQL or Spanner as your DB.\n2. Now `cp config/local.example.toml config/local.toml`. Open `config/local.toml` and make sure you have the desired settings configured. For a complete list of available configuration options, check out [docs/config.md](docs/config.md).\n3. `make run` starts the server in debug mode, using your new `local.toml` file for config options. Or, simply `cargo run` with your own config options provided as env vars.\n4. Visit `http://localhost:8000/__heartbeat__` to make sure the server is running.\n\n### MySQL\n\nDurable sync needs only a valid mysql DSN in order to set up connections to a MySQL database. The database can be local and is usually specified with a DSN like:\n\n`mysql://_user_:_password_@_host_/_database_`\n\nTo setup a fresh MySQL DB and user: (`mysql -u root`):\n\n```sql\nCREATE USER \"sample_user\"@\"localhost\" IDENTIFIED BY \"sample_password\";\nCREATE DATABASE syncstorage_rs;\n\nGRANT ALL PRIVILEGES on syncstorage_rs.* to sample_user@localhost;\n```\n\n### Spanner\n\n#### Authenticating via OAuth\nThe correct way to authenticate with Spanner is by generating an OAuth token and pointing your local application server to the token. In order for this to work, your Google Cloud account must have the correct permissions; contact the Ops team to ensure the correct permissions are added to your account.\n\nFirst, install the Google Cloud command-line interface by following the instructions for your operating system [here](https://cloud.google.com/sdk/docs/install). Next, run the following to log in with your Google account (this should be the Google account associated with your Mozilla LDAP credentials):\n```sh\ngcloud auth application-default login\n```\nThe above command will prompt you to visit a webpage in your browser to complete the login process. Once completed, ensure that a file called `application_default_credentials.json` has been created in the appropriate directory (on Linux, this directory is `$HOME/.config/gcloud/`). The Google Cloud SDK knows to check this location for your credentials, so no further configuration is needed.\n\n##### Key Revocation\nAccidents happen, and you may need to revoke the access of a set of credentials if they have been publicly leaked. To do this, run:\n```sh\ngcloud auth application-default revoke\n```\nThis will revoke the access of the credentials currently stored in the `application_default_credentials.json` file. **If the file in that location does not contain the leaked credentials, you will need to copy the file containing the leaked credentials to that location and re-run the above command.** You can ensure that the leaked credentials are no longer active by attempting to connect to Spanner using the credentials. If access has been revoked, your application server should print an error saying that the token has expired or has been revoked.\n\n#### Authenticating via Service Account\nAn alternative to authentication via application default credentials is authentication via a service account. **Note that this method of authentication is not recommended. Service accounts are intended to be used by other applications or virtual machines and not people. See [this article](https://cloud.google.com/iam/docs/service-accounts#what_are_service_accounts) for more information.**\n\nYour system administrator will be able to tell you which service account keys have access to the Spanner instance to which you are trying to connect. Once you are given the email identifier of an active key, log into the [Google Cloud Console Service Accounts](https://console.cloud.google.com/iam-admin/serviceaccounts) page. Be sure to select the correct project.\n\n- Locate the email identifier of the access key and pick the vertical dot menu at the far right of the row.\n- Select \"_Create Key_\" from the pop-up menu.\n- Select \"JSON\" from the Dialog Box.\n\nA proper key file will be downloaded to your local directory. It's important to safeguard that key file. For this example, we're going to name the file\n`service-account.json`.\n\nThe proper key file is in JSON format. An example file is provided below, with private information replaced by \"`...`\"\n\n```json\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"...\",\n  \"private_key_id\": \"...\",\n  \"private_key\": \"...\",\n  \"client_email\": \"...\",\n  \"client_id\": \"...\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"...\"\n}\n```\n\n**Note that the name `service-account.json` must be exactly correct to be ignored by `.gitignore`.**\n\n#### Connecting to Spanner\nTo point to a GCP-hosted Spanner instance from your local machine, follow these steps:\n\n1. Authenticate via either of the two methods outlined above.\n2. Open `local.toml` and replace `database_url` with a link to your spanner instance.\n3. Open the Makefile and ensure you've correctly set you `PATH_TO_GRPC_CERT`.\n4. `make run_spanner`.\n5. Visit `http://localhost:8000/__heartbeat__` to make sure the server is running.\n\nNote, that unlike MySQL, there is no automatic migrations facility. Currently, the Spanner schema must be hand edited and modified.\n\n#### Emulator\nGoogle supports an in-memory Spanner emulator, which can run on your local machine for development purposes. You can install the emulator via the gcloud CLI or Docker by following the instructions [here](https://cloud.google.com/spanner/docs/emulator#installing_and_running_the_emulator). Once the emulator is running, you'll need to create a new instance and a new database. To create an instance using the REST API (exposed via port 9020 on the emulator), we can use `curl`:\n```sh\ncurl --request POST \\\n  \"localhost:9020/v1/projects/$PROJECT_ID/instances\" \\\n  --header 'Accept: application/json' \\\n  --header 'Content-Type: application/json' \\\n  --data \"{\\\"instance\\\":{\\\"config\\\":\\\"emulator-test-config\\\",\\\"nodeCount\\\":1,\\\"displayName\\\":\\\"Test Instance\\\"},\\\"instanceId\\\":\\\"$INSTANCE_ID\\\"}\"\n```\nNote that you may set `PROJECT_ID` and `INSTANCE_ID` to your liking. To create a new database on this instance, we'll use a similar HTTP request, but we'll need to include information about the database schema. Since we don't have migrations for Spanner, we keep an up-to-date schema in `src/db/spanner/schema.ddl`. The `jq` utility allows us to parse this file for use in the JSON body of an HTTP POST request:\n```sh\nDDL_STATEMENTS=$(\n  grep -v ^-- schema.ddl \\\n  | sed -n 's/ \\+/ /gp' \\\n  | tr -d '\\n' \\\n  | sed 's/\\(.*\\);/\\1/' \\\n  | jq -R -s -c 'split(\";\")'\n)\n```\nFinally, to create the database:\n```sh\ncurl -sS --request POST \\\n  \"localhost:9020/v1/projects/$PROJECT_ID/instances/$INSTANCE_ID/databases\" \\\n  --header 'Accept: application/json' \\\n  --header 'Content-Type: application/json' \\\n  --data \"{\\\"createStatement\\\":\\\"CREATE DATABASE \\`$DATABASE_ID\\`\\\",\\\"extraStatements\\\":$DDL_STATEMENTS}\"\n```\nNote that, again, you may set `DATABASE_ID` to your liking. Make sure that the `database_url` config variable reflects your choice of project name, instance name, and database name (i.e. it should be of the format `spanner://projects/<your project ID here>/instances/<your instance ID here>/databases/<your database ID here>`).\n\nTo run an application server that points to the local Spanner emulator:\n```sh\nSYNC_SPANNER_EMULATOR_HOST=localhost:9010 make run_spanner\n```\n\n### Running via Docker\nThis requires access to the mozilla-rust-sdk which is now available at `/vendor/mozilla-rust-adk`.\n\n1. Make sure you have [Docker installed](https://docs.docker.com/install/) locally.\n2. Copy the contents of mozilla-rust-sdk into top level root dir here.\n3. Change cargo.toml mozilla-rust-sdk entry to point to `\"path = \"mozilla-rust-sdk/googleapis-raw\"` instead of the parent dir.\n4. Comment out the `image` value under `syncstorage-rs` in either docker-compose.mysql.yml or docker-compose.spanner.yml (depending on which database backend you want to run), and add this instead:\n    ```yml\n      build:\n        context: .\n    ```\n5. If you are using MySQL, adjust the MySQL db credentials in docker-compose.mysql.yml to match your local setup.\n6. `make docker_start_mysql` or `make docker_start_spanner` - You can verify it's working by visiting [localhost:8000/\\_\\_heartbeat\\_\\_](http://localhost:8000/__heartbeat__)\n\n### Connecting to Firefox\n\nThis will walk you through the steps to connect this project to your local copy of Firefox.\n\n1. Follow the steps outlined above for running this project using [MySQL](https://github.com/mozilla-services/syncstorage-rs#mysql).\n\n2. Setup a local copy of [syncserver](https://github.com/mozilla-services/syncserver), with a few special changes to [syncserver.ini](https://github.com/mozilla-services/syncserver/blob/master/syncserver.ini); make sure that you're using the following values (in addition to all of the other defaults):\n\n    ```ini\n    [server:main]\n    port = 5000\n\n    [syncserver]\n    public_url = http://localhost:5000/\n\n    # This value needs to match your \"master_secret\" for syncstorage-rs!\n    secret = INSERT_SECRET_KEY_HERE\n\n    [tokenserver]\n    node_url = http://localhost:8000\n    sqluri = pymysql://sample_user:sample_password@127.0.0.1/syncstorage_rs\n\n    [endpoints]\n    sync-1.5 = \"http://localhost:8000/1.5/1\"```\n\n\n3. In Firefox, go to `about:config`. Change `identity.sync.tokenserver.uri` to `http://localhost:5000/token/1.0/sync/1.5`.\n4. Restart Firefox. Now, try syncing. You should see new BSOs in your local MySQL instance.\n\n## Logging\n\n### Sentry:\n1. If you want to connect to the existing [Sentry project](https://sentry.prod.mozaws.net/operations/syncstorage-local/) for local development, login to Sentry, and go to the page with [api keys](https://sentry.prod.mozaws.net/settings/operations/syncstorage-local/keys/). Copy the `DSN` value.\n2. Comment out the `human_logs` line in your `config/local.toml` file.\n3. You can force an error to appear in Sentry by adding a `panic!` into main.rs, just before the final `Ok(())`.\n4. Now, `SENTRY_DSN={INSERT_DSN_FROM_STEP_1_HERE} make run`.\n5. You may need to stop the local server after it hits the panic! before errors will appear in Sentry.\n\n### RUST_LOG\n\nWe use [env_logger](https://crates.io/crates/env_logger): set the `RUST_LOG` env var.\n\n## Tests\n\n### Unit tests\n\n`make test` - open the Makefile to adjust your `SYNC_DATABASE_URL` as needed.\n\n#### Debugging unit test state\n\nIn some cases, it is useful to inspect the mysql state of a failed test. By\ndefault, we use the diesel test_transaction functionality to ensure test data\nis not committed to the database. Therefore, there is an environment variable\nwhich can be used to turn off test_transaction.\n\n        SYNC_DATABASE_USE_TEST_TRANSACTIONS=false cargo test [testname]\n\nNote that you will almost certainly want to pass a single test name. When running\nthe entire test suite, data from previous tests will cause future tests to fail.\n\nTo reset the database state between test runs, drop and recreate the database\nin the mysql client:\n\n        drop database syncstorage_rs; create database syncstorage_rs; use syncstorage_rs;\n\n### End-to-End tests\n\nFunctional tests live in [server-syncstorage](https://github.com/mozilla-services/server-syncstorage/) and can be run against a local server, e.g.:\n\n1.  If you haven't already followed the instructions [here](https://mozilla-services.readthedocs.io/en/latest/howtos/run-sync-1.5.html) to get all the dependencies for the [server-syncstorage](https://github.com/mozilla-services/server-syncstorage/) repo, you should start there.\n\n2.  Install (Python) server-syncstorage:\n\n         $ git clone https://github.com/mozilla-services/server-syncstorage/\n         $ cd server-syncstorage\n         $ make build\n\n3.  Run an instance of syncstorage-rs (`cargo run` in this repo).\n\n4.  To run all tests:\n\n         $ ./local/bin/python syncstorage/tests/functional/test_storage.py http://localhost:8000#<SOMESECRET>\n\n5.  Individual tests can be specified via the `SYNC_TEST_PREFIX` env var:\n\n        $ SYNC_TEST_PREFIX=test_get_collection \\\n            ./local/bin/python syncstorage/tests/functional/test_storage.py http://localhost:8000#<SOMESECRET>\n\n## Creating Releases\n\n1. Switch to master branch of syncstorage-rs\n1. `git pull` to ensure that the local copy is up-to-date.\n1. `git pull origin master` to make sure that you've incorporated any changes to the master branch.\n1. `git diff origin/master` to ensure that there are no local staged or uncommited changes.\n1. Bump the version number in [Cargo.toml](https://github.com/mozilla-services/syncstorage-rs/blob/master/Cargo.toml) (this new version number will be designated as `<version>` in this checklist)\n1. create a git branch for the new version `git checkout -b release/<version>`\n1. `cargo build --release` - Build with the release profile [release mode](https://doc.rust-lang.org/book/ch14-01-release-profiles.html).\n1. `clog -C CHANGELOG.md` - Generate release notes. We're using [clog](https://github.com/clog-tool/clog-cli) for release notes. Add a `-p`, `-m` or `-M` flag to denote major/minor/patch version, ie `clog -C CHANGELOG.md -p`.\n1. Review the `CHANGELOG.md` file and ensure all relevant changes since the last tag are included.\n1. Create a new [release in Sentry](https://docs.sentry.io/product/releases/#create-release): `VERSION={release-version-here} bash scripts/sentry-release.sh`. If you're doing this for the first time, checkout the [tips below](https://github.com/mozilla-services/syncstorage-rs#troubleshooting) for troubleshooting sentry cli access.\n1. `git commit -am \"chore: tag <version>\"` to commit the new version and changes\n1. `git tag -s -m \"chore: tag <version>\" <version>` to create a signed tag of the current HEAD commit for release.\n1. `git push origin release/<version>` to push the commits to a new origin release branch\n1. `git push --tags origin release/<version>` to push the tags to the release branch.\n1. Submit a Pull Request (PR) on github to merge the release branch to master.\n1. Go to the [GitHub release](https://github.com/mozilla-services/syncstorage-rs/releases), you should see the new tag with no release information.\n1. Click the `Draft a new release` button.\n1. Enter the \\<version> number for `Tag version`.\n1. Copy and paste the most recent change set from `CHANGELOG.md` into the release description, omitting the top 2 lines (the name and version)\n1. Once your PR merges, click [Publish Release] on the [GitHub release](https://github.com/mozilla-services/syncstorage-rs/releases) page.\n\nSync server is automatically deployed to STAGE, however QA may need to be notified if testing is required. Once QA signs off, then a bug should be filed to promote the server to PRODUCTION.\n\n## Troubleshooting\n\n- `rm Cargo.lock; cargo clean;` - Try this if you're having problems compiling.\n\n- Some versions of OpenSSL 1.1.1 can conflict with grpcio's built in BoringSSL. These errors can cause syncstorage to fail to run or compile.\nIf you see a problem related to `libssl` you may need to specify the `cargo` option `--features grpcio/openssl`  to force grpcio to use OpenSSL.\n\n### Sentry\n\n- If you're having trouble working with Sentry to create releases, try authenticating using their self hosted server option that's outlined [here](https://docs.sentry.io/product/cli/configuration/) Ie, `sentry-cli --url https://selfhosted.url.com/ login`. It's also recommended to create a `.sentryclirc` config file. See [this example](https://github.com/mozilla-services/syncstorage-rs/blob/master/.sentryclirc.example) for the config values you'll need.\n\n## Related Documentation\n\n- [API docs](https://mozilla-services.readthedocs.io/en/latest/storage/apis-1.5.html)\n\n- [Code docs](https://mozilla-services.github.io/syncstorage-rs/syncstorage/)\n\n[mpl-svg]: https://img.shields.io/badge/License-MPL%202.0-blue.svg\n[mpl]: https://opensource.org/licenses/MPL-2.0\n[circleci-badge]: https://circleci.com/gh/mozilla-services/syncstorage-rs.svg?style=shield\n[circleci]: https://circleci.com/gh/mozilla-services/syncstorage-rs\n[matrix-badge]: https://img.shields.io/badge/chat%20on%20[m]-%23services%3Amozilla.org-blue\n[matrix]: https://chat.mozilla.org/#/room/#services:mozilla.org\n"
},
{
  "name": "socorro",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".editorconfig",
      ".env",
      ".eslintrc",
      ".github",
      ".gitignore",
      ".log4brains.yml",
      ".prettierrc",
      ".readthedocs.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "LICENSE",
      "Makefile",
      "README.rst",
      "WHATSNEW.rst",
      "bin",
      "contribute.json",
      "docker-compose.yml",
      "docker",
      "docs",
      "product_details",
      "renovate.json",
      "requirements-compose.txt",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "socorro-cmd",
      "socorro",
      "webapp-django"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "adr",
      "adr_log.rst",
      "annotations.rst",
      "conf.py",
      "contributing.rst",
      "correlations.rst",
      "crashqueue.rst",
      "crashstorage.rst",
      "dev.rst",
      "drawio",
      "exts",
      "flows",
      "index.rst",
      "overview.rst",
      "products.rst",
      "reprocessing.rst",
      "schemas.rst",
      "service",
      "signaturegeneration.rst",
      "spec_crashreport.rst",
      "stackwalk.rst",
      "telemetry_socorro_crash.rst",
      "tests",
      "whatsnew.rst"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n# Include my.env and export it so variables set in there are available\n# in the Makefile.\ninclude my.env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\nSOCORRO_UID ?= 10001\nSOCORRO_GID ?= 10001\n\n# Set this in the environment to force --no-cache docker builds.\nDOCKER_BUILD_OPTS :=\nifeq (1, ${NOCACHE})\nDOCKER_BUILD_OPTS := --no-cache\nendif\n\nDC := $(shell which docker-compose)\n\n.DEFAULT_GOAL := help\n.PHONY: help\nhelp:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@grep -E '^[a-zA-Z0-9_-]+:.*?## .*$$' Makefile \\\n\t\t| grep -v grep \\\n\t    | sed -n 's/^\\(.*\\): \\(.*\\)##\\(.*\\)/\\1\\3/p' \\\n\t    | column -t  -s '|'\n\t@echo \"\"\n\t@echo \"See https://socorro.readthedocs.io/ for more documentation.\"\n\nmy.env:\n\t@if [ ! -f my.env ]; \\\n\tthen \\\n\techo \"Copying my.env.dist to my.env...\"; \\\n\tcp docker/config/my.env.dist my.env; \\\n\tfi\n\n.docker-build:\n\tmake build\n\n.PHONY: build\nbuild: my.env  ## | Build docker images.\n\t${DC} build ${DOCKER_BUILD_OPTS} --build-arg userid=${SOCORRO_UID} --build-arg groupid=${SOCORRO_GID} app\n\t${DC} build oidcprovider fakesentry\n\ttouch .docker-build\n\n.PHONY: setup\nsetup: my.env .docker-build  ## | Set up Postgres, Elasticsearch, local SQS, and local S3 services.\n\t${DC} run --rm app shell /app/bin/setup_services.sh\n\n.PHONY: updatedata\nupdatedata: my.env  ## | Add/update necessary database data.\n\t${DC} run --rm app shell /app/bin/update_data.sh\n\n.PHONY: run\nrun: my.env  ## | Run processor, webapp, fakesentry, and all required services.\n\t${DC} up processor webapp fakesentry\n\n.PHONY: runservices\nrunservices: my.env  ## | Run service containers (Postgres, SQS, etc)\n\t${DC} up -d statsd postgresql memcached localstack elasticsearch\n\n.PHONY: stop\nstop: my.env  ## | Stop all service containers.\n\t${DC} stop\n\n.PHONY: shell\nshell: my.env .docker-build  ## | Open a shell in the app container.\n\t${DC} run --rm app shell\n\n.PHONY: clean\nclean:  ## | Remove all build, test, coverage, and Python artifacts.\n\t-rm .docker-build*\n\t-rm -rf .cache\n\n.PHONY: docs\ndocs: my.env .docker-build  ## | Generate Sphinx HTML documetation.\n\t${DC} run --rm --user ${SOCORRO_UID} app shell make -C docs/ html\n\n.PHONY: lint\nlint: my.env  ## | Lint code.\n\t${DC} run --rm --no-deps app shell ./bin/lint.sh\n\n.PHONY: lintfix\nlintfix: my.env  ## | Reformat code.\n\t${DC} run --rm --no-deps app shell ./bin/lint.sh --fix\n\n.PHONY: test\ntest: my.env .docker-build  ## | Run unit tests.\n\t# Make sure services are started and start localstack before the others to\n\t# give it a little more time to wake up\n\t${DC} up -d localstack\n\t${DC} up -d elasticsearch postgresql statsd\n\t# Run tests\n\t${DC} run --rm test shell ./bin/test.sh\n\n.PHONY: test-ci\ntest-ci: my.env .docker-build  ## | Run unit tests in CI.\n\t# Make sure services are started and start localstack before the others to\n\t# give it a little more time to wake up\n\t${DC} up -d localstack\n\t${DC} up -d elasticsearch postgresql statsd\n\t# Run tests in test-ci which doesn't volume mount local directory\n\t${DC} run --rm test-ci shell ./bin/test.sh\n\n.PHONY: testshell\ntestshell: my.env .docker-build  ## | Open a shell in the test environment.\n\t${DC} run --rm test shell\n\n.PHONY: rebuildreqs\nrebuildreqs: .env .docker-build  ## | Rebuild requirements.txt file after requirements.in changes.\n\tdocker-compose run --rm --no-deps app shell pip-compile --generate-hashes\n\n.PHONY: updatereqs\nupdatereqs: .env .docker-build  ## | Update deps in requirements.txt file.\n\tdocker-compose run --rm --no-deps app shell pip-compile --generate-hashes -U\n",
  "readme": "=========================================\nSocorro: Mozilla crash ingestion pipeline\n=========================================\n\nSocorro is a Mozilla-centric ingestion pipeline and analysis tools for\ncrash reports using the `Breakpad libraries\n<http://code.google.com/p/google-breakpad/>`_.\n\n* Free software: Mozilla Public License version 2.0\n* Community Participation Guidelines `Guidelines <https://github.com/mozilla-services/socorro/blob/main/CODE_OF_CONDUCT.md>`_\n* Chat: `#crashreporting matrix channel <https://chat.mozilla.org/#/room/#crashreporting:mozilla.org>`__\n* Socorro (processor/webapp/cron jobs)\n\n  * Code: https://github.com/mozilla-services/socorro/\n  * Documentation: https://socorro.readthedocs.io/\n  * Bugs: `Report a bug <https://bugzilla.mozilla.org/enter_bug.cgi?format=__standard__&product=Socorro&component=General>`_\n\n* Antenna (collector)\n\n  * Code: https://github.com/mozilla-services/antenna/\n  * Documentation: https://antenna.readthedocs.io/\n  * Bugs: `Report an Antenna bug <https://bugzilla.mozilla.org/enter_bug.cgi?format=__standard__&product=Socorro&component=Antenna>`_\n\n\n.. Note::\n\n   This is a very Mozilla-specific product. We do not currently have the\n   capacity to support non-Mozilla uses.\n\n\nCode of Conduct\n===============\n\nThis project and repository is governed by Mozilla's code of conduct and\netiquette guidelines. For more details please see the `CODE_OF_CONDUCT.md file\n<https://github.com/mozilla-services/socorro/blob/main/CODE_OF_CONDUCT.md>`_.\n\n\nDocumentation\n=============\n\nDocumentation for setting up Socorro, configuration, specifications,\ndevelopment, and other related things are at\n`<https://socorro.readthedocs.io/>`_.\n\n\nReleases\n========\n\nWe use continuous development and we release often. See our list of releases\nfor what changes were deployed to production when:\n\nhttps://github.com/mozilla-services/socorro/tags\n\n\nSupport\n=======\n\nNote: This is a Mozilla-specific product. We do not currently have the capacity\nto support external users.\n\nIf you are looking to use Socorro for your product, maybe you want to consider\nthis non-exhaustive list of alternatives:\n\n* run your own: `electron/mini-breakpad-server\n  <https://github.com/electron/mini-breakpad-server>`_\n* run your own: `wk8/sentry_breakpad <https://github.com/wk8/sentry_breakpad>`_\n* hosted/on-premise: `Backtrace <https://backtrace.io/>`_, `BugSplat <https://bugsplat.com/>`_\n"
},
{
  "name": "merino",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".czrc",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "book.toml",
      "config",
      "deduped-dashmap",
      "dev",
      "docs",
      "merino-adm",
      "merino-cache",
      "merino-integration-tests-macro",
      "merino-integration-tests",
      "merino-settings",
      "merino-showroom",
      "merino-suggest-providers",
      "merino-suggest-traits",
      "merino-web",
      "merino-wikipedia",
      "merino",
      "test-engineering",
      "version.json"
    ],
    "/docs": [
      "SUMMARY.md",
      "adrs",
      "api.md",
      "data.md",
      "dev",
      "firefox.md",
      "intro.md",
      "ops.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "ISSUE_TEMPLATE",
      "dependabot.yml"
    ],
    "/.circleci": [
      "check-cargo-version.sh",
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Merino\n\nA service to provide address bar suggestions to Firefox. For more details, see\nthe [service docs](https://mozilla-services.github.io/merino/).\n\n## About the Name\n\nThis project drives an important part of Firefox's \"felt experience\". That is,\nthe feeling of using Firefox, hopefully in a delightful way. The word \"felt\" in\nthis phrase refers to feeling, but it can be punned to refer to the\n[textile](https://en.wikipedia.org/wiki/Felt). Felt is often made of wool, and\nMerino wool (from Merino sheep) produces exceptionally smooth felt.\n"
},
{
  "name": "shavar-list-creation-config",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "prod.ini",
      "stage.ini"
    ]
  },
  "makefile": null,
  "readme": "# shavar-list-creation-config\ncontains config files needed to run the jenkins task that builds the shavar lists\n"
},
{
  "name": "tecken",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".editorconfig",
      ".flake8",
      ".github",
      ".gitignore",
      ".log4brains.yml",
      ".readthedocs.yaml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "LICENSE",
      "Makefile",
      "README.rst",
      "bin",
      "contribute.json",
      "docker-compose.yml",
      "docker",
      "docs",
      "eliot-service",
      "favicons",
      "frontend",
      "manage.py",
      "package.json",
      "pytest.ini",
      "requirements.in",
      "requirements.txt",
      "schemas",
      "setup.cfg",
      "setup.py",
      "systemtests",
      "tecken"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "adr",
      "adr_log.rst",
      "conf.py",
      "configuration.rst",
      "contributing.rst",
      "dev.rst",
      "diagrams",
      "download.rst",
      "drawio",
      "endtoendtesting.rst",
      "exts",
      "frontend.rst",
      "index.rst",
      "logo.png",
      "overview.rst",
      "redis.rst",
      "symbolication.rst",
      "upload.rst"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n# Include my.env and export it so variables set in there are available\n# in the Makefile.\ninclude .env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\nUSE_UID ?= 10001\nUSE_GID ?= 10001\n\n.DEFAULT_GOAL := help\n.PHONY: help\nhelp:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@grep -E '^[a-zA-Z0-9_-]+:.*?## .*$$' Makefile \\\n\t\t| grep -v grep \\\n\t    | sed -n 's/^\\(.*\\): \\(.*\\)##\\(.*\\)/\\1\\3/p' \\\n\t    | column -t  -s '|'\n\t@echo \"\"\n\t@echo \"Adjust your .env file to set configuration.\"\n\t@echo \"\"\n\t@echo \"See https://tecken.readthedocs.io/ for more documentation.\"\n\n# Dev configuration steps\n.docker-build:\n\tmake build\n\n.env:\n\t./bin/cp-env-file.sh\n\n.PHONY: build\nbuild: .env  ## | Build docker images.\n\tdocker-compose build --build-arg userid=${USE_UID} --build-arg groupid=${USE_GID} base frontend\n\ttouch .docker-build\n\n.PHONY: setup\nsetup: .env  ## | Initialize services.\n\tdocker-compose run --rm web bash /app/bin/setup-services.sh\n\n.PHONY: run\nrun: .env .docker-build  ## | Run the web app and services.\n\tdocker-compose up web eliot frontend fakesentry\n\n.PHONY: stop\nstop: .env  ## | Stop docker containers.\n\tdocker-compose stop\n\n.PHONY: shell\nshell: .env .docker-build  ## | Open a shell in web container.\n\tdocker-compose run --rm web bash\n\n.PHONY: clean\nclean: .env stop  ## | Stop and remove docker containers and artifacts.\n\tdocker-compose rm -f\n\trm -fr .docker-build\n\trm -rf frontend/build/\n\n.PHONY: clear-cache\nclear-cache:  ## | Clear Redis cache.\n\tdocker-compose run --rm redis-cache redis-cli -h redis-cache FLUSHDB\n\n.PHONY: redis-cache-cli\nredis-cache-cli: .env .docker-build  ## | Open Redis CLI to cache Redis server.\n\tdocker-compose run --rm redis-cache redis-cli -h redis-cache\n\n.PHONY: psql\npsql: .env .docker-build  ## | Open psql cli.\n\t@echo \"NOTE: Password is 'postgres'.\"\n\tdocker-compose run --rm db psql -h db -U postgres -d tecken\n\n.PHONY: test\ntest: .env .docker-build  ## | Run Python unit test suite.\n\tdocker-compose up -d db redis-cache localstack statsd oidcprovider\n\tdocker-compose run --rm test bash ./bin/run_test.sh\n\n.PHONY: testshell\ntestshell: .env .docker-build  ## | Open shell in test environment.\n\tdocker-compose up -d db redis-cache localstack statsd oidcprovider\n\tdocker-compose run --rm test bash ./bin/run_test.sh --shell\n\n.PHONY: docs\ndocs: .env .docker-build  ## | Build docs.\n\tdocker-compose run --rm --user ${USE_UID} --no-deps web bash make -C docs/ clean\n\tdocker-compose run --rm --user ${USE_UID} --no-deps web bash make -C docs/ html\n\n.PHONY: lint\nlint: .env .docker-build  ## | Lint code.\n\tdocker-compose run --rm --no-deps test bash ./bin/run_lint.sh\n\tdocker-compose run --rm frontend lint\n\tdocker-compose run --rm --no-deps test bash python bin/license_check.py\n\n.PHONY: lintfix\nlintfix: .env .docker-build  ## | Reformat code.\n\tdocker-compose run --rm --no-deps test bash ./bin/run_lint.sh --fix\n\tdocker-compose run --rm frontend lintfix\n\n.PHONY: rebuildreqs\nrebuildreqs: .env .docker-build  ## | Rebuild requirements.txt file after requirements.in changes.\n\tdocker-compose run --rm --no-deps web bash pip-compile --generate-hashes\n\n.PHONY: updatereqs\nupdatereqs: .env .docker-build  ## | Update deps in requirements.txt file.\n\tdocker-compose run --rm --no-deps web bash pip-compile --generate-hashes -U\n",
  "readme": "===============================\nTecken - Mozilla Symbols Server\n===============================\n\nTecken is the umbrella project for managing and using symbols at Mozilla. It\nconsists of two services:\n\n1. Mozilla Symbols Server which manages symbols generated by builds of Mozilla\n   products (:ref:`upload <upload>`, :ref:`download <download>`).\n2. Mozilla Symbolication Server which has a symbolication API for converting\n   memory addresses into symbols (:ref:`symbolication <symbolication>`).\n\n\nProject details\n===============\n\n.. image:: https://circleci.com/gh/mozilla-services/tecken.svg?style=svg\n   :alt: Circle CI status\n   :target: https://circleci.com/gh/mozilla-services/tecken\n.. image:: https://readthedocs.org/projects/tecken/badge/?version=latest\n   :alt: ReadTheDocs status\n   :target: https://tecken.readthedocs.io/\n\n* Free software: Mozilla Public License version 2.0\n* Documentation: `<https://tecken.readthedocs.io/>`_\n* Community Participation Guidelines: `<https://github.com/mozilla-services/tecken/blob/main/CODE_OF_CONDUCT.md>`_\n* Bugs: `Report a Tecken bug <https://bugzilla.mozilla.org/enter_bug.cgi?format=__standard__&product=Tecken>`_\n* Chat: `#crashreporting matrix channel <https://chat.mozilla.org/#/room/#crashreporting:mozilla.org>`_\n"
},
{
  "name": "consvc-shepherd",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".env.example",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "consvc_shepherd",
      "contile",
      "docker-compose.yml",
      "manage.py",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "stubs",
      "templates"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Contextual Services Shepherd\n\nThis is a tool to manage dynamic settings for Contextual Services projects like\nMerino and Contile. The settings managed here can be changed at runtime,\nreducing the need for deployments of the services that use it.\n\n## Quick Start\n\nTo use consvc-shepherd, you'll need a Python 3.10 development environment.\n\nYou'll need to specify some minimal configuration. Create a file `.env` and put\nin it at least, if using the docker workflow, you may want to use `.env.example` \nand rename it to `env`:\n\n```shell\nDEBUG=true\nSECRET_KEY=keyboard-mash\n```\n\nWith that ready, you can set up the Django site:\n\n```shell\n# Prepare a virtual environment (customize this as you see fit)\n$ python -m venv .venv\n$ source .venv/bin/activate\n\n# Install bootstrap-dependencies\n$ pip install -U pip pip-tools\n\n# Install dependencies\n$ pip-sync\n\n# Set up Django\n$ ./manage.py migrate\n$ ./manage.py createsuperuser\n```\n\nAfter that is done, you can run the development server:\n\n```shell\n# Activate the virtualenv, if not already active\n$ source .venv/bin/activate\n\n# Start the app\n$ ./manage.py runserver\n```\n\nThis will start a development server that reloads on changes to the files. You\ncan access the configuration part of the site at\n[localhost:8000/admin][http://localhost:8000/admin].\n\n### Docker set up\n\nStart with building and get it up with:\n```\ndocker compose build\ndocker compose up\n```\nTo create a user, shell into the container and run\n``` \n./manage.py createsuperuser\n```"
},
{
  "name": "remote-settings-client",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE-OF-CONDUCT.md",
      "CONTRIBUTING.md",
      "COPYRIGHT",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "rs-client-demo",
      "src"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Remote Settings Client\n\nA Rust Remote Settings Client to fetch collection data.\n\n- Read-Only\n- Customizable Signature Verification\n<!-- - Cross-Platform\n- Robust -->\n\nConsumers can define their own HTTP implementation by implementing the `net::Requester` trait.\nThis library provides an implementation of the the `net::ViaductClient` HTTP requester based on on Mozilla's [viaduct](https://github.com/mozilla/application-services/tree/v93.5.0/components/viaduct) for its pluggable HTTP backend (eg. `reqwest` or `FFI` on Android).\n\n## Quick start\n\n`Cargo.toml`:\n\n```toml\n[dependencies]\nremote-settings-client = { version = \"0.1\", features = [\"ring_verifier\", \"viaduct_client\"] }\ntokio = { version = \"1.8.2\", features = [\"macros\"] }\n```\n\nMinimal example:\n\n```rust\nuse remote_settings_client::{Client, client::net::ViaductClient};\n\n#[tokio::main]\nasync fn main() {\n  viaduct::set_backend(&viaduct_reqwest::ReqwestBackend).unwrap();\n\n  let client = Client::builder()\n    .collection_name(\"search-config\")\n    .http_client(Box::new(ViaductClient))\n    .build();\n\n  match client.get().await {\n    Ok(records) => println!(\"{:?}\", records),\n    Err(error) => println!(\"Error fetching/verifying records: {:?}\", error),\n  };\n}\n```\n\nSee also our [demo project](rs-client-demo)!\n\n## Documentation\n\n[Crate documentation](https://docs.rs/remote_settings_client)\n\n## Logging\n\nUsing [env_logger](https://docs.rs/env_logger), the log level can be set via an environ variable:\n\n`RUSTLOG={debug/info} cargo run`\n\n```rust\nfn main() {\n  env_logger::init() // initialize logger\n  ..\n}\n```\n\n## License\n\nLicensed under Mozilla Public License, Version 2.0 (https://www.mozilla.org/en-US/MPL/2.0/)\n"
},
{
  "name": "python-canonicaljson-rs",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGELOG.rst",
      "CODE-OF-CONDUCT.md",
      "CONTRIBUTORS.rst",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.rst",
      "pyproject.toml",
      "src",
      "tests"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "canonicaljson-rs\n################\n\nPython package leveraging our Canonical JSON implementation in Rust.\n\nIn order to validate content signatures of our data, Canonical JSON gives us a predictable JSON serialization.\nAnd Rust allows us to reuse the same implementation between our server in Python (this package) and our diverse clients (Rust, Android/iOS, JavaScript).\n\nUsage\n=====\n\n.. code-block ::\n\n    pip install canonicaljson-rs\n\n.. code-block :: python\n\n    >>> import canonicaljson\n    >>>\n    >>> canonicaljson.dumps({\"h\u00e9o\": 42})\n    '{\"h\\\\u00e9o\":42}'\n\n\n* ``canonicaljson.dumps(obj: Any) -> str``\n* ``canonicaljson.dump(obj: Any, stream: IO) -> str``\n\n\nDevelopment\n===========\n\nWe rely on a specific Python builder that automates everything around Rust bindings.\n\n.. code-block ::\n\n    pip install maturin\n\nIn order to install the package in the current environment:\n\n.. code-block ::\n\n    maturin develop\n\nBuild and Release:\n\n.. code-block ::\n\n    vim Cargo.toml\n    git ci -am \"Bump version\"\n    git tag -a v1.2.3\n\nUpdate version in ``Cargo.toml`` and:\n\n.. code-block ::\n\n    maturin build\n    maturin publish\n\nSee Also\n========\n\n* https://github.com/gibson042/canonicaljson-spec\n* The code to build a ``serde_json::Value`` from a ``pyo3::PyObject`` was greatly inspired by Matthias Endler's `hyperjson <https://github.com/mre/hyperjson/>`_\n\nOther specs:\n\n* https://github.com/Kinto/kinto-signer/blob/6.1.0/kinto_signer/canonicaljson.py\n* https://searchfox.org/mozilla-central/rev/b2395478c/toolkit/modules/CanonicalJSON.jsm\n* https://github.com/matrix-org/python-canonicaljson\n\nLicense\n=======\n\n* Mozilla Public License 2.0\n"
},
{
  "name": "pkcs7",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE",
      "Makefile",
      "README.md",
      "ber.go",
      "ber_test.go",
      "decrypt.go",
      "decrypt_test.go",
      "encrypt.go",
      "encrypt_test.go",
      "go.mod",
      "pkcs7.go",
      "pkcs7_test.go",
      "sign.go",
      "sign_test.go",
      "verify.go",
      "verify_test.go",
      "verify_test_dsa.go"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "all: vet staticcheck test\n\ntest:\n\tgo test -covermode=count -coverprofile=coverage.out .\n\nshowcoverage: test\n\tgo tool cover -html=coverage.out\n\nvet:\n\tgo vet .\n\nlint:\n\tgolint .\n\nstaticcheck:\n\tstaticcheck .\n\ngettools:\n\tgo get -u honnef.co/go/tools/...\n\tgo get -u golang.org/x/lint/golint\n",
  "readme": "# pkcs7\n\n[![GoDoc](https://godoc.org/go.mozilla.org/pkcs7?status.svg)](https://godoc.org/go.mozilla.org/pkcs7)\n[![Build Status](https://github.com/mozilla-services/pkcs7/workflows/CI/badge.svg?branch=master&event=push)](https://github.com/mozilla-services/pkcs7/actions/workflows/ci.yml?query=branch%3Amaster+event%3Apush)\n\npkcs7 implements parsing and creating signed and enveloped messages.\n\n```go\npackage main\n\nimport (\n\t\"bytes\"\n\t\"crypto/rsa\"\n\t\"crypto/x509\"\n\t\"encoding/pem\"\n\t\"fmt\"\n\t\"os\"\n\n    \"go.mozilla.org/pkcs7\"\n)\n\nfunc SignAndDetach(content []byte, cert *x509.Certificate, privkey *rsa.PrivateKey) (signed []byte, err error) {\n\ttoBeSigned, err := NewSignedData(content)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"Cannot initialize signed data: %s\", err)\n\t\treturn\n\t}\n\tif err = toBeSigned.AddSigner(cert, privkey, SignerInfoConfig{}); err != nil {\n\t\terr = fmt.Errorf(\"Cannot add signer: %s\", err)\n\t\treturn\n\t}\n\n\t// Detach signature, omit if you want an embedded signature\n\ttoBeSigned.Detach()\n\n\tsigned, err = toBeSigned.Finish()\n\tif err != nil {\n\t\terr = fmt.Errorf(\"Cannot finish signing data: %s\", err)\n\t\treturn\n\t}\n\n\t// Verify the signature\n\tpem.Encode(os.Stdout, &pem.Block{Type: \"PKCS7\", Bytes: signed})\n\tp7, err := pkcs7.Parse(signed)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"Cannot parse our signed data: %s\", err)\n\t\treturn\n\t}\n\n\t// since the signature was detached, reattach the content here\n\tp7.Content = content\n\n\tif bytes.Compare(content, p7.Content) != 0 {\n\t\terr = fmt.Errorf(\"Our content was not in the parsed data:\\n\\tExpected: %s\\n\\tActual: %s\", content, p7.Content)\n\t\treturn\n\t}\n\tif err = p7.Verify(); err != nil {\n\t\terr = fmt.Errorf(\"Cannot verify our signed data: %s\", err)\n\t\treturn\n\t}\n\n\treturn signed, nil\n}\n```\n\n\n\n## Credits\nThis is a fork of [fullsailor/pkcs7](https://github.com/fullsailor/pkcs7)\n"
},
{
  "name": "autograph",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "authorize.go",
      "authorize_test.go",
      "autograph.encrypted.yaml",
      "autograph.yaml",
      "bin",
      "context.go",
      "database",
      "docker-compose.yml",
      "docs",
      "errors.go",
      "formats",
      "go.mod",
      "go.sum",
      "handlers.go",
      "handlers_racing_test.go",
      "handlers_test.go",
      "id.go",
      "id_test.go",
      "logging.go",
      "main.go",
      "main_racing_test.go",
      "main_test.go",
      "memory_backend.go",
      "middleware.go",
      "monitor.go",
      "monitor_handler.go",
      "monitor_handler_racing_test.go",
      "monitor_handler_test.go",
      "profiler.go",
      "rds-combined-ca-bundle.pem",
      "signer",
      "stats.go",
      "tools",
      "verifier",
      "version.sh"
    ],
    "/docs": [
      "architecture.md",
      "configuration.md",
      "endpoints.md",
      "hsm.md",
      "statics",
      "troubleshooting.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\nPACKAGE_NAMES := github.com/mozilla-services/autograph github.com/mozilla-services/autograph/database github.com/mozilla-services/autograph/formats github.com/mozilla-services/autograph/signer github.com/mozilla-services/autograph/signer/apk2 github.com/mozilla-services/autograph/signer/contentsignature github.com/mozilla-services/autograph/signer/contentsignaturepki github.com/mozilla-services/autograph/signer/genericrsa github.com/mozilla-services/autograph/signer/gpg2 github.com/mozilla-services/autograph/signer/mar github.com/mozilla-services/autograph/signer/xpi github.com/mozilla-services/autograph/verifier/contentsignature\n\nall: generate test vet lint staticcheck install\n\n# update the vendored version of the wait-for-it.sh script\ninstall-wait-for-it:\n\tcurl -o bin/wait-for-it.sh https://raw.githubusercontent.com/vishnubob/wait-for-it/master/wait-for-it.sh\n\tsha256sum -c bin/wait-for-it.sh.sha256\n\tchmod +x bin/wait-for-it.sh\n\ninstall-golint:\n\tgo get -u golang.org/x/lint/golint\n\ninstall-cover:\n\tgo get -u golang.org/x/tools/cmd/cover\n\ninstall-goveralls:\n\tgo get -u github.com/mattn/goveralls\n\ninstall-staticcheck:\n\tgo get -u honnef.co/go/tools/cmd/staticcheck\n\ninstall-go-mod-upgrade:\n\tgo get -u github.com/oligot/go-mod-upgrade\n\ninstall-dev-deps: install-golint install-staticcheck install-cover install-goveralls install-go-mod-upgrade\n\ninstall:\n\tgo install github.com/mozilla-services/autograph\n\nvendor:\n\tgo-mod-upgrade\n\ntag: all\n\tgit tag -s $(TAGVER) -a -m \"$(TAGMSG)\"\n\nlint:\n\tgolint $(PACKAGE_NAMES) | tee /tmp/autograph-golint.txt\n\ttest 0 -eq $(shell cat /tmp/autograph-golint.txt | grep -Pv 'stutters|suggestions' | wc -l)\n\n# refs: https://github.com/mozilla-services/autograph/issues/247\ncheck-no-crypto11-in-signers:\n\ttest 0 -eq $(shell grep -Ri crypto11 signer/*/ | tee /tmp/autograph-crypto11-check.txt | wc -l)\n\nshow-lints:\n\t-cat /tmp/autograph-golint.txt /tmp/autograph-crypto11-check.txt /tmp/autograph-staticcheck.txt\n\t-rm -f /tmp/autograph-golint.txt /tmp/autograph-crypto11-check.txt /tmp/autograph-staticcheck.txt\n\nvet:\n\tgo vet $(PACKAGE_NAMES)\n\nfmt-diff:\n\tgofmt -d *.go database/ signer/ tools/autograph-client/ $(shell ls tools/autograph-monitor/*.go) tools/softhsm/ tools/hawk-token-maker/ tools/make-hsm-ee/ tools/makecsr/ tools/genpki/\n\nfmt-fix:\n\tgo fmt $(PACKAGE_NAMES)\n\tgofmt -w tools/autograph-client/ $(shell ls tools/autograph-monitor/*.go) tools/softhsm/ tools/hawk-token-maker/ tools/make-hsm-ee/ tools/makecsr/ tools/genpki/\n\nbenchmarkxpi:\n\tgo test -run=XXX -benchtime=15s -bench=. -v -cpuprofile cpu.out github.com/mozilla-services/autograph/signer/xpi ;\\\n\nshowbenchmarkxpi:\n\tgo tool pprof -web cpu.out\n\nrace:\n\tgo test -race -covermode=atomic -count=1 $(PACKAGE_NAMES)\n\nstaticcheck:\n\tstaticcheck -go 1.16 $(PACKAGE_NAMES) | tee /tmp/autograph-staticcheck.txt\n\t# ignore errors in pkgs\n\t# ignore SA1019 for DSA being deprecated refs: GH #667\n\ttest 0 -eq $(shell cat /tmp/autograph-staticcheck.txt | grep -Pv '^/go/pkg/mod/|SA1019' | wc -l)\n\ntest:\n\tgo test -v -coverprofile coverage.out -covermode=count -count=1 $(PACKAGE_NAMES)\n\nshowcoverage: test\n\tgo tool cover -html=coverage.out\n\ngenerate:\n\tgo generate\n\ngpg-test-clean:\n\trm -rf ~/.gnupg /tmp/autograph_gpg2*\n\tkillall gpg-agent\n\n# image build order:\n#\n# app -> {app-hsm,monitor}\n# monitor -> monitor-lambda-emulator,monitor-hsm-lambda-emulator\n# app-hsm -> monitor-hsm-lambda-emulator (app-hsm writes chains and updated config to shared /tmp volume)\n#\nbuild: generate\n\tDOCKER_BUILDKIT=0 COMPOSE_DOCKER_CLI_BUILD=0 docker-compose build --no-cache --parallel app db\n\tDOCKER_BUILDKIT=0 COMPOSE_DOCKER_CLI_BUILD=0 docker-compose build --no-cache --parallel app-hsm monitor\n\tDOCKER_BUILDKIT=0 COMPOSE_DOCKER_CLI_BUILD=0 docker-compose build --no-cache --parallel monitor-lambda-emulator monitor-hsm-lambda-emulator\n\nintegration-test:\n\t./bin/run_integration_tests.sh\n\ndummy-statsd:\n\tnc -kluvw 0 localhost 8125\n\n.SUFFIXES:            # Delete the default suffixes\n.PHONY: all dummy-statsd test generate vendor integration-test check-no-crypto11-in-signers\n",
  "readme": "# Autograph\nAutograph is a cryptographic signature service that implements\n[Content-Signature](signer/contentsignaturepki/README.md),\n[XPI Signing](signer/xpi/README.md) for Firefox web extensions,\n[MAR Signing](signer/mar/README.md) for Firefox updates,\n[APK Signing](signer/apk2/README.md) for Android,\n[GPG2](signer/gpg2/README.md)\nand [RSA](signer/genericrsa/README.md).\n\n[![CircleCI](https://circleci.com/gh/mozilla-services/autograph/tree/main.svg?style=svg)](https://circleci.com/gh/mozilla-services/autograph/tree/main)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla-services/autograph/badge.svg?branch=main)](https://coveralls.io/github/mozilla-services/autograph?branch=main)\n[![Dependabot Status](https://api.dependabot.com/badges/status?host=github&repo=mozilla-services/autograph)](https://dependabot.com)\n\nWhy is it called \"autograph\"? Because it's a service to sign stuff.\n\n## Installation\n\n### Using Docker\n\n`docker pull mozilla/autograph && docker run mozilla/autograph`\n\nThis will download the latest build of autograph from DockerHub and run it with its dev configuration.\n\n### Using go get\n\nIf you don't yet have a GOPATH, export one:\n```bash\n$ export GOPATH=$HOME/go\n$ mkdir $GOPATH\n```\n\nInstall ltdl:\n* on Ubuntu: ltdl-dev\n* on RHEL/Fedora/Arch: libtool-ltdl-devel\n* on MacOS: libtool (NB: this might require `brew unlink libtool && brew link libtool`)\n\nThen download and build autograph:\n```bash\n$ go get github.com/mozilla-services/autograph\n```\n\nThe resulting binary will be placed in `$GOPATH/bin/autograph`. To run autograph with the example conf, do:\n```bash\n$ cd $GOPATH/src/github.com/mozilla-services/autograph\n$ $GOPATH/bin/autograph -c autograph.yaml\n```\n\nExample clients are in the `tools` directory. You can install the Go one like this:\n```bash\n$ go get github.com/mozilla-services/autograph/tools/autograph-client\n$ $GOPATH/bin/autograph-client -u alice -p fs5wgcer9qj819kfptdlp8gm227ewxnzvsuj9ztycsx08hfhzu -t http://localhost:8000/sign/data -r '[{\"input\": \"Y2FyaWJvdW1hdXJpY2UK\"}]'\n2016/08/23 17:25:55 signature 0 pass\n```\n\n## Documentation\n\n* [Architecture](docs/architecture.md)\n* [Configuration](docs/configuration.md)\n* [Endpoints](docs/endpoints.md)\n* [HSM Support](docs/hsm.md)\n\n### Signers\n\n* [APK](signer/apk2/README.md)\n* [Content-Signature PKI](signer/contentsignaturepki/README.md)\n* [Content-Signature protocol](signer/contentsignature/README.md)\n* [GPG](signer/gpg2/README.md)\n* [MAR](signer/mar/README.md)\n* [RSA](signer/genericrsa/README.md)\n* [XPI Signing protocol](signer/xpi/README.md)\n\n## Signing\n\nAutograph exposes a REST API that services can query to request signature of\ntheir data. Autograph knows which key should be used to sign the data of a\nservice based on the service's authentication token. Access control and rate\nlimiting are performed at that layer as well.\n\n![signing.png](docs/statics/Autograph%20signing.png)\n"
},
{
  "name": "antenna",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".github",
      ".gitignore",
      ".readthedocs.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "antenna",
      "bin",
      "docker-compose.yml",
      "docker",
      "docs",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "systemtest",
      "testlib",
      "tests"
    ],
    "/docs": [
      "Makefile",
      "_templates",
      "conf.py",
      "configuration.rst",
      "contributing.rst",
      "deploy.rst",
      "dev.rst",
      "index.rst",
      "overview.rst",
      "spec_v1.rst"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at https://mozilla.org/MPL/2.0/.\n\n# Include my.env and export it so variables set in there are available\n# in Makefile.\ninclude my.env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\nANTENNA_UID ?= 10001\nANTENNA_GID ?= 10001\n\n# Set this in the environment to force --no-cache docker builds.\nDOCKER_BUILD_OPTS :=\nifeq (1, ${NOCACHE})\nDOCKER_BUILD_OPTS := --no-cache\nendif\n\nDC := $(shell which docker-compose)\n\n.DEFAULT_GOAL := help\n.PHONY: help\nhelp:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@grep -E '^[a-zA-Z0-9_-]+:.*?## .*$$' Makefile \\\n\t\t| grep -v grep \\\n\t    | sed -n 's/^\\(.*\\): \\(.*\\)##\\(.*\\)/\\1\\3/p' \\\n\t    | column -t  -s '|'\n\t@echo \"\"\n\t@echo \"Adjust your my.env file to set configuration.\"\n\t@echo \"\"\n\t@echo \"See https://antenna.readthedocs.io/ for more documentation.\"\n\n# Dev configuration steps\n.docker-build:\n\tmake build\n\nmy.env:\n\t@if [ ! -f my.env ]; \\\n\tthen \\\n\techo \"Copying my.env.dist to my.env...\"; \\\n\tcp docker/config/my.env.dist my.env; \\\n\tfi\n\n.PHONY: build\nbuild: my.env  ## | Build docker images.\n\t${DC} build ${DOCKER_BUILD_OPTS} --build-arg userid=${ANTENNA_UID} --build-arg groupid=${ANTENNA_GID} deploy-base\n\t${DC} build fakesentry\n\ttouch .docker-build\n\n.PHONY: setup\nsetup: my.env .docker-build  ## | Set up services.\n\t${DC} run --rm web shell ./bin/run_setup.sh\n\n.PHONY: run\nrun: my.env .docker-build  ## | Run the webapp and services.\n\t${DC} up web fakesentry\n\n.PHONY: shell\nshell: my.env .docker-build  ## | Open a shell in the web image.\n\t${DC} run --rm web shell\n\n.PHONY: my.env clean\nclean:  ## | Remove build, test, coverage, and Python artifacts.\n\t# python related things\n\t-rm -rf build/\n\t-rm -rf dist/\n\t-rm -rf .eggs/\n\tfind . -name '*.egg-info' -exec rm -rf {} +\n\tfind . -name '*.egg' -exec rm -f {} +\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -rf {} +\n\n\t# test related things\n\t-rm -f .coverage\n\n\t# docs files\n\t-rm -rf docs/_build/\n\n\t# state files\n\t-rm .docker-build\n\n.PHONY: lint\nlint: my.env .docker-build  ## | Lint code.\n\t${DC} run --rm --no-deps base shell ./bin/run_lint.sh\n\n.PHONY: lintfix\nlintfix: my.env .docker-build  ## | Reformat code.\n\t${DC} run --rm --no-deps base shell ./bin/run_lint.sh --fix\n\n.PHONY: test\ntest: my.env .docker-build  ## | Run unit tests.\n\t# Make sure services are started up\n\t${DC} up --detach localstack\n\t${DC} up --detach statsd\n\t${DC} up --detach fakesentry\n\t# Run tests\n\t${DC} run --rm test shell ./bin/run_tests.sh\n\n.PHONY: testshell\ntestshell: my.env .docker-build  ## | Open a shell in the test container.\n\t${DC} run --rm test shell\n\n.PHONY: test-coverage\ntest-coverage: my.env .docker-build  ## | Run test coverage report.\n\t${DC} run --rm test shell ./bin/run_tests.sh --cov=antenna --cov-report term-missing\n\n.PHONY: docs\ndocs: my.env .docker-build  ## | Generate Sphinx HTML documentation.\n\t${DC} run -u ${ANTENNA_UID} base shell ./bin/build_docs.sh\n\n.PHONY: rebuildreqs\nrebuildreqs: my.env .docker-build  ## | Rebuild requirements.txt file after requirements.in changes.\n\t${DC} run --rm --no-deps base shell pip-compile --generate-hashes\n\n.PHONY: updatereqs\nupdatereqs: my.env .docker-build  ## | Update deps in requirements.txt file.\n\t${DC} run --rm --no-deps base shell pip-compile --generate-hashes -U\n",
  "readme": "===================================================\nAntenna: Mozilla crash ingestion pipeline collector\n===================================================\n\nAntenna is the collector for the `Socorro crash ingestion pipeline\n<https://socorro.readthedocs.io/>`_ that supports crash reports with minidumps\ngenerated by `breakpad <https://chromium.googlesource.com/breakpad/breakpad>`_.\n\nUses Python 3, `Gunicorn <https://gunicorn.org/>`_, `gevent\n<https://www.gevent.org/>`_, `Falcon <https://falconframework.org/>`_ and some\nother things.\n\n* Free software: Mozilla Public License version 2.0\n* Code: https://github.com/mozilla-services/antenna/\n* Documentation: https://antenna.readthedocs.io/\n* Bugs: `Report a bug <https://bugzilla.mozilla.org/enter_bug.cgi?format=__standard__&product=Socorro&component=Antenna>`_\n* Community Participation Guidelines: `Guidelines <https://github.com/mozilla-services/antenna/blob/main/CODE_OF_CONDUCT.md>`_\n\n\nCode of Conduct\n===============\n\nThis project and repository is governed by Mozilla's code of conduct and\netiquette guidelines. For more details please see the `CODE_OF_CONDUCT.md file\n<https://github.com/mozilla-services/antenna/blob/main/CODE_OF_CONDUCT.md>`_.\n\n\nDocumentation\n=============\n\nDocumentation for setting up Antenna, configuration, specifications,\ndevelopment, and other related things are at\n`<https://antenna.readthedocs.io/>`_.\n\n\nReleases\n========\n\nWe use continuous development and we release often. Each release corresponds with\na git tag.\n\nhttps://github.com/mozilla-services/antenna/tags\n\n\nCommunication\n=============\n\nWe all hang out in `#crashreporting matrix channel\n<https://chat.mozilla.org/#/room/#crashreporting:mozilla.org>`_.\n"
},
{
  "name": "iprepd",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "auth.go",
      "auth_test.go",
      "client.go",
      "client_test.go",
      "cmd",
      "compose",
      "docker_push.sh",
      "exception.go",
      "go.mod",
      "go.sum",
      "http.go",
      "http_test.go",
      "iprepd.go",
      "iprepd.yaml.sample",
      "iprepd_test.go",
      "redis.go",
      "score.go",
      "statsd.go",
      "testdata",
      "validators.go",
      "validators_test.go",
      "write_version_json.sh"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "build:\n\tdocker-compose -f compose/docker-compose.base.yml build\n\nrun:\n\tCONFIG_VOLUME=$(shell pwd) docker-compose -f compose/docker-compose.base.yml -f compose/docker-compose.run.yml up\n\ntest:\n\tdocker-compose -f compose/docker-compose.base.yml -f compose/docker-compose.test.yml run --rm iprepd\n\tdocker-compose -f compose/docker-compose.base.yml -f compose/docker-compose.test.yml down\n\n\n.PHONY: build run test\n",
  "readme": "# iprepd\n\niprepd is a centralized reputation daemon that can be used to store reputation information\nfor various objects such as IP addresses and retrieve reputation scores for the objects.\n\nThe project initially focused on managing reputation information for only IP addresses, but\nhas since been expanded to allow reputation tracking for other types of values such as account\nnames or email addresses.\n\nThe daemon provides an HTTP API for requests, and uses a Redis server as the backend storage\nmechanism. Multiple instances of the daemon can be deployed using the same Redis backend.\n\n__Note__: Support for legacy endpoints, such as `GET /127.0.0.1` has been discontinued. Typed\nendpoints must now be used. For more information see the API documentation below. Some minor\nmodification to client code is required to convert from the old endpoints.\n\n## Configuration\n\nConfiguration is done through the configuration file, by default `./iprepd.yaml`. The location\ncan be overridden with the `-c` flag.\n\nSee [iprepd.yaml.sample](./iprepd.yaml.sample) for an example configuration.\n\n## Building the Docker image\n\n```bash\n./write_version_json.sh\ndocker build -t iprepd:latest .\n```\n\nDocker images are also [published](https://hub.docker.com/r/mozilla/iprepd/).\n\n```bash\ndocker pull mozilla/iprepd:latest\ndocker run -ti --rm -v `pwd`/iprepd.yaml:/app/iprepd.yaml mozilla/iprepd:latest\n```\n\n## API\n\n### Authentication\n\niprepd supports two forms of authentication. Clients can authenticate to the service using either\nstandard API keys, or by using [Hawk authentication](https://github.com/hapijs/hawk).\n\nStandard API key authentication can be configured in the configuration file in the `apikey` (for\nread/write) and `ROapikey` (for a read-only user) sections under the `auth` section in the\nconfiguration file. To use API key authentication, clients should send an `Authorization` header\nthat is of format `APIKey <apikey>`.\n\nSimilarly, Hawk authentication is configured under the `hawk` and `ROhawk` sections in the\nconfiguration file. To use Hawk authentication clients need to include the hawk authentication\nheader in the `Authorization` header when making a request.\n\n### Endpoints\n\n#### GET /type/ip/10.0.0.1\n\nRequest the reputation for an object of a given type. Responds with 200 and a JSON\ndocument describing the reputation if found. Responds with a 404 if the object is\nunknown to iprepd, or is in the exceptions list.\n\nThe current supported object types are `ip` for an IP address and `email` for an\nemail address.\n\nThe response body may include a `decayafter` element if the reputation for the address was changed\nwith a recovery suppression applied. If the timestamp is present, it indicates the time after which\nthe reputation for the address will begin to recover.\n\n##### Response body\n\n```json\n{\n\t\"object\": \"10.0.0.1\",\n        \"type\": \"ip\",\n\t\"reputation\": 75,\n\t\"reviewed\": false,\n\t\"lastupdated\": \"2018-04-23T18:25:43.511Z\"\n}\n```\n\n#### DELETE /type/ip/10.0.0.1\n\nDeletes the reputation entry for the requested object of the specified type.\n\n#### PUT /type/ip/10.0.0.1\n\nSets a reputation score for the specified object of the specified type. A reputation JSON\ndocument must be provided with the request body. The `reputation` field must be provided\nin the document. The reviewed field can be included and set to true to toggle the reviewed\nfield for a given reputation entry.\n\nNote that if the reputation decays back to 100, if the reviewed field is set on the entry it will\ntoggle back to false.\n\nThe reputation will begin to decay back to 100 immediately for the address based on the decay\nsettings in the configuration file. If it is desired that the reputation should not decay for a\nperiod of time, the `decayafter` field can be set with a timestamp to indicate when the reputation\ndecay logic should begin to be applied for the entry.\n\n##### Request body\n\n```json\n{\n\t\"object\": \"10.0.0.1\",\n        \"type\": \"ip\",\n\t\"reputation\": 75\n}\n```\n\n#### PUT /violations/type/ip/10.0.0.1\n\nApplies a violation penalty to the specified object of the specified type.\n\nIf an unknown violation penalty is submitted, this endpoint will still return 200, but the\nerror will be logged.\n\nIf desired, `suppress_recovery` can be included in the request body and set to an integer which\nindicates the number of seconds that must elapse before the reputation for this entry will begin\nto decay back to 100. If this setting is not included, the reputation will begin to decay\nimmediately. If the violation is being applied to an existing entry, the `suppress_recovery` field\nwill only be applied if the existing entry has no current recovery suppression, or the specified\nrecovery suppression time frame would result in a time in the future beyond which the entry\ncurrently has. If `suppress_recovery` is included it must be less than `1209600` (14 days).\n\n##### Request body\n\n```json\n{\n\t\"object\": \"10.0.0.1\",\n        \"type\": \"ip\",\n\t\"violation\": \"violation1\"\n}\n```\n\n#### PUT /violations/type/ip\n\nApplies a violation penalty to a multiple objects of a given type.\n\nIf an unknown violation penalty is submitted, this endpoint will still return 200, but the\nerror will be logged.\n\n##### Request body\n\n```json\n[\n\t{\"object\": \"10.0.0.1\", \"type\": \"ip\", \"violation\": \"violation1\"},\n\t{\"object\": \"10.0.0.2\", \"type\": \"ip\", \"violation\": \"violation1\"},\n\t{\"object\": \"10.0.0.3\", \"type\": \"ip\", \"violation\": \"violation2\"}\n]\n```\n\n#### GET /violations\n\nReturns violations configured in iprepd in a JSON document.\n\n##### Response body\n\n```json\n[\n\t{\"name\": \"violation1\", \"penalty\": 5, \"decreaselimit\": 50},\n\t{\"name\": \"violation2\", \"penalty\": 25, \"decreaselimit\": 0},\n]\n```\n\n#### GET /dump\n\nReturns all reputation entries.\n\n**Note: This makes use of the [KEYS](https://redis.io/commands/keys) redis command, which is known to be very slow. Use with care.**\n\n##### Response body\n\n```json\n[\n  {\"ip\": \"10.0.0.1\", \"reputation\": 75, \"reviewed\": false, \"lastupdated\": \"2018-04-23T18:25:43.511Z\"},\n  {\"ip\": \"10.0.0.2\", \"reputation\": 50, \"reviewed\": false, \"lastupdated\": \"2018-04-23T18:31:27.457Z\"},\n  {\"ip\": \"10.0.20.2\", \"reputation\": 25, \"reviewed\": false, \"lastupdated\": \"2018-04-23T17:22:42.230Z\"},\n]\n```\n\n\n#### GET /\\_\\_heartbeat\\_\\_\n\nService heartbeat endpoint.\n\n#### GET /\\_\\_lbheartbeat\\_\\_\n\nService heartbeat endpoint.\n\n#### GET /\\_\\_version\\_\\_\n\nReturn version data.\n\n## Acknowledgements\n\nThe API design and overall concept for this project are based on work done in\n[Tigerblood](https://github.com/mozilla-services/tigerblood).\n"
},
{
  "name": "google-cloud-rust",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "CONTRIBUTING.md",
      "Cargo.toml",
      "LICENSE.md",
      "README.md",
      "docker",
      "googleapis-raw",
      "googleapis",
      "tools"
    ]
  },
  "makefile": null,
  "readme": "# Google Cloud Rust Client\n\nThis repository contains [`Rust`](https://www.rust-lang.org/) client libraries to interact with various [`Google Cloud Platform`](https://cloud.google.com/) services.\n\n### Currently in production:\n\n[`Cloud Spanner`](https://cloud.google.com/spanner) (Horizontally scalable relational database)\n\n\n### Auto-generated:\n\n[`Cloud BigTable`](https://cloud.google.com/bigtable) (Petabyte-scale, low-latency, NoSQL non-relational database)\n\n[`Cloud Pub/Sub`](https://cloud.google.com/pubsub) (Messaging and ingestion for event-driven systems and streaming analytics)\n\n[`Cloud Storage`](https://cloud.google.com/storage) (Multi-class, multi-region, RESTful object storeage)\n\n\n**NOTE: These generated clients are under development and should be considered\nexperimental!**\n\n\n## Usage\n\n## Rust bindings for Google APIs\n\nSee [`googleapis-raw`](googleapis-raw/examples) for raw bindings based on\n[`grpcio`](https://github.com/pingcap/grpc-rs).\n\nSee [`googleapis`](googleapis/examples) for high-level bindings (not ready for use yet).\n\n## Contributing\n\nContributions to this library are always welcome and highly encouraged.\n\nSee [CONTRIBUTING](CONTRIBUTING.md) for more information on how to get started.\n\nPlease note that this project is released with a Contributor Code of Conduct. By participating in this project you agree to abide by its terms.\n\n## License\n\nApache 2.0 - See [LICENSE](LICENSE.md) for more information.\n\n## Disclaimer\n\nThis is not an officially supported Google product, but was initially created via an agreement between [`Mozilla`](https://www.mozilla.org/), [`Google Cloud`](https://cloud.google.com/), [`Ferrous Systems`](https://ferrous-systems.com/), and [`IGNW`](https://www.ignw.io/).\n\nThank you\n"
},
{
  "name": "tokenserver",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "alembic.ini",
      "bin",
      "dev-requirements.txt",
      "docker-entrypoint.sh",
      "docs",
      "etc",
      "loadtest",
      "requirements.txt",
      "setup.py",
      "tokenserver"
    ],
    "/docs": [
      "RELEASE.md",
      "configuration.rst"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "VIRTUALENV = virtualenv\nVENV := $(shell echo $${VIRTUAL_ENV-local})\nPTYPE = pypy\nPYTHON = $(VENV)/bin/python\nNOSE = $(VENV)/bin/nosetests\nDEV_STAMP = $(VENV)/.dev_env_installed.stamp\nINSTALL_STAMP = $(VENV)/.install.stamp\nTEMPDIR := $(shell mktemp -d)\n\n# Hackety-hack around OSX system python bustage.\n# The need for this should go away with a future osx/xcode update.\nARCHFLAGS = -Wno-error=unused-command-line-argument-hard-error-in-future\nCFLAGS = -Wno-error=write-strings\n\nINSTALL = ARCHFLAGS=$(ARCHFLAGS) CFLAGS=$(CFLAGS) $(VENV)/bin/pip install\n\n.IGNORE: clean distclean maintainer-clean\n.PHONY: all install install-dev virtualenv tests\n\nhelp:\n\t@echo \"Since Python 2.7 is no longer supported, but is used by this project,\"\n\t@echo \"please be sure to install pypy 2.7 <http://pypy.org/download.html> and\"\n\t@echo \"the system appropriate `pypy-dev` package\"\n\t@echo \"\"\n\t@echo \"Please use 'make <target>' where <target> is one of\"\n\t@echo \"  install                     install dependencies and prepare environment\"\n\t@echo \"  install-dev                 install dependencies and everything needed to run tests\"\n\t@echo \"  build-requirements          install all requirements and freeze them in requirements.txt\"\n\t@echo \"  flake8                      run the flake8 linter\"\n\t@echo \"  tests                       run all the tests with all the supported python interpreters (same as travis)\"\n\t@echo \"  version-file                update the version.json file\"\n\t@echo \"  clean                       remove *.pyc files and __pycache__ directory\"\n\t@echo \"  distclean                   remove *.egg-info files and *.egg, build and dist directories\"\n\t@echo \"  maintainer-clean            remove the .tox and the .venv directories\"\n\t@echo \"Check the Makefile to know exactly what each target is doing.\"\n\nall: install\ninstall: $(INSTALL_STAMP)\n$(INSTALL_STAMP): $(PYTHON) setup.py\n\t$(INSTALL) -Ue .\n\ttouch $(INSTALL_STAMP)\n\ninstall-dev: $(INSTALL_STAMP) $(DEV_STAMP)\n$(DEV_STAMP): $(PYTHON) dev-requirements.txt\n\t$(INSTALL) -Ur dev-requirements.txt\n\ttouch $(DEV_STAMP)\n\nvirtualenv: $(PYTHON)\n$(PYTHON):\n\t# The latest `pip` doesn't work with pypy 2.7 on some platforms.\n\t# Pin to a working version; ref https://github.com/pypa/pip/issues/8653\n\t$(VIRTUALENV) -p $(PTYPE) --no-pip $(VENV)\n\t$(VENV)/bin/easy_install pip==20.1.1\n\nbuild-requirements:\n\t$(VIRTUALENV) -p $(PTYPE) --no-pip $(TEMPDIR)\n\t$(TEMPDIR)/bin/easy_install pip==20.1.1\n\tARCHFLAGS=$(ARCHFLAGS) $(TEMPDIR)/bin/pip install -Ue .\n\t$(TEMPDIR)/bin/pip freeze | grep -v -- '^-e' > requirements.txt\n\ntests: install-dev\n\t# By default nose will skip tests in executable files, but that's annoying\n\t# when working in WSL with a checkout mounted from the native filesystem.\n\t$(VENV)/bin/nosetests --exe tokenserver/tests\n\nflake8: install-dev\n\t$(VENV)/bin/flake8 tokenserver\n\nclean:\n\tfind . -name '*.pyc' -delete\n\tfind . -name '__pycache__' -type d | xargs rm -fr\n\trm -fr docs/_build/\n\ndistclean: clean\n\trm -fr *.egg *.egg-info/ dist/ build/\n\nmaintainer-clean: distclean\n\trm -fr local/ .tox/\n\nNAME := tokenserver\nSOURCE := $(shell git config remote.origin.url | sed -e 's|git@|https://|g' | sed -e 's|github.com:|github.com/|g')\nVERSION := $(shell git describe --always --tag)\nCOMMIT := $(shell git log --pretty=format:'%H' -n 1)\nversion-file:\n\techo '{\"name\":\"$(NAME)\",\"version\":\"$(VERSION)\",\"source\":\"$(SOURCE)\",\"commit\":\"$(COMMIT)\"}' > version.json\n",
  "readme": "[![Build Status](https://travis-ci.org/mozilla-services/tokenserver.png?branch=master)](https://travis-ci.org/mozilla-services/tokenserver)\n[![Docker Build Status](https://circleci.com/gh/mozilla-services/tokenserver/tree/master.svg?style=shield&circle-token=0fdb6d8d80e18f180132ea25cf9f75a38828591a)](https://circleci.com/gh/mozilla-services/tokenserver)\n\n# Firefox Sync TokenServer\n\nThis service is responsible for allocating Firefox Sync users to one of several Sync Storage nodes.\nIt provides the \"glue\" between [Firefox Accounts](https://github.com/mozilla/fxa/) and the\n[SyncStorage](https://github.com/mozilla-services/server-syncstorage) API, and handles:\n\n* Checking the user's credentials as provided by FxA\n* Sharding users across storage nodes in a way that evenly distributes server load\n* Re-assigning the user to a new storage node if their FxA encryption key changes\n* Cleaning up old data from e.g. deleted accounts\n\nThe service was originallly conceived to be a general-purpose mechanism for connecting users\nto multiple different Mozilla-run services, and you can see some of the historical context\nfor that original design [here](https://wiki.mozilla.org/Services/Sagrada/TokenServer)\nand [here](https://mozilla-services.readthedocs.io/en/latest/token/index.html).\n\nIn practice today, it is only used for connecting to Firefox Sync.\n\n## How to run the server\n\nLike this:\n\n    $ make install\n    $ ./local/bin/pip install gunicorn\n    $ ./local/bin/gunicorn --paste etc/tokenserver-dev.ini\n\n## API\n\nFirfox Sync clients must first obtain user credentials from FxA, which can be either:\n\n* A BrowserID assertion with audience of `https://token.services.mozilla.com/`\n* An OAuth access token bearing the scope `https://identity.mozilla.com/apps/oldsync`\n\nThey then provide this in the `Authorization` header of a `GET` request to the Tokenserver,\nwhich will respond with the URL of the user's sync storage node, and some short-lived credentials\nthat can be used to access it.\n\nMore detailed API documentation is available [here](https://mozilla-services.readthedocs.io/en/latest/token/apis.html).\n\n### Using BrowserID\n\nTo access the user's sync data using BrowserID, the client must obtain a BrowserID assertion\nwith audience matching the tokenserver's public URL, as well as the user's Sync encryption key.\nThey send the BrowserID assertion in the `Authorization` header, and the first half of the\nhex-encoded SHA256 digest of the encryption key in the `X-Client-State` header, like so:\n\n```\nGET /1.0/sync/1.5\nHost: token.services.mozilla.com\nAuthorization: BrowserID <assertion>\nX-Client-State: <hex(sha256(kSync))[:32]>\n```\n\n### Using OAuth\n\nTo access the user's sync data using OAuth, the client must obtain an FxA OAuth access_token\nwith scope `https://identity.mozilla.com/apps/oldsync`, and the corresponding encryption key\nas a JWK. They send the OAuth token in the `Authorization` header, and the `kid` field of the\nencryption key in the `X-KeyID` header, like so:\n\n```\nGET /1.0/sync/1.5\nHost: token.services.mozilla.com\nAuthorization: Bearer <access_token>\nX-KeyID: <JWK['kid']>\n```\n\n### Response\n\nThe tokenserver will validate the provided credentials, and either look up the user's existing\nstorage node allocation or assign them to a new one.  It responds with the location of the\nstorage node and a set of short-lived credentials that can be used to access it:\n\n```\n{\n 'id': <token>,\n 'key': <request-signing secret>,\n 'api_endpoint': 'https://db42.sync.services.mozilla.com/1.5/12345',\n 'uid': 12345,\n 'duration': 300,\n}\n```\n\n### Storage Token\n\nThe value of `<token>` is intended to be opaque to the client, but is in fact an encoded JSON blob\nsigned using a secret key shared between the tokenserver and the storage nodes.  This allows\nthe tokenserver to securely communicate information about the user to their storage node.\nThe fields contained therein include:\n\n* `uid`: A numeric userid that uniquely identifies this user, on this storage node, using this encryption key\n* `node`: The intended storage node on which these credentials can be used\n* `expires`: A timestamp for when the credentials expire\n* `fxa_uid`: The user's stable FxA user id, as a hex string\n* `fxa_kid`: The key-id of the JWK representing the user's sync encryption key\n\n## Specifying the Data Store\n\nThe data store is specified in the *[tokenserver]* section as `sqluri`. If you want to use a longer\nlived data store than `sqlite3`, you will need to specify the Data Source Name (DSN) as a Universal\nResource Locator (URL) such as: `sqluri = mysql+pymysql://scott:tiger@localhost/tokenserver`. See the\n[SQLAlchemy Engine](https://docs.sqlalchemy.org/en/13/core/engines.html) specification for details.\n\nTokenServer comes with support for `pymysql`, `mysqldb`, and `sqlite3` by default. Additional databases\nwill require `pip install`.\n\n## Data Model\n\nThe core of the TokenServer's data model is a table named `users` that maps each user to their storage\nnode, and that provides enough information to update that mapping over time.  Each row in the table\ncontains the following fields:\n\n* `uid`: Auto-incrementing numeric userid, created automatically for each row.\n* `service`: The service the user is accessing; in practice this is always `sync-1.5`.\n* `email`: Stable identifier for the user; in practice this is always `<fxa_uid>@api.accounts.firefox.com`.\n* `nodeid`: The storage node to which the user has been assigned.\n* `generation`: A monotonically increasing number provided by the FxA server, indicating\n                the last time at which the user's login credentials were changed.\n* `client_state`: The hash of the user's sync encryption key.\n* `keys_changed_at`: A monotonically increasing timestamp provided by the FxA server, indicating\n                     the last time at which the user's encryption keys were changed.\n* `created_at`: Timestamp at which this node-assignment record was created.\n* `replaced_at`: Timestamp at which this node-assignment record was replaced by a newer assignment, if any.\n\nAs you can see, this table contains some unnecessarily general names; these are a legacy of earlier plans\nto re-use Tokenserver for multiple Mozilla services and with multiple identity providers.\n\nThe `generation` column is used to detect when the user's FxA credentials have been changed\nand to lock out clients that have not been updated with the latest credentials.\nTokenserver tracks the highest value of `generation` that it has ever seen for a user,\nand rejects BrowserID assertions in which the `generation` number is less than that high-water mark.\nNote that OAuth clients do not provide a `generation` number, because OAuth tokens get\nrevoked immediately when the user's credentials are changed.\n\nThe `client_state` column is used to detect when the user's encryption key changes.\nWhen it sees a new value for `client_state`, Tokenserver will replace the user's node assignment\nwith a new one, so that data encrypted with the new key will be written into a different\nstorage \"bucket\" on the storage nodes.\n\nThe `keys_changed_at` column tracks the timestamp at which the user's encryption keys were\nlast changed. BrowserID clients provide this as a field in the assertion, while OAuth clients\nprovide it as part of the `X-KeyID` header. Tokenserver will check that changes in the value\nof `keys_changed_at` always correspond to a change in `client_state`, and will use this pair of\nvalues to construct the `fxa_kid` field that is communicated to the storage nodes.\n\nWhen replacing a user's node assignment, the previous column is not deleted immediately.\nInstead, it is marked as \"replaced\" by setting the `replaced_at` timestamp, and then a background\njob periodically purges replaced rows (including making a `DELETE` request to the storage node\nto clean up any old data stored under that `uid`).\n\nFor this scheme to work as intended, it's expected that storage nodes will index user data by either:\n\n1. The tuple `(fxa_uid, fxa_kid)`, which identifies a consistent set of sync data for a particular\n   user, encrypted using a particular key.\n2. The numeric `uid`, which changes whenever either of the above two values change.\n"
},
{
  "name": "autopush-rs",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".clog.toml",
      ".dockerignore",
      ".gitguardian.yml",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "PR_TEMPLATE.md",
      "README.md",
      "autoendpoint",
      "autopush-common",
      "autopush",
      "configs",
      "entrypoint.sh",
      "scripts",
      "tests",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "SHELL := /bin/sh\nCARGO = cargo\n\n.PHONY: ddb\n\nddb:\n\tmkdir $@\n\tcurl -sSL http://dynamodb-local.s3-website-us-west-2.amazonaws.com/dynamodb_local_latest.tar.gz | tar xzvC $@\n\nupgrade:\n\t$(CARGO) install cargo-edit ||\n\t\techo \"\\n$(CARGO) install cargo-edit failed, continuing..\"\n\t$(CARGO) upgrade\n\t$(CARGO) update\n",
  "readme": "[![Build](https://travis-ci.org/mozilla-services/autopush-rs.svg?branch=master)](https://travis-ci.org/mozilla-services/autopush-rs)\n[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0)\n[![Connect to Matrix via the Riot webapp][matrix-badge]][matrix]\n\n# Autopush-rs\n\nMozilla Push server built with [Rust](https://rust-lang.org).\n\nThis is the fourth generation of the Mozilla Web Push server. It currently supports websocket connections\nand support for\n[Megaphone](https://github.com/mozilla-services/megaphone) broadcast.\n\nPlease consult the [autopush\ndocumentation](http://autopush.readthedocs.io/en/latest/index.html)\nfor information about how this server works, as well as any [error\nmessages](http://autopush.readthedocs.io/en/latest/http.html#error-codes)\nyou may see when sending push messages to our server.\n\nMDN has information about [how to use\nWebPush](https://developer.mozilla.org/en-US/docs/Web/API/Push_API)\n\n***Note*** while `rust-doc` style comments are used prolifically\nthrough the source, only public structures are rendered automatically.\nFor those curious about the inner workings, You may wish to read the\ncode files directly.\n\n[matrix-badge]: https://img.shields.io/badge/chat%20on%20[m]-%23push%3Amozilla.org-blue\n[matrix]: https://chat.mozilla.org/#/room/#push:mozilla.org\n"
},
{
  "name": "autopush",
  "files": {
    "/": [
      ".circleci",
      ".clog.toml",
      ".coveragerc",
      ".dockerignore",
      ".editorconfig",
      ".gitignore",
      ".noserc",
      ".pyup.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "Dockerfile.python27",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "autokey.py",
      "autopush",
      "configs",
      "default.nix",
      "doc-requirements.txt",
      "docker-compose.yml",
      "docs",
      "entrypoint.sh",
      "hekad.toml",
      "keys",
      "maintenance.py",
      "mypy.ini",
      "parquet_schemas",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "states.dot",
      "test-requirements.txt",
      "tox.ini",
      "version.json"
    ],
    "/docs": [
      "Makefile",
      "adm.rst",
      "api.rst",
      "api",
      "apns.rst",
      "architecture.rst",
      "conf.py",
      "glossary.rst",
      "http.rst",
      "index.rst",
      "install.rst",
      "releasing.rst",
      "running.rst",
      "rust.rst",
      "style.rst",
      "testing.rst"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "SHELL := /bin/sh\nAPPNAME = autopush\nDEPS =\nHERE = $(shell pwd)\nPTYPE=pypy\nREQS=requirements.txt\nBIN = $(HERE)/$(PTYPE)/bin\nVIRTUALENV = virtualenv\nTESTS = $(APPNAME)/tests\nPYTHON = $(BIN)/$(PTYPE)\nINSTALL = $(BIN)/pip install\nPATH := $(BIN):$(PATH)\n\nBUILD_DIRS = bin build deps include lib lib64 lib_pypy lib-python\\\n\tsrc site-packages .tox .eggs .coverage\n\n\n.PHONY: all build test coverage lint clean clean-env\n\nall:\tbuild\n\nddb:\n\tmkdir $@\n\tcurl -sSL http://dynamodb-local.s3-website-us-west-2.amazonaws.com/dynamodb_local_latest.tar.gz | tar xzvC $@\n\n$(BIN)/pip: $(PYTHON)\n\tcurl -O https://bootstrap.pypa.io/get-pip.py\n\t$(PYTHON) get-pip.py\n\trm get-pip.py\n\n$(BIN)/tox: $(BIN)/pip\n\t$(INSTALL) tox\n\n$(BIN)/flake8: $(BIN)/pip\n\t$(INSTALL) flake8\n\n$(BIN)/paster: lib $(BIN)/pip\n\t$(INSTALL) -r $(REQS)\n\t$(PYTHON) setup.py develop\n\n$(PYTHON):\n\t$(VIRTUALENV) $(PTYPE) -p $(PTYPE)\n\nclean-env:\n\trm -rf *.egg-info\n\trm -rf $(BUILD_DIRS)\n\nclean:\tclean-env\n\nbuild: $(BIN)/pip\n\t$(INSTALL) -r $(REQS)\n\t$(PYTHON) setup.py develop\n\nrequirements: $(BIN)/pip\n\t$(INSTALL) pip-tools\n\tpip-compile --no-header --upgrade -o $(REQS) requirements.in > /dev/null\n\ntest: $(BIN)/tox ddb\n\t$(BIN)/tox\n\ncoverage: $(BIN)/tox\n\t$(BIN)/tox -- --with-coverage --cover-package=autopush\n\nlint: $(BIN)/flake8\n\t$(BIN)/flake8 autopush\n",
  "readme": "[![codecov.io][codecov-svg]][codecov]\n[![License: MPL 2.0][mpl-svg]][mpl]\n[![Test Status][travis-badge]][travis]\n[![Build Status][circleci-badge]][circleci]\n[![docs][docs-badge]][docs]\n[![Connect to Matrix via the Riot webapp][matrix-badge]][matrix]\n\n# Autopush\n\nMozilla Push server and Push Endpoint utilizing built with:\n\n- Python\n- twisted\n- DynamoDB\n\n[codecov-svg]: https://img.shields.io/codecov/c/github/mozilla-services/autopush/master.svg\n[codecov]: https://codecov.io/github/mozilla-services/autopush?branch=master\n[mpl-svg]: https://img.shields.io/badge/License-MPL%202.0-blue.svg\n[mpl]: https://opensource.org/licenses/MPL-2.0\n[travis-badge]: https://travis-ci.org/mozilla-services/autopush.svg?branch=master\n[travis]: https://travis-ci.org/mozilla-services/autopush\n[circleci-badge]: https://circleci.com/gh/mozilla-services/autopush.svg?style=shield&circle-token=074ae89011d1a7601378c41a4351e1e03f1e8177\n[circleci]: https://circleci.com/gh/mozilla-services/autopush\n[docs-badge]: https://readthedocs.org/projects/docs/badge/?version=latest\n[docs]: https://autopush.readthedocs.io/\n[matrix-badge]: https://img.shields.io/badge/chat%20on%20[m]-%23push%3Amozilla.org-blue\n[matrix]: https://chat.mozilla.org/#/room/#push:mozilla.org\n"
},
{
  "name": "guardian-vpn-windows-deprecated",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".gitmodules",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "docs",
      "installer",
      "taskcluster",
      "test",
      "tunnel",
      "ui"
    ],
    "/docs": [
      "releasenotes"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla VPN for Windows\n[![CircleCI](https://circleci.com/gh/mozilla-services/guardian-vpn-windows.svg?style=svg&circle-token=d0d916754d2f18a3ec876dcdf2c79f6b45b334e0)](https://circleci.com/gh/mozilla-services/guardian-vpn-windows)\n\n## Structure\n\n| Folder        | Description           | Language  |\n| ------------- |:-------------|:-----:|\n| `ui`          | VPN client UI and the tunnel service interface | C# |\n| `tunnel`      | Wrapper around [wireguard-windows](https://git.zx2c4.com/wireguard-windows/about/)'s [embeddable-dll-service](https://git.zx2c4.com/wireguard-windows/tree/embeddable-dll-service) for running a [WireGuard](https://www.wireguard.com/) process as a service. | Go |\n| `installer`   | WiX installer scripts, for setting up and creating an MSI installer | XML |\n| `test`        | Integration and end-to-end tests | Go |\n\n## Environment Preparation\n- Nuget CLI: Download Nuget.exe CLI from [nuget.org](https://dist.nuget.org/win-x86-commandline/latest/nuget.exe). [More information from Microsoft Doc](https://docs.microsoft.com/nuget/consume-packages/install-use-packages-nuget-cli)\n- Add MSBuild.exe to System Environment Path. For Visual Studio 2019 user, the file is placed at `C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\MSBuild\\Current\\Bin`\n\n## Building\n\n- First, [build the embeddable-dll-service's tunnel.dll](tunnel/README.md).\n- Next, [build the the UI](ui/README.md).\n- Finally, [build the installer](installer/README.md).\n\nAlternatively, this may all be done at once, like so:\n\n```\nC:\\guardian-vpn> tunnel\\build.cmd\nC:\\guardian-vpn> cd ui\nC:\\guardian-vpn\\ui> nuget.exe restore -SolutionDirectory .\\\nC:\\guardian-vpn\\ui> MSBuild.exe /t:Rebuild /p:Configuration=Release /p:Platform=\"x86\"\nC:\\guardian-vpn\\ui> MSBuild.exe /t:Rebuild /p:Configuration=Release /p:Platform=\"x64\"\nC:\\guardian-vpn\\ui> cd ..\nC:\\guardian-vpn> installer\\build.cmd\nC:\\guardian-vpn> msiexec /i installer\\x64\\MozillaVPN.msi\n```\n## Testing\n\nSee [the instructions](test/README.md) in the `test` folder.\n\n## Code of Conduct\n\nThis repository is governed by Mozilla's [Community Participation Guidelines](CODE_OF_CONDUCT.md)\nand [Developer Etiquette Guidelines][etiquette].\n\n[etiquette]: https://bugzilla.mozilla.org/page.cgi?id=etiquette.html\n"
},
{
  "name": "mozillapulse",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "HACKING.md",
      "README.md",
      "mozillapulse",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "test",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "# MozillaPulse\n\nMozillaPulse is a Python package for interacting with [Mozilla Pulse][].\nIt contains classes for consumers, publishers, and messages.\n\nIn order to use a Mozilla Pulse consumer, you must register with\n[PulseGuardian][] to create a Pulse user.  Here's an example of\ncreating a Buildbot consumer:\n\n    from mozillapulse.consumers import BuildConsumer\n\n    def callback(body, msg):\n        print 'Received message: %s' % body\n\n    c = BuildConsumer(user=<PulseGuardian user>,\n                      password=<PulseGuardian password>,\n                      topic='#',\n                      callback=callback)\n    c.listen()\n\nSee the HACKING.md file for instructions on setting up a local Pulse\ninstance for development.\n\n[Mozilla Pulse]: https://wiki.mozilla.org/Auto-tools/Projects/Pulse\n[PulseGuardian]: https://wiki.mozilla.org/Auto-tools/Projects/Pulse/PulseGuardian\n\n\n## Running tests\n\nIf you want to run tests against this package:\n\n- Install tox: `pip install tox`\n- Run the vagrant instance. In the test repository run: `vagrant up`\n- Run tox: `tox`\n"
},
{
  "name": "pulseguardian",
  "files": {
    "/": [
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "__init__.py",
      "alembic.ini",
      "docker-compose.yml",
      "docker",
      "flask_secure_headers",
      "gen_secret_key.py",
      "migration",
      "pulseguardian",
      "renovate.json",
      "requirements.txt",
      "runtime.txt",
      "setup.py",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# PulseGuardian\n\nA system to manage Pulse: administers RabbitMQ users and handles overgrowing\nqueues. More information on [the wiki][].\n\n[![Build Status](https://travis-ci.org/mozilla-services/pulseguardian.svg?branch=main)](https://travis-ci.org/mozilla-services/pulseguardian)\n\n## Pre-requisites\n\n* RabbitMQ (tested on 3.5.7)\n* Python 3.9\n* pip (to install external dependencies)\n* PostgreSQL (for production; testing environments can use sqlite)\n* docker-compose (to stand up a local Docker-based environment)\n\n## Setup\n\n### Docker Compose\n\nThe easiest way to start a local instance of PulseGuardian is via\n[docker-compose][]:\n\n    $ docker-compose up --build\n\nThis will launch four containers: a RabbitMQ instance, a PostGreSQL database,\nthe PulseGuardian web process, and the PulseGuardian daemon\n(`guardian.py`).  Pressing control-C will stop all the containers.\nYou can also add `-d` to run docker-compose in the background, in\nwhich case you will need to run `docker-compose down` to stop the containers.\n\nKnown issue: a local install of PostGreSQL will likely result in a port\nconflict.\n\nThe PulseGuardian code is mounted as `/code` in the web and daemon\ncontainers.  You can edit the code locally and restart the container(s) to\npick up changes: `docker-compose restart web` and/or `docker-compose\nrestart guardian`.  The RabbitMQ cluster and PulseGuardian database data are\npreserved across restarts as via Docker volumes.\n\nBecause PulseGuardian uses cookies, it is necessary to add an entry to\nyour local hosts file for `pulseguardian-web`, mapping to 127.0.0.1.\nAfter adding this, you can access the web server via\nhttp://pulseguardian-web:5000/.\n\nRabbitMQ is available via localhost:5672 (AMQP) and\nhttp://localhost:15672/ (management interface).  The PostGreSQL\ndatabase is available at localhost:5432.  If you have `psql` installed\nlocally, you can connect to the database via the following:\n\n    psql -h localhost -p 5432 -d postgres -U postgres\n\nYou can change the logged-in user by overriding the `FAKE_ACCOUNT`\nenvironment variable.  One way to do this is by creating a file named\n`docker-compose.override.yml` that contains something like this:\n\n```yml\nversion: \"2\"\nservices:\n  pulseguardian-web:\n    environment:\n      - FAKE_ACCOUNT=fake-override@example.com\n```\n\n### Local\n\nYou can also run the web and/or daemon processes locally.\n\nOn Linux (or the Linux subsystem on Windows 10), you will need some\nsystem packages installed before you can install the prerequisite Python\npackages.  On Ubuntu, these are\n\n* python\n* python-dev\n* libssl-dev\n* libffi-dev\n\nYou will also need PostgreSQL installed in order to install the psycopg2\nPython package.\n\nUsing a virtualenv is highly recommended. One possible installation would be\n\n* Clone the repository and cd into it.\n* Create and activate a virtualenv:\n\n        virtualenv venv\n        source venv/bin/activate\n\nWithin the chosen environment, install and configure PulseGuardian:\n\n* Install the requirements:\n\n        pip install -r requirements.txt\n\n* Install the package.  This will ensure you have access to the `pulseguardian`\n  package from anywhere in your virtualenv.\n\n        python setup.py develop\n\nIf you will be running PulseGuardian with SSL enabled (i.e. over https),\nyou will also need the pyOpenSSL package:\n\n    pip install pyOpenSSL\n\nYou will also need a RabbitMQ instance running somewhere.  Docker provides a\nlightweight and isolated solution.  See the [docker installation docs][] for\nyour system.\n\nTo create a Pulse Docker image, run this from within the root PulseGuardian\nsource directory:\n\n    docker build -t=\"pulse:testing\" test\n\nWhen that finishes building, you can run a RabbitMQ instance in a Docker\ncontainer with\n\n    docker run -d -p 5672:5672 -p 15672:15672 --name pulse pulse:testing\n\nThis will run RabbitMQ in a container in the background.  It will also forward\nthe AMQP and management API ports, 5672 and 15672, respectively, from the\ncontainer to your local host.\n\nTo stop the container, run\n\n    docker stop pulse\n\nYou can remove the container with\n\n    docker rm pulse\n\nAnd you can remove the images with\n\n    docker rmi pulse:testing\n    docker rmi ubuntu:14.04\n\nYou can also use either a local RabbitMQ server or a VM.  See the\nmozillapulse [HACKING.md][] file for instructions on setting up both of these.\n\nFinally, you need your environment set up correctly.  If you are running\nRabbitMQ in the Docker configuration described above, the defaults should\nbe mostly fine.  You will need to set `FLASK_SECRET_KEY` and probably\n`DATABASE_URL`.  To use a random secret key and a local sqlite database named\n`pulseguardian.db`, run the following from within the root PulseGuardian\nsource directory:\n\n```bash\nexport FLASK_SECRET_KEY=`python gen_secret_key.py`\nexport DATABASE_URL=sqlite:///`pwd`/db\n```\n\nSee the complete listing of options in `pulseguardian/config.py`.\n\nTODO: Each of these options should be documented in the source.\n\nInitialize the db with `python pulseguardian/dbinit.py`. *WARNING*:\nThis removes any existing data the app might have previously stored in\nthe database.\n\nSet the environment variable `FAKE_ACCOUNT` to a valid email address.\nThis setting makes development easier by bypassing OIDC\nauthentication, logging the user in automatically with the provided\naddress.  It will also create the given user, if necessary.\n\nYou can also test with a real Auth0 account.  You can create an account at\nhttps://auth0.com and use the provided credentials in the `OIDC_*` config\nvariables.\n\nRun the Pulse Guardian daemon with: `python pulseguardian/guardian.py`\n\nRun the web app (for development) with: `python pulseguardian/web.py`\n\nFor production, the web app can be run with [gunicorn][] and such.\n\n## Testing\n\nTODO: This process should be updated to run the tests with a\ndocker-compose environment.\n\nTests are automatically run against the GitHub repository via [Travis\nCI][]. Before submitting a patch, it is highly recommended that you\nget a Travis CI account and activate it on a GitHub fork of the\npulseguardian repo.\n\nFor local testing, PulseGuardian uses docker to run its test\nsuite. Please follow the [docker installation docs][] on how to\ninstall it in your system.  Note that these tests are not yet hooked\nup to the environment created with `docker-compose` above.\n\nWith docker installed and configured appropriately, run\n\n    python test/runtests.py\n\nThe required docker image will be built and container started before the tests\nare run.\n\nThe docker container forwards ports 5672 and 15672. Please be sure that\nthey are available.\n\nSince PulseGuardian is configured via environment variables, you must ensure\nthat you have a clean environment before running the tests, i.e., no\nPulseGuardian environment variables set. (FIXME: set up a full test environment\nfrom within runtests.py rather than relying on defaults.)\n\nSome Linux-specific notes (TODO: are these still valid/necessary?):\n\n* The docker daemon must always run as the root user, but you need to be able\n  to run docker client commands without `sudo`. To achieve that you can:\n\n * Add the docker group if it doesn't already exist:  `sudo groupadd docker`\n\n * Add the connected user \"${USER}\" to the docker group. Change the user name\nto match your preferred user:  `sudo gpasswd -a ${USER} docker`\n\n * Restart the Docker daemon:  `sudo service docker restart`\n\n * You need to log out and log back in again if you added the currently\n   logged-in user.\n\nIf you prefer, you can run the tests against a local RabbitMQ installation. For\nthat you can run: `python test/runtests.py --use-local`.\n\n**WARNING**: If you use your local RabbitMQ instance the tests will mess with it\n(wiping out existing queues, possibly deleting users) so make sure you don't\nrun the tests on a production instance.\n\n## Database migration\n\nPulseGuardian uses [Alembic][] for database migrations.  SQLite doesn't support\nall the SQL commands required for migrations, so you may want to use PostgreSQL\neven in your development environment, at least if you are testing migrations.\n\nBecause PulseGuardian is designed to run on Heroku, which doesn't support\nlocally modified files, the Alembic configuration file, `alembic.ini`, must be\nused for all installations, including for local development.  The database URL,\n`sqlalchemy.url`, is not used; instead, `migration/env.py` is set to use the\n`DATABASE_URL` environment variable, as the rest of PulseGuardian does.\n\nTo migrate the database,\n\n* Install the alembic package (if you haven't yet): `pip install -r requirements.txt`\n* Run `alembic upgrade head`\n\n## Deployment\n\nThis project is deployed to the Heroku app `pulseguardian`.\nThis is via Git pushes to Heroku, rather than the more common pull-from-GitHub approach.\nTo set this up, run `heroku git:remote -a pulseguardian`.\nThen just push the latest `main` branch to the `heroku` remote: `git push heroku main`\n\n[the wiki]: https://wiki.mozilla.org/Auto-tools/Projects/Pulse/PulseGuardian\n[docker-compose]: https://docs.docker.com/compose/\n[HACKING.md]: https://hg.mozilla.org/automation/mozillapulse/file/tip/HACKING.md\n[Travis CI]: https://travis-ci.org/mozilla/pulseguardian\n[gunicorn]: https://www.digitalocean.com/community/articles/how-to-deploy-python-wsgi-apps-using-gunicorn-http-server-behind-nginx\n[docker installation docs]: https://docs.docker.com/installation/#installation\n[Alembic]: https://alembic.readthedocs.org\n\n"
},
{
  "name": "cjms",
  "files": {
    "/": [
      ".config",
      ".github",
      ".gitignore",
      ".vscode",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "migrations",
      "settings.yaml.example",
      "sqlx-data.json",
      "src",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# cjms\n\n[![Coverage Status](https://codecov.io/gh/mozilla-services/cjms/branch/main/graph/badge.svg?token=JW9B9YTOE0)](https://codecov.io/gh/mozilla-services/cjms)\n[![Security audit](https://github.com/mozilla-services/cjms/actions/workflows/scheduled-audit.yml/badge.svg)](https://github.com/mozilla-services/cjms/actions/workflows/scheduled-audit.yml)\n\nMicro-service supporting VPN activities\n\n## Endpoints\n\n### AIC - Affiliate Identifier Cookie\n\n`/aic`:\n- POST only\n- Accepts: JSON data with `flow_id` (required), `cj_id` (required)\n- Returns: JSON data with  `aic_id`, `expires` (a timestamp)\n- A cookie should then be set with the stated expiration time and the returned `aic_id`\n- Success - 201\n- All errors - 500\n\n`/aic/<aicID>` endpoint:\n- PUT only\n- Accepts: JSON data with `flow_id` (required), `cj_id` (optional)\n- Returns: JSON data with `aic_id`, `expires` (a timestamp)\n- A cookie should then be set with the stated expiration time and the returned `aic_id`\n- Success - 201\n- Unknown aicID - 404\n- All other errors - 500\n\n## Settings\n\nThe required settings are listed in `settings.yaml.example`. There may be other local setting needs  (see \"Auto-magic behavior based on environment\" below).\n\n* aic_expiration_days: How long for an aic cookie to expire\n* authentication: Used for basic_auth on the the corrections detail page\n* cj_api_access_token: For CJ querying\n* cj_cid: For CJ S2S configuration\n* cj_sftp_user: For CJ corrections\n* cj_signature: For CJ S2S configuration\n* cj_subid: For CJ corrections\n* cj_type: For CJ S2S configuration\n* database_url: the database url for connecting to postgres database\n* environment: the environment (see \"Auto-magic behavior based on envrionment\" below)\n* gcp_project: the gcp project where the big query data lives that the check_subscriptions binary pulls from\n* host: the host the web service runs on\n* log_level: The lowest priority log level that is logged to the output sink. Value can be one of `error`, `warn`, `info`, `debug`, or `trace`.\n* port: the port the web service runs on\n* sentry_dsn: The [DSN identifier] for the Sentry instance\n* sentry_environment: The environment passed to Sentry. Should be the same as `environment`.\n* statsd_host: The host of the statsd server.\n* statsd_port: The port of the statsd server.\n\n## Development pre-requisites\n\n### Rust\n\nhttps://www.rust-lang.org/tools/install\n\nWe use clippy and fmt which can be added with `rustup component add clippy rustfmt`.\n\nMany optional utilities are useful when developing cjms and can be installed with `cargo install`. e.g. `cargo install cargo-edit` adds the ability to add dependencies to the project by running `cargo add <name of package>`. Consider the following additions:\n* cargo-edit\n* cargo-audit\n* cargo-tarpaulin\n* cargo-udeps (requires nightly compiler toolchain)\n\nI have found the VSCode extension `rust-analyzer` to give me the richest experience while developing.\n\n### Postgres\n\nhttps://www.postgresql.org/docs/14/index.html\n\n- Have postgres running on your machine.\n- Know your database url `postgres://${DB_USER}:${DB_PASSWORD}@localhost:${DB_PORT}/${DB_NAME}`\n- The user will need permission to create databases (at a minimum, used by integration tests) - `ALTER USER username CREATEDB;`\n- Install [sqlx-cli](https://github.com/launchbadge/sqlx/tree/master/sqlx-cli) `cargo install sqlx-cli`\n- If needed create your database `sqlx database create --database-url=\"<database url>\"`\n\nWhen adding migrations, create reversible migrations using `sqlx migrate add -r <name>`.\n\n## Run server\n\nYou can configure cjms and tests either with environment variables or a settings file.\n\nTo use a settings file, copy `settings.yaml.example` to `settings.yaml` and update with your local settings values.\nYou will also need a `version.yaml` which can be made by running `cargo run --bin make_version_file`.\n\nThen run the server:\n\n`cargo run --bin web`\n\nIf configuring with environment variables, all variables, listed in settings.yaml.example must be available.\n\n### Auto-magic behavior based on environment\n\nValid values for environment are: local | dev | stage | prod.\n\n* If using local, you must have BQ_ACCESS_TOKEN set in your environment when running bins that access big query.\n* CORS changes based on environment (see appconfig)\n\n\n## Run tests\n\n### With nextest\n\nWe have a few flaky tests due to the challenges of mocking time in rust.\n\nNextest makes your tests run faster and does automatic retries.\n\nInstall nextest: https://nexte.st/book/pre-built-binaries.html\n\nRun tests with nextest: `cargo nextest run`\n\n### Without nextest\n\n`cargo test`\n\nIntegration tests go under tests folder, unit tests go into associated files under src.\n\n### Test databases\n\nRunning the integration tests will cause lots of test databases with the prefix `<your_db_name>_test_<random id>` to be created in\nyour local database. If you want to remove them, you can use a bash script along the lines of:\n\n```\nfor db in `psql -U cjms -c '\\l' | grep cjms_test_ | cut -d '|' -f 1`; do psql -U cjms -c \"drop database \\\"$db\\\" \"; done\n```\n\nIn this case my db_name is `cjms` and my user is `cjms` and my password is in the environment variables `export PGPASSWORD=<>`.\n\n## Tips\n\n### VSCode Settings\n\n```\n{\n\t\"files.trimTrailingWhitespace\": true,\n\t\"files.insertFinalNewline\": true,\n    \"rust-analyzer.diagnostics.disabled\": [\n        \"unresolved-macro-call\"\n    ]\n}\n```\n\n### Git hooks\n\nTo save time in CI, add a pre-commit or pre-push git hook locally that runs, at least, clippy and fmt.\n\nFor example, in `.git/hooks/` copy `pre-push.sample` to `pre-push`. Update the file, so that the end of the file reads:\n\n```\nif ! cargo fmt -- --check; then\n    exit 1\nfi\nif ! cargo clippy --all-targets --all-features -- -D warnings; then\n    exit 1\nfi\nif ! cargo sqlx prepare --check -- --bin cjms; then\n    exit 1\nfi\n\nexit 0\n```\n\nNote: I've found the cargo sqlx prepare check to not work as expected for me and always suggest that a new prepare is needed :(\n\n### Working with sqlx\n\nIn general, be mindful of the difference between compile time checks and running the app.\n\nsqlx does compile time checks. For these to work one of two things has to be true:\n* There is a `DATABASE_URL` in the environment pointing to a database that is fully migrated\n* There is NOT a `DATABASE_URL` in the environment and then sqlx-cli will run in \"offline\" mode and use `sqlx-data.json`.\n\nsqlx does some implicit things looking for the DATABASE_URL in your environment, which includes looking for a `.env` file. If things aren't working as expected make sure you don't have a rogue `.env` file or environment variables.\n\n#### Writing new queries\n\nIt took me a long time to figure out the mechanics of working with sqlx. Here's the key points:\n* We want to use the macro functions like `query_as!` so that we get compile time checks as we're developing.\n* They require DATABASE_URL to be in the environment. Either exposed in the environment or in a .env file.\n* This works by sqlx connecting to a live database to check the queries at compile time. So the database that's listed\nin your DATABASE_URL must be available and migrated to the latest migrations as needed. This is not the same as the dynamically\ngenerated test databases.\n* However, in CI where the final compile will happen this won't be the case, so we use sqlx's offline mode. This works by running\na command that generates the necessary data into `sqlx-data.json`. This can then be used at compile time instead of the database\nbeing available.\n* The command `cargo sqlx prepare -- --bin cjms` will auto generate `sqlx-data.json` for you (make sure DATABASE_URL is available).\n* But, it only runs against one target so sql queries in the integration tests do not work. So, I've established a pattern where\n  all sql queries as functions in the relevant model and the integration tests call functions on the model. While this is somewhat\n  polluting production code with test code it was the best tradeoff I could find.\n\nThe general steps that I took:\n* Add a .env file with my database url\n* Run `sqlx migrate run` and sometimes `sqlx migrate revert` (and then run) to get my database correctly migrated\n* Once everythings working as expected, run `cargo sqlx prepare -- --bin cjms` to update `sqlx-data.json`. If you don't CI\n  will fail at compile time so this should be fairly easy to spot.\n\nLinks to sqlx cli information including offline mode https://github.com/launchbadge/sqlx/blob/v0.5.11/sqlx-cli/README.md.\n\nIf you have forgotten to prepare, CI will fail with error messages like:\n\n```\nFailed to compile tests! Error: cjms: failed to find data for query UPDATE aic\n            SET flow_id = $1\n            WHERE id = $2\n\t\t\tRETURNING *\nError: \"Failed to compile tests! Error: cjms: failed to find data for query UPDATE aic\\n            SET flow_id = $1\\n            WHERE id = $2\\n\\t\\t\\tRETURNING *\"\n```\nor\n```\nerror: failed to find data for query UPDATE aic\n                   SET flow_id = $1\n                   WHERE id = $2\n                   RETURNING *\n  --> src/models/aic.rs:40:9\n   |\n40 | /         query_as!(\n41 | |             AIC,\n42 | |             r#\"UPDATE aic\n43 | |             SET flow_id = $1\n...  |\n47 | |             id,\n48 | |         )\n   | |_________^\n   |\n   = note: this error originates in the macro `$crate::sqlx_macros::expand_query` (in Nightly builds, run with -Z macro-backtrace for more info)\n```\n\n#### Version compatibility\n\nOne thing that took me a while to figure out was I was using sqlx features like \"time\" or \"uuid\" but I was\ngetting error messages like `expected struct sqlx::types::time::OffsetDateTime, found struct time::OffsetDateTime`.\nThis was confusing because I was using the modules as documented and the whole point is that these types become\nmagically compatible.\n\nIn both cases the reason was because the versions of `time` and `uuid` that I had installed were ahead of what sqlx\ncurrently supports. This meant that the, I think, trait implementations for them to interoperate weren't present.\n\n### Working with logs and metrics\n\nThe telemetry module contains utilities for writing out logs and metrics. The most common pattern we use it to use a macro that writes out a log with a pre-defined key, while at the same time incrementing a statsd counter with the same name. The name of the log key should be a known value in the `LogKey` macro.\n\nFor example:\n\n```rust\ninfo_and_incr!(\n    StatsD::new(&settings),\n    LogKey::CleanupAicArchive,\n    key = \"value\",\n    \"Some log message\"\n)\n```\n\nThe value of the `LogKey` enum will be stringified to kebab case before it is sent to the logger and statsd. Additionally statsd adds a project prefix. So for this example, the final result will be to write a Mozlog-compatible log message to stdout with a `type` value of `cleanup-aic-archive`, and increment a statsd counter with the name `cjms.cleanup-aic-archive`.\n\n#### Running a local statsd client\n\nIf configured using the defaults from settings.yaml.example, the statsd client\nwill send metrics to a statsd server on localhost at UDP port 8125. Since statsd\ndoesn't actually check if the metrics have been received by the server, this\nwill work even if you don't have a local statsd server configured. However, it\nis trivial to run statsd and graphite locally for debugging purposes. A [Docker\ncontainer](https://hub.docker.com/r/graphiteapp/graphite-statsd/) can be run\nusing the following command:\n\n```\ndocker run -d \\\n --name graphite \\\n --restart=always \\\n -p 80:80 \\\n -p 2003-2004:2003-2004 \\\n -p 2023-2024:2023-2024 \\\n -p 8125:8125/udp \\\n -p 8126:8126 \\\n graphiteapp/graphite-statsd\n```\n\nThe graphite server is exposed at `localhost:80`.\n\n## Deployment\n\nService is deployed using docker containers.\n\nInstall docker.\n\nTo build, for example\n\n`docker build -t cjms:latest .`\n\nTo run, set environment variables (can be done with a file) and forward ports e.g.\n\n`docker run -e HOST=0.0.0.0 -e PORT=8484 -p 8484:8484 cjms:latest`\n\n### Version numbers\n\n#### Pre 1.0\n\nVersion numbers will increment 0.1, 0.2 etc as pre-releases as we work towards launch.\n\n#### 1.0+\n\nVersion numbering will follow the guardian schema:\n- version numbers will increase v1.1, v1.2 etc\n- in emergency situations a minor release e.g. v1.2.1 will be made for a patch release\n- release candidates will have a \"b\" suffix e.g. ahead of v1.1 release we will push\n  v1.1.b1, v1.1.b2 to staging for QA to review\n\n#### 2.0+\n\nA major version bump from 1.x to 2.x would happen in the case of a breaking change to APIs\nwe provide.\n\nFor CJMS, this is unlikely to happen.\n\n## Release, branching, and merging\n\n### Pre 1.0\n\nAll development will happen against main\n\n### 1.0+\n\nmain should reflect the state of production code.\n\nAt the start of a new development cycle a release branch is made and development continues\nagainst that branch. A draft PR is opened that proposes a merge of the whole release branch\ninto main. The supports the following workflow elements:\n- The test suite reviews the complete set of changes in the release branch\n- The release PR should only be merged after QA sign-off and the QA report should be linked in\n  the commit\n- If a patch release is needed, make a new release branch off main and when completed cherry\n  pick into the active release branch\n\nPRs should be squash merged into the release branch.\n\nRelease branch should be merged with a merge commit into main.\n\n[DSN Identifier]: https://docs.sentry.io/product/sentry-basics/dsn-explainer/\n"
},
{
  "name": "shavar-prod-lists",
  "files": {
    "/": [
      ".github",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "disconnect-blacklist.json",
      "disconnect-entitylist.json",
      "scripts",
      "social-tracking-protection-blacklist.json",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# shavar-prod-lists\n\n[![Build Status](https://travis-ci.org/mozilla-services/shavar-prod-lists.svg?branch=master)](https://travis-ci.org/mozilla-services/shavar-prod-lists)\n\nThis repo serves as a staging area for\n[shavar](https://github.com/mozilla-services/shavar) /\n[tracking protection](https://wiki.mozilla.org/Security/Tracking_protection)\nlists prior to\n[production deployment to Firefox](https://mana.mozilla.org/wiki/display/SVCOPS/Shavar+-+aka+Mozilla's+Tracking+Protection).\nThis repo gives Mozilla a chance to manually review all updates before they go\nlive, a fail-safe to prevent accidental deployment of a list that could break\nFirefox.\n\nNot all domains in this repository are blocked in all versions of Firefox.\nThe master branch represents the base list blocked by Nightly. Beta, release,\nand past versions of Firefox all use versions of this list, accessible as\n[branches](https://github.com/mozilla-services/shavar-prod-lists/branches) of\nthis repository. We may also unblock certain domains through our anti-tracking\ninterventions temporarily when we discover site breakage. These temporary\nexceptions are tracked in [Bug 1537702](https://bugzilla.mozilla.org/show_bug.cgi?id=1537702),\nand the policy governing their use is [described below](#temporary-exceptions).\n\nThese lists are processed and transformed and sent to Firefox via\n[Shavar](https://mana.mozilla.org/wiki/display/SVCOPS/Shavar+-+aka+Mozilla's+Tracking+Protection).\n\n## Disconnect's Lists\nFirefox's Enhanced Tracking Protection features rely on lists of trackers\nmaintained by [Disconnect](https://disconnect.me/trackerprotection).\nMozilla does not maintain these lists. As such, we will close all issues and\npull requests related to making changes to the list contents. These issues\nshould be reported to Disconnect.\n\n#### `disconnect-blacklist.json`\nA version controlled copy of Disconnect's\n[list of trackers](https://github.com/disconnectme/disconnect-tracking-protection/blob/master/services.json).\nThis blocklist is the core of tracking protection in Firefox.\n\nA vestige of the list is the \"Disconnect\" category, which contains Facebook,\nTwitter, and Google domains. Domains from this category are remapped into the\nSocial, Advertising, or Analytics categories as described\n[here](https://github.com/mozilla-services/shavar-list-creation/blob/master/disconnect_mapping.json).\nThis remapping occurs at the time of list creation, so the Social, Analytics,\nand Advertising lists consumed by Firefox will contain these domains.\n\nFirefox consumes the list as follows:\n* **Tracking**: anything in the Advertising, Analytics, Social, Content, or\n    Disconnect category. Firefox ships two versions of the tracking lists: the\n    \"Level 1\" list, which excludes the \"Content\" category, and the\n    \"Level 2\" list which includes the \"Content\" category.\n* **Cryptomining**: anything in the Cryptomining category\n* **Fingerprinting**: anything in the FingerprintingInvasive category.\n    By default, ETP's fingerprinting blocking only blocks\n    _Tracking Fingerprinters_, that is domains which appear in both the\n    FingerprintingInvasive category and one of the Tracking categories.\n    Firefox does not use the FingerprintingGeneral category at this time.\n\n#### `disconnect-entitylist.json`\n\nA version controlled copy of Disconnect's\n[list of entities](https://github.com/disconnectme/disconnect-tracking-protection/blob/master/entities.json).\nETP classifies a resource as a tracking resource when it is present on\nblocklist and loaded as a third-party. The Entity list is used to allow\nthird-party subresources that are wholly owned by the same company that owns\nthe top-level website that the user is visiting. For example, if abcd.com owns\nefgh.com and efgh.com is on the blocklist, it will not be blocked on abcd.com.\nInstead, efgh.com will be treated as first party on abcd.com, since the same\ncompany owns both. But since efgh.com is on the blocklist it will be blocked on\nother third-party domains that are not all owned by the same parent company.\n\n## Other lists\n\nIn addition, Mozilla maintains several lists for Firefox-specific features and\nexperiments. The lists currently in active use are:\n* `social-tracking-protection-blacklist.json`: a subset of trackers from\n    Disconnect's blocklist. This list is used to identify \"social media\"\n    trackers within Firefox's UI. All of the origins on this list should also\n    be included in Disconnect's `disconnect-blacklist.json` list.\n\n## List Versioning and Release Process\n\nAs of Firefox 72, all desktop releases use versioned blocklists, i.e., each\nversion of Firefox uses a version of `disconnect-blacklist.json` and\n`disconnect-entitylist.json` specific to that version. These versions are\ntracked by [branches](https://github.com/mozilla-services/shavar-prod-lists/branches)\nof this repository. For the current cycle (Dec. 2019) this means there is a\n73 list (Nightly), a 72 list (Beta), a 71 list (Release), and a 68 list (ESR).\n\nNightly uses a staging version of the blocklist; the staging blocklist pulls in\nchanges from Disconnect as soon as they are available. When a new version of\nFirefox is released, we will also release a new version of the list that\ncorresponds to the version of Firefox moving from Nightly (main branch) --> Beta\n(versioned branch). That version of the list will ride the trains along with its\nrespective Firefox version. Releases older than Firefox 69 use the 69 version of\nthe blocklist.\n\nThis means that all changes will be tested for at least the full beta cycle and\npart of the Nightly cycle. We may choose to shorten the testing cycle in the\nfuture.\n\nThere are three possible exceptions to this process:\n1. **Fast-tracked changes** which are deployed immediately to all channels\n2. **Temporary exceptions** which are deployed using Remote Settings\n3. **List freezes** for when we\u2019d like to test changes for a longer duration.\n   These are tracked in Github issues on this repository.\n\n#### Fast-tracked changes\n\nWe will fast track breakage-related updates or policy-related updates, both\nof which may only be done by Disconnect. Fast-tracked changes should have\nminimal, if any, risk of breakage.\n\nChanges that may be fast-tracked include:\n* Deleting a domain from the blocklist and its respective domains from the entity list.\n* Adding new domains to the entity list.\n* Replacing a domain currently on the list with a new domain at the request of\n  the company that owns the domain. These requests must go through Disconnect.\n* Moving a domain between list categories of the same feature.\n\nAs soon as Disconnect makes changes of this type we will merge\nthem into each versioned list and deploy them across all channels.\n\n#### Temporary exceptions\n\nWe may choose to grant a temporary domain-based exemption in response to website\nbreakage as detailed in our\n[anti-tracking policy](https://wiki.mozilla.org/Security/Anti_tracking_policy#Temporary_Web_Compatibility_Interventions).\n\n#### List freezes\n\nWe may want to let certain changes bake in our pre-release browsers for a\ncouple extra cycles. This provides more time for us to discover user-reported\nbreakage or run breakage studies on the lists. In these cases we may hold back\nthe changes from moving to a new release of Firefox. These freezes will either\napply to the entire blocklist, or to specific categories of the blocklist\n(e.g., we shipped cookie blocking for the Level 1 list while we\n[further tested](https://bugzilla.mozilla.org/show_bug.cgi?id=1501461)\nthe Level 2 list). We will not freeze specific domains or commits.\n\n## List update process\nThis repo is configured with [Travis CI\nbuilds](https://travis-ci.org/mozilla-services/shavar-prod-lists/builds) that\nrun the `scripts/json_verify.py` script to verify all pull request changes to\nthe list are valid.\n\nThis Travis CI status check must pass before any commit can be merged or pushed\nto master.\n\n### Making changes to the format\nWhen making changes to the list formats, corresponding changes to the\n`scripts/json_verify.py` script must also be made.\n\nTo help validate the validator (such meta!), use the list fixtures in the\n`tests` directory. Run the script against a specific file like this:\n\n```\n./scripts/json_verify.py -f <filename>\n```\n\n* `tests/disconnect_blacklist_invalid.json` - copy of\n  `disconnect-blacklist.json` with an invalid `\"dnt\"` value\n* `tests/disconnect_blacklist_valid.json` - copy of `disconnect-blacklist.json`\n  with all valid values\n\n\n```\n$ ./scripts/json_verify.py -f tests/disconnect_blacklist_valid.json\n\ntests/disconnect_blacklist_valid.json : valid\n\n$ ./scripts/json_verify.py -f tests/disconnect_blacklist_invalid.json\n\ntests/disconnect_blacklist_invalid.json : invalid\nFacebook has bad DNT value: bogus\n```\n\n# License\nFind more details about license [here](LICENSE)\n"
},
{
  "name": "topsites-proxy",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".prettierrc.js",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTE.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "lib",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "server.js",
      "test",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![Docker Build Status](https://circleci.com/gh/mozilla-services/topsites-proxy/tree/master.svg?style=shield&circle-token=e7ebd9840084fe030514be02a23db6a78973379d)](https://circleci.com/gh/mozilla-services/topsites-proxy)\n\n# Top Sites proxy\n\nThis project started as a fork of [Dockerflow](https://github.com/mozilla-services/Dockerflow),\nused it as a solid foundation to implement a simple Express-based proxy server\nfor link replacement experiments in Firefox.\n\n## Contributing\n\n- [Contribution Guidelines](CONTRIBUTE.md)\n"
},
{
  "name": "megaphone",
  "files": {
    "/": [
      ".circleci",
      ".clog.toml",
      ".dockerignore",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "Rocket.toml",
      "docker-compose.yaml",
      "migrations",
      "rust-toolchain",
      "src",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![License: MPL 2.0][mpl-svg]][mpl] [![Test Status][travis-badge]][travis] [![Build Status][circleci-badge]][circleci] [![Connect to Matrix via the Riot webapp][matrix-badge]][matrix]\n\n# Megaphone\n\n## What is it?\n\nMegaphone is an internal Mozilla system providing global broadcasts for Firefox.\n\nTraditionally Firefox has polled multiple services at different frequencies (e.g. every 24 hours) to check for updates. Megaphone serves as an alternative, notifying user agents of new updates in near real time (within 5 minutes) over the [WebPush WebSocket protocol].\n\nThis enables Firefox to:\n\n* [Revoke HTTPS Certificates] or [malicious extensions] immediately after security incidents occur\n* Update quicker (Firefox itself, [tracking protection lists], or [general settings])\n* Provide faster turn-around/feedback loops for studies/experiments ([Shield])\n\nAll via one unified, simpler client-side service that doesn't require polling.\n\nThis repository provides a Rust based Megaphone endpoint (API). Broadcasts are sent to the Megaphone endpoint. The [autopush-rs service] polls the endpoint as the source of truth for current broadcasts, ultimately delivering them to clients.\n\nAlso see the [API doc].\n\n\n## Requirements\n\n * Rust nightly as specified in the `rust-toolchain` file (recognized by cargo) in the root of the project (recognized by cargo).\n * MySQL 5.7 (or compatible)\n * libmysqlclient (brew install mysql on macOS, apt-get install libmysqlclient-dev on Ubuntu)\n\n * *For running the docker image locally: docker-compose v1.21.0 or later\n\n## Setting Up\n\n1) [Install Rust]\n\n2) Create a `megaphone` user/database\n\n3) Run:\n\n  $ export ROCKET_DATABASE_URL=mysql://scott:tiger@mydatabase/megaphone\n  $ cargo run\n\n## Running the Docker Image\n\n1) [Install docker-compose]\n\n2) From a dedicated screen (or tmux window)\n\n$ `docker-compose up`\n\nThis will create two intertwined docker images:\n\n***db_1*** - the database image. This image is a local test image. The database can be accessed via `mysql -umegaphone -ptest -h localhost --port 4306 megaphone`.\n\n***app_1*** - the **megaphone** application. This is accessible via port 8000.\n\n\n# API\n\nMegaphone is normally called via a HTTP interface using Authorized calls. Responses are generally JSON objects with appropriate HTTP status codes to indicate success/failure.\n\n## Authorization\n\nAll calls to Megaphone (minus the Dockerflow Status Checks) require authorization. Authorization is specified by the `Authorization` header via Bearer tokens specified in the application's configuration.\n\ne.g.\n\n```\nexport ROCKET_BROADCASTER_AUTH={test=[\"foobar\"]}\nexport ROCKET_READER_AUTH={autopush=[\"quux\"]}\n```\n\nThe *test* broadcaster would specify:\n\n```\nAuthorization: Bearer foobar\n```\n\nThe *autopush* reader would specify:\n\n```\nAuthorization: Bearer quux\n```\n\n\n## PUT /v1/broadcasts/< broadcaster_id > /< bchannel_id >\n\nBroadcast a new version.\n\nThe body of the PUT request becomes the new version value.\n\nA special version value of \"____NOP____\" (the string \"NOP\" prefixed and suffixed by four underscores) signals a \"No Operation\" to clients: that no action should take place, effectively overwriting and cancelling any pending version update.\n\nThe return value is a JSON structure including the HTTP status of the result: successful results either being a `201` code for newly created broadcasts or `200` for an update to an existing broadcast.\n\n```javascript\n{\n   \"code\": 200\n}\n```\n\n\n## GET /v1/broadcasts\n\nRead the current broadcasts.\n\nThe return value is a JSON structure including the HTTP status of the result and a `broadcasts` object consisting of broadcastIDs and their current versions.\n\n```javascript\n{\n   \"code\": 200,\n   \"broadcasts\": {\n      \"test/broadcast1\": \"v3\",\n      \"test/broadcast2\": \"v0\"\n   }\n}\n```\n\n## Dockerflow Status Checks:\n\n## GET /\\_\\_heartbeat__\n\nReturn the status of the server.\n\nThis call is only used for server status checks.\n\n\n## GET /\\_\\_lbheartbeat__\n\nReturn a light weight status check (200 OK).\n\nThis call is only used for the Load Balancer's check.\n\n\n## GET /\\_\\_version__\n\nReturn a JSON response of the version information of the server.\n\n\n[mpl-svg]: https://img.shields.io/badge/License-MPL%202.0-blue.svg\n[mpl]: https://opensource.org/licenses/MPL-2.0\n[travis-badge]: https://travis-ci.org/mozilla-services/megaphone.svg?branch=master\n[travis]: https://travis-ci.org/mozilla-services/megaphone\n[circleci-badge]: https://circleci.com/gh/mozilla-services/megaphone.svg?style=shield&circle-token=074ae89011d1a7601378c41a4351e1e03f1e8177\n[circleci]: https://circleci.com/gh/mozilla-services/megaphone\n[matrix-badge]: https://img.shields.io/badge/chat%20on%20[m]-%23services%3Amozilla.org-blue\n[matrix]: https://chat.mozilla.org/#/room/#services:mozilla.org\n\n[WebPush WebSocket protocol]: https://mozilla-push-service.readthedocs.io/en/latest/design/#simplepush-protocol\n[revoke HTTPS Certificates]: https://blog.mozilla.org/security/2015/03/03/revoking-intermediate-certificates-introducing-onecrl/\n[malicious extensions]: https://wiki.mozilla.org/Blocklisting\n[tracking protection lists]: https://wiki.mozilla.org/Security/Safe_Browsing\n[general settings]: https://wiki.mozilla.org/Firefox/RemoteSettings\n[Shield]: https://wiki.mozilla.org/Firefox/Shield/Shield_Studies\n[autopush-rs service]: https://github.com/mozilla-services/autopush-rs\n[API doc]: https://docs.google.com/document/d/1Wxqf1a4HDkKgHDIswPmhmdvk8KPoMEh2q6SPhaz4LNE\n\n[Install Rust]: https://rustup.rs/\n[Install docker-compose]: https://docs.docker.com/compose/install/\n"
},
{
  "name": "pushbox",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".clog.toml",
      ".dockerignore",
      ".flooignore",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.lock",
      "Cargo.toml",
      "DOCUMENTATION.md",
      "Dockerfile",
      "LICENSE",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "Rocket.toml",
      "docker-compose.yaml",
      "migrations",
      "rust-toolchain",
      "scripts",
      "src",
      "test.sh",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![License: MPL 2.0][mpl-svg]][mpl]\n[![Test Status][travis-badge]][travis]\n[![Build Status][circleci-badge]][circleci]\n[![Connect to Matrix via the Riot webapp][matrix-badge]][matrix]\n\n# Pushbox - A rust implementation of Push/Sync long term storage\n\n## What is it?\n\nThis is an internal project. mozilla needs the ability to store large data\nchunks that may not fit perfectly within a standard WebPush message. PushBox\nacts as an intermediary store for those chunks.\n\nMessages are created by Firefox Accounts (FxA), stored here, and then a\nWebPush message containing a URL that points back to this storage is sent\nto the User Agent.\n\nThe User Agent can then fetch the data, decrypt it and do whatever it needs\nto.\n\nThis project, once completed, will eventually replace the AWS Severless\nPushBox project. It's being developed here because serverless can be a bit\ngreedy about what it grabs, and since PushBox is a rapid prototype, it's\ngood to treat it in a clean room environment.\n\nSee [API doc](\nhttps://docs.google.com/document/d/1YT6gh125Tu03eM42Vb_LKjvgxc4qrGGZsty1_ajf2YM/)\n\n## Requirements\n\nThe project requires Rust Nightly, a MySQL compliant data store, and\naccess to a [Firefox Accounts token verifier](https://github.com/mozilla/fxa-auth-server) system.\n\n\n## Setting Up\n\n1) Install Rust Nightly.\n\nThe rocket.rs [Getting Started](https://rocket.rs/guide/getting-started/)\ndocument lists information on how to set that up.\n\n2) create the pushbox MySQL user and database.\n\nBecause I'm horribly creative and because this is a WIP, I use \"`test:test@localhost/pushbox`\".\nThis is not recommended for production use. You can set your preferred\nMySQL access credential information as \"database_url\" in the `Rocket.toml`\nsettings file (See [Rocket Config](https://rocket.rs/guide/configuration/#rockettoml)\ninformation.)\n\n3) Run `cargo run` to start the application, which (depending on the last\n  commit) may actually start the program. YMMV.\n\n## Running Docker Image\n\nIt is ***NOT*** advised to run the docker image in a production environment.\nIt's best suited for local development ***ONLY***.\n\nBe sure to have `docker-compose` v. 1.21.0 or later installed.\n\n1) From a dedicated screen (or tmux window)\n\n$ `docker-compose up`\n\nThis will create two intertwined docker images:\n\n***db_1*** - the database image. This image is a local test image. The database\ncan be accessed via `mysql -utest -ptest -h localhost --port 4306 pushbox`.\n\n***app_1*** - the **pushbox** application. This is accessible via port\n8000,\nand uses the server authentication key of \"[Correct_Horse_Battery_Staple_1](https://www.xkcd.com/936/)\".\n\ne.g.\n` curl -H \"Authorization: FxA-Server-Key Correct_Horse_Battery_Staple_1\" \"http://localhost:8000/v1/userid/deviceid\"`\n\nNote: No garbage collection is currently done for the database. Heavy use\nmight warrant deleting old records every so often.\n\n# API\n\nPushbox is normally called via a HTTP interface using Authorized calls. Responses are generally JSON objects with appropriate HTTP status codes to indicate success/failure.\n\n## Authorization\n\nAll calls to Pushbox require authorization. Authorization is specified by the `Authorization` header and can be either via a Server-Key or using Firefox Accounts OAuth token scopes. The method of authorization shall be determined and set by operations. It is **strongly** suggested that if Server-Key is used, access to PushBox be limited to an ACL of calling Sync servers. \n\ne.g.\n\n```\nAuthorization: fxa-server-key Correct-Horse-Battery-Staple-1\n```\n\n\n## GET /v1/store/< user> /< device >[?< options >]\n\nFetch data for a `<user>` on a `<device>`. \n\nOptions may be one or more of the following:\n\n* *index* - offset index to being new messages.\n* *limit* - maximum number of messages to return. This will include the next index value to use. \n* *status* - Quick set the index and limit values:\n  * *new* - client is new, and needs all records.\n  * *lost* - client only needs latest index.\n\nThe return value is JSON structure: \n\n```javascript\n{ \"last\": true, /* boolean indicating this is the last data block */\n  \"index\": 123, /* the highest message index value returned */\n  \"status\": 200, /* HTTP status for result */\n  \"messages\": [\n    {\"index\": 123, /* Message block index */\n     \"data\": \"aBc1...\" /* encrypted data block */\n    }, ...\n  ]\n}\n```\n\n## POST /v1/store/< user >/< device >\n\nWrite a databblock for a `<user>` on `<device>`\n\nThe body of the post message is a JSON structure:\n\n```javascript\n{\"ttl\": 3600, /* Time for the data to live in seconds.*/\n \"data\": \"aBc1...\" /* Encrypted data block to store */\n}\n```\n\n**NOTE:** Please be certain to encrypt the body of the data you wish to store. No encryption is done on the server side, and even if it was, there's no guarantee that it couldn't be reversed by a disgruntled employee or malicious agent.\n\nThe returned value is a JSON structure:\n\n```javascript\n{\n  \"status\": 200, /* The HTTP status for the result */\n  \"index\": 123 /* the index number of the stored record */\n}\n```\n\n## DELETE /v1/store/< user >[/< device >]\n\nDelete all records for a given user or just a given user's device.\n\nThis call returns just an empty object.\n\n\n## GET /__heartbeat__\n\nReturn the status of the server.\n\nThis call is only used for server status checks.\n\n\n## GET /__lbheartbeat__\n\nReturn a light weight status check (200 OK).\n\nThis call is only used for the Load Balancer's check.\n\n\n## GET /__version__\n\nReturn a JSON response of the version information of the server.\n\n\n# Database\n\nPushbox requires a configured MySQL compliant server. Pushbox requires the configuration file to contain the proper credentials, but will create any require table or indexes. Currently, there is no garbage collection done for expired, unread records. It is suggested that a regularly scheduled command be created to run \n\n```sql\nDELETE from pushboxv1 where TTL < unix_timestamp();\n```\n\nThis function may be added to pushbox at a later date.\n\n[mpl-svg]: https://img.shields.io/badge/License-MPL%202.0-blue.svg\n[mpl]: https://opensource.org/licenses/MPL-2.0\n[travis-badge]: https://travis-ci.org/mozilla-services/pushbox.svg?branch=master\n[travis]: https://travis-ci.org/mozilla-services/pushbox\n[circleci-badge]: https://circleci.com/gh/mozilla-services/pushbox.svg?style=shield&circle-token=074ae89011d1a7601378c41a4351e1e03f1e8177\n[circleci]: https://circleci.com/gh/mozilla-services/pushbox\n[matrix-badge]: https://img.shields.io/badge/chat%20on%20[m]-%23push%3Amozilla.org-blue\n[matrix]: https://chat.mozilla.org/#/room/#push:mozilla.org\n"
},
{
  "name": "channelserver",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "build.sh",
      "channelserver",
      "docs",
      "test_chan"
    ],
    "/docs": [
      "PULL_REQUEST_TEMPLATE.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "SHELL := /bin/bash\nPWD := `pwd`\n\n.PHONY: test\n\ntest_chan/venv/bin/python:\n\tpushd test_chan && \\\n\tvirtualenv -p python3 venv && \\\n\tvenv/bin/pip install --upgrade pip && \\\n\tvenv/bin/python setup.py develop && \\\n\tpopd\n\n# Fetch the MMDB database and set up the Python integration testing\ninstall: channelserver/mmdb/latest test_chan/venv/bin/python\n\techo \"Done\"\n\n# Do the actual testing.\n# TODO: Need to switch this to pytest, however __main__.py does all the same things.\ntest: channelserver/mmdb/latest test_chan/venv/bin/python\n\tpushd test_chan && \\\n\tPAIR_MMDB_LOC=../channelserver/mmdb/latest/GeoLite2-City.mmdb venv/bin/python test_chan && \\\n\tpopd\n",
  "readme": "[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0)\n[![Build](https://travis-ci.org/mozilla-services/channelserver.svg?branch=master)](https://travis-ci.org/mozilla-services/channelserver)\n[![Connect to Matrix via the Riot webapp][matrix-badge]][matrix]\n\n# ChannelServer\n\nA project to make Firefox Account log-in, pairing and sync easier\nbetween devices or applications.\n\nContains:\n\n- [channelserver](./channelserver/) - websocket message relay server.\n- [test_chan](./test_chan/) - python based external integration tester\n  for channelserver.\n\nFor client code that uses this facility to create\nan encrypted and authenticated channel between two\nclient devices, see [fxa-pairing-channel](https://github.com/mozilla/fxa-pairing-channel).\n\n[matrix-badge]: https://img.shields.io/badge/chat%20on%20[m]-%23services%3Amozilla.org-blue\n[matrix]: https://chat.mozilla.org/#/room/#services:mozilla.org\n"
},
{
  "name": "a2",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      ".travis",
      "CHANGELOG.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "a2_travis.enc",
      "examples",
      "rustfmt.toml",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# a2\n\n[![Travis Build Status](https://travis-ci.org/pimeys/a2.svg?branch=master)](https://travis-ci.org/pimeys/a2)\n[![MIT licensed](https://img.shields.io/badge/license-MIT-blue.svg)](./LICENSE)\n[![crates.io](http://meritbadge.herokuapp.com/a2)](https://crates.io/crates/a2)\n\nHTTP/2 Apple Push Notification Service for Rust using Tokio and async sending.\n\n## Help needed\n\nThe main author is not currently owning any Apple phones, so would be nice to have some help from a co-author with needed devices and an Apple developer account. If you happen to have them and are willing to help, please contact!\n\n## Requirements\n\nNeeds a Tokio executor version 0.2 or later and Rust compiler version 1.39.0 or later.\n\n## Documentation\n\n* [Released](https://docs.rs/a2/)\n* [Master](https://pimeys.github.io/a2/master/)\n\n## Features\n\n* Fast asynchronous sending, based on [h2](https://github.com/carllerche/h2) and\n  [hyper](https://github.com/hyperium/hyper) crates.\n* Payload serialization/deserialization with\n  [serde](https://github.com/serde-rs/serde).\n* Provides a type-safe way of constructing different types of payloads. Custom\n  data through `Serialize`, allowing use of structs or dynamic hashmaps.\n* Supports `.p12` certificate databases to connect using a custom certificate.\n* Supports `.p8` private keys to connect using authentication tokens.\n* If using authentication tokens, handles signature renewing for Apple's guidelines\n  and caching for maximum performance.\n\n## Examples\n\nThe library supports connecting to Apple Push Notification service [either using\na\ncertificate](https://github.com/pimeys/a2/blob/master/examples/certificate_client.rs)\nwith a password [or a private\nkey](https://github.com/pimeys/a2/blob/master/examples/token_client.rs) with\na team id and key id. Both are available from your Apple account and with both\nit is possible to send push notifications to one application.\n\nTo see it used in a real project, take a look to the [XORC\nNotifications](https://github.com/xray-tech/xorc-notifications), which is a\nfull-fledged consumer for sending push notifications.\n\n## Gotchas\n\nWe've been pushing some millions of notifications daily through this library and\nare quite happy with it. Some things to know, if you're evaluating the library\nfor production use:\n\n* Do not open new connections for every request. Apple will treat it as Denial of Service attack and block the sending IP address. When using the same `Client` for multiple requests, the `Client` keeps the connection alive if pushing steady traffic through it.\n\n* For one app, one connection is quite enough already for certain kind of\n  loads. With http2 protocol, the events are asynchronous and the pipeline can\n  hold several outgoing requests at the same time. The biggest reason to open\n  several connections is for redundancy, running your sender service on different\n  machines.\n\n* It seems to be Apple doesn't like when sending tons of notifications with\n  faulty device tokens and it might lead to `ConnectionError`s. Do not send more\n  notifications with tokens that return `Unregistered`, `BadDeviceToken` or\n  `DeviceTokenNotForTopic`.\n\n## Tests\n\n`cargo test`\n\n## Contact\n\noh_lawd @\u00a0IRC (Freenode, Mozilla)\n"
},
{
  "name": "services-engineering",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      ".prettier.rc",
      "PR_TEMPLATE.md",
      "README.md",
      "docs",
      "package-lock.json",
      "package.json",
      "scripts"
    ],
    "/docs": [
      "estimation.md"
    ]
  },
  "makefile": null,
  "readme": "# services-engineering\n\nThis is a meta repo for all things related to the Services Engineering team that don't belong specifically to another repo.\n\n## How we work:\n* We follow these [Rust security related best practices](https://github.com/mozilla-services/websec-check/blob/master/rust.md) within our Rust based projects. Other general Rust tips...\n  * If you're having trouble getting Rust to compile, try `rm Cargo.lock && cargo clean` - this forces an upgrade to your dependencies.\n* We use [this estimation guide](https://github.com/mozilla-services/services-engineering/blob/master/docs/estimation.md) when adding estimates to tasks. You can see these estimates used as labels throughout our GitHub repos.\n* You can usually get a sense of what we're up to on our [our GitHub project board](https://github.com/orgs/mozilla-services/projects/10).\n\n## Where else to find us:\n\n* [chat.mozilla.org - #services](https://chat.mozilla.org/#/room/#services:mozilla.org)\n* [Slack - #services-engineering](https://app.slack.com/client/T027LFU12/CK6MA5133)\n* [Mana](https://mana.mozilla.org/wiki/pages/viewpage.action?spaceKey=SE&title=Services+Engineering)\n"
},
{
  "name": "syncstorage-loadtest",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.rst",
      "loadtest.py",
      "molotov.json",
      "requirements.txt",
      "run_test.sh",
      "storage"
    ]
  },
  "makefile": null,
  "readme": "Storage load test\n-----------------\n* Note: this requires Python version 3.6+\n\nTo run it locally::\n\n    $ virtualenv .\n    $ bin/pip install -r requirements.txt\n    $ bin/molotov --max-runs 5 -cxv loadtest.py\n\n\nTo run it locally, directly from GitHub (assuming Molotov is installed)::\n\n    $ moloslave https://github.com/mozilla-services/syncstorage-loadtest test\n\nSee the molotov.json file for the different tests available to moloslave\n\nTo run it inside docker::\n\n    $ docker run -e TEST_REPO=https://github.com/mozilla-services/syncstorage-loadtest -e TEST_NAME=test tarekziade/molotov:latest\n\n\nHappy Breaking!\n"
},
{
  "name": "autopush-loadtester",
  "files": {
    "/": [
      ".circleci",
      ".clog.toml",
      ".dockerignore",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "LICENSE",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "SCENARIOS.md",
      "SMOKETEST.md",
      "aplt",
      "config.ini.sample",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![codecov.io](https://codecov.io/github/mozilla-services/ap-loadtester/coverage.svg?branch=master)](https://codecov.io/github/mozilla-services/ap-loadtester?branch=master) [![Build Status](https://travis-ci.org/mozilla-services/ap-loadtester.svg?branch=feature%2Fbug-1)](https://travis-ci.org/mozilla-services/ap-loadtester)\n\n# Autopush Load-Tester\n\nThe Autopush Load-Tester is an integrated API and load-tester for the Mozilla\nServices autopush project. It's intented to verify proper functioning of\nautopush deployments under various load conditions.\n\n## Supported Platforms \n\n`ap-loadtester` should run on most Linux distro(s).  Though we provide some \nnotes for OSX users (see below), please note that we only support usage\nof this tool on Linux. \n\n## Getting Started\n\n`ap-loadtester` uses PyPy 5.3.1 which can be downloaded here:\nhttp://pypy.org/download.html\n\nYou will also need virtualenv installed on your system to setup a virtualenv for\n`ap-loadtester`. Assuming you have virtualenv and have downloaded pypy, you\ncould then setup the loadtester for use with the following commands:\n\n**Linux:**\n\n    $ tar xjvf pypy2-v5.3.1-linux64.tar.bz2\n    $ virtualenv -p pypy2-v5.3.1-linux64/bin/pypy apenv\n\n**OSX:**\n\n    $ tar xjvf pypy2-v5.3.1-osx64.tar.bz2\n    $ virtualenv -p pypy2-v5.3.1-osx64/bin/pypy apenv\n\n**Activate Virtualenv:**\n\n    $ source apenv/bin/activate\n    $ pip install --upgrade pip\n\nThe last two commands activate the virtualenv so that running python or pip on\nthe shell will run the virtualenv pypy, and upgrade the installed pip to the\nlatest version.\n\nYou can now either install `ap-loadtester` as a [program](#program-use) to run\ntest scenarios you create, or if adding scenarios/code to `ap-loadtester`\ncontinue to [Developing](#developing).\n\n\n## Program Use\n\nInstall the `ap-loadtester` package:\n\n    $ pip install ap-loadtester\n\nRun the basic scenario against the dev server:\n\n    $ aplt_scenario aplt.scenarios:basic wss://autopush.dev.mozaws.net/ \n\nRun 5 instances of the basic scenario, starting one every second, against the\ndev server:\n\n    $ aplt_testplan \"aplt.scenarios:basic,5,1,0\" wss://autopush.dev.mozaws.net/ \n\nEither of these scripts can be run with `-h` for full help documentation.\n\nSee [SCENARIOS](SCENARIOS.md) for guidance on writing a scenario function for\nuse with `ap-loadtester`.\n\n## Developing\n\nCheckout the code from this repository and run the package setup after the\nvirtualenv is active:\n\n    $ pip install -r requirements.txt -e .\n\nSee [Contributing](CONTRIBUTING.md) for contribution guidelines.\n\n## Notes on Installation\n\n**'openssl/aes.h' file not found**\n\nIf you get the following error:\n\n    $ fatal error: 'openssl/aes.h' file not found\n\nLinux: You'll need to install OpenSSL:\n\n    $ sudo apt-get install libssl-dev\n\nOSX: Apple has deprecated OpenSSL in favor of its own TLS and crypto libraries.\nIf you get this error on OSX (El Capitan), install OpenSSL with brew, then\nlink brew libraries and install cryptography.  \nNOTE: /usr/local/opt/openssl is symlinked to brew Cellar:\n\n    $ brew install openssl\n    $ ARCHFLAGS=\"-arch x86_64\" LDFLAGS=\"-L/usr/local/opt/openssl/lib\" \\\n      CFLAGS=\"-I/usr/local/opt/openssl/include\" pip install cryptography\n\n**missing required distribution pyasn1**\n\nIf you get the following error:\n\n    $ error: Could not find required distribution pyasn1\n\nre-run:\n\n    $ python setup.py develop\n"
},
{
  "name": "syncserver",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".python-version",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.rst",
      "bin",
      "dev-requirements.txt",
      "docker-entrypoint.sh",
      "requirements.txt",
      "setup.py",
      "syncserver.ini",
      "syncserver.wsgi",
      "syncserver"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# Once there is Python 3 support in this package and it's dependencies\n# the detection of the used Python version should be changed.\n# The following line prefers `python` as this will allow this to work\n# in CI (e.g. Travis CI) and put up the configured Python version:\n# SYSTEMPYTHON = `which python python3 python2 | head -n 1`\nSYSTEMPYTHON = `which python2 python2.7 python | head -n 1`\nVIRTUALENV = $(SYSTEMPYTHON) -m virtualenv --python=$(SYSTEMPYTHON)\nENV = ./local\nTOOLS := $(addprefix $(ENV)/bin/,flake8 nosetests)\n\n# Hackety-hack around OSX system python bustage.\n# The need for this should go away with a future osx/xcode update.\nARCHFLAGS = -Wno-error=unused-command-line-argument-hard-error-in-future\n\n# Hackety-hack around errors duing compile of ultramemcached.\nCFLAGS = \"-Wno-error -Wno-error=format-security\"\n\nINSTALL = CFLAGS=$(CFLAGS) ARCHFLAGS=$(ARCHFLAGS) $(ENV)/bin/pip install\n\n\n.PHONY: all\nall: build\n\n.PHONY: build\nbuild: | $(ENV)/COMPLETE\n$(ENV)/COMPLETE: requirements.txt\n\t# Install the latest Python 2 compatible setuptools manually:\n\t# https://github.com/mozilla-services/syncserver/issues/239\n\t$(VIRTUALENV) $(ENV) --no-setuptools\n\t$(INSTALL) -U \"setuptools<45\"\n\t$(INSTALL) -r requirements.txt\n\t$(ENV)/bin/python ./setup.py develop\n\ttouch $(ENV)/COMPLETE\n\n.PHONY: test\ntest: | $(TOOLS)\n\t$(ENV)/bin/flake8 ./syncserver\n\t$(ENV)/bin/nosetests -s syncstorage.tests\n\t# Tokenserver tests currently broken due to incorrect file paths\n\t# $(ENV)/bin/nosetests -s tokenserver.tests\n\n\t# Test against a running server.\n\t$(ENV)/bin/gunicorn --paste syncserver/tests.ini 2> /dev/null & SERVER_PID=$$!; \\\n\tsleep 2; \\\n\t$(ENV)/bin/python -m syncstorage.tests.functional.test_storage \\\n\t\t--use-token-server http://localhost:5000/token/1.0/sync/1.5; \\\n\tkill $$SERVER_PID\n\n$(TOOLS): | $(ENV)/COMPLETE\n\t$(INSTALL) -r dev-requirements.txt\n\n.PHONY: serve\nserve: | $(ENV)/COMPLETE\n\t$(ENV)/bin/gunicorn --paste ./syncserver.ini\n\n.PHONY: clean\nclean:\n\trm -rf $(ENV)\n",
  "readme": "Run-Your-Own Firefox Sync Server\n================================\n\n.. image:: https://circleci.com/gh/mozilla-services/syncserver/tree/master.svg?style=svg\n   :target: https://circleci.com/gh/mozilla-services/syncserver/tree/master\n\n.. image:: https://img.shields.io/docker/automated/mozilla-services/syncserver.svg?style=flat-square\n   :target: https://hub.docker.com/r/mozilla/syncserver/\n\n**Note that this repository is no longer being maintained**. Use this at your own risk, and\nwith the understanding that it is not being maintained, work is being done on its replacement,\nand that no support or assistance will be offered.\n\nThis is an all-in-one package for running a self-hosted Firefox Sync server.\nIt bundles the \"tokenserver\" project for authentication and the \"syncstorage\"\nproject for storage, to produce a single stand-alone webapp.\n\nComplete installation instructions are available at:\n\n   https://mozilla-services.readthedocs.io/en/latest/howtos/run-sync-1.5.html\n\n\nQuickstart\n----------\n\nThe Sync Server software runs using **python 2.7**, and the build\nprocess requires **make** and **virtualenv**.  You will need to have the\nfollowing packages (or similar, depending on your operating system) installed:\n\n- python2.7\n- python2.7-dev\n- python-virtualenv\n- gcc and g++\n- make\n- libstdc++\n- libffi-dev\n- mysql-dev\n- musl-dev\n- ncurses-dev\n- openssl-dev\n\nTake a checkout of this repository, then run \"make build\" to pull in the\nnecessary python package dependencies::\n\n    $ git clone https://github.com/mozilla-services/syncserver\n    $ cd syncserver\n    $ make build\n\nTo sanity-check that things got installed correctly, do the following::\n\n    $ make test\n\nNow you can run the server::\n\n    $ make serve\n\nThis should start a server on http://localhost:5000/.\n\nNow go into Firefox's `about:config` page, search for a setting named\n\"tokenServerURI\", and change it to point to your server::\n\n    identity.sync.tokenserver.uri:  http://localhost:5000/token/1.0/sync/1.5\n\n(Prior to Firefox 42, the TokenServer preference name for Firefox Desktop was\n\"services.sync.tokenServerURI\". While the old preference name will work in\nFirefox 42 and later, the new preference is recommended as the old preference\nname will be reset when the user signs out from Sync causing potential\nconfusion.)\n\nFirefox should now sync against your local server rather than the default\nMozilla-hosted servers.\n\nFor more details on setting up a stable deployment, see:\n\n   https://mozilla-services.readthedocs.io/en/latest/howtos/run-sync-1.5.html\n\n\nCustomization\n-------------\n\nAll customization of the server can be done by editing the file\n\"syncserver.ini\", which contains lots of comments to help you on\nyour way.  Things you might like to change include:\n\n    * The client-visible hostname for your server.  Edit the \"public_url\"\n      key under the [syncerver] section.\n\n    * The database in which to store sync data.  Edit the \"sqluri\" setting\n      under the [syncserver] section.\n\n    * The secret key to use for signing auth tokens.  Find the \"secret\"\n      entry under the [syncserver] section and follow the instructions\n      in the comment to replace it with a strong random key.\n\n\nDatabase Backend Modules\n------------------------\n\nIf your python installation doesn't provide the \"sqlite\" module by default,\nyou may need to install it as a separate package::\n\n    $ ./local/bin/pip install pysqlite2\n\nSimilarly, if you want to use a different database backend you will need\nto install an appropriate python module, e.g::\n\n    $ ./local/bin/pip install PyMySQL\n    $ ./local/bin/pip install psycopg2\n\n\nRunner under Docker\n-------------------\n\n`Dockerhub Page <https://hub.docker.com/r/mozilla/syncserver>`_\n\nThere is experimental support for running the server inside a Docker\ncontainer. The docker image runs with UID/GID 1001/1001.\nBuild the image like this::\n\n    $ docker build -t syncserver:latest .\n\nThen you can run the server by passing in configuration options as\nenvironment variables, like this::\n\n    $ docker run --rm \\\n        -p 5000:5000 \\\n        -e SYNCSERVER_PUBLIC_URL=http://localhost:5000 \\\n        -e SYNCSERVER_SECRET=<PUT YOUR SECRET KEY HERE> \\\n        -e SYNCSERVER_SQLURI=sqlite:////tmp/syncserver.db \\\n        -e SYNCSERVER_BATCH_UPLOAD_ENABLED=true \\\n        -e SYNCSERVER_FORCE_WSGI_ENVIRON=false \\\n        -e SYNCSERVER_DEBUG_ENABLED=true \\\n        -e PORT=5000 \\\n        mozilla/syncserver:latest\n\n    or\n\n    $ docker run --rm \\\n        -p 5000:5000 \\\n        -e SYNCSERVER_PUBLIC_URL=http://localhost:5000 \\\n        -e SYNCSERVER_SECRET_FILE=<PUT YOUR SECRET KEY FILE LOCATION HERE> \\\n        -e SYNCSERVER_SQLURI=sqlite:////tmp/syncserver.db \\\n        -e SYNCSERVER_BATCH_UPLOAD_ENABLED=true \\\n        -e SYNCSERVER_FORCE_WSGI_ENVIRON=false \\\n        -e PORT=5000 \\\n        -v /secret/file/at/host:<PUT YOUR SECRET KEY FILE LOCATION HERE>  \\\n        mozilla/syncserver:latest\n\nDon't forget to `generate a random secret key <https://mozilla-services.readthedocs.io/en/latest/howtos/run-sync-1.5.html#further-configuration>`_\nto use in the `SYNCSERVER_SECRET` environment variable or mount your secret key file!\n\nAnd you can test whether it's running correctly by using the builtin\nfunction test suite, like so::\n\n    $ /usr/local/bin/python -m syncstorage.tests.functional.test_storage \\\n        --use-token-server http://localhost:5000/token/1.0/sync/1.5\n\nIf you'd like a persistent setup, you can mount a volume as well::\n\n    $ docker run -d \\\n        -v /syncserver:/data \\\n        -p 5000:5000 \\\n        -e SYNCSERVER_PUBLIC_URL=http://localhost:5000 \\\n        -e SYNCSERVER_SECRET=<PUT YOUR SECRET KEY HERE> \\\n        -e SYNCSERVER_SQLURI=sqlite:////data/syncserver.db \\\n        -e SYNCSERVER_BATCH_UPLOAD_ENABLED=true \\\n        -e SYNCSERVER_FORCE_WSGI_ENVIRON=false \\\n        -e PORT=5000 \\\n        mozilla/syncserver:latest\n\nMake sure that /syncserver is owned by 1001:1001\n\n\n`Docker Compose <https://docs.docker.com/compose>`_ can also be used for structured deployments::\n\n    version: '3.7'\n    services:\n        syncserver:\n            container_name: syncserver\n            image: mozilla/syncserver:latest\n            volumes:\n                - /syncserver:/data\n            ports:\n                - 5000:5000\n            environment:\n                SYNCSERVER_PUBLIC_URL: 'http://localhost:5000'\n                SYNCSERVER_SECRET: '<PUT YOUR SECRET KEY HERE>'\n                SYNCSERVER_SQLURI: 'sqlite:////data/syncserver.db'\n                SYNCSERVER_BATCH_UPLOAD_ENABLED: 'true'\n                SYNCSERVER_FORCE_WSGI_ENVIRON: 'false'\n                PORT: '5000'\n            restart: always\n\nRemoving Mozilla-hosted data\n----------------------------\n\nIf you have previously uploaded Firefox Sync data\nto the Mozilla-hosted storage service\nand would like to remove it,\nyou can use the following script to do so::\n\n    $ pip install PyFxA\n    $ python ./bin/delete_user_data.py user@example.com\n\n\nQuestions, Feedback\n-------------------\n\n- Matrix: https://wiki.mozilla.org/Matrix#Getting_Started\n- Mailing list: https://mail.mozilla.org/listinfo/services-dev\n"
},
{
  "name": "server-syncstorage",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".hgignore",
      ".hgtags",
      ".idea",
      ".pyup.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTORS.txt",
      "Dockerfile",
      "LICENSE.txt",
      "MANIFEST.in",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "bin",
      "docker-entrypoint.sh",
      "docs",
      "example.ini",
      "requirements.txt",
      "setup.py",
      "syncstorage"
    ],
    "/docs": [
      "RELEASE.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "SYSTEMPYTHON = `which python2.7 python | head -n 1`\nVIRTUALENV = virtualenv --python=$(SYSTEMPYTHON)\nNOSE = local/bin/nosetests -s\nTESTS = syncstorage/tests\nPYTHON = local/bin/python\nEASY_INSTALL = local/bin/easy_install\nPIP = local/bin/pip\nPIP_CACHE = /tmp/pip-cache.${USER}\nBUILD_TMP = /tmp/syncstorage-build.${USER}\nPYPI = https://pypi.org/simple\n\nexport MOZSVC_SQLURI = sqlite:///:memory:\n\n# Hackety-hack around OSX system python bustage.\n# The need for this should go away with a future osx/xcode update.\nARCHFLAGS = -Wno-error=unused-command-line-argument-hard-error-in-future\nCFLAGS = -Wno-error=write-strings\n\nINSTALL = ARCHFLAGS=$(ARCHFLAGS) CFLAGS=$(CFLAGS) $(PIP) install -U -i $(PYPI)\n\n.PHONY: all build test\n\nall:\tbuild\n\nbuild:\n\t# The latest `pip` doesn't work with pypy 2.7 on some platforms.\n\t# Pin to a working version; ref https://github.com/pypa/pip/issues/8653\n\t$(VIRTUALENV) --no-pip ./local\n\t$(EASY_INSTALL) pip==20.1.1\n\t$(INSTALL) --upgrade \"setuptools>=0.7\"\n\t$(INSTALL) -r requirements.txt\n\t$(PYTHON) ./setup.py develop\n\ntest:\n\t# Check that flake8 passes before bothering to run anything.\n\t# This can really cut down time wasted by typos etc.\n\t./local/bin/flake8 syncstorage\n\t# Run the actual testcases.\n\t$(NOSE) $(TESTS)\n\t# Test that live functional tests can run correctly, by actually\n\t# spinning up a server and running them against it.\n\t./local/bin/gunicorn --paste ./syncstorage/tests/tests.ini --workers 1 --worker-class mozsvc.gunicorn_worker.MozSvcGeventWorker & SERVER_PID=$$! ; sleep 2 ; ./local/bin/python syncstorage/tests/functional/test_storage.py http://localhost:5000 ; kill $$SERVER_PID\n\nsafetycheck:\n\t$(INSTALL) safety\n\t# Check for any dependencies with security issues.\n\t# We ignore a known issue with gevent, because we can't update to it yet.\n\t./local/bin/safety check --full-report --ignore 25837\n",
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla-services/server-syncstorage.svg?style=svg)](https://circleci.com/gh/mozilla-services/server-syncstorage)\n\n# Storage Engine for Firefox Sync Server, version 1.5\n\nThis is the storage engine for version 1.5 of the Firefox Sync Server.\nIt implements the API defined at:\n\n   * https://mozilla-services.readthedocs.io/en/latest/storage/apis-1.5.html\n\nThis code is only one part of the full Sync Server stack and is unlikely\nto be useful in isolation.\n\nIf you want to run a self-hosted version of the Sync Server,\nyou should start from here:\n\n   * https://mozilla-services.readthedocs.io/en/latest/howtos/run-sync-1.5.html\n\nMore general information can be found at the following links:\n\n   * https://mozilla-services.readthedocs.io/en/latest/storage\n   * https://wiki.mozilla.org/Services/Sync\n"
},
{
  "name": "lua_sandbox_extensions",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE.txt",
      "README.md",
      "amqp",
      "artifact_push.sh",
      "aws",
      "bit",
      "bloom_filter",
      "circular_buffer",
      "cjson",
      "cmake",
      "common",
      "compat",
      "cuckoo_filter",
      "docker.arrow.keys.txt",
      "docker_push.sh",
      "elasticsearch",
      "gcp",
      "gen_gh_pages.lua",
      "geoip",
      "gzfile",
      "heka",
      "hindsight",
      "hyperloglog",
      "irc",
      "jose",
      "kafka",
      "lfs",
      "libinjection",
      "lpeg",
      "lsb",
      "lyaml",
      "maxminddb",
      "moz_ingest",
      "moz_logging",
      "moz_pioneer",
      "moz_security",
      "moz_telemetry",
      "openssl",
      "papertrail",
      "parquet",
      "postgres",
      "rjson",
      "sax",
      "snappy",
      "socket",
      "ssl",
      "struct",
      "syslog",
      "systemd",
      "taskcluster",
      "xxhash",
      "zlib"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Lua Sandbox Extensions\n\n## Overview\n\nPackage management for [Lua Sandbox](http://mozilla-services.github.io/lua_sandbox/)\nmodules and sandboxes. The goal is to simplify the lua_sandbox core by\ndecoupling the module and business logic maintenance and deployment.\n\n[Full Documentation](http://mozilla-services.github.io/lua_sandbox_extensions)\n\n## Installation\n\n### Prerequisites\n* C compiler (GCC 4.7+, Visual Studio 2013)\n* CMake (3.6+) - http://cmake.org/cmake/resources/software.html\n* Git http://git-scm.com/download\n* luasandbox (1.2+) https://github.com/mozilla-services/lua_sandbox\n* Module specific (i.e. if building the ssl module openssl will be required)\n\n#### Optional (used for documentation)\n* gitbook (2.3) - https://www.gitbook.com/\n* lua (5.1) - https://www.lua.org/download.html\n\n### CMake Build Instructions\n\n    git clone https://github.com/mozilla-services/lua_sandbox_extensions.git\n    cd lua_sandbox_extensions\n    mkdir release\n    cd release\n\n    # UNIX\n    cmake -DCMAKE_BUILD_TYPE=release -DENABLE_ALL_EXT=true -DCPACK_GENERATOR=TGZ ..\n    # or cherry pick using -DEXT_xxx=on i.e. -DEXT_lpeg=on (specifying no\n    # extension will provide a list of all available extensions)\n    make\n    ctest\n    make packages\n\n    # Windows Visual Studio 2013\n    cmake -DCMAKE_BUILD_TYPE=release -G \"NMake Makefiles\" -DEXT_lpeg=on ..\n    nmake\n    ctest\n    nmake packages\n\n### Docker Images\n\nDocker images [are built](https://hub.docker.com/r/mozilla/lua_sandbox_extensions/) from\nthe `dev` and `main` branches. These images contain [hindsight](https://github.com/mozilla-services/hindsight),\n[lua_sandbox](https://github.com/mozilla-services/lua_sandbox) and have all of the extensions\nfrom this repository installed.\n\n    # Get main branch docker image\n    docker pull mozilla/lua_sandbox_extensions:main\n\n    # Get dev branch docker image\n    docker pull mozilla/lua_sandbox_extensions:dev\n\n## Releases\n\n* The main branch is the current release and is considered stable at all\n  times.\n* New versions can be released as frequently as every two weeks (our sprint\n  cycle). The only exception would be for a high priority patch.\n* All active work is flagged with the sprint milestone and tracked in the\n  project dashboard.\n* New releases occur the day after the sprint finishes.\n  * The version in the dev branch is updated\n  * The changes are merged into main\n  * A new tag is created\n\n## Contributions\n\n* All pull requests must be made against the dev branch, direct commits to\n  main are not permitted.\n* All non trivial contributions should start with an issue being filed (if it is\n  a new feature please propose your design/approach before doing any work as not\n  all feature requests are accepted).\n\n# Heka Sandbox\n\n## Decoder API Convention\n\nEach decoder module should implement a decode function according to the\nspecification below. Also, if the decoder requires configuration options it\nshould look for a table, in the cfg, with a variable name matching the module\nname (periods replaced by underscores \"decoders.foo\" -> \"decoders_foo\"). This\nnaming convention only allows for a single instance of each decoder per sandbox\ni.e., if you need to parse more than one type of Nginx access log format you\nshould use multiple input sandboxes (one for each).\n\n### decode\n\nThe decode function should parse, decode, and/or transform the original data and\ninject one or more Heka messages into the system.\n\n*Arguments*\n- data (string) - Raw data from the input sandbox that needs\n  parsing/decoding/transforming\n- default_headers (table/nil/none) - Heka message table containing the default\n  header values to use, if they are not populated by the decoder. If 'Fields'\n  is specified it should be in the hashed based format see:\n  http://mozilla-services.github.io/lua_sandbox/heka/message.html. In the case\n  of multiple decoders this may be the message from the previous input/decoding\n  step.\n- mutable (bool/nil/none) - Flag indicating if the decoder can modify the\n  default_headers/msg structure in place or if it has to be copied first.\n\n*Return*\n- err (nil, string) or throws an error on invalid data or an inject message\n  failure\n    - nil - if the decode was successful\n    - string - error message if the decode failed (e.g. no match)\n\n## Encoder API Convention\n\nEach encoder module should implement an encode function according to the\nspecification below. Also, if the encoder requires configuration options it\nshould look for a table, in the cfg, with a variable name matching the module\nname (periods replaced by underscores \"encoders.foo\" -> \"encoders_foo\").\n\n### encode\n\nThe encode function should concatenate, encode, and/or transform the Heka\nmessage into a byte array.\n\n*Arguments*\n- none\n\n*Return*\n- data (string, userdata, nil)\n    - string - raw data ready to be output\n    - userdata - a userdata object that supports the lua_sandbox zero copy API\n    - nil - the output sandbox should skip the message and return -2\n    - error - throws an error on an invalid transformation or incompatible\n      userdata\n"
},
{
  "name": "lua_sandbox",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      ".travis.yml",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "README.md",
      "build",
      "cmake",
      "covfn.txt",
      "docs",
      "gen_gh_pages.lua",
      "include",
      "src"
    ],
    "/docs": [
      "cli",
      "heka",
      "index.md",
      "sandbox.md",
      "util"
    ]
  },
  "makefile": null,
  "readme": "# Lua Sandbox Library\n\n## Overview\n\nSandboxes provide a dynamic and isolated execution environment\nfor data parsing, transformation, and analysis.  They allow access to data\nwithout jeopardizing the integrity or performance of the processing\ninfrastructure. This broadens the audience that the data can be\nexposed to and facilitates new uses of the data (i.e. debugging, monitoring,\ndynamic provisioning,  SLA analysis, intrusion detection, ad-hoc reporting,\netc.)\n\nThe Lua sandbox is a library allowing customized control over the Lua execution\nenvironment including functionality like global data preservation/restoration on\nshutdown/startup, output collection in textual or binary formats and an array of\nparsers for various data types (Nginx, Apache, Syslog, MySQL and many RFC grammars)\n\nThese libraries and utilities have been mostly extracted from\n[Hindsight](https://github.com/mozilla-services/hindsight). The goal was to\ndecouple the Heka/Hindsight functionality from any particular infrastructure and\nmake it embeddable into any tool or language.\n\n### Features\n\n- small - memory requirements are as little as 8 KiB for a basic sandbox\n- fast - microsecond execution times\n- stateful - ability to resume where it left off after a restart/reboot\n- isolated - failures are contained and malfunctioning sandboxes are terminated.\n  Containment is defined in terms of restriction to the operating system,\n  file system, libraries, memory use, Lua instruction use, and output size.\n\n[Full Documentation](http://mozilla-services.github.io/lua_sandbox)\n\n## Installation\n\n### Prerequisites\n* C compiler (GCC 4.7+, Visual Studio 2013)\n* CMake (3.0+) - http://cmake.org/cmake/resources/software.html\n* Git http://git-scm.com/download\n\n#### Optional (used for documentation)\n* Graphviz (2.28.0) - http://graphviz.org/Download..php\n* Doxygen (1.8.11+) - http://www.stack.nl/~dimitri/doxygen/download.html#latestsrc\n* gitbook (2.3) - https://www.gitbook.com/\n* lua (5.1) - https://www.lua.org/download.html\n\n### CMake Build Instructions\n\n    git clone https://github.com/mozilla-services/lua_sandbox.git\n    cd lua_sandbox\n    mkdir release\n    cd release\n\n    # UNIX\n    cmake -DCMAKE_BUILD_TYPE=release ..\n    make\n\n    # Windows Visual Studio 2013\n    cmake -DCMAKE_BUILD_TYPE=release -G \"NMake Makefiles\" ..\n    nmake\n\n    ctest\n    cpack -G TGZ # (DEB|RPM|ZIP)\n\n## Releases\n\n* The main branch is the current release and is considered stable at all\n  times.\n* New versions can be released as frequently as every two weeks (our sprint\n  cycle). The only exception would be for a high priority patch.\n* All active work is flagged with the sprint milestone and tracked in the\n  project dashboard.\n* New releases occur the day after the sprint finishes.\n  * The version in the dev branch is updated\n  * The changes are merged into main\n  * A new tag is created\n\n## Contributions\n\n* All pull requests must be made against the dev branch, direct commits to\n  main are not permitted.\n* All non trivial contributions should start with an issue being filed (if it is\n  a new feature please propose your design/approach before doing any work as not\n  all feature requests are accepted).\n"
},
{
  "name": "hindsight",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "benchmarks",
      "cmake",
      "docker_push.sh",
      "docs",
      "gen_gh_pages.lua",
      "hindsight.cfg",
      "src",
      "util"
    ],
    "/docs": [
      "architecture.md",
      "configuration.md",
      "hindsight_cli.md",
      "hindsight_data_flow.png",
      "hindsight_timer_report.md",
      "index.md",
      "performance.md",
      "tutorials"
    ]
  },
  "makefile": null,
  "readme": "# Hindsight\n\n## Overview\n\nHindsight is a C based data processing infrastructure based on the\n[lua sandbox](https://github.com/mozilla-services/lua_sandbox) project.  I have\nreceived several inquiries about a lighter weight and faster data pipeline with\ndelivery guarantees to replace [Heka](https://github.com/mozilla-services/heka).\nHindsight is that light weight skeleton around the same lua sandbox offering\n'at least once' delivery semantics. The skeleton is supplemented by\n[extension packages](https://mozilla-services.github.io/lua_sandbox_extensions)\nincluding hundreds of data structures, algorithms, plugins, parsers and\ngrammars. The extensions repository is where most of the active development is\nhappening now as the core infrastructure (Hindsight and the [Lua Sandbox](https://github.com/mozilla-services/lua_sandbox))\nis stable and changes infrequently.  There is also a [Hindsight Administration UI](https://github.com/mozilla-services/hindsight_admin)\navailable for monitoring, debugging and plugin management (you can check out a\nrunning instance here: [hsadmin](https://hsadmin.trink.com/))\n\n* [Full Documentation](http://mozilla-services.github.io/hindsight)\n* Support\n    * Chat: [Matrix](https://chat.mozilla.org/#/room/#hindsight:mozilla.org)\n    * Mailing list: https://mail.mozilla.org/listinfo/hindsight\n\n## Build\n\n### Prerequisites\n\n* Clang 3.1 or GCC 4.7+\n* CMake (3.6+) - http://cmake.org/cmake/resources/software.html\n* lua_sandbox (1.2.3+) - https://github.com/mozilla-services/lua_sandbox\n* OpenSSL (1.0.x+, optional)\n\n### CMake Build Instructions\n\n    git clone https://github.com/mozilla-services/hindsight.git\n    cd hindsight\n    mkdir release\n    cd release\n\n    # Linux\n    cmake -DCMAKE_BUILD_TYPE=release ..\n    make\n    ctest\n    cpack -G TGZ # (DEB|RPM|ZIP)\n\n    # Cross platform support is planned but not supported yet\n\nBy default hindsight is linked against OpenSSL and configured to set locking callbacks in the\nlibrary to ensure proper threaded operation. If this functionality is not desired the cmake\nbuild option `-DWITHOUT_OPENSSL=true` can be used to disable this, for example if you are not\nusing any sandboxes/modules that make use of OpenSSL and do not want the dependency.\n\n## Releases\n\n* The main branch is the current release and is considered stable at all\n  times.\n* New versions can be released as frequently as every two weeks (our sprint\n  cycle). The only exception would be for a high priority patch.\n* All active work is flagged with the sprint milestone and tracked in the\n  project dashboard.\n* New releases occur the day after the sprint finishes.\n  * The version in the dev branch is updated\n  * The changes are merged into main\n  * A new tag is created\n\n## Docker Images\n\n[Docker images](https://hub.docker.com/r/mozilla/hindsight/tags) are constructed from the\nmain and dev branches and can be pulled, or built using the Dockerfile.\n\nNote that the Docker image built here is only a bare bones image containing just lua_sandbox\nand hindsight. For a more full featured image that also contains all of the extensions, see\nthe Docker image for the [extensions](https://github.com/mozilla-services/lua_sandbox_extensions)\nrepo.\n\n## Contributions\n\n* All pull requests must be made against the dev branch, direct commits to\n  main are not permitted.\n* All non trivial contributions should start with an issue being filed (if it is\n  a new feature please propose your design/approach before doing any work as not\n  all feature requests are accepted).\n"
},
{
  "name": "hindsight_admin",
  "files": {
    "/": [
      ".travis.yml",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "artifact_push.sh",
      "cmake",
      "css",
      "docker_push.sh",
      "external",
      "hindsight_admin_server.xml",
      "resource_bundle",
      "resources",
      "run.sh",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Hindsight Administration User Interface\n\n## Overview\n\nPrototype administration user interface for Hindsight. Live demo:\nhttps://hsadmin.trink.com/\n\n## Build\n\n### Prerequisites\n\n* GCC (4.8+)\n* CMake (3.5+) - http://cmake.org/cmake/resources/software.html\n* Boost (1.65.1) - http://www.boost.org/users/download/\n* Wt (3.3.9) - http://www.webtoolkit.eu/wt/download\n* sqlite (3.11+) - https://sqlite.org/\n\n### Runtime requirements\n\n* source-highlight (3.1+) - https://www.gnu.org/software/src-highlite/source-highlight.html\n\n### CMake Build Instructions (Linux only)\n\n    git clone https://github.com/mozilla-services/hindsight_admin.git\n    cd hindsight_admin\n    mkdir release\n    cd release\n    cmake -DCMAKE_BUILD_TYPE=release ..\n    make\n\n    # to run the web server locally\n    # update the hindsight_admin_server.xml with your configuration keys\n    cpack -G DEB\n    sudo dpkg -i hindsight-admin-0.0.1-Linux.deb\n    /usr/share/hindsight_admin/run.sh\n    # http://localhost:2020/\n\n## Releases\n\n* The main branch is the current release and is considered stable at all\n  times.\n* New versions can be released as frequently as every two weeks (our sprint\n  cycle). The only exception would be for a high priority patch.\n* All active work is flagged with the sprint milestone and tracked in the\n  project dashboard.\n* New releases occur the day after the sprint finishes.\n  * The version in the dev branch is updated\n  * The changes are merged into main\n  * A new tag is created\n\n## Contributions\n\n* All pull requests must be made against the dev branch, direct commits to\n  main are not permitted.\n* All non trivial contributions should start with an issue being filed (if it is\n  a new feature please propose your design/approach before doing any work as not\n  all feature requests are accepted).\n"
},
{
  "name": "autograph-edge",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Makefile",
      "README.md",
      "autograph-edge.yaml",
      "client.go",
      "client_test.go",
      "context.go",
      "docker-compose.yml",
      "go.mod",
      "go.sum",
      "handlers.go",
      "handlers_test.go",
      "integration_test",
      "main.go",
      "main_test.go",
      "middleware.go",
      "mock_main",
      "version.json"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "GO := GO111MODULE=on go\n\nall: lint vet test install\n\ninstall:\n\t$(GO) install .\ntest:\n\tMOCK_AUTOGRAPH_CALLS=1 $(GO) test -v -count=1 -covermode=count -coverprofile=coverage.out .\nshowcoverage: test\n\t$(GO) tool cover -html=coverage.out\nlint:\n\tgolint *.go\nvet:\n\t$(GO) vet *.go\n",
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla-services/autograph-edge.svg?style=svg)](https://circleci.com/gh/mozilla-services/autograph-edge)\n\n[![Coverage Status](https://coveralls.io/repos/github/mozilla-services/autograph-edge/badge.svg?branch=main)](https://coveralls.io/github/mozilla-services/autograph-edge?branch=main)\n\nAutograph edge\n==============\n\nThis is a small webapp that provides a public endpoint to autograph,\nwithout exposing the entire service to the internet. It only supports XPI and\nAPK signing, and provides fine grained access control to only give clients the\nability to sign a given apk or xpi.\n\nClient are expected to use curl - or similar - to interact with the webapp. An\nunsigned file is submitted to the `/sign/` endpoint along with an authorization\nclient_token. The HTTP response contains the signed file.\n\n```bash\ncurl -F \"input=@/tmp/unsigned.apk\" -o /tmp/signed.apk \\\n    -H \"Authorization: <secret token>\" \\\n    https://autograph-edge.example.com/sign\n```\n\nConfiguration\n-------------\n\n\nThe yaml file `autograph-edge.yaml` the location of the autograph server in\n`url` and a list of authorizations.\n\n```yaml\nauthorizations:\n    - client_token: c4180d2963fffdcd1cd5a1a343225288b964d8934b809a7d76941ccf67cc8547\n      addonid: myaddon@allizom.org\n      user: alice\n      key: fs5wgcer9qj819kfptdlp8gm227ewxnzvsuj9ztycsx08hfhzu\n      signer: extensions-ecdsa\n```\n\nEach authorization has a `client_token` that clients send in their `Authorization` HTTP\nheaders.\n\nThe authorization also has a `user`, `key` and `signer` that are used to call\nautograph (therefore these configuration items must come from the autograph\nconfig).\n\nIf the authorization is for an add-on, it must also contain an `addonid`, which\nis the ID of the add-on being signed. It can also include the optional params:\n\n* `addonpkcs7digest`, a string of the PKCS7 digest algorithm to use\n  (`\"SHA1\"` or `\"SHA256\"`). Defaults to `\"SHA1\"`.\n* `addoncosealgorithms`, an array of strings for COSE Algorithms to\n  sign the addon with. Defaults to an empty list [].\n\nThe sample configuration file in this repository can get you started.\n\n\nNote that the client_token must be longer than 60 characters. You should use `openssl\nrand -hex 32` to generate it.\n"
},
{
  "name": "ip-reputation-js-client",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".eslintrc.js",
      ".gitignore",
      ".nsprc",
      "CHANGELOG",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Gruntfile.js",
      "LICENSE",
      "README.md",
      "docker-compose.yml",
      "grunttasks",
      "lib",
      "package-lock.json",
      "package.json",
      "scripts",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "## iprepd (IP Reputation Service) node.js client library\n\nClient library to send object reputations to [the iprepd service](https://github.com/mozilla-services/iprepd).\n\n[![npm version](https://badge.fury.io/js/ip-reputation-js-client.svg)](https://www.npmjs.com/package/ip-reputation-js-client) [![Coverage Status](https://coveralls.io/repos/github/mozilla-services/ip-reputation-js-client/badge.svg?branch=main)](https://coveralls.io/github/mozilla-services/ip-reputation-js-client?branch=main) [![CircleCI](https://circleci.com/gh/mozilla-services/ip-reputation-js-client/tree/main.svg?style=svg)](https://circleci.com/gh/mozilla-services/ip-reputation-js-client/tree/main)\n\n### Overview\n\niprepd is a service that supports storing and retrieving reputations associated with various object\ntypes, the most common being IP addresses but including others such as account names and email\naddresses. This library can be used by Node applications to integrate directly with this API.\n\n### Usage\n\n#### Functions\n\nCreate a client:\n\n```js\nconst IPReputationClient = require('ip-reputation-service-client-js')\n\nconst client = new IPReputationClient({\n    serviceUrl: 'http://<iprepd service host without trailing slash>',\n    id: '<a hawk ID>',\n    key: '<a hawk key>',\n    timeout: <number in ms>\n})\n```\n\nGet the reputation for an IP:\n\n```js\nclient.getTyped('ip', '127.0.0.1').then(function (response) {\n    if (response && response.statusCode === 404) {\n        console.log('No reputation found for 127.0.0.1');\n    } else {\n        console.log('127.0.0.1 has reputation: ', response.body.reputation);\n    }\n});\n```\n\nSet the reputation for an IP:\n\n```js\nclient.updateTyped('ip', '127.0.0.1', 79).then(function (response) {\n    console.log('Set reputation for 127.0.0.1 to 79.');\n});\n```\n\nRemove an IP:\n\n```js\nclient.removeTyped('ip', '127.0.0.1').then(function (response) {\n    console.log('Removed reputation for 127.0.0.1.');\n});\n```\n\nSend a violation for an IP:\n\n```js\nclient.sendViolationTyped('ip', '127.0.0.1', 'exceeded-password-reset-failure-rate-limit').then(function (response) {\n    console.log('Applied violation to 127.0.0.1.');\n});\n```\n\n#### Legacy functions\n\nPrevious versions of iprepd only supported IP addresses; these functions remain as a compatibility\nlayer for applications that still make use of them, and are essentially wrappers around the typed\nfunction calls.\n\nGet the reputation for an IP:\n\n```js\nclient.get('127.0.0.1').then(function (response) {\n    if (response && response.statusCode === 404) {\n        console.log('No reputation found for 127.0.0.1');\n    } else {\n        console.log('127.0.0.1 has reputation: ', response.body.reputation);\n    }\n});\n```\n\nSet the reputation for an IP:\n\n```js\nclient.update('127.0.0.1', 79).then(function (response) {\n    console.log('Set reputation for 127.0.0.1 to 79.');\n});\n```\n\nRemove an IP:\n\n```js\nclient.remove('127.0.0.1').then(function (response) {\n    console.log('Removed reputation for 127.0.0.1.');\n});\n```\n\nSend a violation for an IP:\n\n```js\nclient.sendViolation('127.0.0.1', 'exceeded-password-reset-failure-rate-limit').then(function (response) {\n    console.log('Applied violation to 127.0.0.1.');\n});\n```\n\n### Development\n\nTests run against [the iprepd service](https://github.com/mozilla-services/iprepd) with [docker-compose](https://docs.docker.com/compose/) from the ip-reputation-js-client repo root:\n\n1. Install [docker](https://docs.docker.com/install/) and [docker-compose](https://docs.docker.com/compose/install/)\n1. Run `docker-compose build`.\n1. Run `docker-compose run --rm test npm install` to collect package dependencies.\n1. Run `docker-compose run --rm test` to test.\n1. Open `coverage/lcov-report/index.html` to see the coverage report\n1. Run `docker-compose down` when you are finished running tests to remove cache and web containers.\n"
},
{
  "name": "GitHub-Audit",
  "files": {
    "/": [
      ".gitignore",
      ".pre-commit-config.yaml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "docs",
      "extract_service_results.py",
      "get_branch_protections.py",
      "moz_scripts",
      "poetry.lock",
      "produce-report.sh",
      "protection_report",
      "pyproject.toml",
      "report_branch_status.py",
      "show_all_terms",
      "term_search.py"
    ],
    "/docs": [
      "README.md",
      "checklist.md",
      "graph-1.mermaid",
      "graph-1.mermaid.svg",
      "graph-2.mermaid",
      "graph-2.mermaid.svg",
      "graph.md",
      "threat.md"
    ]
  },
  "makefile": null,
  "readme": "# GitHub-Audit\n\nReport on GitHub organizations and repositories for adherence to\n[Mozilla's Guidelines for Sensitive Repositories][guidelines_url]\n(additional [background][background_url]).\n\n<!-- I hope to do this in future, so leaving as template for now\n[![Build Status][travis-image]][travis-url]\n[![Downloads Stats][npm-downloads]][npm-url]\n-->\n\nGitHub-Audit is a set of scripts which can be used to query various\naspects of an organization or repository.\n\nThese scripts are intended to be usable both from the command line (CLI)\nand via automation (using 12 Factor principles whenever possible).\n\n\n## Installation\n\nFor now, users should clone the repository, and install the requirements\nusing [``poetry``][poetry_url]:\n\n```sh\ngit clone https://GitHub.com/Mozilla-Services/GitHub-Audit\ncd GitHub-Audit\npoetry install\n```\n\n\n## Usage example\n\nNOTE: run all scripts in the virtual environment created by poetry. From\nwithin the checkout, either activate the virtualenv:\n\n```sh\n$ poetry shell\n$ # run scripts\n$ exit  # deactivate virtual env\n```\nOr run each script within the virtual env:\n\n```sh\n$ poetry run {script}\n```\n\nAll scripts should respond to the ``--help`` option. Additional options\nare often described there.\n\n### Docker\n\nUsing docker to produce CSV output:\n\n```\n$ docker build -t audit .\n$ docker run -e GITHUB_TOKEN -e GITHUB_ORG=mozilla-services audit\n```\n\n### Checks via API\n\nThese checks require a PAT token available. The PAT\ntoken should be on the second line of a file named ``.credentials`` in\nthe current directory (s/a #3).\n\nEach of the scripts below supports a ``--help`` option. Use that for\nadditional information on invoking each script.\n\n- ``get_branch_protections.py`` * to extract the information about\n  protected branches. Outputs JSON file, which\n  ``report_branch_status.py`` can summarize to csv. Import that into a\n  spreadsheet, and play.\n\n- ``show_all_terms`` is a wrapper script around ``term_search.py``. It\n  makes local shallow clones of repos that match, and uses ``rg`` to\n  search for additional occurances. Use the ``--help`` option.\n\n- ``term_search.py`` search orgs or repos for a specific term, such as\n  an API token name. Outputs list of repos that do have the term (per\n  GitHub's index, which can be out of date).\n\n_For more examples and usage, please refer to the [Wiki][wiki]._\n\n## Development setup\n\n### Prerequisites\n\nThis project uses [Black][black_url] to format all python code. A\n``.pre-commit-config.yaml`` file is included, and use of the\n[pre-commit][pre_commit_url] is recommended.\n\nTo ready your environment for development, do:\n```sh\npoetry install --dev\npre-commit install\n```\n\n## Release History\n\nSee [Changes]\n\n## License\n\nDistributed under the Mozilla Public License, version 2 (MPL-2) license. See ``LICENSE`` for more information.\n\n## Contributing\n\n1. Discuss any new feature first by opening an issue.\n1. Fork it (<https://github.com/mozilla-services/GitHub-Audit/fork>)\n1. Clone your fork to your development environment.\n2. Create your feature branch (`git checkout -b feature/fooBar`)\n3. Commit your changes (`git commit -am 'Add some fooBar'`)\n4. Push to the branch (`git push origin feature/fooBar`)\n5. Create a new Pull Request\n\n<!-- Markdown link & img dfn's -->\n[wiki]: https://github.com/mozilla-services/Github-Audit/wiki\n[black_url]: https://black.readthedocs.io/en/stable/index.html\n[pre_commit_url]: https://pre-commit.com/\n[poetry_url]: https://github.com/sdispater/poetry\n[guidelines_url]: https://wiki.mozilla.org/GitHub/Repository_Security\n[background_url]: docs/README.md\n"
},
{
  "name": "go-cose",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".golangci.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "algorithms.go",
      "cbor.go",
      "cbor_test.go",
      "common_headers.go",
      "common_headers_test.go",
      "core.go",
      "core_test.go",
      "errors.go",
      "example",
      "fuzz.go",
      "go.mod",
      "go.sum",
      "helpers_test.go",
      "samples",
      "sign_verify.go",
      "sign_verify_cose_rust_cli_test.go",
      "sign_verify_cose_wg_examples_test.go",
      "sign_verify_test.go"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "\ninstall:\n\t# dev requirements\n\tgo get -u github.com/stretchr/testify/assert\n\n\tmkdir -p test\n\tcd test && git clone https://github.com/cose-wg/Examples.git cose-wg-examples || true\n\tcd test && git clone https://github.com/g-k/cose-rust.git || true\n\tcd test/cose-rust && git checkout test-verify-cli\n\n\ninstall-go-fuzz:\n\t# dev requirement\n\tgo get -u github.com/dvyukov/go-fuzz/...\n\n# sample generated with:\n# for file in $(find . -name *.json); do jq '.output.cbor' < $file | tr -d \\\" | base64 --decode > $(echo $file | sed s/..// | tr '/' '_').cose; done\nfuzz: install-go-fuzz\n\tmkdir -p workdir/corpus\n\tcp samples/*.cose workdir/corpus\n\tgo-fuzz-build go.mozilla.org/cose\n\tgo-fuzz -bin=./cose-fuzz.zip -workdir=workdir\n\nlint:\n\tgolint\n\nvet:\n\tgo vet\n\ncoverage:\n\tgo test -v -cover -race -coverprofile=coverage.out && go tool cover -html=coverage.out\n\nwhat-todo:\n\trg -g '**/*.go' -i TODO\n\ninstall-golint:\n\tgo get -u golang.org/x/lint/golint\n\ngoveralls:\n\tgo get -u github.com/mattn/goveralls\n\nsmoketest-examples:\n\tgo run example/sign.go\n\tgo run example/verify.go\n\nci: install-golint goveralls install coverage lint vet\n\tgoveralls -coverprofile=coverage.out -service=circle-ci -repotoken=$(COVERALLS_TOKEN)\n",
  "readme": "# go-cose\n\n[![CircleCI](https://circleci.com/gh/mozilla-services/go-cose.svg?style=svg)](https://circleci.com/gh/mozilla-services/go-cose) \n[![Coverage Status](https://coveralls.io/repos/github/mozilla-services/go-cose/badge.svg)](https://coveralls.io/github/mozilla-services/go-cose)\n\nA [COSE](https://tools.ietf.org/html/rfc8152) library for go.\n\nIt currently supports signing and verifying the SignMessage type with the ES{256,384,512} and PS256 algorithms.\n\n[API docs](https://godoc.org/go.mozilla.org/cose)\n\n## Usage\n\n### Install\n\n```console\ngo get -u go.mozilla.org/cose\n```\n\n### Signing a message\n\nSee [example/sign.go](example/sign.go) and run it with:\n\n```console\n$ go run example/sign.go\nBit lengths of integers r and s (256 and 256) do not match the key length 255\nMessage signature (ES256): 043685f99421f9e80c7c3c50d0fc8266161d3d614aaa3b63d2cdf581713fca62bb5d2e34d2352dbe41424b31d0b4a11d6b2d4764c18e2af04f4520fbe494d51c\n```\n\n### Verifying a message\n\nSee [example/verify.go](example/verify.go) and run it with:\n\n```console\n$ go run example/verify.go\nBit lengths of integers r and s (256 and 254) do not match the key length 254\nMessage signature (ES256): 9411dc5200c1cb67ccd76424ade09ce89c4a8d8d2b66f2bbf70edf63beb2dc3cbde83250773e659b635d3715442a1efaa6b0c030ee8a2523c3e37a22ddb055fa\nMessage signature verified\n```\n\n## Development\n\nRunning tests:\n\n1. Install [rust and cargo](https://www.rustup.rs/)\n\n1. On OSX: `brew install nss` [nss](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS) then in `sign_verify_cose_rust_cli_test.go` add `NSS_LIB_DIR` to `cmd` or `-L /usr/local/opt/nss/lib` to RUSTFLAGS e.g. `cmd.Env = append(os.Environ(), \"NSS_LIB_DIR=/usr/local/opt/nss/lib\", \"RUSTFLAGS=-A dead_code -A unused_imports\")`\n\n1. If you already have `dep` and `golint` commands installed, run `make install-godep install-golint`\n\n1. Run `go test`\n"
},
{
  "name": "axe-selenium-python",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "MANIFEST.in",
      "README.rst",
      "axe_selenium_python",
      "package-lock.json",
      "package.json",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "axe-selenium-python\n====================\n\naxe-selenium-python integrates aXe and selenium to enable automated web accessibility testing.\n\n**This version of axe-selenium-python is using axe-core@4.0.2.**\n\n.. image:: https://img.shields.io/badge/license-MPL%202.0-blue.svg\n   :target: https://github.com/mozilla-services/axe-selenium-python/blob/master/LICENSE.txt\n   :alt: License\n.. image:: https://img.shields.io/pypi/v/axe-selenium-python.svg\n   :target: https://pypi.org/project/axe-selenium-python/\n   :alt: PyPI\n.. image:: https://img.shields.io/travis/mozilla-services/axe-selenium-python.svg\n   :target: https://travis-ci.org/mozilla-services/axe-selenium-python\n   :alt: Travis\n.. image:: https://img.shields.io/github/issues-raw/mozilla-services/axe-selenium-python.svg\n   :target: https://github.com/mozilla-services/axe-selenium-python/issues\n   :alt: Issues\n.. image:: https://api.dependabot.com/badges/status?host=github&repo=mozilla-services/axe-selenium-python\n   :target: https://dependabot.com\n   :alt: Dependabot\n.. image:: https://coveralls.io/repos/github/mozilla-services/axe-selenium-python/badge.svg?branch=master\n   :target: https://coveralls.io/github/mozilla-services/axe-selenium-python?branch=master\n   :alt: Coveralls\n\n\n\nRequirements\n------------\n\nYou will need the following prerequisites in order to use axe-selenium-python:\n\n- selenium >= 3.0.0\n- Python 2.7 or 3.6\n- The appropriate driver for the browser you intend to use, downloaded and added to your path, e.g. geckodriver for Firefox:\n\n  - `geckodriver <https://github.com/mozilla/geckodriver/releases>`_ downloaded and `added to your PATH <https://stackoverflow.com/questions/40208051/selenium-using-python-geckodriver-executable-needs-to-be-in-path#answer-40208762>`_\n\nInstallation\n------------\n\nTo install axe-selenium-python:\n\n.. code-block:: bash\n\n  $ pip install axe-selenium-python\n\n\nUsage\n------\n\n.. code-block:: python\n\n  from selenium import webdriver\n  from axe_selenium_python import Axe\n\n  def test_google():\n      driver = webdriver.Firefox()\n      driver.get(\"http://www.google.com\")\n      axe = Axe(driver)\n      # Inject axe-core javascript into page.\n      axe.inject()\n      # Run axe accessibility checks.\n      results = axe.run()\n      # Write results to file\n      axe.write_results(results, 'a11y.json')\n      driver.close()\n      # Assert no violations are found\n      assert len(results[\"violations\"]) == 0, axe.report(results[\"violations\"])\n\nThe method ``axe.run()`` accepts two parameters: ``context`` and ``options``.\n\nFor more information on ``context`` and ``options``, view the `aXe documentation here <https://github.com/dequelabs/axe-core/blob/master/doc/API.md#parameters-axerun>`_.\n\nContributing\n------------\n\nFork the repository and submit PRs with bug fixes and enhancements;\ncontributions are very welcome.\n\nNode dependencies must be installed by running `npm install` inside the axe-selenium-python directory.\n\nYou can run the tests using\n`tox <https://tox.readthedocs.io/en/latest/>`_:\n\n.. code-block:: bash\n\n  $ tox\n\nResources\n---------\n\n- `Issue Tracker <http://github.com/mozilla-services/axe-selenium-python/issues>`_\n- `Code <http://github.com/mozilla-services/axe-selenium-python/>`_\n- `pytest-axe <http://github.com/mozilla-services/pytest-axe/>`_\n\nCHANGELOG\n^^^^^^^^^^^^^^\n\nversion 2.1.5\n*************\n**Breaks backwards compatibility**:\n\n- The Axe class method ``execute`` has been renamed to ``run`` to mirror the method in the axe-core API.\n\nversion 2.1.0\n**************\n- Created package.json file to maintain axe-core dependency\n- Replaced unit tests with more meaningful integration tests\n  - included a sample html file for integration tests\n\nversion 2.0.0\n**************\n- All functionalities that are not part of axe-core have been moved into a separate package, ``pytest-axe``. This includes:\n\n  - ``run_axe`` helper method\n  - ``get_rules`` Axe class method\n  - ``run`` Axe class method\n  - ``impact_included`` Axe class method\n  - ``analyze`` Axe class method.\n\nThe purpose of this change is to separate implementations that are specific to the Mozilla Firefox Test Engineering team, and leave the base ``axe-selenium-python`` package for a more broad use case. This package was modeled off of Deque's Java package, axe-selenium-java, and will now more closely mirror it.\n\nAll functionalities can still be utilized when using ``axe-selenium-python`` in conjunction with ``pytest-axe``.\n\nversion 1.2.3\n**************\n- Added the analyze method to the Axe class. This method runs accessibility checks, and writes the JSON results to file based on the page URL and the timestamp.\n- Writing results to file can be enabled by setting the environment variable ``ACCESSIBILITY_REPORTING=true``. The files will be written to ``results/`` directory, which must be created if it does not already exist.\n- Accessibility checks can be disabled by setting the environment variable ``ACCESSIBILITY_DISABLED=true``.\n\nversion 1.2.1\n**************\n- Updated axe to ``axe-core@2.6.1``\n- Modified impact_included class method to reflect changes to the aXe API:\n- There are now only 3 impact levels: 'critical', 'serious', and 'minor'\n\nversion 1.0.0\n**************\n- Updated usage examples in README\n- Added docstrings to methods lacking documentation\n- Removed unused files\n\nversion 0.0.3\n**************\n- Added run method to Axe class to simplify the usage in existing test suites\n- run method includes the ability to set what impact level to test for: 'minor', 'moderate', 'severe', 'critical'\n\nversion 0.0.28\n****************\n- Added selenium instance as a class attribute\n- Changed file paths to OS independent structure\n- Fixed file read operations to use with keyword\n\n\nversion 0.0.21\n***************\n- Fixed include of aXe API file and references to it\n- Updated README\n"
},
{
  "name": "common-rs",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".clog.toml",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "actix-web-location",
      "mozsvc-common",
      "tracing-actix-web-mozlog"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![License: MPL 2.0]][mpl 2.0] [![Build Status]][circleci]\n\n[license: mpl 2.0]: https://img.shields.io/badge/License-MPL%202.0-blue.svg\n[mpl 2.0]: https://opensource.org/licenses/MPL-2.0\n[build status]:\n  https://img.shields.io/circleci/build/github/mozilla-services/common-rs\n[circleci]: https://app.circleci.com/pipelines/github/mozilla-services/common-rs\n\nA common set of utilities for Mozilla server side applications.\n\n---\n\n- [`mozsvc-common`](mozsvc-common)\n  [![version-badge::mozsvc-common]][crates.io::mozsvc-common]\n  [![rustdoc-badge::mozsvc-common]][docs::mozsvc-common]\n\n  Some generically helpful utilities. This library should be considered\n  deprecated.\n\n[version-badge::mozsvc-common]:\n  https://img.shields.io/crates/v/mozsvc-common.svg\n[crates.io::mozsvc-common]: https://crates.io/crates/mozsvc-common\n[docs::mozsvc-common]: https://docs.rs/mozsvc-common\n[rustdoc-badge::mozsvc-common]: https://img.shields.io/docsrs/mozsvc-common\n\n- [`tracing-actix-web-mozlog`](tracing-actix-web-mozlog)\n  [![version-badge::tracing-actix-web-mozlog]][crates.io::tracing-actix-web-mozlog]\n  [![rustdoc-badge::tracing-actix-web-mozlog]][docs::tracing-actix-web-mozlog]\n\n  Integration between Tracing, Actix, and MozLog.\n\n[version-badge::tracing-actix-web-mozlog]:\n  https://img.shields.io/crates/v/tracing-actix-web-mozlog.svg\n[crates.io::tracing-actix-web-mozlog]:\n  https://crates.io/crates/tracing-actix-web-mozlog\n[docs::tracing-actix-web-mozlog]: https://docs.rs/tracing-actix-web-mozlog\n[rustdoc-badge::tracing-actix-web-mozlog]:\n  https://img.shields.io/docsrs/tracing-actix-web-mozlog\n\n- [`actix-web-location`](actix-web-location)\n  [![version-badge::actix-web-location]][crates.io::actix-web-location]\n  [![rustdoc-badge::actix-web-location]][docs::actix-web-location]\n\n  A extensible crate to provide location determination for actix-web, using\n  GeoIP or other techniques.\n\n[version-badge::actix-web-location]:\n  https://img.shields.io/crates/v/actix-web-location.svg\n[crates.io::actix-web-location]: https://crates.io/crates/actix-web-location\n[docs::actix-web-location]: https://docs.rs/actix-web-location\n[rustdoc-badge::actix-web-location]:\n  https://img.shields.io/docsrs/actix-web-location\n"
},
{
  "name": "cloud-storage-rs",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".rustfmt.toml",
      "CHANGELOG.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "src",
      "test.sh"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Cloud Storage\n\n[![cloud-storage-rs on crates.io](https://img.shields.io/crates/v/cloud-storage.svg)](https://crates.io/crates/cloud-storage)\n[![stripe-rust on docs.rs](https://docs.rs/cloud-storage/badge.svg)](https://docs.rs/cloud-storage)\n\nA library that can be used to push blobs to [Google Cloud Storage](https://cloud.google.com/storage/), and then generate download links to those files.\n### Usage\nAdd the following line to your Cargo.toml\n```toml\n[dependencies]\ncloud-storage = \"0.10\"\n```\n### Examples\n```rust\n// create a new Bucket\nlet new_bucket = NewBucket { name: \"mybucket\", ..Default::default() }\nlet bucket = Bucket::create(new_bucket).await?;\n// upload a file to our new bucket\nlet content = b\"Your file is now on google cloud storage!\";\nbucket.upload(content, \"folder/filename.txt\", \"application/text\").await?;\nlet mut object = Object::create(\"mybucket\", content, \"folder/filename.txt\", \"application/text\").await?;\n// let's copy the file\nobject.copy(\"mybucket2: electric boogaloo\", \"otherfolder/filename.txt\").await?;\n// print a link to the file\nprintln!(\"{}\", object.download_url(1000)); // download link for 1000 seconds\n// remove the file from the bucket\nobject.delete().await?;\n```\n\nAuthorization can be granted using the `SERVICE_ACCOUNT` or `GOOGLE_APPLICATION_CREDENTIALS` environment variable, which should contain path to the `service-account-*******.json` file that contains the Google credentials. Alternatively, the service account credentials can be provided as JSON directly through the `SERVICE_ACCOUNT_JSON` or `GOOGLE_APPLICATION_CREDENTIALS_JSON` environment variable, which is useful when providing secrets in CI or k8s.\n\nThe service account should also have the roles `Service Account Token Creator` (for generating access tokens) and `Storage Object Admin` (for generating sign urls to download the files).\n\n### Sync\nIf you're not (yet) interested in running an async executor, then `cloud_storage` exposes a sync api. To use it, enable the feature flag `sync`, and then call instead of calling `function().await`, call `function_sync()`.\n\nYou will need to set both the `global-client` and `sync` flags in your Cargo.toml, for example:\n\n```\ncloud-storage = { version = \"0.11.0\", features = [\"global-client\", \"sync\"] }\n```\n\n### Testing\nTo run the tests for this project, first create an enviroment parameter (or entry in the .env file) named TEST_BUCKET. Make sure that this name is not already in use! The tests will create this bucket for its testing purposes. It will also create a couple of other buckets with this name as prefix, but these will be deleted again. Next, you will need a Google Cloud Storage project, for which you must create a service account. Download the service-account.json file and place the path to the file in the `SERVICE_ACCOUNT` environment parameter. Then, run\n```bash\nsh test.sh\n```\nThe `test-threads=1` is necessary so that the tests don't exceed the 2 per second bucket creating rate limit. (Depending on your internet speed, you may be able to use more than 1 test thread)\n"
},
{
  "name": "skeleton",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CONTRIBUTING.md",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "docker-compose.yaml.example",
      "rust-toolchain",
      "src",
      "test.file",
      "tools",
      "version.json"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# New Rust Project Skeleton\n\nRust requires a few bits of setup. These are files that are fairly\nhelpful for that.\n\nIf you've not done so, you should install rust. the `./tools`\ndirectory has scripts that can help.\n\n* *`rustup_install.sh`* is a copy of the rustup setup script, taken\n  from `curl https://sh.rustup.rs`.\n\n* *`rust_setup.sh`* updates rust. It also has comments which can\n  include other very helpful modules like clippy and wasm.\n\n\nTODO: See new project doc.\n\nFRESHNESS DATE:\n\nBest if used or renewed by JUL 2021\n\n\n---\n\n# ProjectName\n\nOne sentence description for outside people.\n\n## What is it?\n\nA brief discussion of the project, with low jargon, that can be read\nby someone at the VP level who is short on time and understand what\nthis project is about. It's also useful for folks outside the group\nwho might be interested and might want to help you out.\n\n## Requirements\n\nIf this project has any external requirements (e.g. a Database,\nspecific logging, compiler versions, account permissions, etc.) note\nthose. You may need to point to locations or additional instructions.\n\n## Setting Up\n\nList steps to set up a development environment (if needed). Include\nany special one-time instructions if required.\n\n\nNote: When picking a project name, it's best to find a name that\nsomeone outside your team or knowledge space can easily understand.\nThis can be useful for high level executives and decision makers who\nmay need to be following dozens of projects, as well as folks looking\nfor interesting projects to work on. Boring names are ok, if they\nclearly convey meaning.\n\ne.g. \"LabtharGyroHammer\" is clever, but coveys little meaning.\nMysqlAuth_Rust conveys far more information and will be less likely to\nbe forgotten.\n\nThink of your project name is a variable name.\n\n"
},
{
  "name": "fernet-rs",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "src",
      "tests"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "fernet-rs\n=========\n\n[![dependency status](https://deps.rs/repo/github/mozilla-services/fernet-rs/status.svg)](https://deps.rs/repo/github/mozilla-services/fernet-rs)\n\nAn implementation of [fernet](https://github.com/fernet/spec) in Rust.\n\nWhat is Fernet?\n---------------\n\nFernet is a small library to help you encrypt parcels of data with optional expiry times. It's\ngreat for tokens or exchanging small strings or blobs of data. Fernet is designed to be easy\nto use, combining cryptographic primitives in a way that is hard to get wrong, prevents tampering\nand gives you confidence that the token is legitimate. You should consider this if you need:\n\n* Time limited authentication tokens in URLs or authorisation headers\n* To send small blobs of encrypted data between two points with a static key\n* Simple encryption of secrets to store to disk that can be read later\n* Many more ...\n\nGreat! How do I start?\n----------------------\n\nAdd fernet to your Cargo.toml:\n\n    [dependencies]\n    fernet = \"0.1\"\n\nAnd then have a look at our [API documentation] online, or run \"cargo doc --open\" in your\nproject.\n\n[API documentation online]: https://docs.rs/fernet\n\nTesting Token Expiry\n--------------------\n\nBy default fernet wraps operations in an attempt to be safe - you should never be able to\n\"hold it incorrectly\". But we understand that sometimes you need to be able to do some\nmore complicated operations.\n\nThe major example of this is having your application test how it handles tokens that\nhave expired past their ttl.\n\nTo support this, we allow you to pass in timestamps to the `encrypt_at_time` and\n`decrypt_at_time` functions, but these are behind a feature gate. To activate these\napi's you need to add the following to Cargo.toml\n\n    [dependencies]\n    fernet = { version = \"0.1\", features = [\"fernet_danger_timestamps\"] }\n\n\n"
},
{
  "name": "socorro-submitter",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".editorconfig",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.rst",
      "bin",
      "docker-compose.yml",
      "docker",
      "fakedata_source",
      "requirements-dev.in",
      "requirements-dev.txt",
      "requirements-runtime.txt",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n# Include my.env and export it so variables set in there are available\n# in the Makefile.\ninclude my.env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\nAPP_UID ?= 10001\nAPP_GID ?= 10001\n\nDC := $(shell which docker-compose)\nHOSTUSER := $(shell id -u):$(shell id -g)\n\n.DEFAULT_GOAL := help\n.PHONY: help\nhelp:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@grep -E '^[a-zA-Z0-9_-]+:.*?## .*$$' Makefile \\\n\t    | grep -v grep \\\n\t    | sed -n 's/^\\(.*\\): \\(.*\\)##\\(.*\\)/\\1\\3/p' \\\n\t    | column -t  -s '|'\n\nmy.env:\n\t@if [ ! -f my.env ]; \\\n\t\tthen \\\n\t\techo \"Copying my.env.dist to my.env...\"; \\\n\t\tcp docker/my.env.dist my.env; \\\n\tfi\n\n.container-test:\n\tmake build-containers\n\n.PHONY: build-containers\nbuild-containers: my.env\n\t${DC} build --build-arg userid=${APP_UID} --build-arg groupid=${APP_GID} test\n\ttouch .container-test\n\n.PHONY: build-libs\nbuild-libs: my.env\n\t${DC} run -u \"${HOSTUSER}\" lambda-build bash -c \"cd /tmp && /tmp/bin/run_build.sh\"\n\n.PHONY: build\nbuild: .container-test build-libs  ## | Build Docker images.\n\n.PHONY: clean\nclean:  ## | Remove build, test, and other artifacts.\n\t${DC} rm --stop --force -v\n\t-rm -rf build\n\t-rm .container-*\n\t-rm -rf fakedata_dest\n\n.PHONY: lint\nlint: .container-test  ## | Lint code.\n\t${DC} run test bin/run_lint.sh\n\n.PHONY: lintfix\nlintfix: .container-test  ## | Reformat code.\n\t${DC} run -u \"${HOSTUSER}\" test bin/run_lint.sh --fix\n\n.PHONY: test\ntest: build  ## | Run tests.\n\t${DC} run test pytest -vv\n\n.PHONY: testshell\ntestshell: build  ## | Open shell in test container.\n\t${DC} run test bash\n\n.PHONY: rebuildreqs\nrebuildreqs: .container-test  ## | Update requirements*.txt files.\n\t${DC} run --rm -u \"${HOSTUSER}\" lambda-build bash -c \"/tmp/bin/list_runtime_reqs.sh > /tmp/requirements-runtime.txt\"\n\t${DC} run --rm --no-deps -u \"${HOSTUSER}\" test /app/bin/rebuild_reqs.sh\n",
  "readme": "Submitter\n=========\n\nAWS Lambda function that reacts to S3 object save events for raw crash\nfiles and submits a specified percentage to another environment.\n\n* Free software: Mozilla Public License version 2.0\n* Documentation: this README\n* Bugs: `Report a bug <https://bugzilla.mozilla.org/enter_bug.cgi?format=__standard__&product=Socorro>`_\n* Community Participation Guidelines: `Guidelines <https://github.com/mozilla-services/socorro-submitter/blob/main/CODE_OF_CONDUCT.md>`_\n\n\nDetails\n=======\n\nRaw crash files have keys like this::\n\n  v2/raw_crash/000/20170413/00007bd0-2d1c-4865-af09-80bc00170413\n\n\nThe submitter will \"roll a die\" to decide whether to submit to a specified\nenvironment.\n\nIf so, it'll pull all the raw crash data from S3, package it up into a valid\ncrash report, and HTTP POST it to the collector of the specified destination\nenvironment.\n\n\nQuickstart\n==========\n\n1. `Install docker 18.00.0+ <https://docs.docker.com/install/>`_ and\n   `install docker-compose 1.20.0+ <https://docs.docker.com/compose/install/>`_\n   on your machine.\n\n   You'll also need git, make, and bash.\n\n2. Clone the repo:\n\n   .. code-block:: shell\n\n      $ git clone https://github.com/mozilla-services/socorro-submitter\n\n3. Download and build Submitter Docker images and Python libraries:\n\n   .. code-block:: shell\n\n      $ make build\n\n   Anytime you change requirements files or code, you'll need to run ``make\n   build`` again.\n\n4. Run tests:\n\n   .. code-block:: shell\n\n      $ make test\n\n   You can also get a shell and run them manually:\n\n   .. code-block:: shell\n\n      $ make testshell\n      app@4205495cfa57:/app$ pytest\n      <test output>\n\n   Using the shell lets you run and debug tests more easily.\n\n5. Run the integration test:\n\n   .. code-block:: shell\n\n      $ ./bin/integration_test.sh\n      <test output>\n\n6. Invoke the function with a sample S3 ObjectCreated:Put event:\n\n   .. code-block:: shell\n\n      $ ./bin/generate_event.py --key v2/raw_crash/000/20170413/00007bd0-2d1c-4865-af09-80bc00170413 > event.json\n      $ cat event.json | ./bin/run_invoke.sh\n      <invoke output>\n\n   FIXME -- check fake collector\n\n\nCaveats of this setup\n=====================\n\n1. Because ``submitter.py`` is copied into ``build/`` and that version is tested\n   and invoked, if you edit ``submitter.py``, you need to run ``make build``\n   again. This is kind of annoying when making changes.\n\n2. Packaging the ``.zip`` file and deploying it are not handled by the\n   scaffolding in this repo.\n\n\nScripts\n=======\n\n* FIXME -- fake collector\n\n* ``bin/generate_event.py``: Generates a sample AWS S3 event.\n\n* ``bin/run_invoke.sh``: Invokes the submitter function in a AWS Lambda Python\n  3.8 runtime environment.\n\n* ``bin/integration_test.sh``: Runs an integration test.\n\n* ``bin/run_circle.sh``: The script that Circle CI runs.\n\n* ``bin/release.py``: Used to do releases.\n\n* ``bin/list_runtime_reqs.sh``: Lists installed Python libraries in\n  mlupin/docker-lambda:python3.8-build image.\n\n  Use ``make rebuildreqs`` to run this.\n\n* ``bin/rebuild_reqs.sh``: Rebuilds the ``requirements.txt`` and ``requirements-dev.txt``\n  files from their source ``.in`` files.\n\n  Use ``make rebuildreqs`` to run this.\n\n\nConfiguration\n=============\n\nRequired environment variables:\n\n* ``SUBMITTER_ENV_NAME``: The environment name. This is for tagging metrics with\n  the environment.\n* ``SUBMITTER_THROTTLE``: The percent of crashes to submit; 0 is none, 100 is\n  all.\n* ``SUBMITTER_DESTINATION_URL``: The full url of the collector to post crashes\n  to.\n* ``SUBMITTER_S3_BUCKET``: The s3 bucket to pull crash data from.\n* ``SUBMITTER_S3_REGION_NAME``: The AWS region to use.\n\nThen for local development, you need these:\n\n* ``SUBMITTER_S3_ACCESS_KEY``: The s3 access key to use to access the bucket.\n* ``SUBMITTER_S3_SECRET_ACCESS_KEY``: The s3 secret access key to use to access\n  the bucket.\n* ``SUBMITTER_S3_ENDPOINT_URL``: The endpoint url for the fake s3.\n\nIf any of these are missing from the environment, Submitter will raise a\n``KeyError``.\n\n\nMaintenance\n===========\n\nUpdating requirements ``.txt`` files\n------------------------------------\n\nUpdate versions, add packages, remove packages in the ``.in`` files and then run::\n\n    make rebuildreqs\n\nTo rebuild the ``.txt`` files.\n\nThe one caveat to this is when you update ``pip-tools``. If it's changed the\noutput, then you'll need to::\n\n    make rebuildreqs\n    make build\n    make rebuildreqs\n\n\nRelease process\n===============\n\n1. Create a submitter release bug::\n\n      $ ./bin/release.py make-bug\n\n2. Create a tag using the bug::\n\n      $ ./bin/release.py make-tag --with-bug=NNNNNNN\n\n   Note that this doesn't trigger a deploy--SRE does that.\n\n3. Notify SRE about the bug and ask them to deploy socorro-submitter\n"
},
{
  "name": "userplex",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "config.go",
      "config.yaml",
      "config_test.go",
      "go.mod",
      "go.sum",
      "main.go",
      "modules",
      "notifications"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nPROJECT\t\t:= go.mozilla.org/userplex\nGO\t\t:= GO111MODULE=on GOPROXY=https://proxy.golang.org go\n\nall: test vet install\n\ndev: lint cyclo all\n\ninstall:\n\t$(GO) install $(PROJECT)\n\nvendor:\n\t$(GO) mod vendor\n\ntest:\n\tgo test ./...\n\nlint:\n\tgolint .\n\tgolint modules/...\n\nvet:\n\t$(GO) vet $(PROJECT)\n\ncyclo:\n\tgocyclo -over 15 *.go modules/\n\ndeb-pkg: install\n\trm -rf tmppkg\n\tmkdir -p tmppkg/usr/local/bin\n\tcp $$GOPATH/bin/userplex tmppkg/usr/local/bin/\n\tfpm -C tmppkg -n userplex --license MPL2.0 --vendor mozilla \\\n\t  --description \"Propagate users from Mozilla's Person API to third party systems.\" \\\n\t\t-m \"AJ Bahnken <ajvb@mozilla.com>\" \\\n\t\t--url https://go.mozilla.org/userplex \\\n\t\t--architecture x86_64 \\\n\t\t-v \"$$(git describe --abbrev=0 --tags)\" \\\n\t\t-s dir -t deb .\n\nrpm-pkg: install\n\trm -rf tmppkg\n\tmkdir -p tmppkg/usr/local/bin\n\tcp $$GOPATH/bin/userplex tmppkg/usr/local/bin/\n\tfpm -C tmppkg -n userplex --license MPL2.0 --vendor mozilla \\\n\t  --description \"Propagate users from Mozilla's Person API to third party systems.\" \\\n\t\t-m \"AJ Bahnken <ajvb@mozilla.com>\" \\\n\t\t--url https://go.mozilla.org/userplex \\\n\t\t--architecture x86_64 \\\n\t\t-v \"$$(git describe --abbrev=0 --tags)\" \\\n\t\t-s dir -t rpm .\n\n.PHONY: all test clean install\n",
  "readme": "# Userplex [![GoDoc](https://godoc.org/go.mozilla.org/userplex?status.svg)](https://godoc.org/go.mozilla.org/userplex) [![Build Status](https://travis-ci.org/mozilla-services/userplex.svg)](https://travis-ci.org/mozilla-services/userplex)\n\nPropagate users from Mozilla's [Person API](https://github.com/mozilla-iam/cis/blob/master/docs/PersonAPI.md) to third party systems.\n\n## Installation\n\nIf you have Go v1.13+ installed, you can install userplex by running:\n\n```bash\n$ go get go.mozilla.org/userplex\n```\n\nOtherwise, you can get a binary from the [releases section](https://github.com/mozilla-services/userplex/releases).\n\n## Configuration\n\nThere is an example configuration file in the repo at [`config.yaml`](https://github.com/mozilla-services/userplex/blob/master/config.yaml)\n\n```yaml\n# Configuration for using Mozilla's Person API\n# https://github.com/mozilla-iam/cis/blob/master/docs/PersonAPI.md\nperson:\n  person_client_id: \"client_id\"\n  person_client_secret: \"client_secret\"\n  person_base_url: \"https://person_url.com\"\n  person_auth0_url: \"https://auth0.com\"\n\n# Configuration for sending notifications. Will only be used\n# if the module block has `notify_new_users` set to `true`.\nnotifications:\n    email:\n        # your smtp relay may require authentication (AWS SES does), so make\n        # sure to set the parameters below to an authorized sender\n        host: \"email-smtp.us-east-1.amazonaws.com\"\n        port: 587\n        from: \"myauthorizedsender@example.net\"\n        cc:   \"bob.kelso@gmail.com\"\n        replyto: \"Something <something@example.com>\"\n        auth:\n            user: \"AKIAI3TZL\"\n            pass: \"AoXAy......\"\n\n\n# AWS Module configuration section.\n#\n# You may have multiple AWS accounts configured and all will\n# be operated on. The way to give different permissions based\n# on the account is to use the `group_mapping` to give\n# different ldap groups different AWS groups. As well, if you do\n# not have a `default` in `group_mapping`, a user without a\n# matching group will just get ignored.\naws:\n  - account_name: \"myawsaccount\"\n    notify_new_users: true\n    ignore_usernames:\n      - legacy_user\n    credentials:\n        # if blank, will use the default aws credential flow\n        access_key: AKIAnnnn\n        secret_key: XXXXXXX\n    # Used to translate ldap usernames into \"local usernames\"\n    # which will be used as the username in AWS (or which ever\n    # module they are present in)\n    username_map:\n      - ldap_username: bkelso\n        local_username: bob\n      - ldap_username: tanderson\n        local_username: neo\n    group_mapping:\n      - ldap_group: \"sysadmins\"\n        iam_groups:\n          - ldapmanaged\n          - admin\n      - ldap_group: \"developers\"\n        iam_groups:\n          - ldapmanaged\n          - dev_only\n      - default: true\n        iam_groups:\n          - ldapmanaged\n\n# Authorized Keys Module configuration section.\n#\n# As with the AWS Module section, you can have multiple\n# authorized keys paths configured. The core\n# configuration here is the list of allowed `ldap_groups`\n# and how the `path` is setup. You can use `{username}`\n# or `{env:<ENV_VAR>}` within the path.\nauthorized_keys:\n    - name: all_authorizedkeys\n      # Used to translate ldap usernames into \"local usernames\"\n      # which will be used as the username in authorized keys\n      # (or which ever module they are present in)\n      username_map:\n        - ldap_username: bkelso\n          local_username: bob\n        - ldap_username: tanderson\n          local_username: neo\n      ldap_groups:\n        - sysadmins\n        - developers\n        - devssh\n      # {username} will be replaced with the primary username for the user being created\n      path: /data/puppet/modules/users/files/{username}/.ssh/authorized_keys\n      # {env:ROOT_DIR} will be replaced with the env var $ROOT_DIR\n      # path: /data/puppet/modules/users/files/{env:ROOT_DIR}/.ssh/authorized_keys\n\n    - name: root_authorizedkeys\n      ldap_groups:\n        - sysadmins\n      # Used to translate ldap usernames into \"local usernames\"\n      # which will be used as the username in authorized keys\n      # (or which ever module they are present in)\n      username_map:\n        - ldap_username: tanderson\n          local_username: neo\n      path: /data/puppet/modules/users/files/root/.ssh/authorized_keys\n```\n\n## Usage\n\n```\nNAME:\n   userplex - Propagate users from Mozilla's Person API to third party systems.\n\nUSAGE:\n   userplex [global options] command [command options] [arguments...]\n\nVERSION:\n   v1.0.0\n\nAUTHORS:\n   AJ Bahnken <ajvb@mozilla.com>\n   Julien Vehent <jvehent@mozilla.com>\n\nCOMMANDS:\n   aws             Operations within AWS\n   authorizedkeys  Operations within authorizedkeys files\n   get-person      Get Person from Person API. Useful for finding the correct identifier\n   help, h         Shows a list of commands or help for one command\n\nGLOBAL OPTIONS:\n   --config value, -c value  Path to userplex config file [$USERPLEX_CONFIG_PATH]\n   --help, -h                show help\n   --version, -v             print the version\n```\n\n#### AWS Usage\n\n```\n$ userplex -c config-encrypted.yaml aws help\nNAME:\n   userplex aws - Operations within AWS\n\nUSAGE:\n   userplex aws [global options] command [command options] [arguments...]\n\nVERSION:\n   v1.0.0\n\nCOMMANDS:\n   create  Create user\n   reset   Reset user credentials\n   delete  Delete user\n   sync    Run sync operation\n   verify  Verify users against Person API. Outputs report, use `sync` to fix discrepancies.\n\nGLOBAL OPTIONS:\n   --help, -h  show help\n\n$ userplex -c config-encrypted.yaml aws create example-user@mozilla.com\nINFO[0001] aws \"example-aws-account\": user \"example-user\" not found, needs to be created\nNotify new users disabled, printing output.\nCreated new user: example-user\n....\n\n$ userplex -c config-encrypted.yaml aws delete example-user@mozilla.com\nINFO[0002] aws \"example-aws-account\": deleted user \"example-user\"\n\n$ userplex -c config-encrypted.yaml aws verify\nUsers not in LDAP:\n  * test-user\n\n$ userplex -c config-encrypted.yaml aws sync\nUsers not in LDAP:\n  * test-user\nWould you like to remove these users from the example-aws-account AWS account?\n  * test-user\n(y/n): y\n```\n\n#### Authorized Keys Usage\n\n```\n$ userplex -c config-encrypted.yaml authorizedkeys help\nNAME:\n   userplex authorizedkeys - Operations within authorizedkeys files\n\nUSAGE:\n   userplex authorizedkeys [global options] command [command options] [arguments...]\n\nVERSION:\n   v1.0.0\n\nCOMMANDS:\n   create  Create user\n   reset   Reset user credentials\n   delete  Delete user\n   sync    Run sync operation\n   verify  Verify users against Person API. Outputs report, use `sync` to fix discrepancies.\n\nGLOBAL OPTIONS:\n   --help, -h  show help\n\n$ userplex -c config-encrypted.yaml authorizedkeys create example-user@mozilla.com\nINFO[0000] Adding user example-user to /puppet/userplex-testing/ak/example-user/.ssh/authorized_keys\nINFO[0000] creating \"/puppet/userplex-testing/ak/example-user/.ssh/authorized_keys\"\nINFO[0000] 1 keys written into \"/puppet/userplex-testing/ak/example-user/.ssh/authorized_keys\"\nINFO[0000] Adding user example-user to /puppet/userplex-testing/ak/root/.ssh/authorized_keys\nINFO[0000] creating \"/puppet/userplex-testing/ak/root/.ssh/authorized_keys\"\nINFO[0000] 1 keys written into \"/puppet/userplex-testing/ak/root/.ssh/authorized_keys\"\n\n$ cat /puppet/userplex-testing/ak/example-user/.ssh/authorized_keys\nssh-rsa AAAAB3.... example-user@mozilla\n\n$ userplex -c config-encrypted.yaml authorizedkeys delete example-user@mozilla.com\nINFO[0000] removing \"/puppet/userplex-testing/ak/example-user/.ssh/authorized_keys\"\nINFO[0000] removing \"/puppet/userplex-testing/ak/root/.ssh/authorized_keys\"\n\n$ cat /puppet/userplex-testing/ak/example-user/.ssh/authorized_keys\ncat: /puppet/userplex-testing/ak/example-user/.ssh/authorized_keys: No such file or directory\n\n```\n\n\n## License\nMozilla Public License 2.0\n\n## Authors\n  * AJ Bahnken <ajvb@mozilla.com>\n  * Julien Vehent <ulfr@mozilla.com>\n"
},
{
  "name": "stubattribution",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Makefile",
      "README.md",
      "attributioncode",
      "errorconverter",
      "go.mod",
      "go.sum",
      "stubmodify",
      "stubservice",
      "testdata",
      "testing",
      "vendor",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "LINTER = golint -set_exit_status\nPACKAGES := $(shell go list -mod vendor ./... | grep -v 'vendor')\n\n.PHONY: test coveralls travis clean\n\ncodecov: clean\n\tmkdir -p codecov\n\ntest: $(PACKAGES)\n\n$(PACKAGES): codecov\n\tmkdir -p codecov/$@\n\tgo test -mod vendor -coverprofile=\"codecov/$@/profile.out\" -covermode=atomic $@\n\ncoveralls: test\n\techo \"mode: atomic\" > coverage.txt\n\t(find ./codecov -name 'profile.out' -print0 | xargs -0 cat | grep -v 'mode: ') >> coverage.txt\n\ntravis: test coveralls\n\nclean:\n\trm -rf codecov\n\trm -f coverage.txt\n",
  "readme": "# Stub Attribution [![CircleCI](https://circleci.com/gh/mozilla-services/stubattribution.svg?style=svg)](https://circleci.com/gh/mozilla-services/stubattribution) [![GoDoc](https://godoc.org/github.com/mozilla-services/stubattribution?status.svg)](https://godoc.org/github.com/mozilla-services/stubattribution)\nA service which accepts an attribution code and returns a modified stub installer.\n"
},
{
  "name": "cloudops-gcs-static-website-reverse-proxy",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "openresty"
    ]
  },
  "makefile": null,
  "readme": "# Intro\nThis is an Openresty reverse proxy for static websites hosted by GCS.\n\nThe main purpose of the reverse proxy is to remove the \"x-goog-meta-\" prefix\nfrom headers returned by GCS. In addition to that, it also does a few things\nincluding:\n\n* remove a few \"x-goog-\" headers that are less useful to the clients.\n* compress the GCS responses for a few file types (html, js, css and etc.).\n* allow logging the request and response headers for troubleshooting purpose.\n* ...\n\n# Release Process\n\nTake the following steps to release a new tag.\n\n1. File a PR with your changes and make sure they work correctly.\n\n2. Merge the PR to the main branch.\n\n3. Locally, sync your main branch with the remote (i.e.\n   `git checkout main && git pull`).\n\n4. Make a tag and push it.\n   * The tag should be in [semver](https://semver.org/) form.\n   * Breaking changes should increase the major number.\n   * Remember to push your tag.\n\n   For example,\n   ```\n   semver=0.0.1\n   git tag \"${semver}\"\n   git push --tags\n   ```\n\n5. Build an image and push it to the repository. For example,\n   ```\n   semver=0.0.1\n   docker build --platform linux/amd64 -t gcr.io/moz-fx-cloudops-images-global/openresty:gcs-website-reverse-proxy-\"${semver}\" .\n   docker push gcr.io/moz-fx-cloudops-images-global/openresty:gcs-website-reverse-proxy-\"${semver}\"\n   ```\nNote:\n  * The build and push process is not automated because as of this writing, the\n    image is not frequently built and it's not worth the effort to set that up\n    (e.g. we would need a service account with proper permissions and we also\n    need to manage its key file and etc.).\n  * The major versions don't have EOL at this point, i.e. they're maintained\n    forever (it's really because we don't expect that many changes in this repo).\n"
},
{
  "name": "foxsec-pipeline",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile-base",
      "Dockerfile-complete",
      "LICENSE",
      "README.md",
      "bin",
      "checkstyle",
      "contrib",
      "docker-entrypoint.sh",
      "docker",
      "docs",
      "pom.xml",
      "src"
    ],
    "/docs": [
      "RELEASE.md",
      "beam-intro",
      "secops-beam"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# foxsec-pipeline\n\n[![Build Status](https://circleci.com/gh/mozilla-services/foxsec-pipeline/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla-services/foxsec-pipeline/tree/master)\n[![Documentation](https://img.shields.io/badge/documentation-link-purple.svg)](https://mozilla-services.github.io/foxsec-pipeline/secops-beam/)\n\n[Apache Beam](https://beam.apache.org/) pipelines for analyzing log data.\n\n## Documentation\n\n* [secops-beam Java documentation](https://mozilla-services.github.io/foxsec-pipeline/secops-beam/)\n\njavadoc documentation is currently updated manually and although should be up to date, may not be current\nwith master.\n\n## Introduction to Beam\n\nTo get familiar with developing pipelines in Beam, this repository also contains a small workshop that\nprovides some guidance on building basic pipelines. The introduction document can be found\n[here](docs/beam-intro/INTRO.md).\n\n## Tests\n\nTests can be executed locally using Docker.\n\n### Run all tests\n\n```bash\ndocker build -f Dockerfile-base -t foxsec-pipeline-base:latest .\nbin/m test\n```\n\n### Run a specific test\n\n```bash\ndocker build -f Dockerfile-base -t foxsec-pipeline-base:latest .\nbin/m test -Dtest=ParserTest\n```\n\n## CLI Usage\n\n### Pipeline [RuntimeSecrets](https://mozilla-services.github.io/foxsec-pipeline/secops-beam/com/mozilla/secops/crypto/RuntimeSecrets.html)\n\nPipeline runtime secrets can be generated locally using the main method in the [`RuntimeSecrets`](https://mozilla-services.github.io/foxsec-pipeline/secops-beam/com/mozilla/secops/crypto/RuntimeSecrets.html) class.\n\n```bash\nbin/m compile exec:java -Dexec.mainClass=com.mozilla.secops.crypto.RuntimeSecrets -Dexec.args='-i testkey -k dataflow -p my-gcp-dataflow-project -r dataflow'\n```\n\nRun the class with no options to see usage information. Note that in this case, the key ring name and key name\nare being specified as `dataflow`. The existing `RuntimeSecrets` class requires the keys to be accessible\nusing these identifiers when the pipeline is executing.\n\nThe output of the command can be prefixed with `cloudkms://` in an option to enable runtime decryption of the secrets\nduring pipeline execution.\n\n### Interacting with Minfraud\n\nReputation data can be fetched from Minfraud locally using the main method in the [`Minfraud`](https://mozilla-services.github.io/foxsec-pipeline/secops-beam/com/mozilla/secops/Minfraud.html) class.\n\nYou must provide the accountid and licensekey plus the IP and/or email you want to get reputation data for. `--accountid` and `--licensekey` can either be provided directly or provided as RuntimeSecrets (`cloudkms://...`).\n\n```bash\nbin/m exec:java \\\n  -Dexec.mainClass=\"com.mozilla.secops.Minfraud\" \\\n  -Dexec.args=\"-p my-gcp-dataflow-project --accountid 'cloudkms://...' --licensekey 'cloudkms://...' --ip '8.8.8.8' --email 'example@example.com'\"\n```\n\n### Creating Watchlist entries\n\nWatchlist entries can be created locally using the main method in the [`Watchlist`](https://mozilla-services.github.io/foxsec-pipeline/secops-beam/com/mozilla/secops/Watchlist.html) class.\n\nYou must also prefix your command with `WITHOUT_DAEMONS=true` so that the entry won't be submitted to the Datastore emulator running within the container.\n\n```\nusage: Watchlist\n -c,--createdby <arg>\n -ne,--neverexpires     Watchlist entry never expires (compared to default\n                        of 2 weeks)\n -o,--object <arg>      Object to watch. Can be an IP or email.\n -p,--project <arg>     GCP project name (required if submitting to\n                        Datastore)\n -s,--severity <arg>    Severity of Watchlist entry. Can be 'info',\n                        'warn', or 'crit'\n -su,--submit           Submit Watchlist entry to Datastore rather than\n                        emit json\n -t,--type <arg>        Type of object to watch. Can be 'ip' or 'email'\n```\n\n#### Example of creating entry without submitting to Datastore\n```bash\n$ bin/m exec:java -Dexec.mainClass=\"com.mozilla.secops.Watchlist\" -Dexec.args=\"--object '127.0.0.1' --type 'ip' --createdby 'example@example.com' --severity 'info'\"\n\n{\"type\":\"ip\",\"severity\":\"info\",\"expires_at\":\"2020-02-26T17:45:01.399Z\",\"created_by\":\"example@example.com\",\"object\":\"127.0.0.1\"}\n```\n\n#### Example of submitting to Datastore\n```bash\n$ WITHOUT_DAEMONS=true bin/m exec:java -Dexec.mainClass=\"com.mozilla.secops.Watchlist\" -Dexec.args=\"--object '127.0.0.1' --type 'ip' --createdby 'example@example.com' --severity 'info' --project foxsec-pipeline-nonprod --submit\"\n\nFeb 12, 2020 5:41:44 PM com.mozilla.secops.state.State initialize\nINFO: Initializing new state interface using com.mozilla.secops.state.DatastoreStateInterface\nFeb 12, 2020 5:41:45 PM com.mozilla.secops.state.StateCursor set\nINFO: Writing state for 127.0.0.1\nFeb 12, 2020 5:41:45 PM com.mozilla.secops.state.State done\nINFO: Closing state interface com.mozilla.secops.state.DatastoreStateInterface\nSuccessfully submitted watchlist entry to foxsec-pipeline-nonprod\n{\"type\":\"ip\",\"severity\":\"info\",\"expires_at\":\"2020-02-26T17:41:43.919Z\",\"created_by\":\"example@example.com\",\"object\":\"127.0.0.1\"}\n```\n\n## Contributing\n\nSee the [contributing guidelines](./CONTRIBUTING.md).\n"
},
{
  "name": "python-autograph-utils",
  "files": {
    "/": [
      ".editorconfig",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      "CONTRIBUTING.rst",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "autograph_utils",
      "docs",
      "requirements.txt",
      "requirements_dev.txt",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "contributing.rst",
      "history.rst",
      "index.rst",
      "readme.rst"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": ".PHONY: clean clean-test clean-pyc clean-build docs help\n.DEFAULT_GOAL := help\n\ndefine BROWSER_PYSCRIPT\nimport os, webbrowser, sys\n\ntry:\n\tfrom urllib import pathname2url\nexcept:\n\tfrom urllib.request import pathname2url\n\nwebbrowser.open(\"file://\" + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\n\ndefine PRINT_HELP_PYSCRIPT\nimport re, sys\n\nfor line in sys.stdin:\n\tmatch = re.match(r'^([a-zA-Z_-]+):.*?## (.*)$$', line)\n\tif match:\n\t\ttarget, help = match.groups()\n\t\tprint(\"%-20s %s\" % (target, help))\nendef\nexport PRINT_HELP_PYSCRIPT\n\nBROWSER := python -c \"$$BROWSER_PYSCRIPT\"\n\nhelp:\n\t@python -c \"$$PRINT_HELP_PYSCRIPT\" < $(MAKEFILE_LIST)\n\nclean: clean-build clean-pyc clean-test ## remove all build, test, coverage and Python artifacts\n\nclean-build: ## remove build artifacts\n\trm -fr build/\n\trm -fr dist/\n\trm -fr .eggs/\n\tfind . -name '*.egg-info' -exec rm -fr {} +\n\tfind . -name '*.egg' -exec rm -f {} +\n\nclean-pyc: ## remove Python file artifacts\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -fr {} +\n\nclean-test: ## remove test and coverage artifacts\n\trm -fr .tox/\n\trm -f .coverage\n\trm -fr htmlcov/\n\trm -fr .pytest_cache\n\nlint: ## check style with flake8\n\tflake8 autograph_utils tests\n\ntest: ## run tests quickly with the default Python\n\tpytest\n\ntest-all: ## run tests on every Python version with tox\n\ttox\n\ncoverage: ## check code coverage quickly with the default Python\n\tcoverage run --source autograph_utils -m pytest\n\tcoverage report -m\n\tcoverage html\n\t$(BROWSER) htmlcov/index.html\n\ndocs: ## generate Sphinx HTML documentation, including API docs\n\trm -f docs/autograph_utils.rst\n\trm -f docs/modules.rst\n\tsphinx-apidoc -o docs/ autograph_utils\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C docs html\n\t$(BROWSER) docs/_build/html/index.html\n\nservedocs: docs ## compile the docs watching for changes\n\twatchmedo shell-command -p '*.rst' -c '$(MAKE) -C docs html' -R -D .\n\nrelease: dist ## package and upload a release\n\ttwine upload dist/*\n\ndist: clean ## builds source and wheel package\n\tpython setup.py sdist\n\tpython setup.py bdist_wheel\n\tls -l dist\n\ninstall: clean ## install the package to the active Python's site-packages\n\tpython setup.py install\n",
  "readme": "==========================\nPython Autograph Utilities\n==========================\n\n\n.. image:: https://img.shields.io/pypi/v/autograph_utils.svg\n        :target: https://pypi.python.org/pypi/autograph_utils\n\n.. image:: https://img.shields.io/travis/glasserc/python_autograph_utils.svg\n        :target: https://travis-ci.org/glasserc/python_autograph_utils\n\n.. image:: https://readthedocs.org/projects/python-autograph-utils/badge/?version=latest\n        :target: https://python-autograph-utils.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\n\n\n\nA library to simplify use of Autograph\n\n\n* Free software: Apache Software License 2.0\n* Documentation: https://python-autograph-utils.readthedocs.io.\n\n\nFeatures\n--------\n\nSignatureVerifier\n=================\n\nThe canonical implementation of certificate chain validation. Although\nsome other implementations seem to exist (such as\nhttps://github.com/river2sea/X509Validation,\nhttps://github.com/alex/x509-validator, and\nhttps://github.com/openstack/cursive), all are marked as\npre-production and/or needing work, so just do it ourselves.\n\nCredits\n-------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n"
},
{
  "name": "outgoing",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".travis.yml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "contrib",
      "outgoing.go",
      "outgoing_test.go",
      "redirect.go",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "outgoing [![CircleCI](https://circleci.com/gh/mozilla-services/outgoing/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla-services/outgoing/tree/master) [![Build Status](https://travis-ci.org/mozilla-services/outgoing.svg?branch=master)](https://travis-ci.org/mozilla-services/outgoing)\n========\n\nA redirector for outgoing links.\n\n\nTesting\n-------\n```bash\ngo test\n```\n\nBuilding\n-----\n```bash\ngo build\n```\n\nRunning\n-------\n```bash\n./outgoing -key ASECRET -addr \":9090\"\n```\n"
},
{
  "name": "autopush-integration-tests",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Jenkinsfile",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "conftest.py",
      "manifest.ini",
      "poetry.lock",
      "prefs.ini",
      "pyproject.toml",
      "requirements",
      "test_env.conf",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "autopush integration tests\n============================\n\nAutomated tests for the autopush server.\n\n[![license](https://img.shields.io/badge/license-MPL%202.0-blue.svg)](https://github.com/mozilla-services/autopush-integration-tests/tree/master#license)\n[![travis](https://img.shields.io/travis/mozilla-services/autopush-integration-tests.svg?label=travis)](http://travis-ci.org/mozilla-services/autopush-integration-tests/)\n[![updates](https://api.dependabot.com/badges/status?host=github&repo=mozilla-services/autopush-integration-tests)](https://dependabot.com)\n\n\nSummary\n---------\n\nA variety of tests are used to verify the integrity of [Mozilla's autopush service](https://autopush.readthedocs.io/).\nThis repository contains integration tests used largely for deployment verification.\n\nOther tests / test tools used for testing autopush include:\nFor API testing: [ap-loadtester](https://github.com/mozilla-service/ap-loadtester)\n\n\n\nSetup\n---------\n\nThis repository uses [Pipenv](https://pipenv.readthedocs.io/en/latest/)\nto manage it's Python dependencies. Please follow the directions from Pipenv\non how to install it.\n\nOnce Pipenv is installed:\n\n* Use the command `pipenv install` to create a Python virtual environment and\ninstall the required dependencies\n* Once the virtual environment is created, use the command `pipenv shell`\nto use the virtual environment\n\n\nRun Tests\n---------\n\nTo run the current set of tests, please use the following command:\n\n`pytest -v --env=<ENV> --api-version=<API_VERSION> tests/`\n\n* `<ENV>` is `stage`, `production`, or `dev` depending on what\nenvironment you are testing.\n\nIf you want the results of this testrun to be recorded to our TestRail\ninstance, please check the output of `pytest -h` to get a list\nof the values that will need to be passed in as additional\nparameters.\n\nLicense\n-------\nThis software is licensed under the [MPL] 2.0:\n\n    This Source Code Form is subject to the terms of the Mozilla Public\n    License, v. 2.0. If a copy of the MPL was not distributed with this\n    file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n[MPL]: http://www.mozilla.org/MPL/2.0/\n"
},
{
  "name": "servicedenuages.fr",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "Makefile",
      "README.rst",
      "add_mozilla_tag.py",
      "content",
      "pelicanconf.py",
      "plugins",
      "publishconf.py",
      "pure",
      "requirements.txt",
      "self-hosting-a-hard-promess.html"
    ]
  },
  "makefile": "PELICANOPTS=\n\nBASEDIR=$(CURDIR)\nINPUTDIR=$(BASEDIR)/content\nOUTPUTDIR=$(BASEDIR)/output\nCONFFILE=$(BASEDIR)/pelicanconf.py\nPUBLISHCONF=$(BASEDIR)/publishconf.py\n\nGITHUB_PAGES_BRANCH=gh-pages\n\nVENV := $(shell echo $${VIRTUAL_ENV-$(shell pwd)/.venv})\nVIRTUALENV = virtualenv --python python2.7\nINSTALL_STAMP = $(VENV)/.install.stamp\n\nPYTHON=$(VENV)/bin/python\nPELICAN=$(VENV)/bin/pelican\nPIP=$(VENV)/bin/pip\n\nDEBUG ?= 0\nifeq ($(DEBUG), 1)\n\tPELICANOPTS += -D\nendif\n\ninstall: $(INSTALL_STAMP)\n$(INSTALL_STAMP): $(PYTHON) requirements.txt\n\t$(VENV)/bin/pip install -r requirements.txt\n\ttouch $(INSTALL_STAMP)\n\nvirtualenv: $(PYTHON)\n$(PYTHON):\n\t$(VIRTUALENV) $(VENV)\n\nhtml: install\n\t$(PELICAN) $(INPUTDIR) -o $(OUTPUTDIR) -s $(CONFFILE) $(PELICANOPTS)\n\tcp self-hosting-a-hard-promess.html $(OUTPUTDIR)/en/\n\nclean:\n\t[ ! -d $(OUTPUTDIR) ] || rm -rf $(OUTPUTDIR)\n\trm -rf $(VENV)\n\nserve: install\nifdef PORT\n\tcd $(OUTPUTDIR) && $(PYTHON) -m pelican.server $(PORT)\nelse\n\tcd $(OUTPUTDIR) && $(PYTHON) -m pelican.server 8080\nendif\n\nregenerate:\n\tcd $(OUTPUTDIR) && $(PYTHON) -m pelican.server &\n\t$(PELICAN) -r $(INPUTDIR) -o $(OUTPUTDIR) -s $(CONFFILE) $(PELICANOPTS)\n\tcp self-hosting-a-hard-promess.html $(OUTPUTDIR)/en/\n\npublish: install\n\t$(PELICAN) $(INPUTDIR) -o $(OUTPUTDIR) -s $(PUBLISHCONF) $(PELICANOPTS)\n\tcp self-hosting-a-hard-promess.html $(OUTPUTDIR)/en/\n\ngithub: publish\n\t$(VENV)/bin/ghp-import -b $(GITHUB_PAGES_BRANCH) $(OUTPUTDIR)\n\tgit push origin $(GITHUB_PAGES_BRANCH) --force\n\n.PHONY: html clean serve devserver github publish\n",
  "readme": "Service de nuages\n=================\n\nLoad and update plugins submodule::\n\n    git submodule init\n    git submodule update --recursive\n    git submodule status\n\nTo build the blog, just run::\n\n    make html\n\nand to publish it::\n\n    make publish\n\nPublish to gh-pages::\n\n    make github\n\nDependencies\n------------\n\nMost of the dependencies are pure Python and are thus handled by ``pip``\ndirectly; however, in order to build graphs dynamically, the ``dot`` binary\nfrom `Graphviz <http://graphviz.org/Download..php>`_ is required.\n\nTo install dependencies you can use::\n\n    sudo apt-get install graphviz libjpeg8-dev libpng-dev python-dev\n"
},
{
  "name": "pyramid_multiauth",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGES.txt",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTORS.txt",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "pyramid_multiauth",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "=================\npyramid_multiauth\n=================\n\n|pypi| |ci|\n\n.. |pypi| image:: https://img.shields.io/pypi/v/pyramid_multiauth.svg\n    :target: https://pypi.python.org/pypi/pyramid_multiauth\n\n.. |ci| image:: https://github.com/mozilla-services/pyramid_multiauth/actions/workflows/test.yml/badge.svg\n    :target: https://github.com/mozilla-services/pyramid_multiauth/actions\n\n\nAn authentication policy for Pyramid that proxies to a stack of other\nauthentication policies.\n\n\nOverview\n========\n\nMultiAuthenticationPolicy is a Pyramid authentication policy that proxies to\na stack of *other* IAuthenticationPolicy objects, to provide a combined auth\nsolution from individual pieces.  Simply pass it a list of policies that\nshould be tried in order::\n\n\n    policies = [\n        IPAuthenticationPolicy(\"127.0.*.*\", principals=[\"local\"])\n        IPAuthenticationPolicy(\"192.168.*.*\", principals=[\"trusted\"])\n    ]\n    authn_policy = MultiAuthenticationPolicy(policies)\n    config.set_authentication_policy(authn_policy)\n\nThis example uses the pyramid_ipauth module to assign effective principals\nbased on originating IP address of the request.  It combines two such\npolicies so that requests originating from \"127.0.*.*\" will have principal\n\"local\" while requests originating from \"192.168.*.*\" will have principal\n\"trusted\".\n\nIn general, the results from the stacked authentication policies are combined\nas follows:\n\n    * authenticated_userid:    return userid from first successful policy\n    * unauthenticated_userid:  return userid from first successful policy\n    * effective_principals:    return union of principals from all policies\n    * remember:                return headers from all policies\n    * forget:                  return headers from all policies\n\n\nDeployment Settings\n===================\n\nIt is also possible to specify the authentication policies as part of your\npaste deployment settings.  Consider the following example::\n\n    [app:pyramidapp]\n    use = egg:mypyramidapp\n\n    multiauth.policies = ipauth1 ipauth2 pyramid_browserid\n\n    multiauth.policy.ipauth1.use = pyramid_ipauth.IPAuthentictionPolicy\n    multiauth.policy.ipauth1.ipaddrs = 127.0.*.*\n    multiauth.policy.ipauth1.principals = local\n\n    multiauth.policy.ipauth2.use = pyramid_ipauth.IPAuthentictionPolicy\n    multiauth.policy.ipauth2.ipaddrs = 192.168.*.*\n    multiauth.policy.ipauth2.principals = trusted\n\nTo configure authentication from these settings, simply include the multiauth\nmodule into your configurator::\n\n    config.include(\"pyramid_multiauth\")\n\nIn this example you would get a MultiAuthenticationPolicy with three stacked\nauth policies.  The first two, ipauth1 and ipauth2, are defined as the name of\nof a callable along with a set of keyword arguments.  The third is defined as\nthe name of a module, pyramid_browserid, which will be procecesed via the\nstandard config.include() mechanism.\n\nThe end result would be a system that authenticates users via BrowserID, and\nassigns additional principal identifiers based on the originating IP address\nof the request.\n\nIf necessary, the *group finder function* and the *authorization policy* can\nalso be specified from configuration::\n\n    [app:pyramidapp]\n    use = egg:mypyramidapp\n\n    multiauth.authorization_policy = mypyramidapp.acl.Custom\n    multiauth.groupfinder  = mypyramidapp.acl.groupfinder\n\n    ...\n\n\nMultiAuthPolicySelected Event\n=============================\n\nAn event is triggered when one of the multiple policies configured is selected.\n\n::\n\n    from pyramid_multiauth import MultiAuthPolicySelected\n\n\n    # Track policy used, for prefixing user_id and for logging.\n    def on_policy_selected(event):\n        print(\"%s (%s) authenticated %s for request %s\" % (event.policy_name,\n                                                           event.policy,\n                                                           event.userid,\n                                                           event.request))\n\n    config.add_subscriber(on_policy_selected, MultiAuthPolicySelected)\n"
},
{
  "name": "tecken-loadtests",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.rst",
      "bin",
      "docker-compose.yml",
      "downloading",
      "found-on-msdl.log",
      "loadtest-eliot",
      "loadtest",
      "locust-eliot",
      "locust_eliot.sh",
      "molotov.json",
      "molotov_eliot.sh",
      "my.env.dist",
      "requirements.txt",
      "schemas",
      "setup.cfg",
      "symbols-uploaded",
      "test_env.sh",
      "unzips.go"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n# Include my.env and export it so variables set in there are available\n# in the Makefile.\ninclude my.env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\nMYUID ?= 10001\nMYGID ?= 10001\n\nDC := $(shell which docker-compose)\nSTACKSDIR = stacks/\n\n.DEFAULT_GOAL := help\n.PHONY: help\nhelp:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@grep -E '^[a-zA-Z0-9_-]+:.*?## .*$$' Makefile \\\n\t\t| grep -v grep \\\n\t    | sed -n 's/^\\(.*\\): \\(.*\\)##\\(.*\\)/\\1\\3/p' \\\n\t    | column -t  -s '|'\n\t@echo \"\"\n\t@echo \"Read README.rst for more details.\"\n\nmy.env:\n\t@if [ ! -f my.env ]; \\\n\tthen \\\n\techo \"Copying my.env.dist to my.env...\"; \\\n\tcp my.env.dist my.env; \\\n\tfi\n\n.docker-build:\n\tmake build\n\n.PHONY: build\nbuild:  ## | Build Docker image for testing with\n\t${DC} build --no-cache --build-arg userid=${MYUID} --build-arg groupid=${MYGID} base\n\ttouch .docker-build\n\n.PHONY: clean\nclean:  ## | Delete artifacts\n\t${DC} stop\n\t${DC} rm -f\n\trm -rf .docker-build\n\n.PHONY: shell\nshell: .docker-build  ## | Create a shell in the Docker image\n\t${DC} run base /bin/bash\n\n.PHONY: buildstacks\nbuildstacks: .docker-build  ## | Build stacks for testing symbolication\n\t-mkdir $(STACKSDIR)\n\t${DC} run base /bin/bash -c \"bin/fetch-crashids.py --num-results=1000 | bin/make-stacks.py save $(STACKSDIR)\"\n\t@echo \"`ls $(STACKSDIR)/*.json | wc -l` stacks total.\"\n\n.PHONY: symbolicate-locally\nsymbolicate-locally:  ## | Run symbolication against localhost\n\tpython symbolication.py stacks http://localhost:8050\n\n.PHONY: symbolicate-stage\nsymbolicate-stage:  ## | Run symbolication against stage\n\tpython symbolication.py stacks https://symbols.stage.mozaws.net\n\n.PHONY: download-locally\ndownload-locally:  ## | Run download test against localhost\n\tpython download.py http://localhost:8000 downloading/symbol-queries-groups.csv downloading/socorro-missing.csv\n\n.PHONY: download-stage\ndownload-stage:  ## | Run download test against stage\n\tpython download.py https://symbols.stage.mozaws.net downloading/symbol-queries-groups.csv downloading/socorro-missing.csv\n\n.PHONY: download-prod\ndownload-prod:  ## | Run download test against prod\n\tpython download.py https://symbols.mozilla.org downloading/symbol-queries-groups.csv downloading/socorro-missing.csv\n\n.PHONY: make-symbol-zip\nmake-symbol-zip:  ## | Make a symbols.zip file for uploading tests\n\tpython make-symbol-zip.py\n",
  "readme": "================\nTecken loadtests\n================\n\nSet of tools for load testing Tecken Symbols Service and Symbolication Service.\n\n\nSetup\n=====\n\n1. Clone the repo.\n2. Run ``make build`` to build the Docker container.\n\n\nTesting download API\n====================\n\nGenerating ``symbol-queries-groups.csv``\n----------------------------------------\n\nFirst of all, you need to enable logging on the\n``org.mozilla.crash-stats.symbols-public`` and\n``org.mozilla.crash-stats.symbols-private`` S3 buckets. Make the logging\ngo to the bucket ``peterbe-symbols-playground-deleteme-in-2018`` and for\neach make the prefix be ``public-symbols/`` and ``private-symbols/``\nrespectively.\n\nThe file ``symbol-queries-groups.csv`` was created by running\n``generate-csv-logs.py`` a bunch of ways:\n\n1. ``AWS_ACCESS_KEY=... AWS_SECRET_ACCESS_KEY=... python generate-csv-logs.py download``\n\n2. ``python generate-csv-logs.py summorize``\n\n3. ``python generate-csv-logs.py group``\n\n\nTesting the download API\n------------------------\n\n1. Run::\n\n       $ make shell\n       app@...:/app$ python bin/download.py HOST downloading/symbol-queries-groups.csv\n\n   to run it against HOST.\n\n2. Sit and watch it or kill it with ``Ctrl-C``. If you kill it before it\n   finishes stats are printed out with what's been accomplished so far.\n\n**Alternatively** you can do the same but add another CSV file that\ncontains looks for ``code_file`` and ``code_id``. For example:\n\n::\n\n   $ make shell\n   app@...:/app$ python download.py HOST downloading/symbol-queries-groups.csv downloading/socorro-missing.csv\n\nThat second file is expected to have the following header:\n\n::\n\n   debug_file,debug_id,code_file,code_id\n\n\nThe results look like this:\n\n::\n\n   JOBS DONE SO FAR     302\n   RAN FOR              173.957s\n   AVERAGE RATE         1.74 requests/s\n\n   STATUS CODE           COUNT        MEDIAN    (INTERNAL)       AVERAGE    (INTERNAL)       % RIGHT\n   404                     274        0.644s        0.651s        0.564s        0.660s         95.62\n   302                      28        0.657s        0.639s        0.693s        0.663s        100.00\n\nThat means that 302 URLs were sent in. In 95.62% of the cases, Tecken also\nfound that the symbol file didn't exist (compared with what was the case when\nthe CSV file was made). And there were 28 requests where the symbol existed and\nwas able to redirect to an absolute url for the symbol file.\n\nThe ``(INTERNAL)`` is the median and average of the seconds it took the\n*server*, internally, to make the lookup. So if a look up took 0.6 seconds and\n0.5 seconds internally, it means there was an 0.1 second overhead of making the\nrequest to Tecken. In that case, the 0.5 is basically purely the time it takes\nTecken to talk to the storage server. One thing to note is that Tecken can\niterate over a list of storage servers so this number covers lookups across all\nof them.\n\n\nTesting symbolication API\n=========================\n\nGetting stack data\n------------------\n\nYou'll need stack data to test symbolication.\n\nTo build stacks::\n\n    $ make buildstacks\n\n\nRun symbolication testing\n-------------------------\n\n1. Run::\n   \n       $ make shell\n       app@...:/app$ python bin/symbolicate.py stacks HOST\n\n   to run it against HOST.\n\n2. Sit and watch it or kill it with ``Ctrl-C``. If you kill it before it\n   finishes stats are printed out with what's been accomplished so far.\n\n\nThe results look like this:\n\n::\n\n   TOTAL SO FAR 369 JOBS DONE\n   KEY                            SUM        AVG     MEDIAN    STD-DEV\n   cache_lookups.count           1083          2          3       1.73\n   cache_lookups.size           1.8GB      5.0MB       0.0B      8.7MB\n   cache_lookups.time          8.505s     0.023s     0.001s     0.038s\n   downloads.count                941          2          2       1.73\n   downloads.size              23.9GB     66.3MB     76.5MB     40.4MB\n   downloads.time           3179.728s     8.594s    10.257s     4.432s\n   loader_time              3350.723s     9.056s    10.758s      4.484s\n   modules.count                 1083          2          3       1.73\n   stacks.count                  8497         22         25       5.00\n   stacks.real                   8334         22         25       5.39\n   time                     3351.656s     9.059s    10.725s      4.443s\n\n\n   IN CONCLUSION...\n   Final Download Speed    7.7MB/s\n   Final Cache Speed       219.0MB/s\n\nHere's the same output but annotated with comments:\n\n::\n\n   TOTAL SO FAR 369 JOBS DONE\n   KEY                            SUM        AVG     MEDIAN    STD-DEV\n\n   # How many times we've tried to look up a module in the LRU cache.\n   cache_lookups.count           1083          2          3       1.73\n\n   # How much data we have successfully extracted out of the LRU cache.\n   cache_lookups.size           1.8GB      5.0MB       0.0B      8.7MB\n\n   # The time spent doing lookups on the LRU cache (hits or misses).\n   cache_lookups.time          8.505s     0.023s     0.001s     0.038s\n\n   # How many distinct URLs that have had to be downloaded.\n   downloads.count                941          2          2       1.73\n\n   # The amount of data that has been downloaded from URLs (uncompressed).\n   downloads.size              23.9GB     66.3MB     76.5MB     40.4MB\n\n   # The time spent doing URL downloads.\n   downloads.time           3179.728s     8.594s    10.257s     4.432s\n\n   # A special one. This wraps the 'downloads.time' plus the time it\n   # takes to make the and getting the response. Should be marginally\n   # bigger than than 'downloads.time'\n   loader_time              3350.723s     9.056s    10.758s      4.484s\n\n   # Distinct number of modules that have been come across. Note\n   # that this number is the same as 'cache_lookups.count' above.\n   modules.count                 1083          2          3       1.73\n\n   # Total number of individual stacks symbolicated.\n   stacks.count                  8497         22         25       5.00\n\n   # Same as 'stacks.count' except sometimes the module index is -1 so we\n   # know we don't have to symbolicate it and can just insert its hex offset\n   # directly.\n   stacks.real                   8334         22         25       5.39\n\n   # Total time spent symbolicating all stacks. This spans cache misses and\n   # cache hits.\n   time                     3351.656s     9.059s    10.725s      4.443s\n\n\n   IN CONCLUSION...\n\n   # The download speed doing downloads. But note! this is UNcompressed so it's\n   # likely to be much higher (how much? roughly the average gzip size of a\n   # symbol text file) than what you get for your broadband when you open\n   # http://fast.com.\n   Final Download Speed    7.7MB/s\n\n   # The speed at which the web service can extract data out of the LRU cache.\n   # This is a really important number if you want to optimize how the LRU\n   # data pipelining works.\n   Final Cache Speed       219.0MB/s\n\n.. Note::\n\n   This script picks sample JSON stacks to send in randomly. Every time.\n   That means that if you start it, kill it and start again, it's unlikely\n   that you'll be able to benefit much from the cache of the first run.\n\n\nLoad testing with Molotov\n-------------------------\n\nTo start a `molotov testing <https://molotov.readthedocs.io/>`_ run, there's\na ``loadtest.py`` script. Basic usage:\n\n::\n\n   $ make shell\n   app@...:/app$ molotov --max-runs 10 -cx loadtest.py\n\nBy default the base URL for this will be ``http://localhost:8000``. If\nyou want to override that, change the environment variable\n``URL_SERVER``. For example:\n\n::\n\n   app@...:/app$ URL_SERVER=https://symbols.dev.mozaws.net molotov --max-runs 10 -cx loadtest.py\n\n\nTesting upload API\n==================\n\nMake Symbol Zips\n----------------\n\nTo load test Tecken with realistic ``.zip`` uploads, you can simulate\nthe uploads sent to Tecken in the past.\n\nThe ``make-symbol-zip.py`` script will look at the logs, pick a recent\none (uploaded by Mozilla RelEng) and then download each and every file\nfrom S3 and make a ``.zip`` file in ``upload-zips`` directory.\n\nSimply run it like this::\n\n   $ make shell\n   app@...:/app$ python bin/make-symbol-zip.py\n\nIn the stdout, it should say where it was saved.\n\nNow you can use that to upload. For example:\n\n::\n\n   curl -X POST -H \"Auth-Token: YYYYYYY\" \\\n       --form myfile.zip=@/tmp/massive-symbol-zips/symbols-2017-06-09T04_01_45.zip \\\n       http://localhost:8000/upload/\n\n\nUpload symbol zips\n------------------\n\nBuilds are made on TaskCluster, as an artifact it builds symbols zip files. To\nget a list of recent ones of these for local development or load testing run\nthe script:\n\n::\n\n   $ make shell\n   app@...:/app$ python bin/list-firefox-symbols-zips.py\n\nEach URL can be used to test symbol upload by URL. Uses the same default\nsave directory as ``upload-symbol-zips.py``.\n\nThis script picks random ``.zip`` files from that directory where they're\ntemporarily saved. This script will actually go ahead and make the upload.\n\nRun::\n\n    $ make shell\n    app@...:/app$ python bin/upload-symbol-zips.py\n\nBy default, it will upload 1 random ``.zip`` file to\n``http://localhost:8000/upload``. All the uploads are synchronous.\n\nThis does require an ``Auth-Token`` (aka. \"API token\") in the\nenvironment called ``AUTH_TOKEN``. Either export it or use like this:\n\n::\n\n    $ make shell\n    app@...:/app$ AUTH_TOKEN=7e353c4f34644ef6ba1cfb02b3c3662d python bin/upload-symbol-zips.py\n\nIf you do the testing using ``localhost:8000`` but actually depend on\nuploading the to an S3 bucket that is on the Internet, the uploads can\nbecome really slow. Especially on a home broad band. To limit it to\n``.zip`` files that aren't too large you can add ``--max-size`` option.\nE.g.\n\n::\n\n    $ make shell\n    app@...:/app$ python bin/upload-symbol-zips.py --max-size 100m\n\nThat will pick (randomly) only from ``.zip`` files that are 100Mb or\nless.\n\n\nGenerating ``symbols-uploaded/YYYY-MM-DD.json.gz``\n--------------------------------------------------\n\nGet an API token from\n`Crash-stats <https://crash-stats.mozilla.com/api/tokens/>`__ with the\n``View all Symbol Uploads`` permission. Then run:\n\n::\n\n    $ make shell\n    app@...:/app$ AUTH_TOKEN=bdf6effac894491a8ebd0d1b15f3ab5a python bin/generate-symbols-uploaded.py\n\n\nAnalyzing Symbol Uploads\n------------------------\n\nThere's a script called ``analyze-symbol-uploads-times.py`` which gives\ninsight into symbol upload times. Use it to analyze how concurrent\nuploads work/optimize. You need an auth token with the \"View All Symbols\nUploads\" permission. Then run:\n\n::\n\n    $ make shell\n    app@...:/app$ AUTH_TOKEN=66...92e python bin/analyze-symbol-uploads-times.py --domain=symbols.stage.mozaws.net --limit=10\n\n\nUploading by Download URL from TaskCluster\n------------------------------------------\n\nIf you run ``python list-firefox-symbols-zips.py 3`` it will find 3\nrecent symbols builds URLs on TaskCluster. You can actually pipe them\ninto the the ``upload-symbol-zips.py`` script. For example, this is how\nyou do it for stage:\n\n::\n\n   $ make shell\n   app@...:/app$ export AUTH_TOKEN=xxxxxxxStageAPITokenxxxxxxxxx\n   app@...:/app$ python bin/list-firefox-symbols-zips.py 1 | python bin/upload-symbol-zips.py https://symbols.stage.mozaws.net --download-urls-from-stdin --max-size=2gb\n"
},
{
  "name": "go-bouncer",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "bouncer",
      "fixtures",
      "go.mod",
      "go.sum",
      "handlers.go",
      "handlers_test.go",
      "main.go",
      "mozlog",
      "params.go",
      "scripts",
      "tests",
      "vendor",
      "version.json",
      "version.sh"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# go-bouncer [![CircleCI](https://circleci.com/gh/mozilla-services/go-bouncer/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla-services/go-bouncer/?branch=master) [![GoDoc](https://godoc.org/github.com/mozilla-services/go-bouncer?status.svg)](https://godoc.org/github.com/mozilla-services/go-bouncer)\n\nA Go port of the [user facing portion](https://github.com/mozilla/tuxedo/tree/master/bouncer) as part of the [Bouncer project](https://wiki.mozilla.org/Bouncer).\n\n## Environment Variables\n### `BOUNCER_PINNED_BASEURL_HTTP`\nIf this is a unset, bouncer will randomly pick a healthy mirror from the database and return its base url. If this option is set, the mirror table is completely ignored and `BOUNCER_PINNED_BASEURL_HTTP` will be returned instead.\n\nThis option acts on non ssl only products.\n\nExample: `BOUNCER_PINNED_BASEURL=download-sha1.cdn.mozilla.net/pub`\n\n### `BOUNCER_PINNED_BASEURL_HTTPS`\nThis option is exactly the same as `BOUNCER_PINNED_BASEURL_HTTP` but acts on ssl only products.\n\n### `BOUNCER_STUB_ROOT_URL`\nIf set, bouncer will redirect requests with `attribution_sig` and `attribution_code` parameters to\n`BOUNCER_STUB_ROOT_URL?product=PRODUCT&os=OS&lang=LANG&attribution_sig=ATTRIBUTION_SIG&attribution_code=ATTRIBUTION_CODE`.\n\nExample: `BOUNCER_STUB_ROOT_URL=https://stubdownloader.services.mozilla.com/`\n"
},
{
  "name": "iprepd-nginx",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "dist.ini",
      "docker-compose.yml",
      "docs",
      "etc",
      "lib",
      "test",
      "vendor"
    ],
    "/docs": [
      "moz-architecture-diagram.png"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "IMAGE_NAME\t:= \"iprepd-nginx\"\n\n# Build docker images for testing and prod\nbuild:\n\tdocker-compose build --no-cache integration-test\n\tdocker-compose build iprepd-nginx\n\n# Build test image\nbuild_test:\n\tdocker-compose build --no-cache test-client\n\n# Run tests from within iprepd-nginx integration stage container\n# Copy configs for fixtures so CI doesn't need volume mount\nintegration_test:\n\tdocker-compose down -v\n\tdocker-compose up --no-start iprepd\n\tdocker cp ./test/configs/fixtures/iprepd/iprepd.yaml iprepd_app:/app/config/iprepd.yaml\n\tdocker-compose run integration-test\n\n# Run all smoke test against production image\n# Copy configs for fixtures so CI doesn't need volume mount\nsmoke_test:\n\tdocker-compose down -v\n\tdocker-compose up --no-start iprepd backend\n\tdocker cp ./test/configs/fixtures/iprepd/iprepd.yaml iprepd_app:/app/config/iprepd.yaml\n\tdocker cp ./test/configs/fixtures/backend/index.html backend:/usr/share/nginx/html/index.html\n\tdocker-compose up -d iprepd-nginx\n\tdocker-compose run test-client\n\n# Run development environment, overriding image installed iprepd-nginx Lua files and config\n# Environment contains: iprepd-nginx, iprepd, redis, backend\nrun_dev_env:\n\tdocker-compose down -v\n\tdocker-compose up iprepd-nginx\n\n# Run development instance, overriding image installed iprepd-nginx Lua files and\n# nginx configuration; requires a .env file in the repository root\n# only container for iprepdnginx\nrun_dev:\n\tdocker run -ti --rm \\\n\t\t--env-file=.env \\\n\t\t-v $(shell pwd)/etc/nginx.conf:/usr/local/openresty/nginx/conf/nginx.conf \\\n\t\t-v $(shell pwd)/etc/conf.d:/usr/local/openresty/nginx/conf/conf.d \\\n\t\t-v $(shell pwd)/lib/resty/iprepd.lua:/usr/local/openresty/site/lualib/resty/iprepd.lua \\\n\t\t-v $(shell pwd)/lib/resty/statsd.lua:/usr/local/openresty/site/lualib/resty/statsd.lua \\\n\t\t--network=\"host\" $(IMAGE_NAME)\n\n.PHONY: build build_test integration_test run_dev run_dev_env smoke_test\n",
  "readme": "# iprepd-nginx module\n\n`iprepd-nginx` is an openresty module for integrating with [iprepd](https://github.com/mozilla-services/iprepd).\n\nYou can use the example configuration in this repo for a standalone proxy or install using [opm](https://github.com/openresty/opm)\nand integrate it yourself.\n\n*Note:* If nginx is behind a load balancer, make sure to use something like\n[ngx_http_realip_module](https://nginx.org/en/docs/http/ngx_http_realip_module.html).\n\n## What exactly does iprepd-nginx do?\n\nBy using the `iprepd` client in `iprepd-nginx`, you can configure nginx to check the reputation of an incoming client IP within `iprepd`. With\nthis reputation, `iprepd-nginx` will attach up to three HTTP headers on the request that is then forwarded to your application and can reject\nrequests that are below the configured threshold.\n\nThese three headers are:\n\n| Header | Values | Description |\n|---|---|---|\n| X-Foxsec-IP-Reputation | int (0-100) | Reputation score returned by iprepd |\n| X-Foxsec-IP-Reputation-Below-Threshold | boolean ('true'/'false') | Whether the reputation is below the configured threshold |\n| X-Foxsec-Block | boolean ('true'/'false') | High-level whether the request should be blocked (subject to change on what this means) |\n\nAs well, `iprepd-nginx` is designed to fail open and prefer performance to accuracy. The preference of performance to accuracy can be changed a bit as an\noperator, but only to a certain extent (discussed further below).\n\n## (Mozilla-specific) Architecture Diagram\n\n![Architecture Diagram](docs/moz-architecture-diagram.png)\n\n## Installation\n\nInstall using [opm](https://github.com/openresty/opm)\n\n```\nopm get mozilla-services/iprepd-nginx\n```\n\n## Operators Guide\n\n### Prerequisites\n\n* [iprepd](https://github.com/mozilla-services/iprepd), preferably near your `iprepd-nginx` servers (e.g. within the same region in AWS or GCP)\n* A mechanism for updating iprepd. At Mozilla, this is done by feeding logs from your load balancer, application server, and potentially other locations into our [fraud detection pipeline](https://github.com/mozilla-services/foxsec-pipeline).\n* (optional) A mechanism for collecting statsd metrics.\n\n### Note on Performance\n\nA core requirement for iprepd-nginx is that it will add no more than 10ms of latency to requests. Of the mechanisms in place to accomplish this, as an operator there are a few you should be aware of:\n\n#### Heavy use of caching of responses from iprepd\nBy default, iprepd-nginx will cache all non-error responses from iprepd for 30 seconds. It is a good idea to cache errors in production, which is done by enabling `cache_errors` (discussed further below). As well, you may want to lengthen the cache ttl.\n\n#### Strict timeouts to iprepd\nBy default, iprepd-nginx\u2019s request to iprepd will timeout after 10ms. This should not be increased in production, and may be worth reducing if the network design can support it.\n\n### Configuration of the client\n\n#### `threshold` parameter\n\nThe `threshold` value in the client is the numerical value inbetween 0 and 100 where clients will be blocked if their\nIP's reputation in iprepd is below this value.\n\nWhat you will want this value to be set to will be highly contextual to your application and environment, with considerations\nof what kind of violations exist, how likely a client is to activate these violations, how often a client will retry, etc.\n\nA decent value to start at is `50`, but you will want to make sure this is tested along side the implemented iprepd\nviolations for your environment.\n\n#### Audit parameters\n\nThe `audit` values in the client allow configuration of recording blocked requests to the log for security auditing purposes. The audit message includes the request body if small enough to be buffered in memory or the name of the body file if it had to be written to disk. This is dependent on the nginx setting `client_body_buffer_size`.\n\n`audit_blocked_requests` enables auditing, by default it is disabled.\n\n`audit_include_headers` includes the request headers in the audit message. By default, it is disabled. When enabled, Authorization and Proxy-Authorization header values are removed.\n\n`audit_uri_list` is a table of uris that should be audited. This is mandatory when auditing is enabled. Uri's can be either a simple string or a Lua pattern.\nFor example,  `audit_uri_list = {\"test\", \"/test/%d/somethingelse\"}` would allow cause requests to `/test/1/somethingelse` as well as `/test` to be recorded.\n\n#### Example\n\n```lua\n-- Parameters within options:\n--  Required parameters:\n--    api_key - An active API key for authenticating to iprepd\n--    threshold - The reputation threshold, where IP's with a reputation below this number will\n--                be blocked. There is no default for this, as it will be application specific,\n--                but as described above 50 is a good starting place.\n--\n--  Optional parameters:\n--    url - The base URL to iprepd (defaults to \"http://localhost:8080/\")\n--    timeout - The timeout for making requests to iprepd in milliseconds (defaults to 10)\n--    cache_ttl - The iprepd response cache ttl in seconds (defaults to 60)\n--    cache_buffer_count - Max number of entries allowed in the cache. (defaults to 5000)\n--    cache_errors - Enables (1) or disables (0) caching errors. Caching errors is a good\n--                   idea in production, as it can reduce the average additional latency\n--                   caused by this module if anything goes wrong with the underlying\n--                   infrastructure. (defaults to disabled)\n--    cache_errors_ttl - The iprepd response cache ttl for error responses (not 200 or 404) in seconds (defaults to 10)\n--    statsd_host - Host of statsd collector. Setting this will enable statsd metrics collection\n--    statsd_port - Port of statsd collector. (defaults to 8125)\n--    statsd_max_buffer_count - Max number of metrics in buffer before metrics should be submitted\n--                              to statsd (defaults to 100)\n--    statsd_flush_timer - Interval for attempting to flush the stats in seconds. (defaults to 5)\n--    blocking_mode - Enables (1) or disables (0) blocking within nginx by returning a\n--                    429. (defaults to disabled)\n--    verbose - Enables (1) or disables (0) verbose logging. Messages are logged with a\n--              severity of \"ERROR\" so that nginx log levels do not need to be changed. (defaults\n--              to disabled)\n--    allowlist - List of allowlisted IP's and IP CIDR's. (defaults to empty)\n-- audit_blocked_requests - records the body and optionally headers of requests that are being blocked within nginx (defaults disabled, if enabled the uris to audit must be given)\n-- audit_include_headers - if audit_blocked_requests is enabled, also record the headers (defaults disabled)\n-- audit_uri_list - a list of endpoints that will be audited if audit_blocked_requests is enabled (defaults to empty)\n--\nclient = require(\"resty.iprepd\").new({\n  api_key = os.getenv(\"IPREPD_API_KEY\"),\n  threshold = 50,\n  url = \"http://127.0.0.1:8080\",\n  timeout = 10,\n  cache_ttl = 30,\n  cache_buffer_count = 1000,\n  cache_errors = 1,\n  cache_errors_ttl = 10,\n  statsd_host = \"127.0.0.1\",\n  statsd_port = 8125,\n  statsd_max_buffer_count = 100,\n  statsd_flush_timer = 10,\n  blocking_mode = 0,\n  verbose = 0,\n  allowlist = \"127.0.0.1,10.10.10.0/24,192.168.0.16/16\",\n  audit_blocked_requests = 0,\n  audit_include_headers = 0,\n  audit_uri_list = \"/uri1,/uri2\",\n})\n```\n\n\n### Metrics (statsd)\n\n#### Metrics that are collected\n\n| name | type | description |\n|---|---|---|\n| iprepd.status.below_threshold | count | The reputation for the client ip is below the configured threshold. |\n| iprepd.status.rejected | count | The request was blocked (won\u2019t be sent if `blocking_mode` is disabled). |\n| iprepd.status.accepted | count | The request was accepted. The reputation can still be below the threshold if `blocking_mode` is disabled.\n| iprepd.get_reputation | count | Request to iprepd |\n| iprepd.cache_hit | count | Got reputation from internal cache |\n| iprepd.err.timeout | count | Request to iprepd timed out |\n| iprepd.err.500 | count | Got a 500 response from iprepd |\n| iprepd.err.401 | count | Got a 401 response from iprepd, usually means the API key in use is invalid or being sent incorrectly by nginx. |\n| iprepd.err.dns_timeout | count | DNS resolution of the iprepd URL's domain name timed out. Make sure to check nginx's [resolver_timeout](https://nginx.org/en/docs/http/ngx_http_core_module.html#resolver_timeout) setting |\n| iprepd.err.* | count | Got an error while sending a request to iprepd. This could be other 4xx or 5xx status codes for example. |\n\n\n#### Setting up custom metrics\n\nYou can use `client.statsd` (where `client = require(\"resty.iprepd\").new({...})`) to submit your\nown custom metrics. Do note that there is no prefix, so it will act as any other statsd client.\n\n##### Available statsd functions\n\n```lua\nclient = require(\"resty.iprepd\").new({...})\n\nclient.statsd.count(name, value)\nclient.statsd.incr(name) # Increments a count by 1\nclient.statsd.time(name, value)\nclient.statsd.set(name, value)\n```\n\n##### Example within nginx config\n```\ninit_by_lua_block {\n  client = require(\"resty.iprepd\").new({\n    url = os.getenv(\"IPREPD_URL\"),\n    api_key = os.getenv(\"IPREPD_API_KEY\"),\n    statsd_host = os.getenv(\"STATSD_HOST\"),\n  })\n}\n\ninit_worker_by_lua_block {\n  # async flushing of metrics\n  client:config_flush_timer()\n}\n\nserver {\n  ...\n\n  location / {\n    ...\n\n    access_by_lua_block {\n      client:check(ngx.var.remote_addr)\n    }\n\n    log_by_lua_block {\n      # This conditional is not required, but can be helpful to not cause problems\n      # if you want to temporarily disable statsd. This will evaluate to false if\n      # `statsd_host` is not set.\n      if client.statsd then\n        # Here is our custom metric\n        client.statsd.set(\"iprepd.ips_seen\", ngx.var.remote_addr)\n      end\n    }\n  }\n}\n```\n\n### Common Gotchas\n\n* Make sure iprepd-nginx is seeing the real client IP. You will usually need to use something like [ngx_http_realip_module](https://nginx.org/en/docs/http/ngx_http_realip_module.html), and confirm that it is configured correctly.\n* Logging is rather inflexible and certain messages can't be suppressed - i.e. those related to flushing of metrics.\nex.\n```\n[alert] 7#7: *381345 send() returned zero, context: ngx.timer\n[alert] 7#7: *381392 send() returned zero (115: Operation now in progress), context: ngx.timer\n```\nThis can be rather confusing but is expected behaviour.\n\n## Running iprepd-nginx dev environment locally\nA dev environment is provided that can be used to test iprepd-nginx with iprepd and a dummy backend service (in this case nginx with static content).\n\n```\n$ make build\n$ make run_dev_env\n```\n\nSanity tests can then be run by:\n```\n$ make build_test\n$ make smoke_test\n```\n\n## Running just iprepd-nginx locally\nCreate a `.env` file in this repo with the needed environment variables (documentation below).\n\nThen run:\n```\n$ make build\n$ make run_dev\n```\n\nThen you will be able to hit this proxy with: `curl http://localhost:80`\n\n### Environment Variables for Dev\n\n#### Note:\n\nQuotations in env vars matter with nginx. Don't use them if you are using `--env-file` in Docker.\n\n```\n# required\nbackend=http://<>               # URL to proxy to\nIPREPD_URL=http://<>            # iprepd url\nIPREPD_API_KEY=\"api-key\"        # iprepd api key\nIPREPD_REPUTATION_THRESHOLD=50  # iprepd reputation threshold, block all IP's with a reputation below the threshold\n\n#\n# optional\n#\nIPREPD_TIMEOUT=10\nIPREPD_CACHE_TTL=30\nIPREPD_CACHE_ERRORS=0\nIPREPD_CACHE_BUFFER_COUNT=5000\nIPREPD_ALLOWLIST=10.0.0.0/8,127.0.0.1/32\nSTATSD_HOST=127.0.0.1\nSTATSD_PORT=8125\nSTATSD_MAX_BUFFER_COUNT=200\nSTATSD_FLUSH_TIMER=2\nBLOCKING_MODE=0\nAUDIT_BLOCKED_REQUESTS=0\nAUDIT_INCLUDE_HEADERS=0\nAUDIT_URI_LIST=/uri1,/uri2\n```\n"
},
{
  "name": "margo",
  "files": {
    "/": [
      ".github",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "abuse_test.go",
      "cmd",
      "doc.go",
      "errors.go",
      "errors_test.go",
      "example_test.go",
      "examples",
      "firefoxkeys.go",
      "fuzz.go",
      "get_firefox_keys.sh",
      "go.mod",
      "mar.go",
      "mar_test.go",
      "oldmar_test.go",
      "parser.go",
      "parser_test.go",
      "sign.go",
      "sign_test.go",
      "tools",
      "verify.go",
      "verify_test.go"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "PARSE_FILE := cmd/margo_verify_firefox/parse.go\n\nall: lint vet test getsamplemar testparser testsigner\n\nlint:\n\tgolint go.mozilla.org/mar\n\nvet:\n\tgo vet -composites=false go.mozilla.org/mar\n\ntest:\n\tgo test -covermode=count -coverprofile=coverage.out go.mozilla.org/mar\n\ncoverage: test\n\tgo tool cover -html=coverage.out\n\ngetkeys:\n\tbash get_firefox_keys.sh\n\ngetsamplemar:\n\t@if [ ! -e firefox-60.0esr-60.0.1esr.partial.mar ]; then \\\n\t\twget http://download.cdn.mozilla.net/pub/firefox/releases/60.0.1esr/update/win64/en-US/firefox-60.0esr-60.0.1esr.partial.mar ;\\\n\tfi\n\ntestparser:\n\tgo run -ldflags \"-X go.mozilla.org/mar.debug=true\" ${PARSE_FILE} firefox-60.0esr-60.0.1esr.partial.mar 2>&1 | grep 'signature: OK, valid signature from release1_sha384'\n\ntestsigner:\n\tgo run -ldflags \"-X go.mozilla.org/mar.debug=true\" examples/sign.go firefox-60.0esr-60.0.1esr.partial.mar /tmp/resigned.mar\n\tgo run ${PARSE_FILE} /tmp/resigned.mar\n\ngetmarcorpus:\n\t@if [ ! -e /tmp/marworkdir ]; then mkdir /tmp/marworkdir; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-1.5rc2-1.5.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/1.5/update/win32/en-US/firefox-1.5rc2-1.5.partial.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-10.0esr-10.0.1esr.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/10.0.1esr/update/linux-x86_64/fr/firefox-10.0esr-10.0.1esr.partial.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-2.0.0.1.complete.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/2.0.0.1/update/win32/en-US/firefox-2.0.0.1.complete.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-2.0-2.0.0.1.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/2.0.0.1/update/mac/ru/firefox-2.0-2.0.0.1.partial.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-3.5.13-3.5.14.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/3.5.14/update/win32/fy-NL/firefox-3.5.13-3.5.14.partial.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-36.0b4-36.0b5.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/36.0b5/update/linux-i686/ga-IE/firefox-36.0b4-36.0b5.partial.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-4.0rc1-4.0rc2.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/4.0rc2/update/win32/sv-SE/firefox-4.0rc1-4.0rc2.partial.mar; fi\n\t@if [ ! -e /tmp/marworkdir/firefox-60.0esr-60.0.1esr.partial.mar ]; then wget -P /tmp/marworkdir http://download.cdn.mozilla.net/pub/firefox/releases/60.0.1esr/update/win64/en-US/firefox-60.0esr-60.0.1esr.partial.mar; fi\n\ntestmarcorpus:\n\tfor f in $$(ls /tmp/marworkdir/firefox*.mar); do go run ${PARSE_FILE} \"$$f\"; done\n\nfuzz: getmarcorpus\n\tgo get -u github.com/dvyukov/go-fuzz/...\n\tgo-fuzz-build go.mozilla.org/mar\n\tgo-fuzz -bin=mar-fuzz.zip -workdir=/tmp/marworkdir\n\n.PHONY: all lint vet test getkeys getsamplemar testparser\n",
  "readme": "# MARGO: Mozilla ARchive library written in Go\n\n[![Test](https://github.com/mozilla-services/margo/actions/workflows/test.yml/badge.svg)](https://github.com/mozilla-services/margo/actions/workflows/testg.yml)\n[![GoDoc](https://godoc.org/go.mozilla.org/mar?status.svg)](https://godoc.org/go.mozilla.org/mar)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla-services/margo/badge.svg?branch=master)](https://coveralls.io/github/mozilla-services/margo?branch=master)\n\n`import \"go.mozilla.org/mar\"`\n\n**Requires Go 1.10**\n\nMargo is a fairly secure MAR parser written to allow\n[autograph](https://github.com/mozilla-services/autograph) to sign Firefox\nMAR files. Its primary focus is signature, but it can also be used to parse,\ncreate and verify signatures on existing MAR files.\n\nTake a look at `example_test.go` for a taste of the API, or run the command line\ntools under `examples/`.\n\n## FAQ\n### Why is it called \"margo\"?\nit's subtle: it's a \"mar\" library, written in \"go\". get it? \"margo\"!\n"
},
{
  "name": "monitoring-reports",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "build.sh",
      "incident",
      "pingdom",
      "slo"
    ]
  },
  "makefile": null,
  "readme": "# Monitoring Reports\n\nThis repo is for storing code related to generating reports based on our\nmonitoring data.\n\nMore documentation is available at https://mana.mozilla.org/wiki/display/SVCOPS/Monitoring+Reports\n\n## Incidents\n\nincident_report.py pulls data from the Pagerduty API, creates a JSON file, and\nuploads it to S3.\n\nThe script is meant to be run daily as a Lambda function. Running `build.sh incident`\nwill produce incident_lambda.zip, which is ready to deploy to Lambda.\n\nYou must set `API_KEY` and `S3_BUCKET` in the enviroment before\nrunning this script.  Other settings exist and are documented in settings.py\n\nThe output is suitable for querying via Athena. A script for setting up Athena,\n`setup_athena.py` is provided which takes one argument, which should be the value\nyou used for `S3_BUCKET`.\n\n## SLO\n\n** Notice: We have stopped using StatusPage and thus this report is no longer run **\n\nslo_report.py pulls data from the Statuspage API, creates a JSON file, and\nuploads it to S3.\n\nThe script is meant to be run daily as a Lambda function. Running `build.sh slo`\nwill produce slo_lambda.zip, which is ready to deploy to Lambda.\n\nYou must set `API_KEY` and `S3_BUCKET` in the enviroment before\nrunning this script.  Other settings exist and are documented in settings.py\n\nThe output is suitable for querying via Athena. A script for setting up Athena,\n`setup_athena.py` is provided which takes one argument, which should be the value\nyou used for `S3_BUCKET`.\n"
},
{
  "name": "contile-loadtests",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "LICENSE",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "loadtest.py",
      "requirements.txt",
      "unicode_cldr_subdivision_codes.xml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Contile Load Tests\n\n## Requirements:\n\n- [Python 3.5+](https://www.python.org/downloads/)\n\n## Installation:\n\n```sh\n$ virtualenv venv -p python3\n$ source ./venv/bin/activate\n$ pip install -r requirements.txt\n```\n\n## Usage:\n\nThe load tests make use of the following environment variables:\n* `TIMEOUT`: The timeout (in seconds) for the request made to Contile (defaults to 5)\n* `TARGET_URL`: The URL of the endpoint to be load tested (defaults to `http://localhost:8000/v1/tiles`)\n* `TEST_LOCATION_HEADER_NAME`: The of the HTTP header used to manually specify the location from which the request originated. This should match the value of CONTILE_LOCATION_TEST_HEADER on Contile (defaults to `X-Test-Location`)\n\nThe load tests were written using\n[Molotov](https://molotov.readthedocs.io/en/stable/) and can be started using\nthe following command:\n\n```sh\n$ molotov -c -v d <duration> -s <scenario name>\n```\n\nwhere `<duration>` is the length of time in seconds that you want the load test\nto run, and `<scenario name>` is one of the following:\n* `request_from_consistent_location_with_consistent_user_agent`: Requests all originate from the same location (USCA) and have the same user agent (Google Chrome on Windows)\n* `request_from_random_location_with_consistent_user_agent`: Requests originate from randomly-chosen locations but have the same user agent (Google Chrome on Windows)\n* `request_from_consistent_location_with_random_user_agent`: Requests originate from the same location but have randomly-chosen user agents\n* `request_from_random_location_with_random_user_agent`: Requests originate from a randomly-chosen location and have randomly-chosen user agents\n\nCheck the Molotov documentation for details on other options available to run\nthe tests.\n\n```sh\n$ molotov -h\n```\n\n"
},
{
  "name": "docs",
  "files": {
    "/": [
      ".gitignore",
      ".hgignore",
      ".python-version",
      "CODE_OF_CONDUCT.md",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.rst",
      "make.bat",
      "requirements.txt",
      "source"
    ]
  },
  "makefile": "# Makefile for Sphinx documentation\n\n# You can set these variables from the command line.\nSPHINXOPTS    =\nSPHINXBUILD   = $(shell echo $${VIRTUAL_ENV-venv})/bin/sphinx-build\nPAPER         =\nBUILDDIR      = build\n\n# Internal variables.\nPAPEROPT_a4     = -D latex_paper_size=a4\nPAPEROPT_letter = -D latex_paper_size=letter\nALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) source\n\n.PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest venv build\n\nhelp:\n\t@echo \"Please use \\`make <target>' where <target> is one of\"\n\t@echo \"  html       to make standalone HTML files\"\n\t@echo \"  dirhtml    to make HTML files named index.html in directories\"\n\t@echo \"  singlehtml to make a single large HTML file\"\n\t@echo \"  pickle     to make pickle files\"\n\t@echo \"  json       to make JSON files\"\n\t@echo \"  htmlhelp   to make HTML files and a HTML help project\"\n\t@echo \"  qthelp     to make HTML files and a qthelp project\"\n\t@echo \"  devhelp    to make HTML files and a Devhelp project\"\n\t@echo \"  epub       to make an epub\"\n\t@echo \"  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter\"\n\t@echo \"  latexpdf   to make LaTeX files and run them through pdflatex\"\n\t@echo \"  text       to make text files\"\n\t@echo \"  man        to make manual pages\"\n\t@echo \"  changes    to make an overview of all changed/added/deprecated items\"\n\t@echo \"  linkcheck  to check all external links for integrity\"\n\t@echo \"  doctest    to run all doctests embedded in the documentation (if enabled)\"\n\nclean:\n\t-rm -rf $(BUILDDIR)/*\n\nhtml:\n\t$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html\n\t@echo\n\t@echo \"Build finished. The HTML pages are in $(BUILDDIR)/html.\"\n\ndirhtml:\n\t$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml\n\t@echo\n\t@echo \"Build finished. The HTML pages are in $(BUILDDIR)/dirhtml.\"\n\nsinglehtml:\n\t$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml\n\t@echo\n\t@echo \"Build finished. The HTML page is in $(BUILDDIR)/singlehtml.\"\n\npickle:\n\t$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle\n\t@echo\n\t@echo \"Build finished; now you can process the pickle files.\"\n\njson:\n\t$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json\n\t@echo\n\t@echo \"Build finished; now you can process the JSON files.\"\n\nhtmlhelp:\n\t$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp\n\t@echo\n\t@echo \"Build finished; now you can run HTML Help Workshop with the\" \\\n\t      \".hhp project file in $(BUILDDIR)/htmlhelp.\"\n\nqthelp:\n\t$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp\n\t@echo\n\t@echo \"Build finished; now you can run \"qcollectiongenerator\" with the\" \\\n\t      \".qhcp project file in $(BUILDDIR)/qthelp, like this:\"\n\t@echo \"# qcollectiongenerator $(BUILDDIR)/qthelp/Sync.qhcp\"\n\t@echo \"To view the help file:\"\n\t@echo \"# assistant -collectionFile $(BUILDDIR)/qthelp/Sync.qhc\"\n\ndevhelp:\n\t$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp\n\t@echo\n\t@echo \"Build finished.\"\n\t@echo \"To view the help file:\"\n\t@echo \"# mkdir -p $$HOME/.local/share/devhelp/Sync\"\n\t@echo \"# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/Sync\"\n\t@echo \"# devhelp\"\n\nepub:\n\t$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub\n\t@echo\n\t@echo \"Build finished. The epub file is in $(BUILDDIR)/epub.\"\n\nlatex:\n\t$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex\n\t@echo\n\t@echo \"Build finished; the LaTeX files are in $(BUILDDIR)/latex.\"\n\t@echo \"Run \\`make' in that directory to run these through (pdf)latex\" \\\n\t      \"(use \\`make latexpdf' here to do that automatically).\"\n\nlatexpdf:\n\t$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex\n\t@echo \"Running LaTeX files through pdflatex...\"\n\tmake -C $(BUILDDIR)/latex all-pdf\n\t@echo \"pdflatex finished; the PDF files are in $(BUILDDIR)/latex.\"\n\ntext:\n\t$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text\n\t@echo\n\t@echo \"Build finished. The text files are in $(BUILDDIR)/text.\"\n\nman:\n\t$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man\n\t@echo\n\t@echo \"Build finished. The manual pages are in $(BUILDDIR)/man.\"\n\nchanges:\n\t$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes\n\t@echo\n\t@echo \"The overview file is in $(BUILDDIR)/changes.\"\n\nlinkcheck:\n\t$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck\n\t@echo\n\t@echo \"Link check complete; look for any errors in the above output \" \\\n\t      \"or in $(BUILDDIR)/linkcheck/output.txt.\"\n\ndoctest:\n\t$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest\n\t@echo \"Testing of doctests in the sources finished, look at the \" \\\n\t      \"results in $(BUILDDIR)/doctest/output.txt.\"\n\nspelling:\n\t$(SPHINXBUILD) -b spelling $(ALLSPHINXOPTS) $(BUILDDIR)/doctest\n\t@echo \"Spell checking of docs is finished.\"\n\nbuild: venv/bin/sphinx-build html\n\nvenv/bin/sphinx-build: requirements.txt\n\tvirtualenv --python=python2 ./venv\n\t ./venv/bin/pip install pip sphinx\n\t ./venv/bin/pip install -r requirements.txt\n\ttouch ./venv/bin/sphinx-build\n",
  "readme": "==============================\nMozilla Services Documentation\n==============================\n\nThis repository hosts the source documentation for the \"docs\" site at:\n\n  https://docs.services.mozilla.com\n\n\nTo build it you will need to install python (see .python-version for supported version) and virtualenv, then do the\nfollowing::\n\n    $ make build\n\nThis should produce a \"build/html\" directory containing the generated HTML\ndocumentation.\n"
},
{
  "name": "fx-sig-verify",
  "files": {
    "/": [
      ".bandit-baseline.json",
      ".cookiecutterrc",
      ".coveragerc",
      ".devcontainer",
      ".dockerignore",
      ".editorconfig",
      ".github",
      ".gitignore",
      ".pre-commit-config.yaml",
      ".secrets.baseline",
      "AUTHORS.rst",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "Dockerfile.buster",
      "Dockerfile.buster.debug",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "ci",
      "docs",
      "entry.sh",
      "requirements-dev.txt",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "src",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "authors.rst",
      "changelog.rst",
      "conf.py",
      "contributing.rst",
      "index.rst",
      "installation-on-aws.rst",
      "installation.rst",
      "readme.rst",
      "reference",
      "requirements.txt",
      "spelling_wordlist.txt",
      "summary.rst",
      "usage.rst"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "# To customize for your aws account & bucket, export the following shell\n# variables:\n#\tS3_BUCKET   <-- bucket to use\n#\tLAMBDA\t    <-- function name\n#\n# All other AWS information is supplied the normal way.\n\nPYTHON_VERSION := 3.6\nVENV_NAME=venv\nNOW := $(shell date -u +%Y-%m-%dT%H:%M:%S)\n\n# dev account defaults\nAWS_ACCOUNT_ID=361527076523\nECR_REPO_NAME=fx-sig-verify\n\n# dev account defaults\nAWS_REGION_DEV := us-west-2\nAWS_PROFILE_DEV := cloudservices-aws-dev\nAWS_ACCOUNT_ID_DEV := 927034868273\nECR_REPO_NAME_DEV := fx-sig-verify-dev\n\n# Defaults for docker-debug\nPRODUCTION_DEFAULT = 0\t# set to 1 for skips\nVERBOSE_DEFAULT = 2\t# 2 is max, 0 is quiet\n# show the details of the build (which build-kit hides by default)\n# DOCKER_BUILD_OPTIONS := --progress plain\n\n.PHONY: help\nhelp:\n\t@echo \"help\t\tthis list\"\n\t@echo \"docker-shell-prod    obtain shell in production docker container\"\n\t@echo \"docker-shell-debug   obtain shell in debug docker container\"\n\t@echo \"\"\n\t@echo \"upload\t\tupload prod container to AWS prod account\"\n\t@echo \"upload-dev\tupload prod container to AWS dev account\"\n\t@echo \"publish\t\tpublish prod container on AWS\"\n\t@echo \"invoke\t\texecute test cases against AWS\"\n\t@echo \"invoke-docker    execute test cases against AWS from a\"\n\t@echo \"                 local docker instance with RIE installed\"\n\t@echo \"docker-debug\tstart local debug container\"\n\t@echo \"docker-build-all\tbuild all containers\"\n\t@echo \"docker-build-prod    build production container\"\n\t@echo \"docker-build-debug   build debug container from prod\"\n\t@echo \"tests\t\texecute local tests locally via tox\"\n\t@echo \"docker-debug-tests   execute local tests in docker\"\n\t@echo \"populate_s3\tupload test data to S3\"\n\t@echo \"\"\n\t@echo \"clean            remove built files\"\n\t@echo \"\"\n\t@echo \"generate-docs    Generate docs locally\"\n\n\n\n.PHONY: clean\nclean:\n\trm -f Dockerfile*built\n\trm -f fxsv.zip\n\n# upload shouldn't rebuild the container -- we already tested locally.\n# (you did test locally first, didn't you?)\n.PHONY: upload upload-dev\nupload:\n\t@echo \"Using AWS credentials for $$AWS_DEFAULT_PROFILE in $$AWS_REGION\"\n\tdocker tag fxsigverify $(AWS_ACCOUNT_ID).dkr.ecr.$(AWS_REGION).amazonaws.com/$(ECR_REPO_NAME)\n\taws ecr get-login-password --region $(AWS_REGION) | docker login --username AWS --password-stdin $(AWS_ACCOUNT_ID).dkr.ecr.$(AWS_REGION).amazonaws.com\n\tdocker push $(AWS_ACCOUNT_ID).dkr.ecr.$(AWS_REGION).amazonaws.com/$(ECR_REPO_NAME)\n\nupload-dev:\n\t$(MAKE) AWS_PROFILE=$(AWS_PROFILE_DEV) AWS_ACCOUNT_ID=$(AWS_ACCOUNT_ID_DEV) ECR_REPO_NAME=$(ECR_REPO_NAME_DEV) AWS_REGION=$(AWS_REGION_DEV) upload\n\n.PHONY: publish publish-dev\npublish: upload\n\t@echo \"Go use the console - aws cli is currently f'd up wrt lambda containers\"\n\t@false\n\t@echo \"Using AWS credentials for $$AWS_DEFAULT_PROFILE in $$AWS_REGION\"\n\twhich aws2\n\taws2 lambda update-function-code \\\n\t    --region $${AWS_REGION} \\\n\t    --function-name hwine_fxsv_container \\\n\t    --image-uri 927034868273.dkr.ecr.us-west-2.amazonaws.com/fx-sig-verify-dev:latest \\\n\t    --dry-run \\\n\npublish-dev:\n\t$(MAKE) AWS_ACCOUNT_ID=$(AWS_ACCOUNT_ID_DEV) ECR_REPO_NAME=$(ECR_REPO_NAME_DEV) publish\n\n# The following targets exercise the actual lambda code, and having that\n# code interact with real AWS services. So valid credentials must be\n# applied.\n#\n# The \"-docker\" versions do so to the code deployed in a container with\n# the lambda Remote Interface Emulater installed\n#\n# The ones without that suffix invoke the lambda on AWS\n.PHONY: invoke invoke-no-error invoke-error invoke-docker invoke-no-error-docker invoke-error-docker\ninvoke-no-error:\n\t@test -n \"$$S3_BUCKET\" || ( echo \"You must define S3_BUCKET\" ; false )\n\t@test -n \"$$LAMBDA\" || ( echo \"You must define LAMBDA\" ; false )\n\t@rm -f invoke_output-no-error.json\n\t@echo \"Using AWS credentials for $$AWS_DEFAULT_PROFILE in $$AWS_REGION\"\n\t@echo \"Should not return error (but some 'fail')\"\n\taws lambda invoke \\\n\t\t--region $${AWS_REGION} \\\n\t\t--function-name $(LAMBDA) \\\n\t\t--payload \"$$(sed -e 's/hwine-ffsv-dev/$(S3_BUCKET)/g' \\\n\t\t                  -e 's/1970-01-01T00:00:00/$(NOW)/g' \\\n\t\t\t\t  tests/data/S3_event_template-no-error.json)\" \\\n\t\tinvoke_output-no-error.json ; \\\n\t    if test -s invoke_output-no-error.json; then \\\n\t\tjq . invoke_output-no-error.json ; \\\n\t    fi\n\ninvoke-no-error-docker:\n\t@test -n \"$$S3_BUCKET\" || ( echo \"You must define S3_BUCKET\" ; false )\n\t@test -n \"$$LAMBDA\" || ( echo \"You must define LAMBDA\" ; false )\n\t@rm -f invoke_output-no-error.json\n\t@echo \"Using AWS credentials for $$AWS_DEFAULT_PROFILE in $$AWS_REGION\"\n\t@echo \"Should not return error (but some 'fail')\"\n\tcurl http://localhost:9000/2015-03-31/functions/function/invocations \\\n\t\t--data \"$$(sed -e 's/hwine-ffsv-dev/$(S3_BUCKET)/g' \\\n\t\t                  -e 's/1970-01-01T00:00:00/$(NOW)/g' \\\n\t\t\t\t  tests/data/S3_event_template-no-error.json)\" \\\n\t\t> invoke_output-no-error.json ; \\\n\t    if test -s invoke_output-no-error.json; then \\\n\t\tjq . invoke_output-no-error.json ; \\\n\t    fi\n\ninvoke-error:\n\t@test -n \"$$S3_BUCKET\" || ( echo \"You must define S3_BUCKET\" ; false )\n\t@test -n \"$$LAMBDA\" || ( echo \"You must define LAMBDA\" ; false )\n\t@rm -f invoke_output-error.json\n\t@echo \"Using AWS credentials for $$AWS_DEFAULT_PROFILE in $$AWS_REGION\"\n\t@echo \"Should return error\"\n\taws lambda invoke \\\n\t\t--region $${AWS_REGION} \\\n\t\t--function-name $(LAMBDA) \\\n\t\t--payload \"$$(sed -e 's/hwine-ffsv-dev/$(S3_BUCKET)/g' \\\n\t\t                  -e 's/1970-01-01T00:00:00/$(NOW)/g' \\\n\t\t\t\t  tests/data/S3_event_template-error.json)\" \\\n\t\tinvoke_output-error.json ; \\\n\t    if test -s invoke_output-error.json; then \\\n\t\tjq . invoke_output-error.json ; \\\n\t    fi\n\ninvoke-error-docker:\n\t@test -n \"$$S3_BUCKET\" || ( echo \"You must define S3_BUCKET\" ; false )\n\t@test -n \"$$LAMBDA\" || ( echo \"You must define LAMBDA\" ; false )\n\t@rm -f invoke_output-error.json\n\t@echo \"Using AWS credentials for $$AWS_DEFAULT_PROFILE in $$AWS_REGION\"\n\t@echo \"Should return error\"\n\tcurl http://localhost:9000/2015-03-31/functions/function/invocations \\\n\t\t--data \"$$(sed -e 's/hwine-ffsv-dev/$(S3_BUCKET)/g' \\\n\t\t                  -e 's/1970-01-01T00:00:00/$(NOW)/g' \\\n\t\t\t\t  tests/data/S3_event_template-error.json)\" \\\n\t\t> invoke_output-error.json ; \\\n\t    if test -s invoke_output-error.json; then \\\n\t\tjq . invoke_output-error.json ; \\\n\t    fi\n\ninvoke: invoke-no-error invoke-error\ninvoke-docker: invoke-no-error-docker invoke-error-docker\n\nPHONY: docker-build-prod\ndocker-build-prod:\n\tdocker build $(DOCKER_BUILD_OPTIONS) -t fxsigverify -f Dockerfile.buster .\n\nPHONY: docker-build-debug\ndocker-build-debug: docker-build-prod\n\tenv PRODUCTION=$(or $(PRODUCTION),$(PRODUCTION_DEFAULT)) \\\n\tdocker build $(DOCKER_BUILD_OPTIONS) -t fxsv-debug -f Dockerfile.buster.debug .\n\nPHONY: docker-build-all\ndocker-build-all: docker-build-prod docker-build-debug\n\n# We don't make docker-debug depend on docker-build-debug, as we may\n# want several runs as we change code\nPHONY: docker-debug\ndocker-debug:\n\tbash -xc ' \\\n\tdeclare -p $${!AWS*} ; \\\n\tenv VERBOSE=2 PRODUCTION=0 \\\n\tdocker run --rm -it \\\n\t    -e AWS_ACCESS_KEY_ID \\\n\t    -e AWS_REGION \\\n\t    -e AWS_DEFAULT_REGION \\\n\t    -e AWS_SECRET_ACCESS_KEY \\\n\t    -e AWS_SECURITY_TOKEN \\\n\t    -e AWS_SESSION_TOKEN \\\n\t    -e AWS_VAULT \\\n\t    -e PRODUCTION \\\n\t    -e SNSARN \\\n\t    -e VERBOSE \\\n\t    -p 9000:8080 \\\n\t    fxsv-debug \\\n\t'\n\nPHONY: docker-shell-prod\ndocker-shell-prod:\n\t@echo \"N.B. no environment variables set\"\n\tdocker run --rm -it --entrypoint bash fxsigverify\n\nPHONY: docker-shell-debug\ndocker-shell-debug:\n\t@echo \"N.B. no environment variables set\"\n\tdocker run --rm -it --entrypoint bash fxsv-debug\n\nPHONY: docker-debug-tests\ndocker-debug-tests:\n\tdocker run --rm --entrypoint pytest fxsv-debug:latest tests\n\n# idea from\n# https://stackoverflow.com/questions/23032580/reinstall-virtualenv-with-tox-when-requirements-txt-or-setup-py-changes#23039826\n.PHONY: tests\ntests: .tox/venv.touch\n\ttox $(REBUILD_FLAG)\n\n.tox/venv.touch: setup.py requirements.txt\n\t$(eval REBUILD_FLAG := --recreate)\n\tmkdir -p $$(dirname $@)\n\ttouch $@\n\n\n$(VENV_NAME):\n\tpython$(PYTHON_VERSION) -m venv  $@\n\t. $(VENV_NAME)/bin/activate && pip install --upgrade pip\n\t. $(VENV_NAME)/bin/activate && echo req*.txt | xargs -n1 pip install -r\n\t@echo \"Virtualenv created in $(VENV_NAME). You must activate before continuing.\"\n\t@false\n\n.PHONY:\tpopulate_s3\npopulate_s3:\n\t@test -n \"$$S3_BUCKET\" || ( echo \"You must define S3_BUCKET\" ; false )\n\t@echo \"Populating s3://$(S3_BUCKET) using current credentials & region\"\n\taws s3 cp tests/data/32bit_new.exe \"s3://$(S3_BUCKET)/32bit new.exe\"\n\taws s3 cp tests/data/32bit.exe \"s3://$(S3_BUCKET)/32bit.exe\"\n\taws s3 cp tests/data/32bit_new.exe \"s3://$(S3_BUCKET)/32bit_new.exe\"\n\taws s3 cp tests/data/32bit_new.exe \"s3://$(S3_BUCKET)/32bit+new.exe\"\n\taws s3 cp tests/data/32bit_sha1.exe \"s3://$(S3_BUCKET)/32bit_sha1.exe\"\n\taws s3 cp tests/data/bad_2.exe \"s3://$(S3_BUCKET)/bad_2.exe\"\n\taws s3 cp tests/data/signtool.exe \"s3://$(S3_BUCKET)/signtool.exe\"\n\taws s3 cp tests/data/32bit.exe \"s3://$(S3_BUCKET)/nightly/test/Firefox bogus thingy.exe\"\n\taws s3 cp tests/data/2019-06-64bit.exe \"s3://$(S3_BUCKET)/2019-06-64bit.exe\"\n\taws s3 cp tests/data/2020-05-32bit.exe \"s3://$(S3_BUCKET)/2020-05-32bit.exe\"\n\taws s3 cp tests/data/FxStub-87.0b2.exe \"s3://$(S3_BUCKET)/FxStub-87.0b2.exe\"\n\taws s3 cp tests/data/2021-05-signable-file.exe \"s3://$(S3_BUCKET)/2021-05-signable-file.exe\"\n\n\n.PHONY: generate-docs\ngenerate-docs:\n\ttox -e docs\n\n# vim: noet ts=8\n",
  "readme": "========\nOverview\n========\n\n\nDocumentation for this project is currently maintained restructured text format.\nA rendered version is available at https://fx-sig-verify.readthedocs.io/en/latest/ or read the source in the docs__ directory.\n\nIf you are just looking to run the scripts locally, use the\n`Dockerfile.dev-environment`_ to build a docker image to use. VS-Code will\noffer to do that for you. This is the recommended way to manually check\nbinaries.\n\n__ docs/\n\n.. start-badges\n\n.. list-table::\n    :stub-columns: 1\n\n    * - docs\n      - |docs|\n    * - tests\n      - |travis| |coveralls| |codecov|\n    * - version status\n      - |commits-since|\n\n\n..\n      - | |travis| |requires| |coveralls| |codecov|\n      - |version| |downloads| |wheel| |supported-versions| |supported-implementations| |commits-since|\n\n.. |docs| image:: https://readthedocs.org/projects/fx-sig-verify/badge/?style=flat\n    :target: https://fx-sig-verify.readthedocs.io/fx-sig-verify\n    :alt: Documentation Status\n\n.. |travis| image:: https://travis-ci.org/mozilla-services/fx-sig-verify.svg?branch=master\n    :alt: Travis-CI Build Status\n    :target: https://travis-ci.org/mozilla-services/fx-sig-verify\n\n.. |requires| image:: https://requires.io/github/mozilla-services/fx-sig-verify/requirements.svg?branch=master\n    :alt: Requirements Status\n    :target: https://requires.io/github/mozilla-services/fx-sig-verify/requirements/?branch=master\n\n.. |coveralls| image:: https://coveralls.io/repos/mozilla-services/fx-sig-verify/badge.svg?branch=master&service=github\n    :alt: Coverage Status\n    :target: https://coveralls.io/r/mozilla-services/fx-sig-verify\n\n.. |codecov| image:: https://codecov.io/github/mozilla-services/fx-sig-verify/coverage.svg?branch=master\n    :alt: Coverage Status\n    :target: https://codecov.io/github/mozilla-services/fx-sig-verify\n\n.. |version| image:: https://img.shields.io/pypi/v/fx-sig-verify.svg\n    :alt: PyPI Package latest release\n    :target: https://pypi.python.org/pypi/fx-sig-verify\n\n.. |commits-since| image:: https://img.shields.io/github/commits-since/mozilla-services/fx-sig-verify/v0.4.10.svg\n    :alt: Commits since latest release\n    :target: https://github.com/mozilla-services/fx-sig-verify/compare/v0.4.10...master\n\n.. |downloads| image:: https://img.shields.io/pypi/dm/fx-sig-verify.svg\n    :alt: PyPI Package monthly downloads\n    :target: https://pypi.python.org/pypi/fx-sig-verify\n\n.. |wheel| image:: https://img.shields.io/pypi/wheel/fx-sig-verify.svg\n    :alt: PyPI Wheel\n    :target: https://pypi.python.org/pypi/fx-sig-verify\n\n.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/fx-sig-verify.svg\n    :alt: Supported versions\n    :target: https://pypi.python.org/pypi/fx-sig-verify\n\n.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/fx-sig-verify.svg\n    :alt: Supported implementations\n    :target: https://pypi.python.org/pypi/fx-sig-verify\n\n\n.. end-badges\n\n.. _Dockerfile.dev-environment: ./Dockerfile.dev-environment\n"
},
{
  "name": "privacy-pack",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# privacy-pack"
},
{
  "name": "hawk-go",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE",
      "Makefile",
      "README.md",
      "go.mod",
      "go.sum",
      "hawk.go",
      "hawk_test.go"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "GO = GO15VENDOREXPERIMENT=1 go\nGOLINT = golint\nPROJECT = go.mozilla.org/hawk\n\ninstall-go-mod-upgrade:\n\tgo get -u github.com/oligot/go-mod-upgrade\n\nupgrade:\n\tgo-mod-upgrade\n\nall: test vet generate\n\ntag: all\n\tgit tag -s $(TAGVER) -a -m \"$(TAGMSG)\"\n\nlint:\n\t$(GOLINT) $(PROJECT)\n\nvet:\n\t$(GO) vet $(PROJECT)\n\ntest:\n\t$(GO) test -covermode=count -coverprofile=coverage.out $(PROJECT)\n\nshowcoverage: test\n\t$(GO) tool cover -html=coverage.out\n\ngenerate:\n\t$(GO) generate\n\n.PHONY: all test generate\n\n",
  "readme": "# hawk-go [![Build Status](https://github.com/mozilla-services/hawk-go/workflows/Test/badge.svg)](https://travis-ci.org/mozilla-services/hawk-go)\n\nhawk-go implements the [Hawk](https://github.com/hueniverse/hawk) HTTP\nauthentication scheme in Go.\n\n[**Documentation**](http://godoc.org/github.com/mozilla-services/hawk-go)\n\n## Installation\n\n```text\ngo get go.mozilla.org/hawk\n```\n"
},
{
  "name": "civet-docker",
  "files": {
    "/": [
      "README",
      "c++.mozilla.properties",
      "ce-mozilla.svg",
      "ce.pp",
      "ce.service",
      "execute.cfg",
      "execution.mozilla.properties",
      "get_mozbuild_exports.py",
      "override.conf",
      "server.conf"
    ]
  },
  "makefile": null,
  "readme": "##\n\nPuppet orchestration for Mozilla's Compiler Explorer instance.\n\n### GCP Requirements\n\nSetup Instructions (approximately...)\n\nThis is designed so that we use Google's HTTPS Load Balancer so they deal with the certificate stuff for us.\nWe run an openresty server on the box configured with the [SSO proxy](https://github.com/mozilla-iam/mozilla.oidc.accessproxy). That openresty server only knows HTTP; it doesn't mess with HTTPS at all.\nCompiler Explorer runs on the machine on port 10240 and openresty proxies to it.\n\n1. Reserve a Static IP Address in GCP; you'll need this. Put it in the 'Standard Network Tier' if you get an option.\n2. Set the domain name to point to that IP address in the registrar\n3. Add the domain to Google Webmaster Central and verify it (probably using the TXT verification method)\n4. (The rest of this is in GCP) Create a Machine Image (and a VM running the image) as per below\n5. Create an 'Instance Group' because Load Balancers can only point to groups.\n6. Add the single VM to the group and set up the group to not do any autoscaling.\n7. Go over to load balancing, and at the bottom go to the 'advanced' menu. Use that to create a SSL certificate.\n8. Go back to the main screen of Load Balancing and set up the HTTPS Load Balancer.  It'll be one backend service pointing to the instance group you made. Nothing in Host/Path rules. Frontend will be HTTPS using your reserved IP and your certificate you made in step 7.\n9. Now go back to Load Balancing and set up the HTTP Load Balancer. There's a [doc on this](https://cloud.google.com/load-balancing/docs/https/setting-up-http-https-redirect) but the gist is: no backend service; advanced host and path rule: full path redirect, 301 moved permanently, and HTTP Redirect Enable; and the frontend is the same IP and port 80.\n10. Go to IAM & Admin -> Service Accounts and create a Service Account. Give it the 'Secret Manager Secret Accessor' role.\n11. Go to Security -> Secrets Manager and create two secrets, one for the client id and one for the client secret. The Service Account should get automatic access to it, but make sure it can.\n\n#### Set up a machine image:\n\nCreate a VM instance using Ubuntu 20.04, at least 4 GB of RAM, 24 GB of disk, an SSH key to get in, give it the username 'ubuntu', allow HTTP/HTTPS traffic, and the correct service account and then do the following:\n\n(The specific username is important.)\n\n```\nsudo apt-get update\nsudo apt-get install puppet emacs\nsudo puppet module install puppetlabs-stdlib --version 7.0.0\nsudo puppet module install puppetlabs-vcsrepo --version 5.0.0\nsudo apt-get update\n\ncd /\nsudo git clone https://github.com/mozilla-services/civet-docker.git\n\nprintf \"#%s/bin/sh -e\\n\\n/usr/bin/puppet apply /civet-docker/ce.pp\" ! | sudo tee /etc/rc.local\nsudo chmod +x /etc/rc.local\n```\n\nEnsure that the machine has done all the puppet stuff. You can make sure compiler-explorer is running locally by doing `curl http://localhost:10240`.  It does take a few minutes for puppet to finish everything as compiling CE takes a few minutes at least.\n\n"
},
{
  "name": "conduit-testing",
  "files": {
    "/": [
      ".env-dist",
      ".gitignore",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# Conduit Testing\n\nThis repo contains tests that are to be run whenever a version of\nConduit is deployed.\n\n## Requirements\n\nThis project using Python 3.6.4 or greater and uses [Pipenv](https://pipenv.readthedocs.io/en/latest/)\nto manage dependencies. To get started using this project, please\ndo the following\n\n1. Make sure you have Python 3.6.4 or greater installed\n2. Follow the instructions for Pipenv to install it\n3. From inside this directory type the command `pipenv install`\n4. Activate the Python environment with the command `pipenv shell`\n\nYou will also need the following:\n\n* 2 sets of login/password credentials for accessing Phabricator in whatever evironment\nyou are testing in\n* 2 sets of login/password credentials that are allowed to create, read, update, and delete security bugs on BugZilla in whatever environment you are testing\n* 1 additional set of login/password credentials that are not allowed to access security bugs on BugZilla in whatever environment you are testing\n\nThis project uses [dotenv](https://github.com/theskumar/python-dotenv) to store credentials\nand other values needed by the tests. Refer to the instructions for `python-dotenv` for\ndetails on how it works and check the `.env-dist` file for the values that you need to set.\nIf you are going to use the `dotenv` approach, copy `.env-dist` to `.env` and fill in the\nrequired values.\n\nIf you are going to run these tests on a build server, please consult the documentation for\nthat specific build server on how to set the environment variables this test is expecting to\nexist.\n\n## Running the tests\n\nTo run the tests, use the following command:\n\n`pytest tests/`\n\n\n"
},
{
  "name": "hawkauthlib",
  "files": {
    "/": [
      ".gitignore",
      ".pylintrc",
      "CHANGES.txt",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTORS.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "hawkauthlib",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": "# -*- coding: utf-8; mode: makefile-gmake -*-\n\n.DEFAULT = help\nTEST ?= .\n\n# python version to use\nPY       ?=3\n# list of python packages (folders) or modules (files) of this build\nPYOBJECTS = hawkauthlib\n# folder where the python distribution takes place\nPYDIST   ?= dist\n# folder where the python intermediate build files take place\nPYBUILD  ?= build\n\nSYSTEMPYTHON = `which python$(PY) python | head -n 1`\nVIRTUALENV   = virtualenv --python=$(SYSTEMPYTHON)\nVENV_OPTS    = \"--no-site-packages\"\nTEST_FOLDER  = ./hawkauthlib/tests\n\nENV     = ./local/py$(PY)\nENV_BIN = $(ENV)/bin\n\n\nPHONY += help\nhelp::\n\t@echo  'usage:'\n\t@echo\n\t@echo  '  build     - build virtualenv ($(ENV)) and install *developer mode*'\n\t@echo  '  lint      - run pylint within \"build\" (developer mode)'\n\t@echo  '  test      - run tests for all supported environments (tox)'\n\t@echo  '  dist      - build packages in \"$(PYDIST)/\"'\n\t@echo  '  pypi      - upload \"$(PYDIST)/*\" files to PyPi'\n\t@echo  '  clean\t    - remove most generated files'\n\t@echo\n\t@echo  'options:'\n\t@echo\n\t@echo  '  PY=3      - python version to use (default 3)'\n\t@echo  '  TEST=.    - choose test from $(TEST_FOLDER) (default \".\" runs all)'\n\t@echo\n\t@echo  'Example; a clean and fresh build (in local/py3), run all tests (py27, py35, lint)::'\n\t@echo\n\t@echo  '  make clean build test'\n\t@echo\n\n\nPHONY += build\nbuild: $(ENV)\n\t$(ENV_BIN)/pip -v install -e .\n\n\nPHONY += lint\nlint: $(ENV)\n\t$(ENV_BIN)/pylint $(PYOBJECTS) --rcfile ./.pylintrc\n\nPHONY += test\ntest:  $(ENV)\n\t$(ENV_BIN)/tox -vv\n\n$(ENV):\n\t$(VIRTUALENV) $(VENV_OPTS) $(ENV)\n\t$(ENV_BIN)/pip install -r requirements.txt\n\n# for distribution, use python from virtualenv\nPHONY += dist\ndist:  clean-dist $(ENV)\n\t$(ENV_BIN)/python setup.py \\\n\t\tsdist -d $(PYDIST)  \\\n\t\tbdist_wheel --bdist-dir $(PYBUILD) -d $(PYDIST)\n\nPHONY += publish\npublish: dist\n\t$(ENV_BIN)/twine upload $(PYDIST)/*\n\nPHONY += clean-dist\nclean-dist:\n\trm -rf ./$(PYBUILD) ./$(PYDIST)\n\n\nPHONY += clean\nclean: clean-dist\n\trm -rf ./local ./.cache\n\trm -rf *.egg-info .coverage\n\trm -rf .eggs .tox html\n\tfind . -name '*~' -exec echo rm -f {} +\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name __pycache__ -exec rm -rf {} +\n\n# END of Makefile\n.PHONY: $(PHONY)\n",
  "readme": "=================================================================\nhawkauthlib:  library for implementing Hawk Access Authentication\n=================================================================\n\nThis is a low-level library for implementing Hawk Access Authentication, a\nsimple HTTP request-signing scheme described in:\n\n    https://npmjs.org/package/hawk\n\nTo access resources using Hawk Access Authentication, the client must have\nobtained a set of Hawk credentials including an id and a secret key.  They use\nthese credentials to make signed requests to the server.\n\nWhen accessing a protected resource, the server will generate a 401 challenge\nresponse with the scheme \"Hawk\" as follows::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n\n    < HTTP/1.1 401 Unauthorized\n    < WWW-Authenticate: Hawk\n\nThe client will use their Hawk credentials to build a request signature and\ninclude it in the Authorization header like so::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n    > Authorization: Hawk id=\"h480djs93hd8\",\n    >                     ts=\"1336363200\",\n    >                     nonce=\"dj83hs9s\",\n    >                     mac=\"bhCQXTVyfj5cmA9uKkPFx1zeOXM=\"\n\n    < HTTP/1.1 200 OK\n    < Content-Type: text/plain\n    <\n    < For your eyes only:  secret data!\n\n\nThis library provices the low-level functions necessary to implement such\nan authentication scheme.  For Hawk Auth clients, it provides the following\nfunction:\n\n    * sign_request(req, id, key, algorithm=\"sha256\"):  sign a request using\n      Hawk Access Auth.\n\nFor Hawk Auth servers, it provides the following functions:\n\n    * get_id(req):  get the claimed Hawk Auth id from the request.\n\n    * check_signature(req, key, algorithm=\"sha256\"):  check that the request\n      was signed with the given key.\n\nThe request objects passed to these functions can be any of a variety of\ncommon object types:\n\n    * a WSGI environment dict\n    * a webob.Request object\n    * a requests.Request object\n    * a string or file-like object of request data\n\nA typical use for a client program might be to install the sign_request\nfunction as an authentication hook when using the requests library, like this::\n\n    import requests\n    import functools\n    import hawkauthlib\n\n    # Hook up sign_request() to be called on every request.\n    def auth_hook(req):\n        hawkauthlib.sign_request(req, id=\"<AUTH-ID>\", key=\"<AUTH-KEY>\")\n        return req\n    session = requests.session(hooks={\"pre_request\": auth_hook})\n\n    # Then use the session as normal, and the auth is applied transparently.\n    session.get(\"http://www.secret-data.com/get-my-data\")\n\n\nA typical use for a server program might be to verify requests using a WSGI\nmiddleware component, like this::\n\n    class HawkAuthMiddleware(object):\n\n        # ...setup code goes here...\n\n        def __call__(self, environ, start_response):\n\n            # Find the identity claimed by the request.\n            id = hawkauthlib.get_id(environ)\n\n            # Look up their secret key.\n            key = self.SECRET_KEYS[id]\n\n            # If the signature is invalid, error out.\n            if not hawkauthlib.check_signature(environ, key):\n                start_response(\"401 Unauthorized\",\n                               [(\"WWW-Authenticate\", \"Hawk\")])\n                return [\"\"]\n\n            # Otherwise continue to the main application.\n            return self.application(environ, start_response)\n\n\nThe following features of the Hawk protocol are not yet supported:\n\n  * Bewits.\n  * Timestamp adjustment.\n  * Calculating or verifying the server's response signature.\n  * Calculating or verifying payload hashes.\n"
},
{
  "name": "autograph-canary",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "Dockerfile",
      "Dockerfile.js-devtools",
      "Dockerfile.lambda-emulator",
      "Makefile",
      "README.md",
      "autograph.py",
      "bin",
      "doc",
      "docker-compose.override.yml.example",
      "docker-compose.yml",
      "package-lock.json",
      "package.json",
      "requirements.txt",
      "tests",
      "version.json",
      "version.sh"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "build:\n\tdocker-compose build\nintegration-test:\n\t./bin/run_integration_tests.sh\nformat-js:\n\tdocker-compose run --rm js-devtools format\nformat: format-js\nformat-check:\n\tgit diff --exit-code tests/  # fail if js isn't formatted\nemulator-shell:\n\tdocker-compose exec emulator /bin/bash\n",
  "readme": "# autograph-canary\n\n[![CircleCI](https://circleci.com/gh/mozilla-services/autograph-canary/tree/main.svg?style=svg)](https://circleci.com/gh/mozilla-services/autograph-canary/tree/main)\n\nautograph-canary is [a containerized AWS\nlambda](https://docs.aws.amazon.com/lambda/latest/dg/lambda-images.html)\nfor running Firefox integration tests against signed\n[autograph](https://github.com/mozilla-services/autograph/)\nartifacts. It uses XPConnect to exercise Firefox client code against\nsigned XPI/Addons and content signtures.\n\n## Usage\n\n### Installation\n\nTo download the built image from dockerhub run:\n\n```sh\ndocker pull mozilla/autograph-canary\n```\n\nor [see\nbelow](https://github.com/mozilla-services/autograph-canary/blob/main/README.md#command-line)\nto build it locally.\n\n## AWS Lambda\n\n#### Environment Varables\n\nThe following environment variables with their default values below\nconfigure logging verbosity, tests to run, and test targets.\n\nWhat log level should be used (use INFO for less verbose logging):\n\n```sh\nCANARY_LOG_LEVEL=debug\n```\n\nWhich XPCShell test files in `tests/` to run (as matched by [pathlib\nglob][py3_pathlib_glob]):\n\n```sh\nTEST_FILES_GLOB=\"*_test.js\"\n```\n\n##### Addon / XPI Signature Verification\n\nWhich PKI root to verify addons against. Defaults to `prod`, use\n`stage` to set `xpinstall.signatures.dev-root` to true (Fx Nightly\nonly):\n\n```sh\nXPI_ENV=prod\n```\n\nWhich XPI URLs to download and install as a CSV:\n\n```sh\nXPI_URLS=https://addons.mozilla.org/firefox/downloads/file/3772109/facebook_container-2.2.1-fx.xpi,https://addons.mozilla.org/firefox/downloads/file/3713375/firefox_multi_account_containers-7.3.0-fx.xpi,https://addons.mozilla.org/firefox/downloads/file/3768975/ublock_origin-1.35.2-an+fx.xpi\n```\n\n##### Content Signature Verification\n\nWhich prefs to use for content signature settings server URL, bucket,\nand root hash (`prod` or `stage` with an optional `-preview` suffix\nsame as [remotesettings devtools][rsdevtools]):\n\n```sh\nCSIG_ENV=prod\n```\n\nWhich content signature collections to verify. Collections must all\nuse the same `CSIG_ENV` and be a CSV list formatted as\n\"$BUCKET_NAME/$COLLECTION_NAME\". Use `bin/list_collections.sh` to list\npublicly available collections:\n\n```sh\nCSIG_COLLECTIONS=blocklists/gfx,blocklists/addons-bloomfilters,blocklists/plugins,blocklists/addons,blocklists/certificates,main/normandy-recipes,main/normandy-recipes-capabilities,main/hijack-blocklists,main/search-config,security-state/onecrl,security-state/intermediates\n```\n\n[py3_pathlib_glob]: https://docs.python.org/3/library/pathlib.html#pathlib.Path.glob\n[rsdevtools]: https://github.com/mozilla-extensions/remote-settings-devtools\n\n#### Event payload\n\nTo support running from scheduled events, autograph-canary ignores event payloads.\n\n### Command line\n\nTo run the default set of autograph-canary tests:\n\n1. install docker and docker-compose\n\n1. Run `docker-compose build canary` to build the canary container\n\n1. Run `docker-compose run canary` to run `autograph.py` from the main entrypoint\n\nTo run integration tests in the containerized AWS lambda emulator:\n\n1. install docker and docker-compose\n\n1. Run `make build` to build the canary and emulator containers\n\n1. Run `make integration-test`, which starts the emulator and runs `bin/run_integration_tests.sh`\n\n## Development\n\n1. install docker and docker-compose\n\n1. run `cp docker-compose.override.yml.example docker-compose.override.yml`\n\n1. Run `make build` to build the canary and emulator containers\n\n1. Run `docker-compose up -d emulator` to start the emulator container\n\n1. Run `make emulator-shell` to log into the emulator container. In\n   the container run `cp local/autograph.py . && python autograph.py`\n   to run tests without rebuilding the container.\n"
},
{
  "name": "tuxedo",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "Dockerfile",
      "README.md",
      "__init__.py",
      "apps",
      "bin",
      "inc",
      "manage.py",
      "media",
      "requirements.txt",
      "requirements",
      "settings-dist.py",
      "sql",
      "templates",
      "urls.py",
      "version.json",
      "wsgi"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "tuxedo\n======\n\nAn improved version of the [Bouncer](https://wiki.mozilla.org/Bouncer) mirror\nmanagement software.\n\nBouncer is a Mozilla project. The new admin backend was originally written by\nFrederic Wenzel (fwenzel@mozilla.com).\n\nGetting Started\n---------------\n\n### Python\nYou need Python 2.6. Also, you probably want to run this application in a\n[virtualenv][virtualenv] environment.\n\nRun ``easy_install pip`` followed by ``pip install -r requirements.txt``\nto install the required Python libraries.\n\n[virtualenv]: http://pypi.python.org/pypi/virtualenv\n\n### git submodules\nThe list of known languages is provided by ``languages.json`` in the directory\n``inc/product-details/json`` and imported as a [git submodule][git-submodule].\nThe source of the data is the [Mozilla product-details library][prod-details].\n\nTo initialize the submodule (or pull updates from it), run this from the tuxedo\nroot directory:\n\n    git submodule update --init\n\n[git-submodule]: http://www.kernel.org/pub/software/scm/git/docs/git-submodule.html\n[prod-details]: http://svn.mozilla.org/libs/product-details/\n\n\n### Initial Database Setup\nIf you're installing a new copy of Bouncer, run ``./manage.py syncdb``\nfollowed by ``./manage.py migrate`` (see \"Database Migrations\" below).\n\n### Database Migrations\nI am using [South](http://south.aeracode.org/) to keep track of database\nmigrations. ``./manage.py migrate`` will apply these migrations when\nnecessary.\n\n### Upgrading an older version of Bouncer\nIf you are upgrading from an earlier version of Bouncer that isn't locale-\naware yet, apply ``sql/bouncer-add-lang.sql`` first.\n\nThen, apply ``sql/incremental.sql`` to bring the DB up to date.\n\nFinally, run the following steps to initialize Django and South:\n\n    ./manage.py syncdb   # initialize django\n    # (answer \"no\" to the \"add a new admin?\" question)\n    ./manage.py migrate mirror 0001 --fake   # initialize South\n    ./manage.py migrate   # apply all existing migrations\n\nTuxedo API and mod_wsgi\n-----------------------\nThere are multiple ways to deploy a Django project on a web server. If you\nwant to use Apache with mod_wsgi, make sure to set it up so it passes\n[HTTP Basic Auth credentials][userauth] on to the application, otherwise\nthe Tuxedo API won't work.\n\n[userauth]: http://code.google.com/p/modwsgi/wiki/ConfigurationGuidelines#User_Authentication\n\nWhy \"tuxedo\"?\n-------------\nIn my silly mind, I chose to interpret \"Bouncer\" as \"doorman\" and subsequently\ndecided that its new version should be codenamed like what fancy bouncers wear:\na tuxedo.\n\nLicensing\n---------\nThis software is licensed under the [Mozilla Tri-License][MPL]:\n\n    ***** BEGIN LICENSE BLOCK *****\n    Version: MPL 1.1/GPL 2.0/LGPL 2.1\n\n    The contents of this file are subject to the Mozilla Public License Version\n    1.1 (the \"License\"); you may not use this file except in compliance with\n    the License. You may obtain a copy of the License at\n    http://www.mozilla.org/MPL/\n\n    Software distributed under the License is distributed on an \"AS IS\" basis,\n    WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License\n    for the specific language governing rights and limitations under the\n    License.\n\n    The Original Code is tuxedo.\n\n    The Initial Developer of the Original Code is Mozilla.\n    Portions created by the Initial Developer are Copyright (C) 2010\n    the Initial Developer. All Rights Reserved.\n\n    Contributor(s):\n      Frederic Wenzel <fwenzel@mozilla.com>\n\n    Alternatively, the contents of this file may be used under the terms of\n    either the GNU General Public License Version 2 or later (the \"GPL\"), or\n    the GNU Lesser General Public License Version 2.1 or later (the \"LGPL\"),\n    in which case the provisions of the GPL or the LGPL are applicable instead\n    of those above. If you wish to allow use of your version of this file only\n    under the terms of either the GPL or the LGPL, and not to allow others to\n    use your version of this file under the terms of the MPL, indicate your\n    decision by deleting the provisions above and replace them with the notice\n    and other provisions required by the GPL or the LGPL. If you do not delete\n    the provisions above, a recipient may use your version of this file under\n    the terms of any one of the MPL, the GPL or the LGPL.\n\n    ***** END LICENSE BLOCK *****\n\n[MPL]: http://www.mozilla.org/MPL/\n\n"
},
{
  "name": "requests-hawk",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CHANGES.txt",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "MANIFEST.in",
      "README.rst",
      "requests_hawk",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "Requests-Hawk\n#############\n\n|pypi| |travis|\n\n.. |travis| image:: https://travis-ci.org/mozilla-services/requests-hawk.png\n    :target: https://travis-ci.org/mozilla-services/requests-hawk\n\n.. |pypi| image:: https://img.shields.io/pypi/v/requests-hawk.svg\n    :target: https://pypi.python.org/pypi/requests-hawk\n\n\nThis project allows you to use `the python requests library\n<http://python-requests.org/>`_ with `the hawk authentication\n<https://github.com/hueniverse/hawk>`_ mechanism.\n\nHawk itself does not provide any mechanism for obtaining or transmitting the\nset of shared credentials required, but this project proposes a scheme we use\nacross mozilla services projects.\n\nGreat, how can I use it?\n========================\n\nFirst, you'll need to install it:\n\n.. code-block:: bash\n\n    pip install requests-hawk\n\nThen, in your project, if you know the `id` and `key`, you can use:\n\n.. code-block:: python\n\n    import requests\n    from requests_hawk import HawkAuth\n\n    hawk_auth = HawkAuth(id='my-hawk-id', key='my-hawk-secret-key')\n    requests.post(\"https://example.com/url\", auth=hawk_auth)\n\nOr if you need to derive them from the hawk session token, instead use:\n\n.. code-block:: python\n\n    import requests\n    from requests_hawk import HawkAuth\n\n    hawk_auth = HawkAuth(\n        hawk_session=resp.headers['hawk-session-token'],\n        server_url=self.server_url\n    )\n    requests.post(\"/url\", auth=hawk_auth)\n\nIn the second example, the ``server_url`` parameter to ``HawkAuth`` was used to\nprovide a default host name, to avoid having to repeat it for each request.\n\nIf you wish to override the default algorithm of ``sha256``, pass the desired\nalgorithm name using the optional ``algorithm`` parameter.\n\nNote: The ``credentials`` parameter has been removed. Instead pass ``id`` and\n``key`` separately (as above), or pass the existing dict as ``**credentials``.\n\nIntegration with httpie\n=======================\n\n`Httpie <https://github.com/jakubroztocil/httpie>`_ is a tool which lets you do\nrequests to a distant server in a nice and easy way. Under the hood, ``httpie``\nuses the requests library. We've made it simple for you to plug hawk with it.\n\nIf you know the id and key, use it like that:\n\n.. code-block:: bash\n\n   http POST localhost:5000/registration\\\n   --auth-type=hawk --auth='id:key'\n\nOr, if you want to use the hawk session token, you can do as follows:\n\n.. code-block:: bash\n\n   http POST localhost:5000/registration\\\n   --auth-type=hawk --auth='c0d8cd2ec579a3599bef60f060412f01f5dc46f90465f42b5c47467481315f51:'\n\nTake care, don't forget to add the extra ``:`` at the end of the hawk session\ntoken for it to be considered like so.\n\nHow are the shared credentials shared?\n======================================\n\nOkay, on to the actual details.\n\nThe server gives you a session token, that you'll need to derive to get the\nhawk credentials.\n\nDo an HKDF derivation on the given session token. You'll need to use the\nfollowing parameters:\n\n.. code-block:: python\n\n    key_material = HKDF(hawk_session, '', 'identity.mozilla.com/picl/v1/sessionToken', 32*2)\n\nThe key material you'll get out of the HKDF needs to be separated into two\nparts, the first 32 hex characters are the ``hawk id``, and the next 32 ones are the\n``hawk key``:\n\n.. code-block:: python\n\n    credentials = {\n        'id': keyMaterial[0:32]\n        'key': keyMaterial[32:64]\n        'algorithm': 'sha256'\n    }\n\nRun tests\n=========\n\nTo run test, you can use tox:\n\n.. code-block:: bash\n\n    tox\n"
},
{
  "name": "pollbot-integration-tests",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Jenkinsfile",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "config-tests",
      "conftest.py",
      "manifest.ini",
      "pipenv.txt",
      "setup.cfg"
    ]
  },
  "makefile": null,
  "readme": "[![License](https://img.shields.io/badge/License-Mozilla%202.0-blue.svg)](https://github.com/mozilla-services/pollbot-integration-tests/blob/master/LICENSE)\n[![Build Status](https://travis-ci.org/mozilla-services/pollbot-integration-tests.svg?branch=master)](https://travis-ci.org/mozilla-services/pollbot-integration-tests)\n[![updates](https://api.dependabot.com/badges/status?host=github&repo=mozilla-services/pollbot-integration-tests)](https://dependabot.com)\n\n\n# Summary\nTests for the Pollbot server fall into 3 categories:\n\n1. unit tests - located here: https://github.com/mozilla/PollBot/tree/master/tests\n2. configuration check tests - located in this repo\n\nThis repo is a clearinghouse for all automated tests that don't need\nto reside in their own repository.  They would include a variety of\ntest types that usually fall in the middle of the test pyramid: API\ntests, config and URL checks, deployment tests, end-2-end tests,\nsecurity tests, etc.\n\n## Preparing the tests\n\nTo run the tests, you need to have the following installed:\n\n* Python 3.6 or greater\n* [Pipenv](https://pipenv.readthedocs.io/en/latest/)\n\n\n## Running the Tests\n\nYou can run these tests using the following commands from inside the root directory for the project.\n\n```shell\npipenv install\npipenv shell\npytest --env=TEST_ENV\n```\n\n* `TEST_ENV` is one of the environments listed in the `manifest.ini` file.\n"
},
{
  "name": "deadpool",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "benches",
      "examples",
      "lapin",
      "postgres",
      "redis",
      "src",
      "sync-readme.py",
      "test-build.sh",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Deadpool [![Latest Version](https://img.shields.io/crates/v/deadpool.svg)](https://crates.io/crates/deadpool) [![Build Status](https://img.shields.io/github/workflow/status/bikeshedder/deadpool/Rust)](https://github.com/bikeshedder/deadpool/actions?query=workflow%3ARust)\n\n\nDeadpool is a dead simple async pool for connections and objects\nof any type.\n\nThis crate provides two implementations:\n\n- Managed pool (`deadpool::managed::Pool`)\n  - Creates and recycles objects as needed\n  - Useful for [database connection pools](#database-connection-pools)\n  - Enabled via the `managed` feature in your `Cargo.toml`\n\n- Unmanaged pool (`deadpool::unmanaged::Pool`)\n  - All objects either need to to be created by the user and added to the\n    pool manually. It is also possible to create a pool from an existing\n    collection of objects.\n  - Enabled via the `unmanaged` feature in your `Cargo.toml`\n\n## Features\n\n| Feature | Description | Extra dependencies | Default |\n| ------- | ----------- | ------------------ | ------- |\n| `managed` | Enable managed pool implementation | `async-trait` | yes |\n| `unmanaged` | Enable unmanaged pool implementation | - | yes |\n| `config` | Enable support for [config](https://crates.io/crates/config) crate | `config`, `serde/derive` | yes |\n| `rt_tokio_1` | Enable support for [tokio](https://crates.io/crates/tokio) crate | `tokio/time` | no |\n| `rt_async-std_1` | Enable support for [async-std](https://crates.io/crates/config) crate | `async-std` | no |\n\nThe runtime features (`rt_*`) are only needed if you need support for\ntimeouts. If you try to use timeouts without specifying a runtime at\npool creation the pool get methods will return an\n`PoolError::NoRuntimeSpecified` error.\n\n## Managed pool (aka. connection pool)\n\nThis is the obvious choice for connection pools of any kind. Deadpool already\ncomes with a couple of [database connection pools](#database-connection-pools)\nwhich work out of the box.\n\n### Example\n\n```rust,ignore\nuse async_trait::async_trait;\n\n#[derive(Debug)]\nenum Error { Fail }\n\nstruct Computer {}\nstruct Manager {}\ntype Pool = deadpool::managed::Pool<Manager>;\n\nimpl Computer {\n    async fn get_answer(&self) -> i32 {\n        42\n    }\n}\n\n#[async_trait]\nimpl deadpool::managed::Manager for Manager {\n    type Type = Computer;\n    type Error = Error;\n    async fn create(&self) -> Result<Computer, Error> {\n        Ok(Computer {})\n    }\n    async fn recycle(&self, conn: &mut Computer) -> deadpool::managed::RecycleResult<Error> {\n        Ok(())\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    let mgr = Manager {};\n    let pool = Pool::new(mgr, 16);\n    let mut conn = pool.get().await.unwrap();\n    let answer = conn.get_answer().await;\n    assert_eq!(answer, 42);\n}\n```\n\n### Database connection pools\n\nDeadpool supports various database backends by implementing the\n`deadpool::managed::Manager` trait. The following backends are\ncurrently supported:\n\nBackend | Crate | Latest Version |\n------- | ----- | -------------- |\n[tokio-postgres](https://crates.io/crates/tokio-postgres) | [deadpool-postgres](https://crates.io/crates/deadpool-postgres) | [![Latest Version](https://img.shields.io/crates/v/deadpool-postgres.svg)](https://crates.io/crates/deadpool-postgres) |\n[lapin](https://crates.io/crates/lapin) (AMQP) | [deadpool-lapin](https://crates.io/crates/deadpool-lapin) | [![Latest Version](https://img.shields.io/crates/v/deadpool-lapin.svg)](https://crates.io/crates/deadpool-lapin) |\n[redis](https://crates.io/crates/redis) | [deadpool-redis](https://crates.io/crates/deadpool-redis) | [![Latest Version](https://img.shields.io/crates/v/deadpool-redis.svg)](https://crates.io/crates/deadpool-redis) |\n[async-memcached](https://crates.io/crates/async-memcached) | [deadpool-memcached](https://crates.io/crates/deadpool-memcached) | [![Latest Version](https://img.shields.io/crates/v/deadpool-memcached.svg)](https://crates.io/crates/deadpool-memcached) |\n\n### Reasons for yet another connection pool\n\nDeadpool is by no means the only pool implementation available. It does\nthings a little different and that is the main reason for it to exist:\n\n- **Deadpool is compatible with any executor.** Objects are returned to the\n  pool using the `Drop` trait. The health of those objects is checked upon\n  next retrieval and not when they are returned. Deadpool never performs any\n  actions in the background. This is the reason why deadpool does not need\n  to spawn futures and does not rely on a background thread or task of any\n  type.\n\n- **Identical startup and runtime behaviour**. When writing long running\n  application there usually should be no difference between startup and\n  runtime if a database connection is temporarily not available. Nobody\n  would expect an application to crash if the database becomes unavailable\n  at runtime. So it should not crash on startup either. Creating the pool\n  never fails and errors are only ever returned when calling `Pool::get()`.\n\n  If you really want your application to crash on startup if objects can\n  not be created on startup simply call\n  `pool.get().await.expect(\"DB connection failed\")` right after creating\n  the pool.\n\n- **Deadpool is fast.** Whenever working with locking primitives they are\n  held for the shortest duration possible. When returning an object to the\n  pool a single mutex is locked and when retrieving objects from the pool\n  a Semaphore is used to make this Mutex as little contested as possible.\n\n- **Deadpool is simple.** Dead simple. There is very little API surface.\n  The actual code is barely 100 lines of code and lives in the two functions\n  `Pool::get` and `Object::drop`.\n\n### Differences to other connection pool implementations\n\n- [`r2d2`](https://crates.io/crates/r2d2) provides a lot more configuration\n  options but only provides a synchroneous interface.\n\n- [`bb8`](https://crates.io/crates/bb8) provides an `async/.await` based\n  interface and provides the same configuration options as `r2d2`. It\n  depends on the tokio executor though and the code is more complex.\n\n- [`mobc`](https://crates.io/crates/mobc) provides an `async/.await` based\n  interface and provides a lot more configuration options. It requires an\n  executor though and the code is a lot more complex.\n\n## Unmanaged pool\n\nAn unmanaged pool is useful when you can't write a manager for the objects\nyou want to pool or simply don't want to. This pool implementation is slightly\nfaster than the managed pool because it does not use a `Manager` trait to\n`create` and `recycle` objects but leaves it up to the user.\n\n### Unmanaged pool example\n\n```rust,ignore\nuse deadpool::unmanaged::Pool;\n\nstruct Computer {}\n\nimpl Computer {\n    async fn get_answer(&self) -> i32 {\n        42\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    let pool = Pool::from(vec![\n        Computer {},\n        Computer {},\n    ]);\n    let s = pool.get().await.unwrap();\n    assert_eq!(s.get_answer().await, 42);\n}\n```\n\n## License\n\nLicensed under either of\n\n- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or <http://www.apache.org/licenses/LICENSE-2.0)>\n- MIT license ([LICENSE-MIT](LICENSE-MIT) or <http://opensource.org/licenses/MIT)>\n\nat your option.\n"
},
{
  "name": "megaphone-tests",
  "files": {
    "/": [
      ".env-dist",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "conftest.py",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# Megaphone Tests\r\n\r\nThis repository contains tests that should be run whenever the [Megaphone](https://github.com/mozilla-services/megaphone)\r\nservice gets deployed.\r\n\r\n## Requirements\r\n\r\nTo run these tests you need to have the following:\r\n\r\n* This repo checked out to your filesystem\r\n* Bearer tokens for use with Megaphone (both for reading and broadcasting)\r\n* Python 3.6.7\r\n* [Pipenv](https://pipenv.readthedocs.io/en/latest)\r\n\r\n## Creating Testing Environment\r\n\r\n* Copy `.env-dist` to `.env`\r\n* Verify that the values in `.env` correspond to the target Megaphone deployment you are testing. `ENDPOINT` is the Megaphone instance being tested. `WS_ENDPOINT` should belong to an appropriate Autopush endpoint as found in the [autopush docs](https://autopush.readthedocs.io/en/latest/#autopush-endpoints). See below for further details.  \r\n* Run the command `pipenv install` to create a virtual environment and install the required dependencies\r\n* Run the command `pipenv shell` to enter the virtual environment (which populates environment variables the tests are expecting)\r\n\r\n## Populating .env\r\n\r\nYou might have been provided with credentials like the following: \r\n\r\n```\r\nFor dev.megaphone.nonprod.cloudops.mozgcp.net: \r\n\tbroadcasterAuth: \r\n\t\tdev-testing-broadcaster=[\"123abc\"]}'\r\n        readerAuth: \r\n\t\tdev-testing-reader=[\"789xyz\"]}'\r\n\r\n```\r\nIn that case, your `.env` might look like: \r\n\r\n```\r\nENDPOINT=\"https://dev.megaphone.nonprod.cloudops.mozgcp.net/\"\r\nWS_ENDPOINT=\"wss://autopush.dev.mozaws.net/\"\r\nBROADCASTER_ID=\"dev-testing-broadcaster\"\r\nBROADCASTER_TOKEN=\"123abc\"\r\nREADER_ID=\"dev-testing-reader\"\r\nREADER_TOKEN=\"789xyz\"\r\n```\r\n\r\n## Running The Tests\r\n\r\nFrom inside the virtual environment, run the following command to execute the tests\r\n\r\n`pytest -v`\r\n"
},
{
  "name": "txstatsd",
  "files": {
    "/": [
      ".bzrignore",
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "example-stats-client.tac",
      "requirements.txt",
      "setup.py",
      "statsd.tac",
      "tox.ini",
      "twisted",
      "txstatsd.conf-example",
      "txstatsd"
    ]
  },
  "makefile": null,
  "readme": "Dependencies\n------------\n\n* Python\n\n* The twisted python package (python-twisted on Debian or similar systems)\n\n[![Build Status](https://travis-ci.org/sidnei/txstatsd.svg?branch=master)](https://travis-ci.org/sidnei/txstatsd)\n\nThings present here\n-------------------\n\n* The txstatsd python package, containing a server and a client implementation\n  for the statsd protocol.\n\n* The metrics support borrows from Coda Hale's Metrics project\n  https://github.com/codahale/metrics.\n\nLicense\n-------\n\ntxStatsD is open source software, MIT License. See the LICENSE file for more\ndetails.\n"
},
{
  "name": "canonicaljson-rs",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CHANGELOG.md",
      "CODE-OF-CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "demo",
      "src"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Canonical JSON library\n\nCanonical JSON is a variant of JSON in which each value has a single,\nunambiguous serialized form. This provides meaningful and repeatable hashes\nof encoded data.\n\nCanonical JSON can be parsed by regular JSON parsers. The most notable differences compared to usual JSON format ([RFC 7159](https://tools.ietf.org/html/rfc7159) or ``serde_json::to_string()``) are:\n\n- Object keys must appear in lexiographical order and must not be repeated\n- No inter-token whitespace\n- Unicode characters and escaped characters are escaped\n\nThis library follows [gibson's Canonical JSON spec](https://github.com/gibson042/canonicaljson-spec).\n\n## Usage\n\nAdd this to your ``Cargo.toml``:\n\n```toml\n[dependencies]\ncanonical_json = \"0.4.0\"\n```\n\n## Examples\n\n```rust,no_run\n   use serde_json::json;\n   use canonical_json::ser::to_string;\n\n   fn main() {\n     to_string(&json!(null)); // returns \"null\"\n\n     to_string(&json!(\"we \u2764 Rust\")); // returns \"we \\u2764 Rust\"\"\n\n     to_string(&json!(10.0_f64.powf(21.0))); // returns \"1e+21\"\n\n     to_string(&json!({\n         \"a\": \"a\",\n         \"id\": \"1\",\n         \"b\": \"b\"\n     })); // returns \"{\"a\":\"a\",\"b\":\"b\",\"id\":\"1\"}\"; (orders object keys)\n\n     to_string(&json!(vec![\"one\", \"two\", \"three\"])); // returns \"[\"one\",\"two\",\"three\"]\"\n   }\n```\n\n## Test suite\n\nRun the projet test suite:\n\n```\n$ cargo test\n```\n\nRun @gibson042's Canonical JSON test suite:\n\n```\n$ git clone git@github.com:gibson042/canonicaljson-spec.git\n$ cd canonicaljson-spec/\n$ ./test.sh ../canonicaljson-rs/demo/target/debug/demo\n```\n\nSome known errors:\n\n- `lone leading surrogate in hex escape`\n- `number out of range`\n- `Non-token input after 896 characters: \"\\u6} surrogate pair\\u2014U+1D306`\n\n\n## See also\n\n* [python-canonicaljson-rs](https://github.com/mozilla-services/python-canonicaljson-rs/): Python bindings for this crate\n* [CanonicalJSON.jsm](https://searchfox.org/mozilla-central/rev/358cef5d1a87172f23b15e1a705d6f278db4cdad/toolkit/modules/CanonicalJSON.jsm) in Gecko\n* [Original python implementation](https://github.com/Kinto/kinto-signer/blob/6.1.0/kinto_signer/canonicaljson.py) in Remote Settings\n* https://github.com/matrix-org/python-canonicaljson/  (encodes unicode with ``\\xDD`` instead of ``\\uDDDD``)\n\n## License\n\nLicensed under MIT\n"
},
{
  "name": "bare-necessities",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CONTRIBUTING.md",
      "Dockerfile",
      "LICENSE.txt",
      "Makefile",
      "Procfile",
      "README.md",
      "docker-compose.yml",
      "migrations",
      "pyproject.toml",
      "requirements",
      "setup.cfg",
      "setup.py",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "NAME := bare-necessities\nVENV := $(shell echo $${VIRTUAL_ENV-.venv})\nVERSION_FILE := $(shell echo $${VERSION_FILE-src/version.json})\nSOURCE := $(shell git config remote.origin.url | sed -e 's|git@|https://|g' | sed -e 's|github.com:|github.com/|g')\nVERSION := $(shell git describe --always --tag)\nCOMMIT := $(shell git log --pretty=format:'%H' -n 1)\nPYTHON := $(VENV)/bin/python3\nVIRTUALENV := virtualenv --python=python3.8\nPIP_INSTALL := $(VENV)/bin/pip install --progress-bar=off\nINSTALL_STAMP := $(VENV)/.install.stamp\n\nall: check-types check-format test\n\n$(PYTHON):\t\n\t$(VIRTUALENV) $(VENV)\n\n$(VERSION_FILE):\n\techo '{\"name\":\"$(NAME)\",\"version\":\"$(VERSION)\",\"source\":\"$(SOURCE)\",\"commit\":\"$(COMMIT)\"}' > $(VERSION_FILE)\n\ninstall: $(INSTALL_STAMP) $(COMMIT_HOOK)\n$(INSTALL_STAMP): $(PYTHON) requirements/dev.txt requirements/defaults.txt\n\t$(PIP_INSTALL) -U -r requirements/defaults.txt -r requirements/dev.txt\n\ttouch $(INSTALL_STAMP)\n\ntest: $(INSTALL_STAMP) $(VERSION_FILE)\n\tSQLALCHEMY_DATABASE_URI=sqlite:///:memory: PYTHONPATH=. $(VENV)/bin/pytest\n\ncheck-types: $(INSTALL_STAMP)\n\t$(VENV)/bin/mypy --config setup.cfg\n\ncheck-format: $(INSTALL_STAMP)\n\t$(VENV)/bin/black --config pyproject.toml --diff --check .\n\nclean:\n\tfind . -type d -name \"__pycache__\" | xargs rm -rf {};\n\trm -rf $(VENV)\n\n.PHONY: clean test check-types src/version.json",
  "readme": "<img align=\"right\" width=\"300\" src=\"https://freesvg.org/img/standing-bear.png\">\n\n*Look for the bare necessities  \nThe simple bare necessities  \nForget about your worries and your strife  \nI mean the bare necessities  \nOld Mother Nature's recipes  \nThat brings the bare necessities of life*\n\n# Bare Necessities\n\nA minimalist web application for Mozilla cloud services that does 99% of what you'll need to run stuff in prod.\n\n\nIt implements [Dockerflow](https://github.com/mozilla-services/Dockerflow/) using the excellent [python-dockerflow](https://python-dockerflow.readthedocs.io/).  \nRequires Python 3.8 with [Black formatting](https://black.readthedocs.io/en/stable/), [mypy type checking](https://mypy.readthedocs.io/en/stable/), etc.  \nUses [Flask](https://flask.palletsprojects.com/en/1.1.x/) for the web side.  \n[Celery](http://docs.celeryproject.org/en/latest/index.html) for the worker side.  \n[SQLAlchemy & PostgreSQL](https://docs.sqlalchemy.org/en/13/dialects/postgresql.html) for the data side.  \n\n## Development\n\nFor local development, setup a virtualenv with\n```bash\n$ virtualenv env\n$ source env/bin/activate\n$ pip install -r requirements/defaults.txt -r requirements/dev.txt\n```\n\nYou can then run the development server using `gunicorn --reload --chdir src src.web.wsgi:app`\n\nIf you want a full stack, with database, workers and so on, use `docker-compose up --build`.\n\n## Heroku\n\nUse your SSO account to [log into heroku](https://sso.mozilla.com/heroku) and verify that you have access to it. (and it not, [follow this doc](https://mana.mozilla.org/wiki/display/TS/Using+SSO+with+your+Heroku+account))\n\nThen from the command line:\n\n``` bash\nheroku login\nheroku create $(whoami)-barenecessities-$(date +%s)\ngit push heroku master\n```\n\nDeployment to heroku is also automated from circleci using a dedicated service account and a non-expiring API key. The documentation on how to get one of these [is on mana](https://mana.mozilla.org/wiki/display/TS/Obtaining+non-expiring+API+keys).\n\n## How does it work?\n\nBare-necessities is a cloud service composed of a web api, a database and an asynchronous worker. The web api and the worker talk to each other via tasks that transit through the database.\n\nThe API exposes its endpoints in `src/web/views.py`. You can add and remove endpoints from that file directly, or add more from another file.\n\n### standard http endpoints\n\nThe [Dockerflow](https://github.com/mozilla-services/Dockerflow/) standard requires web apis to expose several endpoints: **__version**, **__heartbeat__** and **__lbheartbeat__**. Those endpoints are meant to be used by the infrastructure to help operate production services. In this application, the standard http endpoints are transparently enabled by the **python-dockerflow** package, so you won't find them in `src/web/views.py`.\n\n### logging\n\nAll applications must log to standard output in the [JSON mozlog format](https://wiki.mozilla.org/Firefox/Services/Logging). Bare-necessities implements this using the [python-dockerflow](https://python-dockerflow.readthedocs.io/en/latest/logging.html) package, in two different ways:\n\n1. Request logging is configured in `src/web/api.py` to log every request summary using mozlog. (the `request.summary` logger in `src/config.py`)\n2. Application logging is done by obtaining a logger via `log = logging.getLogger(\"web.api\")` and simply logging with `log.info(\"my log message\")`. You can also pass a dictionary of fields to the `extra` parameters of the `log.info` call, like so `log.info(\"hello\", extra={\"world\": name})`. For examples, see `src/web/views.py`.\n\n### docker setup\n\nThe standard dockerflow process to deploy a service to production is to package the application into a docker container and upload that container to hub.docker.com. The [Mozilla's organization on hub.docker.com](https://hub.docker.com/u/mozilla) lists all the applications being hosted there.\n\nBuilding and uploading an application container must be done in CI, typically circleci or taskcluster, such that it doesn't depend on any particular individual running the task by hand.\n\nThe `Dockerfile` at the root of the repository is used to package the application. That Dockerfile is referenced by **docker-compose** in its configuration `docker-compose.yml`, as follows:\n\n```yaml\n  web:\n    container_name: bare-necessities\n    build:\n        context: .\n    image: mozilla/bare-necessities\n```\n\nHere, `build.context: .` means the `Dockerfile` is located in the same folder as the `docker-compose.yml` file. In the circleci configuration at `.circleci/config.yml`, the task `build-images` will then create the docker images by calling `docker-compose build`.\n\nIn order to upload containers, you'll need to ask cloudops to create the dockerhub repository for you. They'll give you back credentials to put in the DOCKERHUB_REPO, DOCKER_USER and DOCKER_PASS environment variables of circleci. Then, the circleci task `upload-docker-images` takes care of publishing images to dockerhub.\n\n## Database\n\nThe postgres database is managed by the application through SQLAlchemy. The tables are defined under `src/db/models` as classes that inherit the `db.Model` class.\n\n### sqlite\n\nFor local development and testing, it is possible to use sqlite instead of postgresql.\n\nTo create a sqlite local database under `/tmp/bare_necessities.sqlite`, use the following command:\n\n```\n$ SQLALCHEMY_DATABASE_URI=sqlite:////tmp/bare_necessities.sqlite \\\n  PYTHONPATH=. \\\n  FLASK_APP=src/web/api.py \\\n  flask db upgrade\n```\n\nYou can then start the web api using this command:\n\n```\n$ SQLALCHEMY_DATABASE_URI=sqlite:////tmp/bare_necessities.sqlite \\\n  PYTHONPATH=. \\\n  FLASK_APP=src/web/api.py \\\n  gunicorn --chdir src --reload src.web.wsgi:app\n```\n\nThe api will be listening on `127.0.0.1:8000`.\n\n### database migrations\n\nThe database schema is entirely managed by flask via the alembic package. Changes to running databases are applied as migration that can be upgraded and downgraded as needed.\n\n*Note: this section is convoluted due to the way docker-compose work. Ideally, these commands would run in the `web` container, but because its volume is read-only, flask can't write migration files. Therefore, we run the flask commands locally by pointing at the running `db` container.*\n\nFirst, spin up a stack in docker-compose. The database instance will be instanciated without any table.\n\n```\n$ docker-compose up --build\n```\n\nThen, retrieve the IP address of the `db` container.\n\n```\n$ export DBIP=$(docker inspect bare-necessities-db| jq -r '.[].NetworkSettings.Networks.barenecessities_default.IPAddress')\n```\n\nRun the `flask db init` command against the `db` container. This will create a local folder named `migrations` with all the boilerplate needed to handle schema migrations.\n\n```\n$ SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://postgres:postgres@${DBIP}:5432/bare_necessities \\\n  PYTHONPATH=. \\\n  FLASK_APP=src/web/api.py \\\n  flask db init\n```\n\nRun the `flask db migrate` command to create the initial schema version. This will create a file under `migrations/versions` with the initial schema of the database.\n\n```\n$ SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://postgres:postgres@${DBIP}:5432/bare_necessities \\\n  PYTHONPATH=. \\\n  FLASK_APP=src/web/api.py \\\n  flask db migrate -m \"Initial migration.\"\n```\n\nFinally, to apply this schema to the database and create the tables, run the `flask db upgrade` command.\n\n```\n$ SQLALCHEMY_DATABASE_URI=postgresql+psycopg2://postgres:postgres@${DBIP}:5432/bare_necessities \\\n  PYTHONPATH=. \\\n  FLASK_APP=src/web/api.py \\\n  flask db upgrade\n```\n\nWe can verify that the tables have been created in the database. The `alembic_version` table contains the version number that maps to the schema version currently applied. You will find a corresponding migration version file under `migrations/versions`.\n\n```\n$ psql -U postgres -h ${DBIP} bare_necessities\n\nbare_necessities=# \\d+\n                             List of relations\n Schema |      Name       |   Type   |  Owner   |    Size    | Description \n--------+-----------------+----------+----------+------------+-------------\n public | alembic_version | table    | postgres | 8192 bytes | \n public | user            | table    | postgres | 8192 bytes | \n public | user_id_seq     | sequence | postgres | 8192 bytes | \n(3 rows)\n\nbare_necessities=# select * from alembic_version;\n version_num  \n--------------\n ddb81cb8c19f\n(1 row)\n```\n\n## Testing\n\nPytest is used for unit testing. All tests are kept under the `tests` directory and can be invoked by calling `make test`.\n\nAn in-memory sqlite database is instantiated using SQLAlchemy every time pytest runs. The database is not preserved between runs.\n"
},
{
  "name": "Dockerflow",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTE.md",
      "Dockerfile",
      "README.md",
      "docs",
      "package.json",
      "server.js",
      "version.json"
    ],
    "/docs": [
      "building-container.md",
      "mozlog.md",
      "serving-static-content.md",
      "version_object.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![Docker Build Status](https://circleci.com/gh/mozilla-services/Dockerflow/tree/main.svg?style=shield&circle-token=c7c606e039cdccd2380782672ac12b2e85550295)](https://circleci.com/gh/mozilla-services/Dockerflow)\n\n\n# Dockerflow\n\nDockerflow is a specification for automated building, testing and publishing of docker web application images that comply to a common set of behaviours. Compliant images are simpler to deploy, monitor and manage in production.\n\nThe specification is this README.md file. This repo contains a reference application that will change with the specification. See the [Contribution document](CONTRIBUTE.md) for details on suggesting changes and providing feedback.\n\n## Automated Creation Pipeline\n\n````\n  +-(1)--+         +-(2)-------------+        +-(3)-------+        +-(4)--------+\n  | Code |         |  CI builds and  |        |   Docker  |        | Container  |\n  | Push | ------> | tests container | -----> |    Hub    | -----> | Deployment |\n  +------+         +-----------------+        +-----------+        +------------+\n\n````\n\n1. A Code push triggers automated image building\n2. CI builds and tests the image\n3. Tested images are published to Docker Hub.\n4. Images are pulled from Docker Hub to be used\n\n## Specification\n\nThe specification has requirements that a container must comply with. Recommendations are optional but encouraged if they are suitable for the application.\n\n### Building and Testing\n\nDockerflow requires an automated build and test tool that meets these requirements:\n\n1. Able to trigger a build from a code change like a pull request or a merge.\n1. Able to run tests on the code and the application in the container.\n1. Able to manually trigger a previous build and test.\n1. Able to publish container to Docker Hub.\n1. Able to provide a build and test log.\n1. Secure and keeps secrets from being exposed.\n\nWithin Mozilla, we support the use of CircleCI or Taskcluster.\n\n### Containerized App Requirements\n\nWhen the application is ran in the container it must:\n\n1. Accept its configuration through environment variables.\n1. Listen on environment variable `$PORT` for HTTP requests.\n1. Must have a [JSON version object](docs/version_object.md) at `/app/version.json`.\n1. Respond to `/__version__` with the contents of `/app/version.json`.\n1. Respond to `/__heartbeat__` with a HTTP 200 or 5xx on error. This should check backing services like a database for connectivity and may respond with the status of backing services and application components as a JSON payload.\n1. Respond to `/__lbheartbeat__` with an HTTP 200. This is for load balancer checks and should **not** check backing services.\n1. Send text logs to `stdout` or `stderr`.\n1. Serve its own [static content](docs/serving-static-content.md).\n\n### Dockerfile requirements\n\n1. Sources should be copied to the `/app` directory in the container.\n1. Create an `app` group with a GID of `10001`.\n1. Create an `app` user with a UID of `10001` and is a member of the `app` group.\n1. Have both `ENTRYPOINT` and `CMD` instructions to start the service.\n1. Have a `USER app` instruction to set the default user.\n\n### Optional Recommendations\n\n1. Each instance of a container should only run one process. If you need to run two processes, just create another instance of the container & run a different command.\n1. Log to `stdout` in the\n   [mozlog](https://wiki.mozilla.org/Firefox/Services/Logging) json schema.\n1. [Containers should be optimized for production use](docs/building-container.md).\n1. Listen on port 8000.\n\n### Docker Hub Credentials\n\nInternal processes for managing `DOCKER_USER` and `DOCKER_PASS` per-project are documented in `hiera-sops/misc/dockerhub/`.\n\n## Contributing\n* [Contribution Guidelines](CONTRIBUTE.md)\n"
},
{
  "name": "foxsec-tools",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "metrics",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "# foxsec-tools\n\nTooling to support common tasks, particularly for working with metrics\n"
},
{
  "name": "data-incident-scripts",
  "files": {
    "/": [
      "2019-05-14-armagaddon",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Record of scripts used to handle clean up of data in response to incidents\n"
},
{
  "name": "edge-validator",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Makefile",
      "README.md",
      "app.py",
      "bin",
      "integration.py",
      "mozilla-pipeline-schemas",
      "requirements.txt",
      "tests",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build\n\nbuild:\n\tdocker build -t edge-validator:latest .\n\nsync:\n\tpython integration.py sync --ignore-data\n\ntest:\n\tpytest tests/\n\nreport:\n\tpython integration.py sync report\n\nserve:\n\tdocker run -e FLASK_ENV -p 8000:8000 -it edge-validator:latest\n\nshell:\n\tdocker run -p 8000:8000 -it edge-validator:latest bash\n",
  "readme": "# edge-validator\n\nA service-endpoint for validating pings against `mozilla-pipeline-schemas`.\n\n[![CircleCI](https://circleci.com/gh/mozilla-services/edge-validator.svg?style=svg)](https://circleci.com/gh/mozilla-services/edge-validator)\n\nSee [bug 1452166](https://bugzilla.mozilla.org/show_bug.cgi?id=1452166) for\nmotivating background.\n\n## Quickstart\n\nStart the docker container to start the local service at `localhost:8000`. The\nfollowing command will fetch the latest image from DockerHub.\n\n```bash\ndocker run -p 8000 -it mozilla/edge-validator:latest\n```\n\nSimply POST to the endpoint to check if a document is valid. The `testing`\nnamespace has an example schema for validation.\n\n```bash\n$ OK_DATA=\"$(echo '{\"payload\": {\"foo\": true, \"bar\": 1, \"baz\": \"hello world\"}}')\"\n$ curl -X POST -H \"Content-Type: application/json\" -d \"${OK_DATA}\" localhost:8000/submit/testing/test/1\n> OK\n```\n\nThe endpoint will return `200 OK` on a successful POST. The response will be\n`400 BAD` if the posted documented does not pass validation. If the URI is\nmalformed, the validator may return with a `404 NOT FOUND`. The status will also\ninclude the exception that caused the error.\n\n```bash\n$ BAD_DATA=\"$(echo '{\"payload\": {\"foo\": null, \"bar\": \"3\", \"baz\": 55}}')\"\n$ curl -X POST -H \"Content-Type: application/json\" -d \"${BAD_DATA}\" localhost:8000/submit/testing/test/1\n> BAD: ('type', '#/properties/payload/properties/foo', '#/payload/foo')\n```\n\nIn this example, `payload.foo` should be a boolean and `payload.baz` should be a\nstring. Currently, only the first validation exception will be propagated to the\nuser.\n\nThe exposed port can be changed through the `PORT` environment variable. It is\npossible to mount a set of local json-schemas by mounting a folder structure\nmirroring `mozilla-services/mozilla-pipeline-schemas` to the container's\n`/app/resources/schemas` directory.\n\n```bash\ncd mozilla-pipeline-schemas\ndocker run -p 8000 -v \"$(pwd)\"/schemas:/app/resources/schemas -it edge-validator\n```\n\n## User Guide\n\n## API\n\n### Generic Ingestion\n\nThe generic ingestion specification provides enough context to map the ping to a\nschema.\n\nThe _namespace_ distinguishes different data collection systems from each other.\nTelemetry is the largest consumer of the ingestion system to date. The _document\ntype_ differentiates messages in the ingestion pipeline. For example, the\nschemas of the main and crash pings share little overlap. The _document version_\nallows for versioning between documents. Finally, the _document id_ is used to\ncheck for duplicates. This is validated in the running pipeline, but not\nsupported here.\n\n```bash\nPOST /submit/<namespace>/<doctype>/<docversion/[<docid>]\n```\n\nThe schemas are mounted under the application directory `/app/resources/schemas`\nwith the following convention:\n\n```bash\n/schemas/<NAMESPACE>/<DOCTYPE>.<DOCVERSION>.schema.json\n```\n\nThe following tree shows a subset of the resource directory.\n\n```bash\n/app/resources\n\u2514\u2500\u2500 schemas\n    \u251c\u2500\u2500 telemetry\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 anonymous\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 anonymous.4.schema.json\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 core\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.1.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.2.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.3.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.4.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.5.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.6.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.7.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.8.schema.json\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 core.9.schema.json\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 crash\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 crash.4.schema.json\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 main\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.4.schema.json\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500\u2500 ...\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 testing\n        \u2514\u2500\u2500 test\n            \u2514\u2500\u2500 test.1.schema.json\n```\n\n### Telemetry Ingestion\n\nThe edge-validator implements the Edge Server [POST request\nspecification](https://docs.telemetry.mozilla.org/concepts/pipeline/http_edge_spec.html#postput-request)\nfor Firefox Telemetry. The validator will reroute the request as a generic\ningestion request.\n\n```bash\nPOST /submit/<namespace>/<docid>/<appName>/<appVersion>/<appUpdateChannel>/<appBuildId>\n```\n\n### Installation\n\n#### Building from source\n\n```bash\n# clone and set the working directory\n$ git clone --recursive https://github.com/mozilla-services/edge-validator.git\n$ cd edge-validator\n\n# if the `--recursive` option was omitted, then update and initialize the submodule\n$ git submodule update --init\n\n# make sure that the system pip is up to date\n$ pip install --user --upgrade pip\n\n# install the dependencies into a virtual environment\n$ python3 -m venv venv\n$ source venv/bin/activate\n$ pip install -r requirements.txt\n\n# bootstrap for test/report/serve\n$ make sync\n```\n\nThe docker environment is suitable for running a local service or for running\nany of the testing suites.\n\n```bash\nmake shell\n\n# Alternatively\n$ docker run -p 8000 -it edge-validator:latest bash\n```\n\nIf you don't require permanent changes to the engine itself, you may pull down a\nprebuilt docker image through DockerHub using the\n`mozilla/edge-validator:latest` image.\n\n### Serving\n\n#### serving via docker host (recommended)\n\n```bash\ndocker --version          # ensure that docker is installed\nmake build                # build the container\nmake serve                # start the service on localhost:8000\n```\n\n#### serving via local host\n\nThe docker host automates the following bootstrap process.\n\n```bash\nflask run --port 8000     # run the application\n```\n\n### Running Tests\n\nUnit tests do not require any dependencies and can be run out of the box. The\nsync command will copy the test resources into the application resource folder.\n\n```bash\nmake sync\nmake test\n```\n\nYou may also run the tests in docker in the same way as CI. A `junit.xml` file\nis generated in a `test-reports` folder. The image must be rebuilt to include\nmodified test files.\n\n```bash\nIMAGE=edge-validator:latest ./docker_env.sh test\n```\n\n### Running Integration Tests\n\nAn integration report gives a performance report based on sampled data.\n\nEnsure that the Google Cloud SDK is correctly configured.\n\n```bash\nbq show moz-fx-data-shared-prod:monitoring.document_sample_nonprod_v1\n```\n\nThen run the report.\n\n```bash\n# export a google service account\nexport GOOGLE_APPLICATION_CREDENTIALS=<path/to/credentials.json>\n\n# Run using the local app context\nmake report\n\n# Run using the docker host\nEXTERNAL=1 PORT=8000 make report\n```\n\nThe report can also be run in Docker when given the correct permissions.\n\n```bash\ndocker run \\\n    -v $GOOGLE_APPLICATION_CREDENTIALS:/tmp/credentials \\\n    -e GOOGLE_APPLICATION_CREDENTIALS=/tmp/credentials \\\n    -it edge-validator:latest \\\n    make report\n```\n\nYou may also be interested in a machine consumable integration report.\n\n```bash\nintegration.py report --report-path test-reports/integration.json\n```\n"
},
{
  "name": "takeonme",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE",
      "README.md",
      "example_service",
      "poetry.lock",
      "pyproject.toml",
      "takeonme"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# takeonme\n\n[![PyPI version](https://badge.fury.io/py/takeonme.svg)](https://badge.fury.io/py/takeonme)\n\n`takeonme` enumerates resources that could be hijacked or taken over\n(e.g. subdomains, hosted zones, IP, or other resources in a common\nglobal namespace).\n\n\n## Example usage\n\n1. Run `pip install takeonme` to install the `takeonme` command from\n   PyPI\n\n1. [Configure\n   credentials](https://github.com/mozilla-services/takeonme/#implemented-resources)\n   for the service you want to enumerate\n\n1. Run `takeonme list <service name> <resource name>` to list\n   resources from a service (e.g. `takeonme list aws domains`)\n\n1. Run a tool to detect whether any of the resources can be hijacked\n   (e.g. [subjack](https://github.com/haccer/subjack) for subdomains)\n\n1. Reserve or take down the hijackable resources (they should be gone in a day or two)\n\n\n## Non-goals\n\n`takeonme` does **not** run against multiple services or accounts or\nuse multiple credentials instead invoke it multiple times with\ndifferent default credentials.\n\n`takeonme` does **not** enumerate resources that cannot be hijacked.\n\n\n## Developing\n\nRequirements:\n\n* git\n* [Python 3.8 or newer](https://www.python.org/downloads/)\n* [poetry](https://python-poetry.org/)\n\n1. `git clone` the repository\n\n1. run `poetry install`\n\n\nTo add a new service or resource, follow [this template](https://github.com/mozilla-services/takeonme/issues/new?template=custom.md).\n\n\n## Implemented resources\n\n* [AWS Route53 DNS records](https://docs.aws.amazon.com/Route53/latest/APIReference/API_ResourceRecord.html)\n* [AWS EC2 Elastic IPs](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_Address.html)\n  * [configuring credentials](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials)\n\n* [GCP Cloud DNS records](https://cloud.google.com/dns/docs/reference/v1/resourceRecordSets#resource)\n  * [configuring credentials](https://cloud.google.com/docs/authentication/production)\n"
},
{
  "name": "cloudops-sentry",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      "9.0",
      "9.1.1",
      "9.1.2",
      "Dockerfile",
      "README.md",
      "build.sh"
    ]
  },
  "makefile": null,
  "readme": "# Introduction\n\nThis repo builds a custom Sentry docker image using the instructions in [[1]](https://github.com/getsentry/docker-sentry) and  [[2]](https://github.com/getsentry/onpremise).\n\nThe jenkins job that builds this repo is at https://ops-master.jenkinsv2.prod.mozaws.net/job/sentry%20image/\n"
},
{
  "name": "websec-check",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "rust.md"
    ]
  },
  "makefile": null,
  "readme": "# websec-check\n\nThe [Firefox Operations Security Team's](https://wiki.mozilla.org/Security/FirefoxOperations) web security checklist for Firefox Services.\n\nMarkdown issue template:\n\n```markdown\nRisk Management\n---------------\n* [ ] The service must have performed a Rapid Risk Assessment\n* [ ] The service must be registered via a [New Service issue](https://github.com/mozilla-services/foxsec/issues/new?template=01_NewService.md&labels=New%20Service&assignee=psiinon&title=New%20Service:%20). You need to have access to the mozilla-services GitHub organization to be able to view and create this issue (ping secops if you don't have access).\n\nInfrastructure\n--------------\n\n* [ ] Access and application logs must be archived for a minimum of 90 days\n* [ ] Use [Modern](https://wiki.mozilla.org/Security/Server_Side_TLS#Modern_compatibility) or [Intermediate](https://wiki.mozilla.org/Security/Server_Side_TLS#Intermediate_compatibility) TLS\n* [ ] Set HSTS to 31536000 (1 year)\n  * `strict-transport-security: max-age=31536000`\n  * [ ] If the service is not hosted under `services.mozilla.com`, it must be manually added to [Firefox's preloaded pins](https://dxr.mozilla.org/mozilla-central/source/security/manager/tools/PreloadedHPKPins.json#184). This only applies to production services, not short-lived experiments.\n* [ ] Correctly set client IP\n  * [ ] Confirm client ip is in the proper location in [X-Forwarded-For](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/X-Forwarded-For), modifying what is sent from the client if needed. AWS and GCP's load balancers will do this automatically.\n  * [ ] Make sure the web server and the application get the true client IP by configuring trusted IP's within Nginx or Apache\n    * Nginx: [ngx_http_realip_module](https://nginx.org/en/docs/http/ngx_http_realip_module.html)\n    * Apache: [mod_remoteip](https://httpd.apache.org/docs/2.4/mod/mod_remoteip.html)\n  * [ ] If you have a service-oriented architecture, you must always be able to find the IP of the client that sent the initial request. We recommend passing along the `X-Forwarded-For` to all back-end services.\n* If service has an admin panels, it must:\n  * [ ] only be available behind Mozilla VPN (which provides MFA)\n  * [ ] require Auth0 authentication\n  * [ ] Enforce a CSP with `frame-ancestors 'none'` to prevent iframe related attacks, such as [DOM-Based CSRF](https://www.youtube.com/watch?v=Femsrx0m9bU)\n* [ ] Build and deploy main or -slim variants of official language-specific base docker images e.g. [node](https://hub.docker.com/_/node/), [python](https://hub.docker.com/_/python/), or [rust](https://hub.docker.com/_/rust/) and contact secops@ if you want to use other variants\n\nDevelopment\n-----------\n* [ ] Ensure your code repository is configured and located appropriately:\n  * [ ] Application built internally should be hosted in trusted GitHub organizations (mozilla, mozilla-services, mozilla-bteam, mozilla-conduit, mozilla-mobile, taskcluster). Sometimes we build and deploy applications we don't fully control. In those cases, the Dockerfile that builds the application container should be hosted in its own repository in a trusted organization.\n  * [ ] Secure your repository by implementing [Mozilla's GitHub security standard](https://github.com/mozilla-services/GitHub-Audit/blob/master/docs/checklist.md).\n* [ ] Sign all release tags, and ideally commits as well\n  * Developers should [configure git to sign all tags](http://micropipes.com/blog//2016/08/31/signing-your-commits-on-github-with-a-gpg-key/) and upload their PGP fingerprint to https://login.mozilla.com\n  * The signature verification will eventually become a requirement to shipping a release to staging & prod: the tag being deployed in the pipeline must have a matching tag in git signed by a project owner. This control is designed to reduce the risk of a 3rd party GitHub integration from compromising our source code.\n* [ ] Enable [automated security fix PRs on GitHub](https://help.github.com/en/articles/configuring-automated-security-fixes#managing-automated-security-fixes-for-your-repository) for vulnerabilities in 3rd-party dependencies\n* [ ] Keep 3rd-party libraries up to date (in addition to the security updates)\n  * For NodeJS applications, use [dependabot](https://dependabot.com/), [renovate](https://renovateapp.com/), or [GreenKeeper](https://greenkeeper.io/)\n  * For Python, use ``pip list --outdated`` or [requires.io](https://requires.io/) or pyup outdated checks\n  * For Rust, use `cargo update` and [cargo upgrade](https://github.com/killercup/cargo-edit#cargo-upgrade) when changing versions\n* [ ] Integrate static code analysis in CI, and avoid merging code with issues\n    * Javascript applications should use ESLint with the [Mozilla ruleset](https://developer.mozilla.org/en-US/docs/ESLint)\n    * Python applications should use [Bandit](https://github.com/openstack/bandit)\n    * Go applications should use the [Go Meta Linter](https://github.com/alecthomas/gometalinter)\n    * Use whitelisting mechanisms in these tools to deal with false positives\n\nDual Sign Off\n------------\n* [ ] Services that push data to Firefox clients must require a dual sign off on every change, implemented in their admin panels\n  * This mechanism must be reviewed and approved by the Firefox Operations Security team before being enabled in production\n\nLogging\n-------\n* [ ] Publish detailed logs in [mozlog](https://github.com/mozilla-services/Dockerflow/blob/master/docs/mozlog.md) format (**APP-MOZLOG**)\n  * Business logic must be logged with app specific codes (see [FxA](https://github.com/mozilla/fxa-auth-server/blob/master/docs/api.md#defined-errors))\n  * Access control failures must be logged at WARN level\n\nWeb Applications\n----------------\n* [ ] Must have a [Content Security Policy (CSP)](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy). The policy:\n  * [ ] should set `default-src none` or only allow specific origins and set `frame-src` and `object-src` to `none` if default-src is not `none`\n  * [ ] web API responses should return `default-src 'none'; frame-ancestors 'none'; base-uri 'none'` to disallow content rendering and framing/redressing\n  * [ ] should not use `unsafe-inline` or `unsafe-eval` in `script-src`, `style-src`, or `img-src` directives (\n  * [ ] may include a `report-uri` directive to provide visibility into CSP violations\n\n* [ ] Third-party javascript must be pinned to specific versions using [Subresource Integrity (SRI)](https://infosec.mozilla.org/guidelines/web_security#subresource-integrity)\n* [ ] Web APIs:\n  * [ ] must set a non-HTML content-type on all responses, including 300s, 400s and 500s\n  * [ ] Web APIs should export an OpenAPI (Swagger) to facilitate automated vulnerability tests\n  * [ ] Should use authentication tokens with a unique pattern which is easily parsed with a regexp. This should allow inclusion into a token scanning service in the future. (E.g. prefix `mgp-` + 20 hex digits would match the regexp `\\bmgp-[0-9A-Fa-f]{20}\\b`)\n* [ ] for [Cookies](https://wiki.mozilla.org/Security/Guidelines/Web_Security#Cookies):\n  * [ ] Set the Secure and HTTPOnly flags\n  * [ ] Use a sensible Expiration\n  * [ ] Use the prefix `__Host-` for the cookie name\n* [ ] Make sure your application gets an A+ on the [Mozilla Observatory](https://observatory.mozilla.org/)\n* [ ] Confirm your application doesn't fail the [ZAP Security Baseline](https://github.com/zaproxy/zaproxy/wiki/ZAP-Baseline-Scan):\n  1. Register your service as described in the Risk Management section (if you are not registering your service you can run the scan as described at: https://github.com/zaproxy/zaproxy/wiki/ZAP-Baseline-Scan)\n  2. Go to [The STMO baseline dashboard](https://sql.telemetry.mozilla.org/dashboard/security-baseline-top-level-scores) (if you do not have access to the dashboard, contact secops@)\n  3. Filter for your service name and check that the \"failures\" column is 0. Click on each individual site for additional information about any failures.\n  4. If you need to document exceptions to the baseline e.g. to mark a search form as CSRF exempt, contact secops@ or ping 'psiinon' on github to get the scan config updated\n\nSecurity Features\n-----------------\n* [ ] Authentication of end-users should be via FxA. Authentication of Mozillians should be via Auth0/SSO. Any exceptions must be approved by the security team.\n* [ ] Session Management should be via existing and well regarded frameworks. In all cases you should contact the security team for a design and implementation review\n  * Store session keys server side (typically in a db) so that they can be revoked immediately.\n  * Session keys must be changed on login to prevent session fixation attacks.\n  * Session cookies must have HttpOnly and Secure flags set and the SameSite attribute set to 'strict' or 'lax' (which allows external regular links to login).\n  * For more information about potential pitfalls see the [OWASP Session Management Cheat Sheet](https://www.owasp.org/index.php/Session_Management_Cheat_Sheet)\n* [ ] When using cookies for session management, make sure you have CSRF protections in place, which in 99% of cases is [SameSite cookies](https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies#SameSite_cookies). If you can't use SameSite, use anti CSRF tokens. There are two exceptions to implementing CSRF protection:\n  * Forms that don't change state (e.g. search forms) don't need CSRF protection and can indicate that by setting the 'data-no-csrf' form attribute (this tells our ZAP scanner to ignore those forms when testing for CSRF).\n  * Sites that don't use cookies for anything sensitive can ignore CSRF protection. A lot of modern sites prefer to use local-storage JWTs for session management, which aren't vulnerable to CSRF (but must have a rock solid CSP).\n* [ ] Access Control should be via existing and well regarded frameworks. If you really do need to roll your own then contact the security team for a design and implementation review.\n* [ ] If you are building a core Firefox service, consider adding it to the list of restricted domains in the preference `extensions.webextensions.restrictedDomains`. This will prevent a malicious extension from being able to steal sensitive information from it, see [bug 1415644](https://bugzilla.mozilla.org/show_bug.cgi?id=1415644).\n\nDatabases\n---------\n* [ ] All SQL queries must be parameterized, not concatenated\n* [ ] Applications must use accounts with limited GRANTS when connecting to databases\n  * In particular, applications **must not use admin or owner accounts**, to decrease the impact of a sql injection vulnerability.\n\nCommon issues\n-------------\n* [ ] User data must be [escaped for the right context](https://www.owasp.org/index.php/XSS_(Cross_Site_Scripting)_Prevention_Cheat_Sheet#XSS_Prevention_Rules_Summary) prior to reflecting it\n  * When inserting user generated html into an html context:\n    * Python applications should use [Bleach](https://github.com/mozilla/bleach)\n    * Javascript applications should use [DOMPurify](https://github.com/cure53/DOMPurify/)\n* [ ] Apply sensible limits to user inputs, see [input validation](https://wiki.mozilla.org/WebAppSec/Secure_Coding_Guidelines#Input_Validation)\n  * POST body size should be small (<500kB) unless explicitly needed\n* [ ] When allowing users to upload or generate content, make sure to host that content on a separate domain (eg. firefoxusercontent.com, etc.). This will prevent malicious content from having access to storage and cookies from the origin.\n  * Also use this technique to host rich content you can't protect with a CSP, such as metrics reports, wiki pages, etc.\n* [ ] When managing permissions, make sure access controls are enforced server-side\n* [ ] If an authenticated user accesses protected resource, make sure the pages with those resource arent cached and served up to unauthenticated users (like via a CDN).\n* [ ] If handling cryptographic keys, must have a mechanism to handle quarterly key rotations\n  * Keys used to sign sessions don't need a rotation mechanism if destroying all sessions is acceptable in case of emergency.\n* [ ] Do not proxy requests from users without strong limitations and filtering (see [Pocket UserData vulnerability](https://www.gnu.gl/blog/Posts/multiple-vulnerabilities-in-pocket/)). Don't proxy requests to [link local, loopback, or private networks](https://en.wikipedia.org/wiki/Reserved_IP_addresses#IPv4) or DNS that resolves to addresses in those ranges (i.e. 169.254.0.0/16, 127.0.0.0/8, 10.0.0.0/8, 100.64.0.0/10, 172.16.0.0/12, 192.168.0.0/16, 198.18.0.0/15).\n* [ ] Do not use `target=\"_blank\"` in external links unless you also use `rel=\"noopener noreferrer\"` (to prevent [Reverse Tabnabbing](https://www.owasp.org/index.php/Reverse_Tabnabbing))\n```\n"
},
{
  "name": "json2hcl",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "fixtures",
      "main.go"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/kvz/json2hcl.svg?branch=master)](https://travis-ci.org/kvz/json2hcl)\n\n# json2hcl (and hcl2json)\n\nConvert JSON to HCL and HCL to JSON via STDIN / STDOUT.\n\n## Warning\n\nWe don't use json2hcl anymore ourselves, so we can't invest time into it. However, we're still welcoming PRs.\n\n## Install\n\nCheck the [releases](https://github.com/kvz/json2hcl/releases) for the latest version.\nThen it's just a matter of downloading the right one for you platform, and making the binary\nexecutable. \n\n### Linux\n\nHere's how it could look for 64 bits Linux, if you wanted `json2hcl` available globally inside\n`/usr/local/bin`:\n\n```bash\ncurl -SsL https://github.com/kvz/json2hcl/releases/download/v0.0.6/json2hcl_v0.0.6_linux_amd64 \\\n  | sudo tee /usr/local/bin/json2hcl > /dev/null && sudo chmod 755 /usr/local/bin/json2hcl && json2hcl -version\n```\n\n### OSX\n\nHere's how it could look for 64 bits Darwin, if you wanted `json2hcl` available globally inside\n`/usr/local/bin`:\n\n```bash\ncurl -SsL https://github.com/kvz/json2hcl/releases/download/v0.0.6/json2hcl_v0.0.6_darwin_amd64 \\\n  | sudo tee /usr/local/bin/json2hcl > /dev/null && sudo chmod 755 /usr/local/bin/json2hcl && json2hcl -version\n```\n\n## Use\n\nHere's an example [`fixtures/infra.tf.json`](fixtures/infra.tf.json) being\nconverted to HCL:\n\n```bash\n$ json2hcl < fixtures/infra.tf.json\n\"output\" \"arn\" {\n  \"value\" = \"${aws_dynamodb_table.basic-dynamodb-table.arn}\"\n}\n... rest of HCL truncated\n```\n\nTypical use would be\n\n```bash\n$ json2hcl < fixtures/infra.tf.json > fixtures/infra.tf\n```\n\n## hcl2json\n\nAs a bonus, the conversion the other way around is also supported via the `-reverse` flag:\n\n```bash\n$ json2hcl -reverse < fixtures/infra.tf\n{\n  \"output\": [\n    {\n      \"arn\": [\n        {\n          \"value\": \"${aws_dynamodb_table.basic-dynamodb-table.arn}\"\n        }\n      ]\n    }, \n  ... rest of JSON truncated\n  ]\n}\n```\n\n## Development\n\n```bash\nmkdir -p ~/go/src/github.com/kvz\ncd ~/go/src/github.com/kvz\ngit clone git@github.com:kvz/json2hcl.git\ncd json2hcl\ngo get\n```\n\n## Why?\n\nIf you don't know HCL, read [Why HCL](https://github.com/hashicorp/hcl#why).\n\nAs for why json2hcl and hcl2json, we're building a tool called Frey that marries multiple underlying\ntools. We'd like configuration previously written in YAML or TOML to now be in HCL now as well. \nIt's easy enough to convert the mentioned formats to JSON, and strictly speaking HCL is already \nable to read JSON natively, so why the extra step?\n\nWe're doing this for readability and maintainability, we wanted to save \nour infra recipes as HCL directly in our repos, instead of only having machine readable intermediate \nJSON that we'd need to hack on. This saves time spotting problems, and makes the experience somewhat \nenjoyable even.\n\nIn the off-chance you too have machine-readable JSON and are interested in converting that\nto the more human-being friendly HCL format, we thought we'd share this.\n\nIt's no rocket science, we're using already available HashiCorp libraries to support the conversion,\nHashiCorp could have easily released their own tools around this, and perhaps they will, but \nso far, they haven't.\n\n## Changelog\n\n### Ideabox (Unplanned)\n\n- [ ] Give the README.md some love\n\n### v0.0.7 (Unreleased)\n\n- [ ] Tests\n\n### v0.0.6 (2016-09-06)\n\n- [x] Deprecate goxc in favor of native builds\n\n### v0.0.5 (2016-09-06)\n\n- [x] Add hcl2json via the `-reverse` flag \n\n### v0.0.4 (2016-09-05)\n\n- [x] Error handling\n- [x] Cross-compiling and shipping releases\n\n## Contributors\n\n- [Marius Kleidl](https://github.com/Acconut)\n- [Kevin van Zonneveld](https://github.com/kvz)\n"
},
{
  "name": "shavar",
  "files": {
    "/": [
      ".boto",
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      "CHANGES.txt",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "constraints.txt",
      "development.ini",
      "howToShavarList.md",
      "mozsvc",
      "production.ini",
      "requirements-test.txt",
      "requirements.txt",
      "scripts",
      "setup.cfg",
      "setup.py",
      "shavar.ini",
      "shavar.testing.ini",
      "shavar",
      "startup.sh"
    ],
    "/.circleci": [
      "_config_yml_",
      "config.yml"
    ]
  },
  "makefile": "VIRTUALENV = virtualenv\nPYTHON = bin/python\nPIP = bin/pip\nPIP_CACHE = /tmp/pip-cache.${USER}\nBUILD_TMP = /tmp/syncstorage-build.${USER}\nPYPI = https://pypi.python.org/simple\nINSTALL = $(PIP) install -U -i $(PYPI)\nFLAKE8 ?= ./bin/flake8\nNOSETESTS ?= ./bin/nosetests\n# Not a selective assignment because command line variable assignment means\n# make ignores any in-Makefile assignments without the \"override\" keyword\nTAG = Who cares? Testing\nREV = $(shell git rev-parse HEAD)\nSOURCE = $(shell git config remote.origin.url)\nVERSION_JSON = $(shell printf '{\"commit\":\"%s\",\"version\":\"%s\",\"source\":\"%s\"}' \\\n\t\t$(REV) \"$(TAG)\" $(SOURCE))\n\n.PHONY: all build test\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages .\n\teasy_install distribute\n\t$(INSTALL) pip\n\t$(INSTALL) nose\n\t$(INSTALL) flake8\n\t$(INSTALL) -r requirements-test.txt\n\t$(PYTHON) ./setup.py develop\n\ntest:\n\t# Check that flake8 passes before bothering to run anything.\n\t# This can really cut down time wasted by typos etc.\n\t$(FLAKE8) shavar\n\t# Generate a version.json just for giggles if one doesn't exist\n\t@if [ ! -f version.json ]; then \\\n\t\techo '{\"commit\":\"1\",\"version\":test\",\"source\":\"testing\"}' > version.json; \\\n\tfi;\n\t# Run the actual testcases.\n\t$(NOSETESTS) -s ./shavar/tests\n\ntag:\n\t@if [ \"$(TAG)\" == 'Who cares? Testing' ]; then \\\n\t\techo \"Missing TAG= variable on command line.\"; \\\n\t\techo \"Usage:\\n\\tmake tag TAG=0.0.0\"; \\\n\t\texit 1; \\\n\tfi;\n\tsed -i -e \"s/version=\\'.*/version=\\'$(TAG)\\',/\" setup.py\n\t@echo '$(VERSION_JSON)' > version.json\n",
  "readme": "shavar - a service that speaks Google's safe browsing protocol\n\nFor more information on safe browsing and the wire protocol this code\nspeaks, see:\n\n  https://developers.google.com/safe-browsing/developers_guide\n\n\nRunning locally\n---------------\n\nFor dev testing, create and activate a virtual environment:\n\n    virtualenv -p python3.7 shavar\n    source shavar/bin/activate\n\nInstall the necessary dependencies:\n\n    pip install -r requirements-test.txt\n\nRun code style check:\n\n    flake8 --exclude ./shavar/lib,./shavar/bin,./build,./deactivate,./.local\n\nRun unit tests:\n\n    nosetests -s --nologcapture ./shavar/tests\n\nConfigure for running locally in a development environment:\n\n    python setup.py develop\n\nRun the service locally:\n\n    pserve shavar.testing.ini\n\nBy default the service listens on port the loopback interface, port 6543.  If\nyou want to change this, modify the values in the INI file's [server:main]\nsection.\n\n\nConfiguration\n-------------\n\nThe shavar service serves changes to a set of hashes of canonicalized URLs.\nBasic configuration consists of specifying the names of the lists to be served\nand a section for each of those lists declaring at least the two minimum\nrequired configuration directives for each list. Read shavar-server-list-config\nfor more examples of Shavar configurations.\n\nSince the tracking protection files (AKA block lists and entity lists) are hosted in S3 by [shavar-list-creation](https://github.com/mozilla-services/shavar-list-creation/), your Shavar dev environment will need to setup AWS keys to retrieve those files. For more information on configuration for `boto` see:\n\n  https://github.com/boto/boto/#getting-started-with-boto\n\nWhen an update is needed on a tracking protection file, Shavar responds with a CDN link to download the latest files. You will need to setup a CDN in front of your S3 bucket using AWS's CloudFront. For more information on setting up CloudFront on your S3 bucket see:\n\n  https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GettingStarted.SimpleDistribution.html\n\nA commented example configuration:\n\n    [shavar]\n    # A newline separated list of the lists to be served.  The names given\n    # here will be used to locate the list specific configuration stanzas\n    # elsewhere in the INI file.\n    lists_served = mozpub-track-digest256\n                   moz-abp-shavar\n                   moz-bananas-shavar\n    # The default protocol version to speak.  As yet, we only speak version\n    # 2 of the protocol even though it has been superceded by Google.\n    # Default value: 2\n    default_proto_ver = 2.0\n    # The root directory for the data files for lists if absolute path names\n    # are not provided in the list specific stanzas.  Not necessary if you\n    # provide absolute paths.\n    lists_root = tests\n\n    # This is the public host and scheme to reach the service\n    # like https://shavar.stage.mozaws.net\n    # when not provided, uses X-Forwarded-Host then fallsback to HTTP_HOST\n    # and X-Forwarded-Proto for the scheme\n    host = shavar.in.production.mozilla.com\n    scheme = https\n\n    [mozpub-track-digest256]\n    # The type of list data that will be shipped.  Presumably this has some\n    # greater impact on the client side but as yet, it isn't used for much\n    # other than making sure the list's name and type match.\n    #\n    # The technical difference between the shavar and digest256 list types\n    # is that digest256 formatted lists use the entire 32 byte SHA256 hash\n    # in the list data while shavar formatted list only send the first 4\n    # bytes of the 32 byte hash.  The client then queries the service if it\n    # encounters a match for a given hash prefix(the first 4 bytes) to\n    # retrieve the entire hash for a given prefix.\n    type = digest256\n    # URL or relative path to the source data for this list.  Possibilities\n    # at the moment include:\n    #\n    # relative/path/to/the/file\n    # /absolute/path/to/the/file\n    # file:///absolute/path/to/the/file\n    # s3+file:///s3_bucket_name/s3_key_name_which_can_include_slashes\n    #\n    # In this usage, \"my_s3_bukkit\" is the S3 bucket name and\n    # \"faux/path/to/file/moz-abp-shavar.data\" is the full key name.  This\n    # just permits slight simulation of a file name.\n    source = s3+file:///my_s3_bukkit/faux/path/to/file/mozpub-track-digest256.data\n\n    [moz-abp-shavar]\n    # Firefox currently (as of 2015-07-13) allows digest256 lists to get away\n    # with breaking the safe browsing wire protocol slightly.  The protocol\n    # actually states that the service's response to a client request for\n    # updated data be formatted as a list of URLs to data files to be\n    # downloaded.\n    #\n    # With Firefox, digest256 changes can be served inline in the initial\n    # response.  Naughty Firefox.  No cookie.\n    #\n    type = shavar\n    # delta_chunk_source is part of the unit test suite and should always be\n    # available to test against.\n    source = shavar/tests/delta_chunk_source\n    # As a result of this use of URLs(referred to as redirects in the protocol\n    # specification document given at the top of the README), it is necessary\n    # for the service to know where the data files will be publicly reachable.\n    # This setting provides the base URL that, when combined with the path\n    # portion of the URL given in the `source` directive above, will be the\n    # redirect served in the response.\n    #\n    # Best practice: make sure it ends in a /\n    redirect_url_base = http://localhost:6543/data/\n\n"
},
{
  "name": "yaml",
  "files": {
    "/": [
      ".travis.yml",
      "LICENSE",
      "LICENSE.libyaml",
      "README.md",
      "apic.go",
      "decode.go",
      "decode_test.go",
      "emitterc.go",
      "encode.go",
      "encode_test.go",
      "parserc.go",
      "readerc.go",
      "resolve.go",
      "scannerc.go",
      "sorter.go",
      "suite_test.go",
      "writerc.go",
      "yaml.go",
      "yaml_test.go",
      "yamlh.go",
      "yamlprivateh.go"
    ]
  },
  "makefile": null,
  "readme": "# YAML support for the Go language\n\nIntroduction\n------------\n\nThe yaml package enables Go programs to comfortably encode and decode YAML\nvalues. It was developed within [Canonical](https://www.canonical.com) as\npart of the [juju](https://juju.ubuntu.com) project, and is based on a\npure Go port of the well-known [libyaml](http://pyyaml.org/wiki/LibYAML)\nC library to parse and generate YAML data quickly and reliably.\n\nCompatibility\n-------------\n\nThe yaml package supports most of YAML 1.1 and 1.2, including support for\nanchors, tags, map merging, etc. Multi-document unmarshalling is not yet\nimplemented, and base-60 floats from YAML 1.1 are purposefully not\nsupported since they're a poor design and are gone in YAML 1.2.\n\nInstallation and usage\n----------------------\n\nThe import path for the package is *gopkg.in/yaml.v2*.\n\nTo install it, run:\n\n    go get gopkg.in/yaml.v2\n\nAPI documentation\n-----------------\n\nIf opened in a browser, the import path itself leads to the API documentation:\n\n  * [https://gopkg.in/yaml.v2](https://gopkg.in/yaml.v2)\n\nAPI stability\n-------------\n\nThe package API for yaml v2 will remain stable as described in [gopkg.in](https://gopkg.in).\n\n\nLicense\n-------\n\nThe yaml package is licensed under the Apache License 2.0. Please see the LICENSE file for details.\n\n\nExample\n-------\n\n```Go\npackage main\n\nimport (\n        \"fmt\"\n        \"log\"\n\n        \"gopkg.in/yaml.v2\"\n)\n\nvar data = `\na: Easy!\nb:\n  c: 2\n  d: [3, 4]\n`\n\ntype T struct {\n        A string\n        B struct {\n                RenamedC int   `yaml:\"c\"`\n                D        []int `yaml:\",flow\"`\n        }\n}\n\nfunc main() {\n        t := T{}\n    \n        err := yaml.Unmarshal([]byte(data), &t)\n        if err != nil {\n                log.Fatalf(\"error: %v\", err)\n        }\n        fmt.Printf(\"--- t:\\n%v\\n\\n\", t)\n    \n        d, err := yaml.Marshal(&t)\n        if err != nil {\n                log.Fatalf(\"error: %v\", err)\n        }\n        fmt.Printf(\"--- t dump:\\n%s\\n\\n\", string(d))\n    \n        m := make(map[interface{}]interface{})\n    \n        err = yaml.Unmarshal([]byte(data), &m)\n        if err != nil {\n                log.Fatalf(\"error: %v\", err)\n        }\n        fmt.Printf(\"--- m:\\n%v\\n\\n\", m)\n    \n        d, err = yaml.Marshal(&m)\n        if err != nil {\n                log.Fatalf(\"error: %v\", err)\n        }\n        fmt.Printf(\"--- m dump:\\n%s\\n\\n\", string(d))\n}\n```\n\nThis example will generate the following output:\n\n```\n--- t:\n{Easy! {2 [3 4]}}\n\n--- t dump:\na: Easy!\nb:\n  c: 2\n  d: [3, 4]\n\n\n--- m:\nmap[a:Easy! b:map[c:2 d:[3 4]]]\n\n--- m dump:\na: Easy!\nb:\n  c: 2\n  d:\n  - 3\n  - 4\n```\n\n"
},
{
  "name": "bmo-iprepd-nginx",
  "files": {
    "/": [
      "Dockerfile",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# bmo-iprepd-nginx\nThe iprepd container build used for BMO\n"
},
{
  "name": "testrail-docker",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "bin",
      "entrypoint.sh",
      "php.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "cloudops-deployment-proxy",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".travis.yml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "go.mod",
      "go.sum",
      "main.go",
      "proxyservice",
      "vendor"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Cloudops Deployment Proxy\nThis service listens for requests from webhooks and messages on pulse.mozilla.org and triggers Jenkins pipelines in response to them.\n"
},
{
  "name": "cose-rust",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "build.rs",
      "examples",
      "rustfmt.toml",
      "src",
      "tools"
    ]
  },
  "makefile": null,
  "readme": "# cose-rust\n\nA Rust library for [COSE](https://tools.ietf.org/html/rfc8152) using [NSS](https://github.com/nss-dev/nss/).\n\n[![Build Status](https://travis-ci.org/franziskuskiefer/cose-rust.svg?branch=master)](https://travis-ci.org/franziskuskiefer/cose-rust/)\n![Maturity Level](https://img.shields.io/badge/maturity-alpha-red.svg)\n\n**THIS IS WORK IN PROGRESS. DO NOT USE YET.**\n\n## Build instructions\n\nIf NSS is not installed in the path, use `NSS_LIB_DIR` to set the library path where\nwe can find the NSS libraries.\n\n    cargo build\n\n### Run Tests and Examples\n\nTo run tests and examples you need NSS in your library path. Tests can be run\nwith\n\n    cargo test\n\nand examples with\n\n    cargo run --example sign_verify\n"
},
{
  "name": "topsites-proxy-target",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# topsites-proxy-target\nTest target to inspect the behavior of the topsites-proxy\n"
},
{
  "name": "fraud-panel",
  "files": {
    "/": [
      ".github",
      "account_handlers.go",
      "auth_handlers.go",
      "cmd",
      "context.go",
      "db",
      "dockerflow.go",
      "dockerflow_test.go",
      "docs",
      "errors",
      "go.mod",
      "go.sum",
      "main.go",
      "middleware.go",
      "middleware_test.go",
      "mozlog",
      "version.json"
    ],
    "/docs": [
      "authentication.md"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "feuerwerk",
  "files": {
    "/": [
      ".env-dist",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "config.ini-dist",
      "runner.py"
    ]
  },
  "makefile": null,
  "readme": "# Feuerwerk\n\n## Introduction\n\nFeuerwerk is a tool designed to run load tests in [Docker](https://docker.com) containers using\n[Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/). It is a command-line tool, so be sure to read up on\nhow to use whatever shell or command prompt is available for your operating system. \n\nFeuerwerk creates a Kubernetes job, launches it and then deletes the job when it has succesfully completed.\n\nPlease consult the documentation for your operating system, terminal emulator and shell if any\nof the command-line instructions provided here do not work for you. \n\n## Requirements\n\nTo use this project you will need the following tools installed:\n\n* [Python]() 3.6 or greater\n* [pipenv](https://pipenv.readthedocs.io/en/latest/) to create a virtual environment and install the required Python dependencies\n* [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) to \n* Google's [Cloud SDK](https://cloud.google.com/sdk/) to configure Google Kubernetes Engine for use\n* Docker to obtain images containing a load test or to turn your own load tests into a Docker image\n\nFeuerwerk was developed\nand tested using the following configurations:\n\n* [macOS](), [iTerm 2]() and [zsh]()\n* [Windows 10]() with [Ubuntu]() running inside [Windows Subsystem for Linux](), [Windows Terminal]() and [bash]()\n\nYou can find an example of creating a Dockerized load test [here](https://github.com/Kinto/kinto-loadtests/blob/master/Dockerfile). \n\n\n## Installation\n\nOnce you checked out this repo, create the virtual environment and activate it using\nthe following commands:\n\n* `pipenv install`\n* `pipenv shell`\n\n\n## Project Creation and Configuration\n\nIf you haven't already created one, use the [Google Cloud Platform Console](https://console.cloud.google.com/) to create a project and\nnote the value Google returns to you for the name of the project.\n\nFor the sake of examples, the project ID is 'yetanothertest-219614'\n\nNext we need to configure our environment to use this project using this CLI command:\n\n`gcloud config set project yetanothertest-219614`\n\nfollowed by setting which [region and zone](https://cloud.google.com/compute/docs/regions-zones/)\nyou wish to run your tests in. In this example we are using the US West zone. Choose a\nzone that provides you with the computing resources you will need to run your tests.\n\n`gcloud config set compute/zone us-west1-a`\n\n\n### Service User and Credentials\n\nCreate a user who has permissions to access the resources you will be deploying up\nto GKE. In this example we're using the same project as above and creating a user named\n'feuerwerk'\n\n`gcloud iam service-accounts create feuerwerk`\n\nThen we need to give that user permission to access your project\n\n`gcloud projects add-iam-policy-binding yetanothertest-219614 --member \"serviceAccount:feuerwerk@yetanothertest-219614.iam.gserviceaccount.com\" --role \"roles/owner\"`\n\nWith the user configured, we need to obtain the access keys and other credentials associated with the\nuser we created. These will be written to a file on your local filesystem.\n\n`gcloud iam service-accounts keys create loadtest.json --iam-account feuerwerk@yetanothertest-219614.iam.gserviceaccount.com`\n\nNext, we need to assign a value representing the fully qualified of path of the file to an\nenvironment variable so the Kubernetes python client libraries can find. For [bash](https://www.gnu.org/software/bash/)\nor zsh, you can use the following command:\n\n`export GOOGLE_APPLICATION_CREDENTIALS=/Users/chartjes/mozilla-services/feuerwerk/loadtest.json`\n\nYou can optionally add the following line to your `.env` file so it is accessible\nthe next time you exit the virtual environment and re-enter it.\n\n`GOOGLE_APPLICATION_CREDENTIALS=/Users/chartjes/mozilla-services/feuerwerk/loadtest.json`\n\n### Cluster Creation\n\nYou will also need to set up a cluster for your load test containers to run in. You can create one by doing\nthe following:\n\n* _Google Cloud Platform_ -> _Kubernetes Engine_ -> _Configuration_\n* In the section that says \"Filter secrets and config maps\" click on the name (ie default-token-k7m4b)\n* Then click on _KUBECTL_ and select _Get YAML_\n\nThis should open up a Cloud Shell. From inside the shell you can run the following command to get a kubectl config:\n\n`cat .kube/config`\n\nIt should look something like this:\n\n```\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: <redacted because I can't share>\n    server: https://34.83.235.67\n  name: gke_autopush-test-01_us-west1-a_autopush-cluster\ncontexts:\n- context:\n    cluster: gke_autopush-test-01_us-west1-a_autopush-cluster\n    user: gke_autopush-test-01_us-west1-a_autopush-cluster\n  name: gke_autopush-test-01_us-west1-a_autopush-cluster\ncurrent-context: gke_autopush-test-01_us-west1-a_autopush-cluster\nkind: Config\npreferences: {}\nusers:\n- name: gke_autopush-test-01_us-west1-a_autopush-cluster\n  user:\n    auth-provider:\n      config:\n        access-token: <redacted because I can't share>\n        cmd-args: config config-helper --format=json\n        cmd-path: /usr/lib/google-cloud-sdk/bin/gcloud\n        expiry: \"2020-07-23T15:33:06Z\"\n        expiry-key: '{.credential.token_expiry}'\n        token-key: '{.credential.access_token}'\n      name: gcp\n```\n\nand then cut-and-paste that into your own kubectl configuration file.\n\n\n### Creating Your Load Test\n\nIf you don't already have a load test in a Docker container, you will need to create one.  Here is a sample that uses [Molotov](https://github.com/loads/molotov):\n\n```python\nfrom molotov import scenario\n\n\n@scenario(weight=7)\nasync def recipe_endpoint_test(session):\n    recipe_endpoint = \"https://localhost/api/v1/recipe/?enabled=true/\"\n    async with session.get(recipe_endpoint) as resp:\n        res = await resp.json()\n        assert resp.status == 200\n        assert len(res) > 0\n\n\n@scenario(weight=7)\nasync def signed_recipe_endpoint_test(session):\n    signed_recipe_endpoint = (\n        \"https://localhost/api/v1/recipe/signed/?enabled=true/\"\n    )\n    async with session.get(signed_recipe_endpoint) as resp:\n        res = await resp.json()\n        assert resp.status == 200\n        assert len(res) > 0\n\n\n@scenario(weight=7)\nasync def heartbeat_test(session):\n    heartbeat_endpoint = \"https://localhost/__heartbeat__\"\n    async with session.get(heartbeat_endpoint) as resp:\n        res = await resp.json()\n        assert resp.status == 200\n        assert \"status\" in res\n\n\n@scenario(weight=50)\nasync def classify_client_test(session):\n    classify_client_endpoint = (\n        \"https://localhost/api/v1/classify_client/\"\n    )\n    async with session.get(classify_client_endpoint) as resp:\n        res = await resp.json()\n        assert resp.status == 200\n        assert \"country\" in res\n        assert \"request_time\" in res\n\n\n@scenario(weight=7)\nasync def implementation_url_tests(session):\n    signed_action_endpoint = \"https://localhost/api/v1/action/signed/\"\n    async with session.get(signed_action_endpoint) as resp:\n        res = await resp.json()\n        assert resp.status == 200\n        count = 0\n\n        while count <= 4 and res[count][\"action\"][\"implementation_url\"] is not None:\n            implementation_url = res[count][\"action\"][\"implementation_url\"]\n            async with session.get(implementation_url) as iu_resp:\n                iu_res = await iu_resp.text()\n                assert iu_resp.status == 200\n                assert len(iu_res) > 0\n                count = count + 1\n```\n\n### Dockerize Your Load Test\n\nOnce you have verified your test is working, you then need to createa Docker image\nthat will run your test. Here is a sample Dockerfile for a load test that was built\nusing Molotov\n\n```\n# Mozilla Load-Tester\nFROM alpine:3.7\n\n# deps\nRUN apk add --update python3; \\\n    apk add --update python3-dev; \\\n    apk add --update openssl-dev; \\\n    apk add --update libffi-dev; \\\n    apk add --update build-base; \\\n    apk add --update git; \\\n    pip3 install --upgrade pip; \\\n    pip3 install molotov; \\\n    pip3 install git+https://github.com/loads/mozlotov.git; \\\n    pip3 install PyFxa;\n\nWORKDIR /molotov\nADD . /molotov\n\n# run the test\nCMD molotov -c -v -d 60 api_tests.py\n```\n\nThis can be customized based on whatever load testing framework you are using but\nis should be able to run your load test.\n\n\n### Run Your Load Test\n\nTo run your load test, run the following command:\n\n`python runner.py`\n\nYou will be prompted to enter:\n\n* a name for the deployment\n* how many copies of your container to run at once\n* the name of the image (ie someproject/sample-loadtest)\n\nYou should see output similar to the following:\n\n```\nInitializing our k8s configuration\nHow many copies of the image do you want running? 1\nWhat Docker image are you using? (ie chartjes/kinto-loadtests) someproject/sample-loadtest:v1\nChecking to see if image exists locally...\nFound the image locally...\nCreating our load testing job...\nStarting the load testing job...\n| |                                                            #                                   | 0 Elapsed Time: 0:05:20\nDeleting load test job...\nLoad test finished\n```\n\n"
},
{
  "name": "setup-go",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".prettierrc.json",
      "CONDUCT",
      "LICENSE",
      "README.md",
      "__tests__",
      "action.yml",
      "dist",
      "docs",
      "jest.config.js",
      "matchers.json",
      "package-lock.json",
      "package.json",
      "src",
      "tsconfig.json"
    ],
    "/docs": [
      "contributors.md"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# setup-go\n\n<p align=\"left\">\n  <a href=\"https://github.com/actions/setup-go/actions\"><img alt=\"GitHub Actions status\" src=\"https://github.com/actions/setup-go/workflows/build-test/badge.svg\"></a>\n\n  <a href=\"https://github.com/actions/setup-go/actions\"><img alt=\"versions status\" src=\"https://github.com/actions/setup-go/workflows/go-versions/badge.svg\"></a>  \n</p>\n\nThis action sets up a go environment for use in actions by:\n\n- optionally downloading and caching a version of Go by version and adding to PATH\n- registering problem matchers for error output\n\n# V2\n\nThe V2 offers:\n- Adds GOBIN to the PATH\n- Proxy Support\n- stable input \n- Bug Fixes (including issues around version matching and semver)\n\nIt will first check the local cache for a version match. If version is not found locally, It will pull it from `main` branch of [go-versions](https://github.com/actions/go-versions/blob/main/versions-manifest.json) repository and on miss or failure, it will fall back to the previous behavior of download directly from [go dist](https://storage.googleapis.com/golang).\n\nMatching by semver spec:\n```yaml\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-go@v2\n  with:\n    go-version: '^1.13.1' # The Go version to download (if necessary) and use.\n- run: go version\n```\n\nMatching an unstable pre-release:\n```yaml\nsteps:\n- uses: actions/checkout@v2\n- uses: actions/setup-go@v2\n  with:\n    stable: 'false'\n    go-version: '1.14.0-rc1' # The Go version to download (if necessary) and use.\n- run: go version\n```\n\n# Usage\n\nSee [action.yml](action.yml)\n\nBasic:\n```yaml\nsteps:\n- uses: actions/checkout@master\n- uses: actions/setup-go@v1\n  with:\n    go-version: '1.9.3' # The Go version to download (if necessary) and use.\n- run: go run hello.go\n```\n\nMatrix Testing:\n```yaml\njobs:\n  build:\n    runs-on: ubuntu-16.04\n    strategy:\n      matrix:\n        go: [ '1.13', '1.12' ]\n    name: Go ${{ matrix.go }} sample\n    steps:\n      - uses: actions/checkout@v2\n      - name: Setup go\n        uses: actions/setup-go@v1\n        with:\n          go-version: ${{ matrix.go }}\n      - run: go run hello.go\n```\n\n# License\n\nThe scripts and documentation in this project are released under the [MIT License](LICENSE)\n\n# Contributions\n\nContributions are welcome!  See [Contributor's Guide](docs/contributors.md)\n\n## Code of Conduct\n\n:wave: Be nice.  See [our code of conduct](CONDUCT)\n"
},
{
  "name": "GitHub-action-testing",
  "files": {
    "/": [
      ".github",
      "LICENSE",
      "README.md",
      "action-a"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "## Nothing to see here\n\nWe're using this repo to test how GitHub Actions might affect the security of Mozilla products developed on GitHub.\n\nAny findings will be published in the normal places, and we'll probably delete this repo.\n"
},
{
  "name": "checkout",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.json",
      ".github",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.json",
      "CHANGELOG.md",
      "LICENSE",
      "README.md",
      "__test__",
      "action.yml",
      "adrs",
      "dist",
      "jest.config.js",
      "package-lock.json",
      "package.json",
      "src",
      "tsconfig.json"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "<p align=\"center\">\n  <a href=\"https://github.com/actions/checkout\"><img alt=\"GitHub Actions status\" src=\"https://github.com/actions/checkout/workflows/test-local/badge.svg\"></a>\n</p>\n\n# Checkout V2\n\nThis action checks-out your repository under `$GITHUB_WORKSPACE`, so your workflow can access it.\n\nOnly a single commit is fetched by default, for the ref/SHA that triggered the workflow. Set `fetch-depth: 0` to fetch all history for all branches and tags. Refer [here](https://help.github.com/en/articles/events-that-trigger-workflows) to learn which commit `$GITHUB_SHA` points to for different events.\n\nThe auth token is persisted in the local git config. This enables your scripts to run authenticated git commands. The token is removed during post-job cleanup. Set `persist-credentials: false` to opt-out.\n\nWhen Git 2.18 or higher is not in your PATH, falls back to the REST API to download the files.\n\n# What's new\n\n- Improved performance\n  - Fetches only a single commit by default\n- Script authenticated git commands\n  - Auth token persisted in the local git config\n- Supports SSH\n- Creates a local branch\n  - No longer detached HEAD when checking out a branch\n- Improved layout\n  - The input `path` is always relative to $GITHUB_WORKSPACE\n  - Aligns better with container actions, where $GITHUB_WORKSPACE gets mapped in\n- Fallback to REST API download\n  - When Git 2.18 or higher is not in the PATH, the REST API will be used to download the files\n  - When using a job container, the container's PATH is used\n\nRefer [here](https://github.com/actions/checkout/blob/v1/README.md) for previous versions.\n\n# Usage\n\n<!-- start usage -->\n```yaml\n- uses: actions/checkout@v2\n  with:\n    # Repository name with owner. For example, actions/checkout\n    # Default: ${{ github.repository }}\n    repository: ''\n\n    # The branch, tag or SHA to checkout. When checking out the repository that\n    # triggered a workflow, this defaults to the reference or SHA for that event.\n    # Otherwise, uses the default branch.\n    ref: ''\n\n    # Personal access token (PAT) used to fetch the repository. The PAT is configured\n    # with the local git config, which enables your scripts to run authenticated git\n    # commands. The post-job step removes the PAT.\n    #\n    # We recommend using a service account with the least permissions necessary. Also\n    # when generating a new PAT, select the least scopes necessary.\n    #\n    # [Learn more about creating and using encrypted secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets)\n    #\n    # Default: ${{ github.token }}\n    token: ''\n\n    # SSH key used to fetch the repository. The SSH key is configured with the local\n    # git config, which enables your scripts to run authenticated git commands. The\n    # post-job step removes the SSH key.\n    #\n    # We recommend using a service account with the least permissions necessary.\n    #\n    # [Learn more about creating and using encrypted secrets](https://help.github.com/en/actions/automating-your-workflow-with-github-actions/creating-and-using-encrypted-secrets)\n    ssh-key: ''\n\n    # Known hosts in addition to the user and global host key database. The public SSH\n    # keys for a host may be obtained using the utility `ssh-keyscan`. For example,\n    # `ssh-keyscan github.com`. The public key for github.com is always implicitly\n    # added.\n    ssh-known-hosts: ''\n\n    # Whether to perform strict host key checking. When true, adds the options\n    # `StrictHostKeyChecking=yes` and `CheckHostIP=no` to the SSH command line. Use\n    # the input `ssh-known-hosts` to configure additional hosts.\n    # Default: true\n    ssh-strict: ''\n\n    # Whether to configure the token or SSH key with the local git config\n    # Default: true\n    persist-credentials: ''\n\n    # Relative path under $GITHUB_WORKSPACE to place the repository\n    path: ''\n\n    # Whether to execute `git clean -ffdx && git reset --hard HEAD` before fetching\n    # Default: true\n    clean: ''\n\n    # Number of commits to fetch. 0 indicates all history for all branches and tags.\n    # Default: 1\n    fetch-depth: ''\n\n    # Whether to download Git-LFS files\n    # Default: false\n    lfs: ''\n\n    # Whether to checkout submodules: `true` to checkout submodules or `recursive` to\n    # recursively checkout submodules.\n    #\n    # When the `ssh-key` input is not provided, SSH URLs beginning with\n    # `git@github.com:` are converted to HTTPS.\n    #\n    # Default: false\n    submodules: ''\n```\n<!-- end usage -->\n\n# Scenarios\n\n- [Fetch all history for all tags and branches](#Fetch-all-history-for-all-tags-and-branches)\n- [Checkout a different branch](#Checkout-a-different-branch)\n- [Checkout HEAD^](#Checkout-HEAD)\n- [Checkout multiple repos (side by side)](#Checkout-multiple-repos-side-by-side)\n- [Checkout multiple repos (nested)](#Checkout-multiple-repos-nested)\n- [Checkout multiple repos (private)](#Checkout-multiple-repos-private)\n- [Checkout pull request HEAD commit instead of merge commit](#Checkout-pull-request-HEAD-commit-instead-of-merge-commit)\n- [Checkout pull request on closed event](#Checkout-pull-request-on-closed-event)\n- [Push a commit using the built-in token](#Push-a-commit-using-the-built-in-token)\n\n## Fetch all history for all tags and branches\n\n```yaml\n- uses: actions/checkout@v2\n  with:\n    fetch-depth: 0\n```\n\n## Checkout a different branch\n\n```yaml\n- uses: actions/checkout@v2\n  with:\n    ref: my-branch\n```\n\n## Checkout HEAD^\n\n```yaml\n- uses: actions/checkout@v2\n  with:\n    fetch-depth: 2\n- run: git checkout HEAD^\n```\n\n## Checkout multiple repos (side by side)\n\n```yaml\n- name: Checkout\n  uses: actions/checkout@v2\n  with:\n    path: main\n\n- name: Checkout tools repo\n  uses: actions/checkout@v2\n  with:\n    repository: my-org/my-tools\n    path: my-tools\n```\n\n## Checkout multiple repos (nested)\n\n```yaml\n- name: Checkout\n  uses: actions/checkout@v2\n\n- name: Checkout tools repo\n  uses: actions/checkout@v2\n  with:\n    repository: my-org/my-tools\n    path: my-tools\n```\n\n## Checkout multiple repos (private)\n\n```yaml\n- name: Checkout\n  uses: actions/checkout@v2\n  with:\n    path: main\n\n- name: Checkout private tools\n  uses: actions/checkout@v2\n  with:\n    repository: my-org/my-private-tools\n    token: ${{ secrets.GitHub_PAT }} # `GitHub_PAT` is a secret that contains your PAT\n    path: my-tools\n```\n\n> - `${{ github.token }}` is scoped to the current repository, so if you want to checkout a different repository that is private you will need to provide your own [PAT](https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line).\n\n\n## Checkout pull request HEAD commit instead of merge commit\n\n```yaml\n- uses: actions/checkout@v2\n  with:\n    ref: ${{ github.event.pull_request.head.sha }}\n```\n\n## Checkout pull request on closed event\n\n```yaml\non:\n  pull_request:\n    branches: [main]\n    types: [opened, synchronize, closed]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n```\n\n## Push a commit using the built-in token\n\n```yaml\non: push\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - run: |\n          date > generated.txt\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n          git add .\n          git commit -m \"generated\"\n          git push\n```\n\n# License\n\nThe scripts and documentation in this project are released under the [MIT License](LICENSE)\n"
},
{
  "name": "gopgagent",
  "files": {
    "/": [
      "gpgagent.go",
      "gpgagent_test.go"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "spanner-client-rs",
  "files": {
    "/": [
      ".gitignore",
      "Cargo.toml",
      "LICENSE",
      "src",
      "vendor"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "gomoz",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "autograph",
      "bouncer",
      "cloudops-deployment-proxy",
      "cloudtrail-streamer",
      "cose",
      "digigo",
      "foxsec-pipeline-contrib",
      "gopgagent",
      "hawk",
      "index.html",
      "iprepd",
      "mar",
      "mozldap",
      "mozlog",
      "mozlogrus",
      "notfound.html",
      "person-api",
      "pineapple",
      "pkcs7",
      "renard",
      "sops",
      "statics",
      "systrack",
      "tigerblood",
      "userplex"
    ]
  },
  "makefile": null,
  "readme": "# go.mozilla.org\n\nThis repository contains the source files of go.mozilla.org.\n\n## Add a new project\n\nCreate a directory with the name of your project and create an index.html file\nin the directory that references the git location of the source code.\n\nYou can copy an existing index.html from another project and edit it.\n\nThe name you use in `go.mozilla.org/<project>` does not need to be the\nsame as the one you host the code under. For example,\n`https://github.com/mozilla-services/hawk-go` uses `go.mozilla.org/hawk`.\n\nIf your project has packages, create a directory structure that matches the\npackages structure, and give each package its own index.html file.\n\nFor example:\n```bash\nuserplex/\n\u251c\u2500\u2500 index.html\n\u2514\u2500\u2500 modules\n    \u251c\u2500\u2500 authorizedkeys\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 index.html\n    \u251c\u2500\u2500 aws\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 index.html\n    \u251c\u2500\u2500 datadog\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 index.html\n    \u2514\u2500\u2500 index.html\n\n```\n\nNote that sub-packages must use the git location of the top-level repository.\nFor example, \n```\n<meta name=\"go-import\"\n    content=\"go.mozilla.org/userplex/modules/aws\n             git https://github.com/mozilla-services/userplex\">\n```\n\nThe sub-package `go.mozilla.org/userplex/modules/aws` uses the top-level git\nrepository `https://github.com/mozilla-services/userplex`.\n\n## Set a custom import path in your packages\n\nThis is only needed for packages that are importable. The `main` package does\nnot need to specify a custom import path since it cannot be imported by other\nGo programs.\n\nIn the source go files of your project, next to the package name, add a line\nthat references the `go.mozilla.org` import path:\n```go\npackage mymodule // import \"go.mozilla.org/myproject/mymodule\"\n```\n\nNote that your project must now live under\n`$GOPATH/src/go.mozilla.org/myproject`. \n\n## Use custom GOPATH in CI\n\n### travis-ci\n\nTravisCI has a directive to indicate the import path of your package, just add\n`go_import_path` to your `.travis.yml`.\n\n```yaml\nlanguage: go\ngo:\n  - 1.5\n  - 1.6\n  - tip\ngo_import_path: go.mozilla.org/myproject\nscript:\n  - go test go.mozilla.org/myproject\n```\n\n## Publish files in S3\n\nAs an AWS admin of the prod IAM, use this command:\n\n```bash\naws s3 sync . s3://go.mozilla.org \\\n    --exclude \"README.md\" --exclude \".git/*\" \\\n    --content-type \"text/html\" --acl public-read\n```\n\nThen invalidate the cloudfront cache\n\n```bash\n$ aws cloudfront create-invalidation --distribution-id ESQYDMA17GDLC --paths '/*'\n```\n"
},
{
  "name": "WebPushDataTestPage",
  "files": {
    "/": [
      "FileSaver.js",
      "LICENSE",
      "README.md",
      "base64.js",
      "common.js",
      "der_lite.js",
      "fetch.js",
      "firefox.mp3",
      "icon.png",
      "index.html",
      "manifest.json",
      "push_test.png",
      "push_test32.png",
      "style.css",
      "sw.js",
      "vapid.js",
      "webpush.js"
    ]
  },
  "makefile": null,
  "readme": "# WebPush Crypto Test Page\n\nMuch like the [OAuth Test Page](http://jrconlin.github.io/OAuthTestPage/), this page is designed to help library authors and developers a way to understand and audit the various encryption bits needed to send data via [WebPush](https://developer.mozilla.org/en-US/docs/Web/API/Push_API).\n\nPlease be advised that sending Data via WebPush is still experimental. Currently this is only supported on Firefox version 45 or later.\n\n[See a live\ndemo](https://mozilla-services.github.io/WebPushDataTestPage/).\n\nThis demo is beta quality. While it works, you may need to refresh several times before things are working correctly. In addition, there's [a known bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1237455) with `fetch()` which prevents the page from sending headers as part of the outbound request. Using the `curl` output should work, however.\n"
},
{
  "name": "pyramid_ipauth",
  "files": {
    "/": [
      ".coveragerc",
      ".gitignore",
      "CHANGES.txt",
      "CONTRIBUTORS.txt",
      "MANIFEST.in",
      "README.rst",
      "pyramid_ipauth",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "==============\npyramid_ipauth\n==============\n\nAn authentication policy for Pyramid that sets identity and effective\nprincipals based on the remote IP address of the request.\n\n\nOverview\n========\n\nTo perform IP-address-based authentication, create an IPAuthenticationPolicy\nand specify the target IP range, userid and effective principals.  Then set it\nas the authentication policy in your configurator::\n\n    authn_policy = IPAuthenticationPolicy(\"127.0.*.*\", \"myuser\", [\"locals\"])\n    config.set_authentication_policy(authn_policy)\n\nThis will cause all requests from IP addresses in the 127.0.*.* range to be\nauthenticated as user \"myuser\" and have the effective principal \"locals\".\n\nIt is also possible to specify the configuration options in your deployment\nfile::\n\n    [app:pyramidapp]\n    use = egg:mypyramidapp\n\n    ipauth.ipaddrs = 127.0.0.* 127.0.1.*\n    ipauth.principals = locals\n\nYou can then simply include the pyramid_ipauth package into your configurator::\n\n    config.include(\"pyramid_ipauth\")\n\nIt will detect the ipauth settings and construct an appropriate policy.\n\nNote that this package only supports matching against a single set of IP\naddresss.  If you need to assign different credentials to different sets\nof IP addresses, you can use the pyramid_multiauth package in conjunction\nwith pyramid_ipauth:\n\n    http://github.com/mozilla-services/pyramid_multiauth\n\nIf you don't want to hard-code the userid or principals at configuration time,\nyou may specify a \"get_userid\" and/or \"get_principals\" callback instead.\n\n\nSpecifying IP Addresses\n=======================\n\nIP addresses can be specified in a variety of forms, including:\n\n    * \"all\":        all possible IPv4 and IPv6 addresses\n    * \"local\":      all local addresses of the machine\n    * \"A.B.C.D\"     a single IP address\n    * \"A.B.C.D/N\"   a network address specification\n    * \"A.B.C.*\"     a glob matching against all possible numbers\n    * \"A.B.C.D-E\"   a glob matching against a range of numbers\n    * a whitespace- or comma-separated string of any of the above\n    * a netaddr IPAddress, IPRange, IPGlob, IPNetork of IPSet object\n    * a list, tuple or iterable of any of the above\n\n\nProxies\n=======\n\nThis module does not respect the X-Forwarded-For header by default, since it\ncan be spoofed easily by malicious clients.  If your server is behind a \ntrusted proxy that sets the X-Forwarded-For header, you should explicitly\ndeclare the set of trusted proxies like so::\n\n    IPAuthenticationPolicy(\"127.0.*.*\",\n                           principals=[\"local\"],\n                           proxies = \"127.0.0.1\")\n\nThe set of trusted proxy addresses can be specified using the same syntax as\nthe set of IP addresses to authenticate.\n"
},
{
  "name": "heroku-admin",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "heroku-2fa.py"
    ]
  },
  "makefile": "UPDATE_BEFORE_COMMIT_FILES := \n\nhelp:\n\t@echo \"Targets:\"\n\t@echo \"    commit   update any files derived from another, prior to committing\"\n\t@echo \"    help     show this text\"\n\t@echo \"    test     test everything!\"\n\t@echo \"    csv      Generate CSV output for today\"\n\t@echo \"    email    Generate email output for today\"\n\t@echo \"    membership Generate membership output for today\"\n\nmembership:\n\theroku-admin --membership > $$(date -u +%Y-%m-%d_heroku_membership.csv)\n\nemail:\n\t./heroku-2fa.py --email > $$(date -u +%Y-%m-%dT%H:%M:%S%Z_heroku_missing_2FA.csv) \\\n\t\t|| echo \"Some users missing 2fa\"\n\ncsv:\n\t./heroku-2fa.py --csv > $$(date -u +%Y-%m-%dT%H:%M:%S%Z_heroku_missing_2FA.csv) \\\n\t\t|| echo \"Some users missing 2fa\"\n\ntest: \n\t@echo \"No tests yet\"\n\tfalse\n\n.PHONY:  test\n",
  "readme": "# heroku-admin\nScripts for managing Heroku accounts\n\n## heroku-2fa.py\n\nGenerates a list of users (members & contributors) who do not have 2FA\nenabled. Outputs a text list, or use option ``--csv`` for a CSV file.\n"
},
{
  "name": "actix-net",
  "files": {
    "/": [
      ".appveyor.yml",
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "actix-codec",
      "actix-connect",
      "actix-ioframe",
      "actix-macros",
      "actix-rt",
      "actix-server",
      "actix-service",
      "actix-testing",
      "actix-threadpool",
      "actix-tls",
      "actix-tracing",
      "actix-utils",
      "examples",
      "router",
      "rustfmt.toml",
      "string"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Actix net [![Build Status](https://travis-ci.org/actix/actix-net.svg?branch=master)](https://travis-ci.org/actix/actix-net) [![codecov](https://codecov.io/gh/actix/actix-net/branch/master/graph/badge.svg)](https://codecov.io/gh/actix/actix-net) [![Join the chat at https://gitter.im/actix/actix](https://badges.gitter.im/actix/actix.svg)](https://gitter.im/actix/actix?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nActix net - framework for composable network services\n\n## Documentation & community resources\n\n* [Chat on gitter](https://gitter.im/actix/actix)\n* Minimum supported Rust version: 1.39 or later\n\n## Example\n\n```rust\nfn main() -> io::Result<()> {\n    // load ssl keys\n    let mut builder = SslAcceptor::mozilla_intermediate(SslMethod::tls()).unwrap();\n    builder.set_private_key_file(\"./examples/key.pem\", SslFiletype::PEM).unwrap();\n    builder.set_certificate_chain_file(\"./examples/cert.pem\").unwrap();\n    let acceptor = builder.build();\n\n    let num = Arc::new(AtomicUsize::new(0));\n\n    // bind socket address and start workers. By default server uses number of\n    // available logical cpu as threads count. actix net start separate\n    // instances of service pipeline in each worker.\n    Server::build()\n        .bind(\n            // configure service pipeline\n            \"basic\", \"0.0.0.0:8443\",\n            move || {\n                let num = num.clone();\n                let acceptor = acceptor.clone();\n\n                // construct transformation pipeline\n                pipeline(\n                    // service for converting incoming TcpStream to a SslStream<TcpStream>\n                    fn_service(move |stream: actix_rt::net::TcpStream| async move {\n                        SslAcceptorExt::accept_async(&acceptor, stream.into_parts().0).await\n                            .map_err(|e| println!(\"Openssl error: {}\", e))\n                    }))\n                // .and_then() combinator chains result of previos service call to argument\n                /// for next service calll. in this case, on success we chain\n                /// ssl stream to the `logger` service.\n                .and_then(fn_service(logger))\n                // Next service counts number of connections\n                .and_then(move |_| {\n                    let num = num.fetch_add(1, Ordering::Relaxed);\n                    println!(\"got ssl connection {:?}\", num);\n                    future::ok(())\n                })\n            },\n        )?\n        .run()\n}\n```\n\n## License\n\nThis project is licensed under either of\n\n* Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0))\n* MIT license ([LICENSE-MIT](LICENSE-MIT) or [http://opensource.org/licenses/MIT](http://opensource.org/licenses/MIT))\n\nat your option.\n\n## Code of Conduct\n\nContribution to the actix-net crate is organized under the terms of the\nContributor Covenant, the maintainer of actix-net, @fafhrd91, promises to\nintervene to uphold that code of conduct.\n"
},
{
  "name": "actix-web",
  "files": {
    "/": [
      ".appveyor.yml",
      ".github",
      ".gitignore",
      ".travis.yml",
      "CHANGES.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "MIGRATION.md",
      "README.md",
      "actix-cors",
      "actix-files",
      "actix-framed",
      "actix-http",
      "actix-identity",
      "actix-multipart",
      "actix-session",
      "actix-web-actors",
      "actix-web-codegen",
      "awc",
      "examples",
      "rustfmt.toml",
      "src",
      "test-server",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "<div align=\"center\">\n <p><h1>Actix web</h1> </p>\n  <p><strong>Actix web is a small, pragmatic, and extremely fast rust web framework</strong> </p>\n  <p>\n\n[![Build Status](https://travis-ci.org/actix/actix-web.svg?branch=master)](https://travis-ci.org/actix/actix-web) \n[![codecov](https://codecov.io/gh/actix/actix-web/branch/master/graph/badge.svg)](https://codecov.io/gh/actix/actix-web) \n[![crates.io](https://meritbadge.herokuapp.com/actix-web)](https://crates.io/crates/actix-web)\n[![Join the chat at https://gitter.im/actix/actix](https://badges.gitter.im/actix/actix.svg)](https://gitter.im/actix/actix?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n[![Documentation](https://docs.rs/actix-web/badge.svg)](https://docs.rs/actix-web)\n[![Download](https://img.shields.io/crates/d/actix-web.svg)](https://crates.io/crates/actix-web)\n[![Version](https://img.shields.io/badge/rustc-1.39+-lightgray.svg)](https://blog.rust-lang.org/2019/11/07/Rust-1.39.0.html)\n![License](https://img.shields.io/crates/l/actix-web.svg)\n\n  </p>\n\n  <h3>\n    <a href=\"https://actix.rs\">Website</a>\n    <span> | </span>\n    <a href=\"https://gitter.im/actix/actix\">Chat</a>\n    <span> | </span>\n    <a href=\"https://github.com/actix/examples\">Examples</a>\n  </h3>\n</div>\n<br>\n\nActix web is a simple, pragmatic and extremely fast web framework for Rust.\n\n* Supported *HTTP/1.x* and *HTTP/2.0* protocols\n* Streaming and pipelining\n* Keep-alive and slow requests handling\n* Client/server [WebSockets](https://actix.rs/docs/websockets/) support\n* Transparent content compression/decompression (br, gzip, deflate)\n* Configurable [request routing](https://actix.rs/docs/url-dispatch/)\n* Multipart streams\n* Static assets\n* SSL support with OpenSSL or Rustls\n* Middlewares ([Logger, Session, CORS, etc](https://actix.rs/docs/middleware/))\n* Includes an asynchronous [HTTP client](https://actix.rs/actix-web/actix_web/client/index.html)\n* Supports [Actix actor framework](https://github.com/actix/actix)\n\n## Example\n\nDependencies:\n\n```toml\n[dependencies]\nactix-web = \"2\"\nactix-rt = \"1\"\n```\n\nCode:\n\n```rust\nuse actix_web::{get, web, App, HttpServer, Responder};\n\n#[get(\"/{id}/{name}/index.html\")]\nasync fn index(info: web::Path<(u32, String)>) -> impl Responder {\n    format!(\"Hello {}! id:{}\", info.1, info.0)\n}\n\n#[actix_rt::main]\nasync fn main() -> std::io::Result<()> {\n    HttpServer::new(|| App::new().service(index))\n        .bind(\"127.0.0.1:8080\")?\n        .run()\n        .await\n}\n```\n\n### More examples\n\n* [Basics](https://github.com/actix/examples/tree/master/basics/)\n* [Stateful](https://github.com/actix/examples/tree/master/state/)\n* [Multipart streams](https://github.com/actix/examples/tree/master/multipart/)\n* [Simple websocket](https://github.com/actix/examples/tree/master/websocket/)\n* [Tera](https://github.com/actix/examples/tree/master/template_tera/) /\n* [Askama](https://github.com/actix/examples/tree/master/template_askama/) templates\n* [Diesel integration](https://github.com/actix/examples/tree/master/diesel/)\n* [r2d2](https://github.com/actix/examples/tree/master/r2d2/)\n* [OpenSSL](https://github.com/actix/examples/tree/master/openssl/)\n* [Rustls](https://github.com/actix/examples/tree/master/rustls/)\n* [Tcp/Websocket chat](https://github.com/actix/examples/tree/master/websocket-chat/)\n* [Json](https://github.com/actix/examples/tree/master/json/)\n\nYou may consider checking out\n[this directory](https://github.com/actix/examples/tree/master/) for more examples.\n\n## Benchmarks\n\n* [TechEmpower Framework Benchmark](https://www.techempower.com/benchmarks/#section=data-r18)\n\n## License\n\nThis project is licensed under either of\n\n* Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or [http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0))\n* MIT license ([LICENSE-MIT](LICENSE-MIT) or [http://opensource.org/licenses/MIT](http://opensource.org/licenses/MIT))\n\nat your option.\n\n## Code of Conduct\n\nContribution to the actix-web crate is organized under the terms of the\nContributor Covenant, the maintainer of actix-web, @fafhrd91, promises to\nintervene to uphold that code of conduct.\n"
},
{
  "name": "actix",
  "files": {
    "/": [
      ".appveyor.yml",
      ".gitignore",
      ".travis.yml",
      "CHANGES.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "MIGRATION.md",
      "Makefile",
      "README.md",
      "examples",
      "rustfmt.toml",
      "src",
      "tests"
    ]
  },
  "makefile": ".PHONY: default build test doc clean\n\nCARGO_FLAGS := --features \"$(FEATURES)\"\n\ndefault: test\n\nbuild:\n\tcargo build $(CARGO_FLAGS)\n\ntest: build\n\tcargo test $(CARGO_FLAGS)\n\tcd examples/chat && cargo build\n\nskeptic:\n\tUSE_SKEPTIC=1 cargo test $(CARGO_FLAGS)\n\n# cd examples/word-count && python setup.py install && pytest -v tests\n\nclippy:\n\tif $$CLIPPY; then cargo clippy $(CARGO_FLAGS); fi\n\ndoc: build\n\tcargo doc --no-deps $(CARGO_FLAGS)\n\tcd guide; mdbook build -d ../target/doc/guide/; cd ..\n\nclean:\n\trm -r target\n\n.PHONY: gh-pages-doc\ngh-pages-doc: doc\n\tcd gh-pages && git pull\n\trm -r gh-pages/doc\n\tcp -r target/doc gh-pages/\n\trm gh-pages/doc/.lock\n\tcd gh-pages && git add .\n\tcd gh-pages && git commit -m \"Update documentation\"\n\npublish: default gh-pages-doc\n\tcargo publish\n\tcd gh-pages && git push\n",
  "readme": "# Actix [![Build Status](https://travis-ci.org/actix/actix.svg?branch=master)](https://travis-ci.org/actix/actix) [![Build status](https://ci.appveyor.com/api/projects/status/aytxo1w6a88x2cxk/branch/master?svg=true)](https://ci.appveyor.com/project/fafhrd91/actix-n9e64/branch/master) [![codecov](https://codecov.io/gh/actix/actix/branch/master/graph/badge.svg)](https://codecov.io/gh/actix/actix) [![crates.io](http://meritbadge.herokuapp.com/actix)](https://crates.io/crates/actix) [![Join the chat at https://gitter.im/actix/actix](https://badges.gitter.im/actix/actix.svg)](https://gitter.im/actix/actix?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nActix is a Rust actors framework.\n\n* [User Guide](https://actix.rs/book/actix/)\n* [API Documentation (Development)](http://actix.github.io/actix/actix/)\n* [API Documentation (Releases)](https://docs.rs/actix/)\n* Cargo package: [actix](https://crates.io/crates/actix)\n* Minimum supported Rust version: 1.39 or later\n\n---\n\n## Features\n\n  * Async/Sync actors.\n  * Actor communication in a local/thread context.\n  * Uses [Futures](https://crates.io/crates/futures) for asynchronous message handling.\n  * HTTP1/HTTP2 support ([actix-web](https://github.com/actix/actix-web))\n  * Actor supervision.\n  * Typed messages (No `Any` type).\n\n## Usage\n\nTo use `actix`, add this to your `Cargo.toml`:\n\n```toml\n[dependencies]\nactix = \"0.9\"\n```\n\n### Initialize Actix\n\nIn order to use actix you first need to create a `System`.\n\n```rust,ignore\nfn main() {\n    let system = actix::System::new(\"test\");\n\n    system.run();\n}\n```\n\nActix uses the [tokio](https://github.com/tokio-rs/tokio) event loop.\n`System::new()` creates a new event loop and starts the `System` actor.\n`system.run()` starts the tokio event loop, and will finish once the `System` actor\nreceives the `SystemExit` message.\n\nLet's create a simple Actor.\n\n### Implement an Actor\n\nIn order to define an actor you need to define a struct and have it implement\nthe [`Actor`](https://actix.github.io/actix/actix/trait.Actor.html) trait.\n\n```rust\nuse actix::{Actor, Addr, Arbiter, Context, System};\n\nstruct MyActor;\n\nimpl Actor for MyActor {\n    type Context = Context<Self>;\n\n    fn started(&mut self, ctx: &mut Self::Context) {\n       println!(\"I am alive!\");\n       System::current().stop(); // <- stop system\n    }\n}\n\nfn main() {\n    let system = System::new(\"test\");\n\n    let addr = MyActor.start();\n\n    system.run();\n}\n```\n\nSpawning a new actor is achieved via the `start` and `create` methods of\nthe [Actor](https://actix.github.io/actix/actix/trait.Actor.html)\ntrait. It provides several different ways of creating actors, for details check docs.\nYou can implement `started`, `stopping` and `stopped` methods of the Actor trait.\n`started` gets called when actor starts and `stopping` when actor finishes.\nCheck [API documentation](https://actix.github.io/actix/actix/trait.Actor.html#actor-lifecycle)\nfor more information on the actor lifecycle.\n\n### Handle messages\n\nAn Actor communicates with another Actor by sending messages. In actix all messages\nare typed. Let's define a simple `Sum` message with two `usize` parameters,\nand an actor which will accept this message and return the sum of those two numbers.\n\n```rust\nuse futures::{future, Future};\nuse actix::*;\n\n// this is our Message\nstruct Sum(usize, usize);\n\n// we have to define the response type for `Sum` message\nimpl Message for Sum {\n    type Result = usize;\n}\n\n// Actor definition\nstruct Summator;\n\nimpl Actor for Summator {\n    type Context = Context<Self>;\n}\n\n// now we need to define `MessageHandler` for the `Sum` message.\nimpl Handler<Sum> for Summator {\n    type Result = usize;   // <- Message response type\n\n    fn handle(&mut self, msg: Sum, ctx: &mut Context<Self>) -> Self::Result {\n        msg.0 + msg.1\n    }\n}\n\nfn main() {\n    let sys = System::new(\"test\");\n\n    let addr = Summator.start();\n    let res = addr.send(Sum(10, 5));  // <- send message and get future for result\n\n    Arbiter::spawn(res.then(|res| {\n        match res {\n            Ok(result) => println!(\"SUM: {}\", result),\n            _ => println!(\"Something wrong\"),\n        }\n\n        System::current().stop();\n        future::result(Ok(()))\n    }));\n\n    sys.run();\n}\n```\n\nAll communications with actors go through an `Addr` object. You can `do_send` a message\nwithout waiting for a response, or `send` an actor with specific message. The `Message`\ntrait defines the result type for a message.\n\n### Actor state and subscription for specific messages\n\nYou may have noticed that methods of `Actor` and `Handler` traits accept `&mut self`, so you are\nwelcome to store anything in an actor and mutate it whenever necessary.\n\nAddress objects require an actor type, but if we just want to send a specific message to\nan actor that can handle the message, we can use the `Recipient` interface. Let's create\na new actor that uses `Recipient`.\n\n```rust\nuse std::time::Duration;\nuse actix::prelude::*;\n\n#[derive(Message)]\nstruct Ping { pub id: usize }\n\n// Actor definition\nstruct Game {\n    counter: usize,\n    addr: Recipient<Ping>,\n}\n\nimpl Actor for Game {\n    type Context = Context<Game>;\n}\n\n// simple message handler for Ping message\nimpl Handler<Ping> for Game {\n    type Result = ();\n\n    fn handle(&mut self, msg: Ping, ctx: &mut Context<Self>) {\n        self.counter += 1;\n\n        if self.counter > 10 {\n            System::current().stop();\n        } else {\n            println!(\"Ping received {:?}\", msg.id);\n\n            // wait 100 nanos\n            ctx.run_later(Duration::new(0, 100), move |act, _| {\n                act.addr.do_send(Ping{id: msg.id + 1});\n            });\n        }\n    }\n}\n\nfn main() {\n    let system = System::new(\"test\");\n\n    // To get a Recipient object, we need to use a different builder method\n    // which will allow postponing actor creation\n    let addr = Game::create(|ctx| {\n        // now we can get an address of the first actor and create the second actor\n        let addr = ctx.address();\n        let addr2 = Game{counter: 0, addr: addr.recipient()}.start();\n\n        // let's start pings\n        addr2.do_send(Ping{id: 10});\n\n        // now we can finally create first actor\n        Game{counter: 0, addr: addr2.recipient()}\n    });\n\n    system.run();\n}\n```\n\n### chat example\n\nThere is a\n[chat example](https://github.com/actix/actix/tree/master/examples/chat)\nwhich provides a basic example of networking client/server service.\n\n### fectl\n\nYou may consider checking out [fectl](https://github.com/fafhrd91/fectl) utility. It is written\nwith `actix` and shows how to create networking application with relatively complex interactions.\n\n## Contributing\n\nAll contributions are welcome, if you have a feature request don't hesitate to open an issue!\n\n## License\n\nThis project is licensed under either of\n\n * Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or\n   http://www.apache.org/licenses/LICENSE-2.0)\n * MIT license ([LICENSE-MIT](LICENSE-MIT) or\n   http://opensource.org/licenses/MIT)\n\nat your option.\n\n## Code of Conduct\n\nContribution to the actix crate is organized under the terms of the\nContributor Covenant, the maintainer of actix, @fafhrd91, promises to\nintervene to uphold that code of conduct.\n"
},
{
  "name": "person-api-go",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "api.go",
      "go.mod",
      "persons.go"
    ]
  },
  "makefile": null,
  "readme": "# person-api-go\nGo client library for CIS Person API (https://github.com/mozilla-iam/cis/blob/master/docs/PersonAPI.md)\n"
},
{
  "name": "ci-testground-minimal",
  "files": {
    "/": [
      ".circleci",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "testground.sh"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# ci-testground\nfor testing ops ci workflows\nsimple bash script that emulates scripts run in a container\n\nsee ci workflows here: https://circleci.com/gh/mozilla-services/workflows/ci-testground-minimal\nsee containers pushed to dockerhub here: https://hub.docker.com/r/mozilla/ci-testground-minimal/tags/\n\nbump 2019 edition\n"
},
{
  "name": "oidc-gateway",
  "files": {
    "/": [
      ".helmignore",
      "Chart.yaml",
      "LICENSE",
      "README.md",
      "docker",
      "templates",
      "values.yaml"
    ]
  },
  "makefile": null,
  "readme": "# OIDC Gateway\n\nA Docker container and Kubernetes Helm chart that gates access to an upstream service\nbased on OpenID Connect authentication. It uses\n[OpenResty](https://github.com/openresty/openresty) with\n[lua-resty-openidc](https://github.com/zmartzone/lua-resty-openidc). Additionally, it\nuses [Jinja2](http://jinja.pocoo.org/docs/2.10/) to template an NGINX config.\n\nIt proxies traffic on port 80. If you want to do TLS, you should terminate it before\nsending the traffic to this gateway.\n\n## Configuration\n\nOIDC Gateway is configured through YAML files. Because parts of the configuration are\nsensitive, it takes its configuration from both a ConfigMap and a Secret. The\nsensitive parts belong in the Secret, and the non-sensitive ones belong in the\nConfigMap. The ConfigMap should contain a YAML file named `config.yaml` and the\nSecret should contain a YAML file named `secrets.yaml`. These two files are merged,\nwith values from `secrets.yaml` taking precedence if they're present in both files.\n\nThe configuration looks as follows:\n\n```yaml\n# The upstream service to which to forward authenticated requests.\n# E.g. \"mozilla.com\", \"myservice\". \nupstream: 'string'\noidc:\n  # The OIDC client ID. E.g. '33PByFLjwvChUk7oaPcD5Z0egBtccacU'\n  client_id: 'string'\n  # The OIDC client secret.\n  # E.g. 'vBOctKc8scalRP__62zh3oNbF2HJWh2lSeyo0EvNcHLpBbokNicnn86InRKlb2P4'\n  client_secret: 'string'\n  # The OIDC discovery URL.\n  # E.g. 'https://auth.mozilla.auth0.com/.well-known/openid-configuration'\n  discovery: 'string'\n# Secret used to encrypt session data. E.g. '623q4hR325t36VsCD3g567922IC0073T'\nsession_secret: 'string'\n```\n\nYou are free to decide what you want to keep secret and what you want to be public.\n\n## Example\n\nSuppose you have a Kubernetes service called `secretservice` that you want to gate\nthrough OpenID Connect. Your OIDC provider gives you the following values:\n\n- Discovery URL is `https://oidc.com/.well-known/openid-configuration`\n- Client ID is `33PByFLjwvChUk7oaPcD5Z0egBtccacU`\n- Client secret is `vBOctKc8scalRP__62zh3oNbF2HJWh2lSeyo0EvNcHLpBbokNicnn86InRKlb2P4`\n\nYou generate a random session secret, `6660b15ad13f9c5bd0b74767d00d05a3`.\n\nYou create the following ConfigMap:\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: oidc-gateway\ndata:\n  config.yaml: |\n    upstream: secretservice\n    oidc:\n      client_id: '33PByFLjwvChUk7oaPcD5Z0egBtccacU'\n      discovery: 'https://oidc.com/.well-known/openid-configuration'\n```\n\nAnd the following Secret:\n\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: auth0-proxy\ntype: Opaque\ndata:\n  secrets.yaml: b2lkYzoKICBjbGllbnRfc2VjcmV0OiB2Qk9jdEtjOHNjYWxSUF9fNjJ6aDNvTmJGMkhKV2gybFNleW8wRXZOY0hMcEJib2tOaWNubjg2SW5SS2xiMlA0CnNlc3Npb25fc2VjcmV0OiA2NjYwYjE1YWQxM2Y5YzViZDBiNzQ3NjdkMDBkMDVhMwo=\n```\n\nThe base64-encoded string under `secrets.yaml` decodes to:\n\n```yaml\noidc:\n  client_secret: vBOctKc8scalRP__62zh3oNbF2HJWh2lSeyo0EvNcHLpBbokNicnn86InRKlb2P4\nsession_secret: 6660b15ad13f9c5bd0b74767d00d05a3\n```\n\nApply these manifests onto your cluster.\n\nYou can then use a command like the following to deploy an instance of the\ngateway. Make sure to pick an appropriate values for all the arguments.\n\n```bash\nhelm template . \\\n  --set config.configMap=oidc-gateway \\\n  --set config.secret=oidc-gateway \\\n  --set app=secretservice \\\n  -n boring-wozniak | kubectl apply -f -\n```  \n\nThis will create a Deployment and a Service. It is up to you to direct HTTP traffic\nto the Service. You can use an Ingress or pass `--set service.type=LoadBalancer` to\nthe `helm` invocation, for example.\n\n## Future plans\n\n- Support redis and/or memcached for session storage so we can scale beyond one pod.\n"
},
{
  "name": "mozilla-services.github.io",
  "files": {
    "/": [
      "images",
      "index.html",
      "params.json",
      "react-jsonschema-form.html",
      "react-jsonschema-form",
      "stylesheets"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "pulsetranslator",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Procfile",
      "README.md",
      "pulsetranslator",
      "requirements.txt",
      "setup.py",
      "test"
    ]
  },
  "makefile": null,
  "readme": "PulseTranslator\n===============\n\nThe pulsetranslator script consumes Mozilla buildbot pulse messages\nand then re-publishes them in a normalized format.  It does this\nbecause native buildbot messages do not share any consistent\nstructure, and change frequently and without warning.  Consuming them\ndirectly is therefore error-prone and subject to frequent failure.\n\nThe normalized messages are published to the exchange\n\"exchange/build/normalized\".\n\n\nInstalling, Configuring, and Running\n------------------------------------\n\nInstall pulsetranslator as any regular Python package:\n\n    python setup.py install\n\nYou'll need to create a ini-style config file with information on the\nPulse accounts.  It should have both [publisher] and [consumer]\nsections, and both must contain at least \"user\" and \"password\"\noptions.  They can also include any of the standard mozillapulse\n[configuration options][].\n\nThe minimum command line to run pulsetranslator is\n\n    runtranslator --pulse-cfg=<path to config file>\n\nYou can get help on other options by running\n\n    runtranslator -h\n\nTo test specific buildbot pulse messages, you can push a locally stored JSON\nfile into the translator. It's recommended to use the --display-only option, to\nonly log the final normalized message to the console.\n\n    runtranslator --display-only --push-message <path_to_json_file>\n\nPlease keep in mind that in case of errors, details can be found in the specific\nlog files under \"logs\" sub directory.\n\nRouting Keys\n------------\n\nFor unittests:\n\n    unittest.%tree%.%platform%.%os%.%buildtype%.%testname%.%product%.%builder%\n\nFor example,\n\n    unittest.mozilla-central.win32.xp.debug.xpcshell.firefox.mozilla-central_xp-debug_test-xpcshell\n\nFor talos tests, the same format applies, except that the first part\nof the key is 'talos' instead of 'unittest'.  For example:\n\n    talos.try.linux64.fedora64.opt.chrome_2.firefox.try_fedora64_test-chrome\n\nFor builds:\n\n    build.%tree%.%platform%.%buildtype%.%builder%\n\nFor example:\n\n    build.try.android.debug.try-android-debug\n\n\nPulsebuildmonitor\n-----------------\n\nFor simple uses, you may be able to consume messages directly from the\n\"exchange/build/normalized\" exchange.  For more complex uses, you may\nfind it easier to use [pulsebuildmonitor][], which can filter messages\nfor you based on a number of criteria.  See the [pulsebuildmonitor README][]\nfor more details.\n\n[configuration options]: https://github.com/mozilla-services/mozillapulse/blob/master/mozillapulse/config.py\n[pulsebuildmonitor]: http://hg.mozilla.org/automation/pulsebuildmonitor\n[pulsebuildmonitor README]: http://hg.mozilla.org/automation/pulsebuildmonitor/file/tip/README.txt\n"
},
{
  "name": "iprepd-cli",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Makefile",
      "README.md",
      "commands",
      "config",
      "main.go"
    ]
  },
  "makefile": "NAME:=repd # $(shell basename `git rev-parse --show-toplevel`)\nRELEASE:=$(shell git rev-parse --verify --short HEAD)\nVERSION = 0.1.0\n\nall: setbin\n\nsetbin: build\n\tcp $(NAME) /usr/local/bin\n\nbuild:\n\tgo build -ldflags \"-X main.version=$(VERSION)-$(RELEASE)\" -o $(NAME)\n",
  "readme": "# iprepd-cli\n\n[![Go Report Card](https://goreportcard.com/badge/github.com/mozilla-services/iprepd-cli)](https://goreportcard.com/report/github.com/mozilla-services/iprepd-cli)\n[![GitHub issues](https://img.shields.io/github/issues/mozilla-services/iprepd-cli.svg)](https://github.com/mozilla-services/iprepd-cli/issues)\n[![Documentation](https://godoc.org/github.com/mozilla-services/iprepd-cli?status.svg)](https://godoc.org/github.com/mozilla-services/iprepd-cli)\n[![license](https://img.shields.io/github/license/mozilla-services/iprepd-cli.svg)](https://github.com/mozilla-services/iprepd-cli/blob/master/LICENSE)\n\nCommand line client for [IPrepd](https://github.com/mozilla-services/iprepd)\n\n### Getting started\n\nOn unix systems, build with ```make```:\n\n```\n$ make\ngo build -ldflags \"-X main.version=0.1.0-7a96e4e\" -o repd\ncp repd /usr/local/bin\n```\n\nSet your local configuration with ```repd config set```:\n\n```\n$ repd config set --url http://localhost:8080 --token \"APIKey test\"\n```\nVerify your configuration with ```repd config show```:\n\n```\n$ repd config show\n+----------+-----------------------+\n| HOST_URL | http://localhost:8080 |\n| AUTH_TK  | APIKey test           |\n+----------+-----------------------+\n```"
},
{
  "name": "gcs-signing-proxy",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Gopkg.lock",
      "Gopkg.toml",
      "LICENSE",
      "Makefile",
      "README.md",
      "cacert.pem",
      "main.go",
      "proxy",
      "vendor"
    ]
  },
  "makefile": "CREDS ?= $(shell pwd)/service_account_key.json\nTAG := local/gcs-signing-proxy:latest\n\nbuild:\n\tCGO_ENABLED=0 gox -osarch=\"linux/amd64\" -output=\"gcs-signing-proxy\"\n\tdocker build --no-cache -t ${TAG} .\n\nrun:\n\tdocker run -i -t -v ${CREDS}:/service_account_key.json --env-file my.env --rm -p 8000:8000 ${TAG}\n\nclean:\n\trm gcs-signing-proxy*\n",
  "readme": "[![Go Report Card](https://goreportcard.com/badge/github.com/mozilla-services/gcs-signing-proxy)](https://goreportcard.com/report/github.com/mozilla-services/gcs-signing-proxy)\n\n# gcs-signing-proxy\n\nProxies incoming HTTP GET requests to signed GCS object retrieval.\n\n## Usage\n\nImages are available from Docker Hub:\n\n```\n$ docker pull mozilla/gcs-signing-proxy\n```\n\nYou'll need a Google Compute Platform service account key JSON file for a service account that has view access to the Google Cloud Storage bucket you're using. You can create a service account key in the Google Compute Platform console.\n\nThen run gcs-signing-proxy like this:\n\n```\n$ docker run -v CREDENTIALSFILE:/service_account_key.json -p 8000:8000 mozilla/gcs-signing-proxy:latest\n```\n\nreplacing `CREDENTIALSFILE` with the filename of your credentials file.\n\nThis mounts your service account key JSON file as `/service_account_key.json` in the container and exposes the 8000 port to the host.\n\nThe signing proxy listens on `0.0.0.0:8000` by default, which means that it will be exposed to the world _if you expose that port externally_.\n\n## Configuration\n\nThe signing proxy is configured via environment variables with the prefix `SIGNING_PROXY_`. The [config struct](https://github.com/mozilla-services/gcs-signing-proxy/blob/master/main.go#L83-L92) has details on default values and variable types. Implementation by Kelsey Hightower's [envconfig](github.com/kelseyhightower/envconfig).\n\nAvailable environment variables:\n\n    - SIGNING_PROXY_LOG_REQUESTS\n        type: bool\n        description: enable logging of request method and path\n        default: \"true\"\n    - SIGNING_PROXY_STATSD\n        type: bool\n        description: enable statsd reporting\n        default: \"true\"\n    - SIGNING_PROXY_STATSD_LISTEN\n        type: string\n        description: address to send statsd metrics to\n        default: \"127.0.0.1:8125\"\n    - SIGNING_PROXY_STATSD_NAMESPACE\n        type: string\n        description: prefix for statsd metrics. \".\" is appended as a separator.\n        default: \"SIGNING_PROXY\"\n    - SIGNING_PROXY_LISTEN\n        type: string\n        description: address for the proxy to listen on\n        default: \"0.0.0.0:8000\"\n    - SIGNING_PROXY_BUCKET\n        type: string\n        description: the GCS bucket\n        default: \"\"\n\n\n## Development/hacking\n\nRequirements:\n\n* docker\n* make\n* [`dep`](https://golang.github.io/dep/)\n* [`gox`](https://github.com/mitchellh/gox)\n\nTo build binary and Docker image, do:\n\n```\n$ make build\n```\n\nTo sync `Gopkg.lock` and vendored packages:\n\n```\n$ dep ensure\n```\n\nYou'll need a Google Cloud Platform service account key JSON file for a service account that has view access to the Google Cloud Storage bucket you're using. You can create a service account key in the Google Compute Platform console.\n\nYou'll need to create a `my.env` env file with the bucket name in it like this:\n\n```\nSIGNING_PROXY_BUCKET=mybucket\n```\n\nYou can set other configuration in that file, too, or use the defaults.\n\nThen run gcs-signing-proxy like this:\n\n```\n$ make run\n```\n\nYou can also set the `CREDS` environment variable to the name of your service account key JSON file if it's not named `service_account_key.json`.\n\n`CTRL-c` will stop the service.\n\nAfter making big changes, update the `version` const in `main.go`.\n"
},
{
  "name": ".github",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "SECURITY.md"
    ]
  },
  "makefile": null,
  "readme": "# .github\nDefaults for varions GitHub Community Health features\n"
},
{
  "name": "go-mozlogrus",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "applog.go",
      "logrus.go",
      "logrus_test.go"
    ]
  },
  "makefile": null,
  "readme": "# mozlogrus [![GoDoc](https://godoc.org/go.mozilla.org/mozlogrus?status.svg)](https://godoc.org/go.mozilla.org/mozlogrus)\nA logging library which conforms to [Mozilla's logging standard](https://wiki.mozilla.org/Firefox/Services/Logging) for [logrus](https://github.com/Sirupsen/logrus).\n\n## Installation\n\n`go get go.mozilla.org/mozlogrus`\n\n## Example\n\n### Basic Usage\n```go\npackage main\n\nimport (\n\tlog \"github.com/Sirupsen/logrus\"\n\t\"go.mozilla.org/mozlogrus\"\n)\n\nfunc init() {\n\tmozlogrus.Enable(\"ApplicationName\")\n}\n\nfunc main() {\n\tlog.WithFields(log.Fields{\n\t\t\"animal\": \"walrus\",\n\t\t\"size\":   10,\n\t}).Info(\"A group of walrus emerges from the ocean\")\n}\n```\n\n```json\n$ go run mozlogrus.go | jq\n{\n  \"Timestamp\": 1487349663973687600,\n  \"Time\": \"2017-02-17T16:41:03Z\",\n  \"Type\": \"app.log\",\n  \"Logger\": \"ApplicationName\",\n  \"Hostname\": \"gator3\",\n  \"EnvVersion\": \"2.0\",\n  \"Pid\": 18061,\n  \"Severity\": 4,\n  \"Fields\": {\n    \"animal\": \"walrus\",\n    \"msg\": \"A group of walrus emerges from the ocean\",\n    \"size\": 10\n  }\n}\n```\n\n### Custom Log Types\n\n```go\nfunc init() {\n    mozlogrus.EnableFormatter(&mozlogrus.MozLogFormatter{\n        LoggerName: \"ApplicationName\",\n        Type: \"udp datagram\",\n    })\n}\n```\n"
},
{
  "name": "slog-mozlog-json",
  "files": {
    "/": [
      ".clog.toml",
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "src"
    ]
  },
  "makefile": null,
  "readme": "[![License: MPL 2.0]][MPL 2.0] [![Build Status]][travis] [![Latest Version]][crates.io] [![Api Rustdoc]][rustdoc]\n\n[License: MPL 2.0]: https://img.shields.io/badge/License-MPL%202.0-blue.svg\n[MPL 2.0]: https://opensource.org/licenses/MPL-2.0\n[Build Status]: https://travis-ci.org/mozilla-services/slog-mozlog-json.svg?branch=master\n[travis]: https://travis-ci.org/mozilla-services/slog-mozlog-json\n[Latest Version]: https://img.shields.io/crates/v/slog-mozlog-json.svg\n[crates.io]: https://crates.io/crates/slog-mozlog-json\n[Api Rustdoc]: https://img.shields.io/badge/api-rustdoc-blue.svg\n[rustdoc]: https://docs.rs/slog-mozlog-json\n\nA [MozLog] JSON drain for [slog-rs].\n\n[MozLog]: https://wiki.mozilla.org/Firefox/Services/Logging\n[slog-rs]: https://github.com/slog-rs/slog\n"
},
{
  "name": "stubattribution-loadtests",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Makefile",
      "README.md",
      "__init__.py",
      "loads.tpl",
      "loadtest.env",
      "loadtest.py",
      "requirements.txt"
    ]
  },
  "makefile": "#!make\n\n# load env vars\ninclude loadtest.env\nexport $(shell sed 's/=.*//' loadtest.env)\n\n\nOS := $(shell uname)\nHERE = $(shell pwd)\nPYTHON = python3\nVTENV_OPTS = --python $(PYTHON)\n\nBIN = $(HERE)/venv/bin\nVENV_PIP = $(BIN)/pip3\nVENV_PYTHON = $(BIN)/python\nINSTALL = $(VENV_PIP) install\n\n.PHONY: all check-os install-elcapitan install build\n.PHONY: configure\n.PHONY: docker-build docker-run docker-export\n.PHONY: test test-heavy clean clean-env\n\nall: build setup_random configure\n\n\n# hack for OpenSSL (cryptography) w/ OS X El Captain:\n# must run make as sudo on OSX\n# https://github.com/phusion/passenger/issues/1630\ncheck-os:\nifeq ($(OS),Darwin)\n  ifneq ($(USER),root)\n    $(info \"clang now requires sudo, use: sudo make <target>.\")\n    $(info \"Aborting!\") && exit 1\n  endif\n  BREW_PATH_OPENSSL=$(shell brew --prefix openssl)\nendif\n\ninstall-elcapitan: check-os\n\tenv LDFLAGS=\"-L$(BREW_PATH_OPENSSL)/lib\" \\\n\t    CFLAGS=\"-I$(BREW_PATH_OPENSSL)/include\" \\\n\t    $(INSTALL) cryptography\n\n$(VENV_PYTHON):\n\tvirtualenv $(VTENV_OPTS) venv\n\ninstall:\n\t$(INSTALL) -r requirements.txt\n\nbuild: $(VENV_PYTHON) install-elcapitan install\n\nconfigure: build\n\tif [[ ! -w loadtest.env ]]; then touch loadtest.env; fi\n\t@bash loads.tpl\n\n\ntest:\n\tbash -c \"$(BIN)/molotov -c -d $(TEST_DURATION) ./loadtest.py\"\n\ntest-heavy:\n\tbash -c \"$(BIN)/molotov -c -d $(TEST_HEAVY_DURATION) \\\n                                   -w $(TEST_HEAVY_WORKERS) ./loadtest.py\"\n\ndocker-build:\n\tbash -c \"docker build -t $(NAME_DOCKER_IMG) .  --build-arg URL_TEST_REPO=$(URL_TEST_REPO) --build-arg NAME_TEST_REPO=$(NAME_TEST_REPO)\"\n\ndocker-run:\n\tbash -c \"docker run -e HMAC_KEY=$(HMAC_KEY) \\\n                         -e TEST_DURATION=$(TEST_DURATION) \\\n                         -e TEST_PROCESSES=$(TEST_PROCESSES) \\\n                         -e TEST_CONNECTIONS=$(TEST_CONNECTIONS) $(NAME_DOCKER_IMG)\"\n\ndocker-export:\n\tdocker save \"$(NAME_DOCKER_IMG)\" | bzip2> $(PROJECT_NAME)-latest.tar.bz2\n\n\nclean-env:\n\t@cp loadtest.env loadtest.env.OLD\n\t@rm -f loadtest.env\n\nclean:\n\t@rm -fr venv/ __pycache__/\n",
  "readme": "[![Build Status](https://travis-ci.org/mozilla-services/stubattribution-loadtests.svg?branch=master)](https://travis-ci.org/mozilla-services/stubattribution-loadtests)\n[![Updates](https://pyup.io/repos/github/mozilla-services/stubattribution-loadtests/shield.svg)](https://pyup.io/repos/github/mozilla-services/stubattribution-loadtests/)\n[![Python 3](https://pyup.io/repos/github/mozilla-services/stubattribution-loadtests/python-3-shield.svg)](https://pyup.io/repos/github/mozilla-services/stubattribution-loadtests/)\n\n\n# stubattribution-loadtests\n\ngeneric load test based on molotov: https://github.com/loads/molotov\n\n## Requirements\n\n- Python 3.5+\n\n\n## How to run the loadtest?\n\n### For STAGE \n\n    molotov -c -d 1 loadtest.py\n\n## How to build the docker image?\n\n    sudo make docker-build\n\n\n## How to run the docker image?\n\n    make docker-run\n\n\n## How to clean the repository?\n\n    make clean\n"
},
{
  "name": "shavar-experiments",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "fastblock1-whitelist.json",
      "fastblock1.json",
      "fastblock2-whitelist.json",
      "fastblock2.json",
      "fastblock3.json"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "systrack",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "README.md",
      "cmd",
      "systrack-lambda"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "SysTrack\n========\n\nSystem tracking for security. This repository contains a command line tool\ncalled `systrack` that dumps metadata and installed packages from linux\nsystems into kinesis, and a lambda function `systrack-lambda` that processes\nkinesis messages and looks for security issues.\n\nThe idea is to run `systrack` from a cronjob on production systems, publishes\nthe state of running systems into kinesis, and analyze that data and raise\nalerts in the `systrack-lambda` function.\n"
},
{
  "name": "sign-xpi",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "aws_lambda",
      "circle.yml",
      "cli",
      "requirements_dev.txt",
      "setup.py",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": "VENV=.venv\nVENV_DEV=.venv-dev\n\nclean:\n\trm -fr venv $(VENV) lambda.zip ami-built-lambda.zip\n\nvirtualenv:\n\tvirtualenv $(VENV) --python=python3.6\n\t$(VENV)/bin/pip install -r aws_lambda/requirements.txt\n\nvirtualenv-dev:\n\tvirtualenv $(VENV_DEV) --python=python3.6\n\t$(VENV_DEV)/bin/pip install -r aws_lambda/requirements.txt\n\t$(VENV_DEV)/bin/pip install -r requirements_dev.txt\n\nzip: clean virtualenv\n\tzip lambda.zip aws_lambda/sign_xpi.py aws_lambda/__init__.py\n\tcd $(VENV)/lib/python3.6/site-packages/; zip -r ../../../../lambda.zip *\n\nbuild_image:\n\tdocker build -t sign-xpi .\n\nget_zip: build_image\n\tdocker rm sign-xpi || true\n\tdocker run --name sign-xpi sign-xpi\n\tdocker cp sign-xpi:/app/lambda.zip .\n\ninstall-autograph: $(VENV_DEV)/bin/autograph\n\n$(VENV_DEV)/bin/autograph:\n\tenv GOPATH=`pwd`/$(VENV_DEV) go get -u go.mozilla.org/autograph\n\nrun-autograph: install-autograph\n\t$(VENV_DEV)/bin/autograph -c $(VENV_DEV)/src/go.mozilla.org/autograph/autograph.yaml\n",
  "readme": "==========\n sign-xpi\n==========\n\nAn AWS Lambda (and a supplementary CLI utility) to sign XPI files.\n\nThe packaging of this repository is meant to be reminiscent of the\namo2kinto-lambdo repo.\n\nUse this script to generate a zip for Amazon Lambda::\n\n  make clean virtualenv zip\n\nYou must run this script on a linux x86_64 arch, the same as Amazon Lambda.\n\nThis will package a lambda with a handler at ``aws_lambda.sign_xpi.handle``.\n\nIf you don't have access to exactly the right kind of machine, you can\nalso use Docker with the command::\n\n  make get_zip\n\nThis will package the same kind of lambda, but build it in a Docker\ncontainer.\n\nDevelopment\n===========\n\nAnother virtualenv is created for doing development, so that we don't\naccidentally include dev tools in the lambda. To build it::\n\n  make virtualenv-dev\n\nRun the tests using::\n\n  . .venv-dev/bin/activate\n  make run-autograph &\n  py.test\n"
},
{
  "name": "PyPOM-aXe",
  "files": {
    "/": [
      ".gitignore",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "MANIFEST.in",
      "README.rst",
      "pypom_axe",
      "setup.cfg",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "pypom-axe\n##########\n\npypom-axe integrates the aXe accessibility testing API with PyPOM.\n\n\n.. image:: https://img.shields.io/badge/license-MPL%202.0-blue.svg?style=plastic\n   :target: https://github.com/kimberlythegeek/pypom-axe/blob/master/LICENSE.txt\n   :alt: License\n.. image:: https://img.shields.io/pypi/v/pypom-axe.svg?style=plastic\n   :target: https://pypi.org/project/pypom-axe/\n   :alt: PyPI\n.. image:: https://img.shields.io/pypi/wheel/pypom-axe.svg?style=plastic\n   :target: https://pypi.org/project/pypom-axe/\n   :alt: wheel\n.. image:: https://img.shields.io/github/issues-raw/kimberlythegeek/pypom-axe.svg?style=plastic\n   :target: https://github.com/kimberlythegeek/pypom-axe/issues\n   :alt: Issues\n\nRequirements\n*************\n\nYou will need the following prerequisites in order to use pypom-axe:\n\n- Python 2.7 or 3.6\n- PyPOM >= 1.2.0\n\nInstallation\n*************\n\nTo install pypom-axe:\n\n.. code-block:: bash\n\n  $ pip install pypom-axe\n\nUsage\n*************\n\n``pypom-axe`` will run the aXe accessibility checks by default whenever its\n``wait_for_page_to_load()`` method is called.\n\nIf you overload ``wait_for_page_to_load()``, you will need to call\n``super([YOUR CLASS NAME], self).wait_for_page_to_load()`` within your\noverloaded method.\n\nYou can disable accessibility tests by setting the environment variable\n``ACCESSIBILITY_DISABLED`` to ``true``.\n\nYou can **enable** output of the JSON results by setting the environment variable\n``ACCESSIBILITY_REPORTING`` to ``true``. This will output files to ./results/,\nwhich must be created if it does not already exist.\n\n*base.py*\n\n.. code-block:: python\n\n   from pypom_axe.axe import AxePage as Page\n\n   class Base(Page):\n\n   def wait_for_page_to_load(self, context=None, options=None, impact=None):\n     super(Base, self).wait_for_page_to_load()\n     self.wait.until(lambda s: self.seed_url in s.current_url)\n     return self\n\nYou also have the option to customize the accessibility analysis using the\nparameters ``context``, ``options``, and ``impact``.\n\n``context`` and ``options`` directly reflect the parameters used in axe-core.\nFor more information on ``context`` and ``options``, view the `aXe\ndocumentation here <https://github.com/dequelabs/axe-core/blob/master/doc/API.md#parameters-axerun>`_.\n\nThe third parameter, ``impact``, allows you to filter violations by their impact\nlevel.\n\nThe options are ``'critical'``, ``'serious'`` and ``'minor'``, with the\ndefault value set to ``None``.\n\nThis will filter violations for the impact level specified, and **all violations with a higher impact level**.\n\n.. code-block:: python\n\n  from pypom_axe.axe import AxePage as Page\n\n  class Base(Page):\n\n  def wait_for_page_to_load(self, context=None, options=None, impact=None):\n    super(Base, self).wait_for_page_to_load(None, None, 'serious')\n    self.wait.until(lambda s: self.seed_url in s.current_url)\n    return self\n\nResources\n===========\n\n- `Issue Tracker <https://github.com/kimberlythegeek/pypom-axe/issues>`_\n- `Code <https://github.com/kimberlythegeek/pypom-axe>`_\n"
},
{
  "name": "fx-test-pubkeys",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "add-user.sh",
      "add_user.service",
      "cchiorean.pub",
      "chartjes.pub",
      "ckolos.pub",
      "dhunt.pub",
      "epedersen.pub",
      "kthiessen.pub",
      "mbrandt.pub",
      "pdehaan.pub",
      "pjenvey.pub",
      "rbillings.pub",
      "rpappalardo.pub",
      "sdonner.pub",
      "tziade.pub",
      "user-setup.sh"
    ]
  },
  "makefile": null,
  "readme": "# fx-test-pubkeys\nssh public keys for the full time Firefox Test Eng team at mozilla\n\n# WARNING\n\nNever just clone this repo and trust the pubkeys! Use the last-known good SHA instead. See the *How to use* section below.\n\n## The problem\n\nThe Firefox Test Engineering team builds test tools using one-off AWS EC2 instances. Some of these tools become indispensable, yet, don't warrant being monitored and managed by ops.\n\nIn the past, tools have broken down while their creators were on vacation or unavailable, leading to bummer-times for everybody involved--either sshing into a box while on PTO, or the tool just being borked for days.\n\n## The solution\n\nThis repo contains public keys for the Firefox Test Engineering team.\n\nIf toolmakers upload their fellow teammates pubkeys to a long-lived EC2 instance, anybody can reboot or troubleshoot a downed machine when its creator is out on vacation.\n\n## How to use \n\nOnly use the pubkeys from the latest SHA which has been verified as good by the Firefox Test Engineering team. (This happens in email, not in github.)\n\n## When to use\n\nIf you create a tool on an EC2 instance that someone might need to maintain while you're away, then you can upload the fx-test-pubkeys to that EC2 instance and relax.\n\n## How to update with new keys\n\nIf people join the team and you add keys to this repo, you must notify the team via the firefox-test-engineering mailing list and ask everyone to verify their keys are correct in the version identified by the new HEAD SHA.\n\nWe trust github and git because you can't modify the keys without also changing the SHA.\n\n# update-user.sh\n\nwget this script to your host, then run.\n\n"
},
{
  "name": "shavar-loadtests",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "configs",
      "lists_shavar.py",
      "loads-broker.json",
      "loadtest.py",
      "molotov.env",
      "requirements.txt"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "#!make\n\n# load env vars\ninclude molotov.env\nexport $(shell sed 's/=.*//' molotov.env)\n\n\nOS := $(shell uname)\nHERE = $(shell pwd)\nPYTHON = python3\nVTENV_OPTS = --python $(PYTHON)\n\nBIN = $(HERE)/venv/bin\nVENV_PIP = $(BIN)/pip3\nVENV_PYTHON = $(BIN)/python\nINSTALL = $(VENV_PIP) install\n\n.PHONY: all check-os install-elcapitan install build\n.PHONY: configure \n.PHONY: docker-build docker-run docker-export\n.PHONY: test test-heavy clean clean-env\n\nall: build broker-config\n\n\n# hack for OpenSSL (cryptography) w/ OS X El Captain: \n# must run make as sudo on OSX\n# https://github.com/phusion/passenger/issues/1630\ncheck-os:\nifeq ($(OS),Darwin)\n  ifneq ($(USER),root)\n\t$(info \"clang now requires sudo, use: sudo make <target>.\")\n\t$(info \"Aborting!\") && exit 1\n  endif  \n  BREW_PATH_OPENSSL=$(shell brew --prefix openssl)\nendif\n\ninstall-elcapitan: check-os \n\tenv LDFLAGS=\"-L$(BREW_PATH_OPENSSL)/lib\" \\\n\t    CFLAGS=\"-I$(BREW_PATH_OPENSSL)/include\" \\\n\t    $(INSTALL) cryptography \n\n$(VENV_PYTHON):\n\tvirtualenv $(VTENV_OPTS) venv\n\ninstall:\n\t$(INSTALL) -r requirements.txt\n\nbuild: $(VENV_PYTHON) install-elcapitan install\n\nbroker-config: \n\tif [[ ! -w molotov.env ]]; then touch molotov.env; fi\n\t@bash loads-broker.tpl\n\n\ntest:\n\tbash -c \"$(BIN)/molotov $(VERBOSE) -c -d $(TEST_DURATION) ./loadtest.py\"\n\ntest-heavy:\n\tbash -c \"$(BIN)/molotov $(VERBOSE) -c -d $(TEST_HEAVY_DURATION) \\\n                                   -w $(TEST_HEAVY_WORKERS) ./loadtest.py\"\n\n\ndocker-build:\n\tbash -c \"docker build -t $(NAME_DOCKER_IMG) .\"\n\ndocker-run:\n\tbash -c 'docker run  -e PY_CONFIG=$(PY_CONFIG) -e URL_SERVER=$(URL_SERVER) -e TEST_DURATION=$(TEST_DURATION) -e TEST_CONNECTIONS=$(TEST_CONNECTIONS) -t $(NAME_DOCKER_IMG)'\n\ndocker-export:\n\tdocker save \"$(NAME_DOCKER_IMG)\" | bzip2> $(PROJECT_NAME)-latest.tar.bz2\n\n\nclean-env: \n\t@cp molotov.env molotov.env.OLD\n\t@rm -f molotov.env\n\t\nclean: \n\t@rm -fr venv/ __pycache__/\n\n",
  "readme": "# shavar-loadtests\n\ngeneric load test based on molotov: https://github.com/loads/molotov\n\n## Requirements\n\n- Python 3.4\n\n\n## How to run the loadtest?\n\n### For STAGE \n\n    make test -e URL_SERVER=https://shavar.stage.mozaws.net\n\n\n## How to build the docker image?\n\n    make docker-build\n\n\n## How to run the docker image?\n\n    make docker-run\n\n\n## How to clean the repository?\n\n    make clean\n"
},
{
  "name": "shavar-e2e-tests",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "conftest.py",
      "helper_prefs.py",
      "os_handler.py",
      "pages",
      "prefs.ini",
      "requirements",
      "scripts",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "# shavar-e2e-tests\nShavar / Tracking Protection\n\nThis repo will hold all of the e2e-tests for Shavar, also known as Tracking Protection.\nFor manual test case steps please refer to TestRail test case repository: https://testrail.stage.mozaws.net/index.php?/projects/overview/32\n\nPlease file an issue here to request permission for TestRail.\n\nDetailed information lives on the internal Mana page:\nhttps://mana.mozilla.org/wiki/display/SVCOPS/Shavar+-+aka+Mozilla%27s+Tracking+Protection\n\n## Pre-reqs\nKnowledge of Python, Selenium, Docker and Tracking Protection[Shavar]\nInstall: python, pytest, Docker\n"
},
{
  "name": "autograph-loadtests",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "chrome.manifest",
      "loadtest.py",
      "requirements.txt",
      "test-addon.xpi"
    ]
  },
  "makefile": null,
  "readme": "# autograph-loadtests\n\n## Description\nLoadtests for Mozilla's autograph service\n\n\n## Docker \n\n### Prerequisites\n1. A working autograph server:  https://github.com/mozilla-services/autograph\n1. HAWK credentials\n1. docker installation\n1. Set the following environment variables: HOST, PORT, HAWK_ID, HAWK_KEY, SIGNER\n\n### Summary\nTo run the autograph loadtests first build the docker image:\n\n```\ndocker build -t autograph-loadtest .\n```\n\nThen execute:\n\n```\ndocker run -e \"HOST=$HOST\" -e \"PORT=$PORT\" -e \"HAWK_ID=$HAWK_ID\" -e \"HAWK_KEY=$HAWK_KEY\" -e \"SIGNER=$SIGNER\" autograph-loadtest\n```\n\n\n"
},
{
  "name": "reaper",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "FILTERS.md",
      "LICENSE",
      "README.md",
      "aws",
      "circle.yml",
      "config",
      "events",
      "filters",
      "glide.lock",
      "glide.yaml",
      "main.go",
      "prices",
      "reapable",
      "reaper",
      "reaperlog",
      "state",
      "token",
      "vendor"
    ]
  },
  "makefile": null,
  "readme": "# AWS Reaper\n\n## About\n\nThe Reaper terminates forgotten AWS resources.\n\nReaper workflow:\n\n1. Find all enabled resources, filter them, then fire events based on config\n    - Event types include sending emails, posting events to Datadog (Statsd), tagging resources on AWS, stopping or killing resources, and more\n    - Reaper uses the `Owner` tag on resources to notify the owner of the resource with options to Ignore (for a time), Whitelist, Terminate, or Stop each resource they own\n2. Report statistics about the resources that were found\n3. Terminate or Stop abandoned resources after a set amount of time\n\n*Caution* This app is experimental because:\n* doesn't have a lot of tests (Read: any), will add when SDK is updated\n\n## Building\n* install glide per `https://github.com/Masterminds/glide`\n* checkout repo\n* install dependencies: `glide install`\n* build binary: `go build main.go`\n* run binary: `./reaper -config config/default.toml`\n* for command line options: `./reaper -help`\n\n## Command Line Flags\n* config: required flag, the path to the Reaper config file. `string` (no default value)\n* dryrun: run Reaper in dryrun (no-op) mode. Events will not be triggered. `boolean` (default: true)\n* withoutCloudformationResources: skip checking for Cloudformation Resource dependencies (throttled by AWS, so it takes ages). `boolean` (default: false)\n\n## Creating a configuration file\nReaper configuration files should be in toml format. See `config/default.toml` for an example config.\n\n* Top level options\n    - LogFile: the full filepath of the file that logs are written to. `string`\n    - WhitelistTag: a string that will be used to tag resources that have been whitelisted. Defaults to `REAPER_SPARE_ME`. (string)\n    - DefaultOwner: all unowned resources will be assigned to this owner. Can be an email address, or can be a username if DefaultEmailHost is specified. `string`\n    - DefaultEmailHost: resources that do not have a complete email address as their owner will have this appended. Should be of the form \"domain.tld\". Works with DefaultOwner in the following way: `DefaultOwner`@`DefaultEmailHost`. `string`\n    - EventTag: a tag that is added to all events that support tagging. Should be of the form `key1:value1,key2:value2`. `string`\n* HTTP options (under `[HTTP]`)\n    - TokenSecret: the secret key used to secure web requests. `string`\n    - ApiURL: used to generate URLs for Reaper's HTTP API. Should be of the form `protocol://host:port`. `string`\n    - Listen: where the HTTP server will listen for requests. Should be of the form `host:port`. `string`\n    - Token: TODO\n    - Action: TODO\n* Logging (under `[Logging]`)\n    - Extras: enables or disables extra logging, such as dry run notifications for EventReporters not triggering. `boolean`\n* States (under `[States]`)\n    - Interval: the interval between Reaper's scans for resources. The time format must be a duration parsable by Go's time.ParseDuration. See: http://godoc.org/time#ParseDuration. Example: `1h`. `string`\n    - FirstStateDuration: the length of the first state assigned to resources that match filters. The time format must be a duration parsable by Go's time.ParseDuration. See: http://godoc.org/time#ParseDuration. Example: `1h`. `string`\n    - SecondStateDuration: the length of the second state assigned to resources that match filters. The time format must be a duration parsable by Go's time.ParseDuration. See: http://godoc.org/time#ParseDuration. Example: `1h`. `string`\n    - ThirdStateDuration: the length of the third state assigned to resources that match filters. After the Third state elapses, resources move to a permanent final state. The time format must be a duration parsable by Go's time.ParseDuration. See: http://godoc.org/time#ParseDuration. Example: `1h`. `string`\n* Events (under `[Events]`)\n    - Datadog (`[Events.Datadog]`)\n        + Enabled: enables or disables the Datadog EventReporter. Note: Datadog statistics and Event depend on this. `boolean`\n        + Triggers: states for which Datadog will trigger Reapable Events. Can be any/all/none of `first`, `second`, `third`, `final`, or `ignore`. `[]string`\n    - Tagger (`[Events.Tagger]`)\n        + Enabled: enables or disables the Tagger EventReporter. `boolean`\n        + Triggers: states for which Tagger will trigger Reapable Events. Can be any/all/none of `first`, `second`, `third`, `final`, or `ignore`. `[]string`\n    - Reaper (`[Events.Reaper]`)\n        + Enabled: enables or disables the Reaper EventReporter. `boolean`\n        + Triggers: states for which Reaper will trigger Reapable Events. Can be any/all/none of `first`, `second`, `third`, `final`, or `ignore`. `[]string`\n        + Mode: when the Reaper EventReporter is triggered on a Reapable Event, it will `Stop` or `Terminate` Reapables per this flag. Note: modes must be capitalized. `string`\n    - Email (`[Events.Email]`)\n        + Enabled: enables or disables the Email EventReporter. `boolean`\n        + Triggers: states for which Email will trigger Reapable Events. Can be any/all/none of `first`, `second`, `third`, `final`, or `ignore`. `[]string`\n        + Host: the mailserver Reaper will use\n        + AuthType: the type of authentication used by the mailserver. Should be one of `none`, `md5` or `plain`. `string`\n        + Port: the port used by the mailserver. `int`\n        + Username: the username to use for the mailserver. `string`\n        + Password: the password to use for the nmailserver. `string`\n        + From: the address that Reaper will send mail from, must be parsable by Go's mail.ParseAddress. See: http://godoc.org/net/mail#ParseAddress. `string`\n* All Supported AWS Resource types have these properties\n    - Enabled: enables or disables reporting of this resource type. Note: resources will still be queried for as they inform Reaper about the dependencies of other resources. `boolean`\n    - FilterGroups (under `[ResourceType.FilterGroups]`): FilterGroups are sets of filters that can be applied to resources. In order for a resource to match a FilterGroup, it must match _all_ filters in the FilterGroup. If an resource matches _any_ FilterGroup, it has satisfied Reaper's filters. `[]FilterGroup`\n        + Example FilterGroup:\n            ```\n            [ResourceType.FilterGroups.Example]\n                    [ResourceType.FilterGroups.Example.Filter1]\n                        function = \"IsDependency\"\n                        arguments = [\"false\"]\n                    [ResourceType.FilterGroups.Example.Filter2]\n                        function = \"Running\"\n                        arguments = [\"true\"]\n            ```\n\n        + In this example, we see a FilterGroup named \"Example\" that has two Filters, Filter1 and Filter2.\n        + A FilterGroup is a `[]Filter`, and a Filter has two components, a `function` and `arguments`. The `function` is the name of the filtering function for the associated resource type (`string`), and `arguments` is a slice of arguments to that function (`[]string`).\n* Currently supported AWS Resource types:\n    - SecurityGroups (under `[SecurityGroups]`)\n    - Cloudformations (under `[Cloudformations]`)\n    - AutoScalingGroups (under `[AutoScalingGroups]`)\n    - Instances (under `[Instances]`)\n    - Volumes (under `[Volumes]`)\n"
},
{
  "name": "switchboard-server",
  "files": {
    "/": [
      ".babelrc",
      ".eslintrc",
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "USAGE.md",
      "config",
      "experiments",
      "index.js",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "![travis-ci status](https://travis-ci.org/mozilla-services/switchboard-server.svg?branch=master)\n\nSwitchboard is a stateless service that allows clients be divided into A/B testing groups based on UUIDs. Clients send GET requests to the server with their UUID as well as other information that can be useful for classification and user group segmentation. The server determines whether the client matches a set of specifications in the JSON experiments file, and sends back a JSON reply that indicates which experiments the client should enable.\n\nSwitchboard makes enabling features easy - once you have conditional logic in your client application you can simply toggle them on or off in Switchboard.\n"
},
{
  "name": "aws-signing-proxy",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Gopkg.lock",
      "Gopkg.toml",
      "LICENSE",
      "README.md",
      "cacert.pem",
      "main.go",
      "proxy",
      "vendor"
    ]
  },
  "makefile": null,
  "readme": "[![Go Report Card](https://goreportcard.com/badge/github.com/mozilla-services/aws-signing-proxy)](https://goreportcard.com/report/github.com/mozilla-services/aws-signing-proxy)\n\n# aws-signing-proxy\nsigns http requests using AWS V4 signer\n\n### Usage:\n\nRunning the signing proxy should be as simple as running a compiled binary or the `main.go` file. You can also run our prebuilt containers: `docker run -p 8000:8000 mozilla/aws-signing-proxy:latest`.\n\nThe signing proxy listens on `0.0.0.0:8000` by default, which means that it will be exposed to the world _if you expose that port externally_.\n\n### Configuration:\n\nThe signing proxy is configured via environment variables with the prefix `SIGNING_PROXY_`. The [config struct](https://github.com/mozilla-services/aws-signing-proxy/blob/master/main.go#L83-L92) has details on default values and variable types. Implementation by Kelsey Hightower's [envconfig](github.com/kelseyhightower/envconfig).\n\nAvailable environment variables:\n\n    - SIGNING_PROXY_LOG_REQUESTS\n        type: bool\n        description: enable logging of request method and path\n        default: \"true\"\n    - SIGNING_PROXY_STATSD\n        type: bool\n        description: enable statsd reporting\n        default: \"true\"\n    - SIGNING_PROXY_STATSD_LISTEN\n        type: string\n        description: address to send statsd metrics to\n        default: \"127.0.0.1:8125\"\n    - SIGNING_PROXY_STATSD_NAMESPACE\n        type: string\n        description: prefix for statsd metrics. \".\" is appended as a separator.\n        default: \"SIGNING_PROXY\"\n    - SIGNING_PROXY_LISTEN\n        type: string\n        description: address for the proxy to listen on\n        default: \"0.0.0.0:8000\"\n    - SIGNING_PROXY_SERVICE\n        type: string\n        description: aws service to sign requests for\n        default: \"s3\"\n    - SIGNING_PROXY_REGION\n        type: string\n        description: aws region to sign requests for\n        default: \"us-east-1\"\n    - SIGNING_PROXY_DESTINATION\n        type: string\n        description: valid URL that serves as a template for proxied requests. Scheme and Host are preserved for proxied requests.\n        default: \"https://s3.amazonaws.com\"\n\n### Building:\n\n`go build` should be sufficient to build a binary\n\nTo build linux binaries on OSX for containers, I use [`gox`](https://github.com/mitchellh/gox): `gox -osarch=\"linux/amd64\"`\n\n### Docker:\n\nWe're using a `Dockerfile` `FROM scratch`, meaning there's nothing in there at the start.\nWe have the [Mozilla CA certificate store](https://curl.haxx.se/docs/caextract.html) in this repo, and copy it into our containers at build time.\nThis makes our image less than 11mb!\n\nImages are available from Docker Hub: `docker pull mozilla/aws-signing-proxy`\n\n### Development:\n\n`dep` is used for package management:\n  `dep ensure` to keep Gopkg.lock and vendored packages in sync\n\nThere is a simple `version` const in `main.go` for now that we can use to manually track versions.\n"
},
{
  "name": "nginx_moz_ingest",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "config",
      "src",
      "test"
    ]
  },
  "makefile": null,
  "readme": "Mozilla Nginx Data Ingestion\n----------------------------\n\n## Overview\nNginx module for taking HTTP requests, transforming them into Heka\nprotobuf messages and sending them directly to a Kafka cluster. There\nis also a 'landfill' option to write the stream out to disk in addition\nto sending the data to Kafka that can be used as a fail safe.\n\n### Prerequisites\n* [librdkafka](https://github.com/edenhill/librdkafka)\n* [Lua Sandbox](https://github.com/mozilla-services/lua_sandbox)\n\n### Build Instructions\n    \n    # from the nginx build directory\n    ./configure --add-module=<repo_path>/nginx_moz_ingest --prefix=<nginx_path>/nginx-1.7.0\n\n### Example Configuration\n```\nworker_processes  4;\ndaemon off;\nerror_log  logs/error.log;\n\nevents {\n    worker_connections  1024;\n}\n\n\nhttp {\n    default_type  application/octet-stream;\n    sendfile        on;\n    server {\n        listen       8880;\n        server_name  localhost;\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n\n        moz_ingest_kafka_brokerlist         localhost:9092;\n        moz_ingest_kafka_max_buffer_size    100000;\n        moz_ingest_kafka_max_buffer_ms      10;\n        moz_ingest_kafka_batch_size         100;\n        moz_ingest_client_ip                on;\n        moz_ingest_max_unparsed_uri_size    32;\n        moz_ingest_max_content_size         100k;\n        moz_ingest_header                   Content-Length;\n        moz_ingest_header                   User-Agent;\n        moz_ingest_landfill_dir             /mnt/landfill;\n        moz_ingest_landfill_roll_size       300M;\n        moz_ingest_landfill_roll_timeout    60m;\n\n\n        location /submit/telemetry/ {\n                   keepalive_timeout 0;\n                   moz_ingest;\n                   moz_ingest_kafka_topic test;\n                   moz_ingest_landfill_name incoming.telemetry.mozilla.org;\n        }\n    }\n}\n```\n\n### Directives\n\n#### moz_ingest\nActivates the handler for the specified location.\n\n    Syntax: moz_ingest;\n    Default: --\n    Context: location\n\n#### moz_ingest_kafka_brokerlist\nSpecifies the Kafka broker list as specified by the \n[librdkafka documentation](https://github.com/trink/librdkafka/blob/dr_no_poll/src/rdkafka.h#L2258)\n\n    Syntax: moz_ingest_kafka_brokerlist string;\n    Default: --\n    Context: main, http, server, location\n\n#### moz_ingest_kafka_topic\nSpecifies the Kafka topic name to send the messages to.\n\n        Syntax: moz_ingest_kafka_topic string;\n        Default: --\n        Context: main, http, server, location\n\n#### moz_ingest_kafka_max_buffer_size\nSpecifies the maximum number of messages allowed on the Kafka producer queue.\n\n    Syntax: moz_ingest_kafka_max_buffer_size size;\n    Default: 100000\n    Context: main, http, server, location\n\n#### moz_ingest_kafka_max_buffer_ms\nSpecifies the maximum time, in milliseconds, for buffering data on the Kafka producer queue.\n\n    Syntax: moz_ingest_kafka_max_buffer_ms time;\n    Default: 1000\n    Context: main, http, server, location\n\n#### moz_ingest_kafka_batch_size\nSpecifies the maximum number of messages batched in one Kafka MessageSet.\n\n    Syntax: moz_ingest_kafka_batch_size time;\n    Default: 1000\n    Context: main, http, server, location\n\n#### moz_ingest_client_ip\nSpecifies whether or not the client IP address should be included as a **remote_addr** message field.\n\n    Syntax: moz_ingest_client_ip on | off;\n    Default: on\n    Context: main, http, server, location\n\n#### moz_ingest_max_unparsed_uri_size\nSpecifies the maximum unparsed URI size to accept before returning [414].\n\n    Syntax: moz_ingest_max_unparsed_uri_size size;\n    Default: 256\n    Context: main, http, server, location\n\n#### moz_ingest_max_content_size\nSpecifies the maximum content size to accept before returning [413].\n\n    Syntax: moz_ingest_max_content_size size;\n    Default: 100k\n    Context: main, http, server, location\n\n#### moz_ingest_header\nSpecifies a header that should be included as message field (if it exists).\n\n    Syntax: moz_ingest_header field;\n    Default: --\n    Context: main, http, server, location\n\n#### moz_ingest_landfill_dir\nSpecifies the directory the landfill files are written to. If this directive is not set in any\ninherited context or is set to an empty string landfill file creation will be disabled for that\nlocation.\n\n    Syntax: moz_ingest_landfill_dir string;\n    Default: --\n    Context: main, http, server, location\n\n#### moz_ingest_landfill_roll_size\nSpecifies the landfill file size at which the file will be finalized (renamed with a `.done` \nextension) and a new file created.\n\n    Syntax: moz_ingest_landfill_roll_size size;\n    Default: 300M\n    Context: main, http, server, location\n\n#### moz_ingest_landfill_roll_timeout\nSpecifies the landfill timeout when the file will be finalized even if it has not\nreached the roll size. This is driven by incoming requests to the location (per process)\nand not by a timer i.e., if a single request is sent to the location the file will never\nbe rolled due to a timeout, it will only be finalized on Nginx shutdown.\n\n    Syntax: moz_ingest_landfill_roll_timeout seconds;\n    Default: 60m\n    Context: main, http, server, location\n\n#### moz_ingest_landfill_name\nSpecifies the landfill file name prefix (must be unique per location). This name should match the\nexpected `Host` header for the location; if the `Host` header does not match the data is written\nto a file prefixed with `<moz_ingest_landfill_name>_other`.\n\n##### Landfill File Naming Convention\n`<moz_ingest_landfill_dir>/<moz_ingest_landfill_name>+YYYMMDD+HHMMSS.#_<hostname>_<pid>`\n\ne.g. /mnt/landfill/incoming.telemetry.mozilla.org+20160807+182815.0_ip-172-31-43-254_6485\n\n    Syntax: moz_ingest_landfill_name string;\n    Default: --\n    Context: location\n"
},
{
  "name": "cloudops-python-docker",
  "files": {
    "/": [
      "Dockerfile",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "push-test",
  "files": {
    "/": [
      ".coveragerc",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "examples",
      "push_test",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tasks.json",
      "test-requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "Autopush Smoke Test Application\n===============================\n\nThis app is designed to exercise the Autopush server.\nTests can be contained as a JSON file.\n\ne.g.\n\n.. code:: json\n\n    [[\"hello\", {}],\n     [\"register\", {}],\n     [\"push\", {\"data\": \"mary had a little lamb\"}],\n     [\"ack\", {}],\n    ]\n\nThe following commands are available:\n\n*hello* - begin session with autopush server.\n\n``args``\n - `uaid` either None if new session or a previous `uaid`\n\n*register* - register a new Channel\n\n``args``\n- `channelID` uuid of the channel to create (None if new)\n\n- `key` - VAPID public key base64 string (optional)\n\n*push* - push to an endpoint\n\n``args``\n\n- `pushEndpoint` - the push endpoint (defaults to registered)\n\n- `data` - Data to push\n\n- `headers` - Dictionary of headers to include in push\n\n*ack* - Acknowledge the last received message\n\n``Args``\n\n- `channelID` - ChannelID of the last message (defaults to last recv'd)\n\n- `version` - message version information (defaults to last recv'd)\n\n*done* - Close down connection\n\n``Args``\n\n- None\n\n\n"
},
{
  "name": "fxa-squid",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# fxa-squid\nFirefox Accounts Squid proxy\n"
},
{
  "name": "megapush",
  "files": {
    "/": [
      ".gitignore",
      "Cargo.toml",
      "src"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "ci-testground",
  "files": {
    "/": [
      ".circleci",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "testground.sh"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# ci-testground\nfor testing ops ci workflows\nsimple bash script that emulates scripts run in a container\n\nsee ci workflows here: https://circleci.com/gh/mozilla-services/workflows/ci-testground\nsee containers pushed to dockerhub here: https://hub.docker.com/r/mozilla/ci-testground/tags/\n"
},
{
  "name": "lambda-test-helper",
  "files": {
    "/": [
      ".bumpversion.cfg",
      ".cookiecutterrc",
      ".coveragerc",
      ".editorconfig",
      ".gitignore",
      ".travis.yml",
      "AUTHORS.rst",
      "CHANGELOG.rst",
      "CONTRIBUTING.rst",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "appveyor.yml",
      "ci",
      "docs",
      "setup.cfg",
      "setup.py",
      "src",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "authors.rst",
      "changelog.rst",
      "conf.py",
      "contributing.rst",
      "index.rst",
      "installation.rst",
      "readme.rst",
      "reference",
      "requirements.txt",
      "spelling_wordlist.txt",
      "usage.rst"
    ]
  },
  "makefile": null,
  "readme": "========\nOverview\n========\n\n.. start-badges\n\n.. list-table::\n    :stub-columns: 1\n\n    * - docs\n      - |docs|\n    * - tests\n      - | |travis| |appveyor| |requires|\n        | |coveralls| |codecov|\n    * - package\n      - | |version| |downloads| |wheel| |supported-versions| |supported-implementations|\n        | |commits-since|\n\n.. |docs| image:: https://readthedocs.org/projects/lambda-test-helper/badge/?style=flat\n    :target: https://readthedocs.org/projects/lambda-test-helper\n    :alt: Documentation Status\n\n.. |travis| image:: https://travis-ci.org/hwine/lambda-test-helper.svg?branch=master\n    :alt: Travis-CI Build Status\n    :target: https://travis-ci.org/hwine/lambda-test-helper\n\n.. |appveyor| image:: https://ci.appveyor.com/api/projects/status/github/hwine/lambda-test-helper?branch=master&svg=true\n    :alt: AppVeyor Build Status\n    :target: https://ci.appveyor.com/project/hwine/lambda-test-helper\n\n.. |requires| image:: https://requires.io/github/hwine/lambda-test-helper/requirements.svg?branch=master\n    :alt: Requirements Status\n    :target: https://requires.io/github/hwine/lambda-test-helper/requirements/?branch=master\n\n.. |coveralls| image:: https://coveralls.io/repos/hwine/lambda-test-helper/badge.svg?branch=master&service=github\n    :alt: Coverage Status\n    :target: https://coveralls.io/r/hwine/lambda-test-helper\n\n.. |codecov| image:: https://codecov.io/github/hwine/lambda-test-helper/coverage.svg?branch=master\n    :alt: Coverage Status\n    :target: https://codecov.io/github/hwine/lambda-test-helper\n\n.. |version| image:: https://img.shields.io/pypi/v/lambda-test-helper.svg\n    :alt: PyPI Package latest release\n    :target: https://pypi.python.org/pypi/lambda-test-helper\n\n.. |commits-since| image:: https://img.shields.io/github/commits-since/hwine/lambda-test-helper/v0.1.0.svg\n    :alt: Commits since latest release\n    :target: https://github.com/hwine/lambda-test-helper/compare/v0.1.0...master\n\n.. |downloads| image:: https://img.shields.io/pypi/dm/lambda-test-helper.svg\n    :alt: PyPI Package monthly downloads\n    :target: https://pypi.python.org/pypi/lambda-test-helper\n\n.. |wheel| image:: https://img.shields.io/pypi/wheel/lambda-test-helper.svg\n    :alt: PyPI Wheel\n    :target: https://pypi.python.org/pypi/lambda-test-helper\n\n.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/lambda-test-helper.svg\n    :alt: Supported versions\n    :target: https://pypi.python.org/pypi/lambda-test-helper\n\n.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/lambda-test-helper.svg\n    :alt: Supported implementations\n    :target: https://pypi.python.org/pypi/lambda-test-helper\n\n\n.. end-badges\n\nSupport testing installed AWS lambda functions.\n\nInstallation\n============\n\n::\n\n    pip install lambda-test-helper\n\nDocumentation\n=============\n\nhttps://lambda-test-helper.readthedocs.io/\n\nDevelopment\n===========\n\nTo run the all tests run::\n\n    tox\n\nNote, to combine the coverage data from all the tox environments run:\n\n.. list-table::\n    :widths: 10 90\n    :stub-columns: 1\n\n    - - Windows\n      - ::\n\n            set PYTEST_ADDOPTS=--cov-append\n            tox\n\n    - - Other\n      - ::\n\n            PYTEST_ADDOPTS=--cov-append tox\n"
},
{
  "name": "elasticsearch-docker",
  "files": {
    "/": [
      "Dockerfile.1.4.5",
      "Dockerfile.2.4.6",
      "README.md",
      "config",
      "docker-entrypoint.sh"
    ]
  },
  "makefile": null,
  "readme": "# docker-elasticsearch\n\nThis repository is used to keep elasticsearch Dockerfiles used by mozilla-services projects.\n\nCurrently, it is used by socorro and elmo.\n\ndocker pull mozilla/socorro_elasticsearch:1.4.5\ndocker pull mozilla/elmo_elasticsearch:2.4.6\n"
},
{
  "name": "tokenserver-loadtests",
  "files": {
    "/": [
      "Makefile",
      "README.rst",
      "loadtest.py",
      "molotov.json",
      "requirements.txt"
    ]
  },
  "makefile": "HERE = $(shell pwd)\nPYTHON = python3\nVTENV_OPTS = --python $(PYTHON)\n\nBIN = $(HERE)/venv/bin\nVENV_PIP = $(BIN)/pip3\nVENV_PYTHON = $(BIN)/python\nINSTALL = $(VENV_PIP) install\n\n.PHONY: build install test\n\n\n$(VENV_PYTHON):\n\tvirtualenv $(VTENV_OPTS) venv\n\nbuild: $(VENV_PYTHON)\n\t$(INSTALL) -r requirements.txt\n\t$(INSTALL) --upgrade git+https://github.com/loads/molotov.git\n\ntest: build\n\t$(BIN)/molotov --config molotov.json $(TESTNAME)\n",
  "readme": "Token Server Load Test\n======================\n\n\n"
},
{
  "name": "ami-build-tests",
  "files": {
    "/": [
      "README.md",
      "localhost",
      "spec_helper.rb"
    ]
  },
  "makefile": null,
  "readme": "# ami-build-tests\nserverspec tests for the ami-build-scripts process\n"
},
{
  "name": "github-perm-auditor",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "config",
      "glide.lock",
      "glide.yaml",
      "main.go",
      "vendor"
    ]
  },
  "makefile": null,
  "readme": "## `github-perm-auditor` checks user account permissions on Github\n\n### Installation\n`go get go.mozilla.org/github-perm-auditor`\n\n### Running\n`github-perm-auditor` -> follow prompts\n\n### Configuration\n`github-perm-auditor` uses env for config.\nThe following env variables can be exported to configure `github-perm-auditor`\n\n`github-perm-auditor` will prompt you for these when necessary.\n\n- `PERM_DEBUG=false`\n- `PERM_GITHUBTOKEN=aGithubAccessToken`\n- `PERM_GITHUBUSERNAME=yourGithubUsername`\n- `PERM_GITHUBPASSWORD=yourGithubPassword`\n"
},
{
  "name": "ailoads-syncstorage",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "loadtest.py",
      "requirements.txt",
      "syncstorage.tpl"
    ]
  },
  "makefile": "OS := $(shell uname)\nHERE = $(shell pwd)\nPYTHON = python\n#PYTHON = $(shell python3.4)\n#PYTHON = '/usr/local/Cellar/python3/3.4.2_1/bin/python3'\nVTENV_OPTS = --python $(PYTHON)\n\nBIN = $(HERE)/venv/bin\nVENV_PIP = $(BIN)/pip\nVENV_PYTHON = $(BIN)/python\nINSTALL = $(VENV_PIP) install\n\nSYNCSTORAGE_SERVER_URL = https://token.stage.mozaws.net\n\n.PHONY: all check-os install-elcapitan install build\n.PHONY: setup configure \n.PHONY: docker-build docker-run docker-export\n.PHONY: test test-heavy refresh clean\n\nall: build setup configure \n\n\n# hack for OpenSSL problems on OS X El Captain: \n# https://github.com/phusion/passenger/issues/1630\ncheck-os:\nifeq ($(OS),Darwin)\n  ifneq ($(USER),root)\n    $(info \"clang now requires sudo, use: sudo make <target>.\")\n    $(info \"Aborting!\") && exit 1\n  endif  \n  BREW_PATH_OPENSSL=$(shell brew --prefix openssl)\nendif\n\ninstall-elcapitan: check-os \n\tenv LDFLAGS=\"-L$(BREW_PATH_OPENSSL)/lib\" \\\n\t    CFLAGS=\"-I$(BREW_PATH_OPENSSL)/include\" \\\n\t    $(INSTALL) cryptography \n\n$(VENV_PYTHON):\n\tvirtualenv $(VTENV_OPTS) venv\n\ninstall:\n\t$(INSTALL) -r requirements.txt\n\nbuild: $(VENV_PYTHON) install-elcapitan install\n\nclean-env: \n\t@rm -f loadtest.env\n\t\n\nsetup: clean-env\n\t$(BIN)/fxa-client -c --browserid --prefix syncstorage-server --audience $(SYNCSTORAGE_SERVER_URL) --out loadtest.env --duration 3600\n\n\nconfigure: build\n\t@bash syncstorage.tpl\n\n\ntest: build loadtest.env\n\tbash -c \"source loadtest.env && SYNCSTORAGE_SERVER_URL=$(SYNCSTORAGE_SERVER_URL):443 $(BIN)/ailoads -v -d 30\"\n\t$(BIN)/flake8 loadtest.py\n\ntest-heavy: build loadtest.env\n\tbash -c \"source loadtest.env && SYNCSTORAGE_SERVER_URL=$(SYNCSTORAGE_SERVER_URL):443 $(BIN)/ailoads -v -d 300 -u 10\"\n\n\ndocker-build:\n\tdocker build -t syncstorage/loadtest .\n\ndocker-run: loadtest.env\n\tbash -c \"source loadtest.env; docker run -e SYNCSTORAGE_DURATION=30 -e SYNCSTORAGE_NB_USERS=4 -e FXA_BROWSERID_ASSERTION=\\$${FXA_BROWSERID_ASSERTION} syncstorage/loadtest\"\n\ndocker-export:\n\tdocker save \"syncstorage/loadtest:latest\" | bzip2> syncstorage-latest.tar.bz2\n\n\nclean: refresh\n\t@rm -fr venv/ __pycache__/ loadtest.env\n\n",
  "readme": "ailoads-syncstorage\n====================\n\nConverting loadsv1 tests to loadsv2\nThese tests are under construction!\nDo not use.\n\nOverview\n------------\nailoads ([[https://github.com/loads/ailoads]]) is an auxiliarly tool for authoring loadtests to be used by the loads-broker tool (https://github.com/loads/loads-broker).\n\nPurpose\n-------------\nThe purpose of this project is to provide a loadtest 'skeleton' (or demo) that you can employ to write your own loadtests.\n\nSummary\n-------------\nailoads allows you to do the following tasks:\n\n1. Create a docker image to 'house' your python loadtests \n2. Generate a configuration (json) file to be used by the loads-broker tool which defines the type of scale you want your loadtests to run in (i.e. number of attack nodes, what AWS region, size of attack nodes, length of tests, etc.)\n\nFile Contents\n--------------\nAn ailoads repo should contain the following files:\n\n1. loadtest.py\n * define your python loadtests in this file\n2. loadtest.env\n * define any environment variables here\n3. Dockerfile\n * define your docker container (to house your loadtests) here\n4. \\<project name\\>.tpl\n * define your loads-broker config file in a tpl (template) format\n5. Makefile\n * use this to generate by products required by the loads-broker tool (i.e. docker image, json config file)\n\n\nRequirements\n--------------\n\n- Python 3.4\n\nExecution\n--------------\n\n\n## How to run the loadtest?\n\n### Example\n\n    $ make setup_random test\n\n\n### How to build the docker image?\n\n    make docker-build\n\n\n### How to run the docker image?\n\n    make docker-run\n\n\n### How to clean the repository?\n\n    make clean\n\n"
},
{
  "name": "go-stubattribution",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "This project has been moved to [mozilla-services/stub_attribution](https://github.com/mozilla-services/stub_attribution).\n"
},
{
  "name": "marionette-wrapper",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "__init__.py",
      "marionettewrapper",
      "setup.cfg",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "==================\nmarionette-wrapper\n==================\n.. image:: https://img.shields.io/pypi/l/marionette-wrapper.svg\n   :target: https://github.com/mozilla-services/marionette-wrapper/blob/master/LICENSE\n   :alt: License\n.. image:: https://travis-ci.org/mozilla-services/marionette-wrapper.svg?branch=master\n    :target: https://travis-ci.org/mozilla-services/marionette-wrapper\n.. image:: https://img.shields.io/github/issues-raw/mozilla-services/marionette-wrapper.svg\n   :target: https://github.com/mozilla-services/marionette-wrapper/issues\n   :alt: Issues\n.. image:: https://img.shields.io/requires/github/mozilla-services/marionette-wrapper.svg\n   :target: https://requires.io/github/mozilla-services/marionette-wrapper/requirements/?branch=master\n   :alt: Requirements\n\nOverview\n--------\nThis package contains a class that serves as a base class for page object classes. The base class wraps commonly used\n`Selenium WebDriver <http://docs.seleniumhq.org/docs/03_webdriver.jsp>`_ functionality into readable functions and gives a clean API for page objects to consume.\n\nInstallation\n------------\n\n:code:`python setup.py develop`\n\nUsage\n-----\n\nTo use the base class, you would create a page object class that inherits from the base class found in base.py. View the API Reference\nfor the exact calls that can be made in the API.\n\n.. code-block:: python\n\n    from marionette-wrapper.base import Base\n\n    class PageObject(Base):\n        #code\n\nRun Tests\n~~~~~~~~~\nThe sample test included with this package uses pytest to run the test. To invoke the test run the following commmand:\n\n:code:`py.test --capture=sys tests/test_marionette_wrapper_commands.py`\n\nAPI Examples For Common Functionality\n-------------------------------------\n\nThis reference shows the marionette-wrapper API call and it's equivalent Selenium counterpart.\n\nLaunch\n~~~~~~\n`This command is to launch the webpage only after checking if the URL is properly formed.`\n\n.. code-block:: python\n\n    if url is not None:\n        regex = re.compile(\n            r'^(?:http|ftp)s?://'\n            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)'\n            r'+(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?)|'\n            r'localhost|'\n            r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})'\n            r'(?::\\d+)?'\n            r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n        if regex.match(url):\n            self.marionette.navigate(url)\n        else:\n            raise ValueError('Url is malformed.')\n\nClick Element\n~~~~~~~~~~~~~\n`This command is to click on the given element.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.click_element(by, locator)\n\n    # Selenium WebDriver\n    self.marionette.find_element(by, locator).click()\n\nWait For Element Present\n~~~~~~~~~~~~~~~~~~~~~~~~\n`This command is to check that an element exists in the DOM. Does not necessarily have to be visible.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.wait_for_element_present(by, locator)\n\n    # Selenium WebDriver\n    Wait(self.marionette).until(expected.element_present(by, locator))\n\nWait For Element Not Present\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n`This command waits for an element to leave the DOM. If this does not happen before the timeout expires, an exception is thrown.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.wait_for_element_not_present(by, locator)\n\n    # Selenium WebDriver\n    Wait(self.marionette).until(expected.element_not_present(by, locator))\n\nWait For Element Displayed\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n`This command waits for an element display itself on the page. If this does not happen before the timeout expires, an exception is thrown.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.wait_for_element_displayed(by, locator)\n\n    # Selenium WebDriver\n    Wait(self.marionette).until(\n        expected.element_displayed(\n            Wait(self.marionette).until(\n                expected.element_present(by, locator))))\n\nWait For Element Not Displayed\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n`This command waits for an element to become invisible on the page. If this does not happen before the timeout expires, an exception is thrown.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.wait_for_element_not_displayed(by, locator)\n\n    # Selenium WebDriver\n    Wait(self.marionette).until(\n        expected.element_not_displayed(\n            Wait(self.marionette).until(\n                expected.element_present(by, locator))))\n\nWait For Element Enabled\n~~~~~~~~~~~~~~~~~~~~~~~~\n`This command waits for an element to become enabled. If this does not happen before the timeout expires, an exception is thrown.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.wait_for_element_enabled(by, locator)\n\n    # Selenium WebDriver\n    Wait(self.marionette).until(\n            expected.element_enabled(lambda m: m.find_element(by, locator)))\n\nWait For Element Not Enabled\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n`This command waits for an element to become disabled. If this does not happen before the timeout expires, an exception is thrown.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.wait_for_element_not_enabled(by, locator)\n\n    # Selenium WebDriver\n    Wait(self.marionette).until(\n            expected.element_not_enabled(lambda m: m.find_element(by, locator)))\n\nIs Element Present\n~~~~~~~~~~~~~~~~~~\n`This command is to return a boolean as to whether an element exists in the DOM. Does not have to be visible.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.is_element_present(by, locator)\n\n    # Selenium WebDriver\n    try:\n        self.marionette.find_element(by, locator)\n        return True\n    except NoSuchElementException:\n        return False\n\nIs Element Displayed\n~~~~~~~~~~~~~~~~~~~~\n`This command is to return a boolean as to whether an element is present and visible on the page.`\n\n.. code-block:: python\n\n    # marionette-wrapper\n    self.is_element_displayed(by, locator)\n\n    # Selenium WebDriver\n    try:\n        return self.marionette.find_element(by, locator).is_displayed()\n    except NoSuchElementException:\n        return False\n"
},
{
  "name": "site-recipes-admin",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "config.json",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "# site-recipes-kinto\n\nThis is an experimental admin console for managing Firefox [site recipes](https://dxr.mozilla.org/mozilla-central/source/toolkit/components/passwordmgr/content/recipes.json) data using [Kinto](http://kinto-storage.org/).\n\n## Demo\n\n![](http://i.imgur.com/RfkmVuT.png)\n\nA live demo is hosted [on gh-pages](http://mozilla-services.github.io/site-recipes-admin/).\n\nSettings should be configured as following:\n\n![](http://i.imgur.com/MvfsnVl.png)\n\nThen click on the *site-recipes* link in the sidebar, and press the *Synchronize* button.\n\nPublished JSON data are retrievable using [this url](https://test:test@kinto-ota.dev.mozaws.net/v1/buckets/site-recipes/collections/site-recipes/records).\n\n## Install\n\n```\n$ git clone https://github.com/mozilla-services/site-recipes-admin.git\n$ npm install\n```\n\n## Run localy\n\n```\n$ npm start\n```\n\nThen head to [localhost:3000](http://localhost:3000/).\n\n## License\n\nApache-2.0\n"
},
{
  "name": "go-mozlog",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "mozlog.go",
      "mozlog_test.go"
    ]
  },
  "makefile": null,
  "readme": "# mozlog [![GoDoc](https://godoc.org/go.mozilla.org/mozlog?status.svg)](https://godoc.org/go.mozilla.org/mozlog) [![Build Status](https://travis-ci.org/mozilla-services/go-mozlog.svg?branch=master)](https://travis-ci.org/mozilla-services/go-mozlog)\nA logging library which conforms to [Mozilla's logging standard](https://wiki.mozilla.org/Firefox/Services/Logging).\n\n## Example Usage\n```\nimport \"go.mozilla.org/mozlog\"\n\nfunc init() {\n    mozlog.Logger.LoggerName = \"ApplicationName\"\n}\n```\n"
},
{
  "name": "cloud-ops",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# cloud-ops\nPlaceholder for the CloudOps's WaffleBoard - https://waffle.io/mozilla-services/cloud-ops\n"
},
{
  "name": "go-syncstorage",
  "files": {
    "/": [
      ".editorconfig",
      ".gitignore",
      "Dockerfile",
      "LICENSE.txt",
      "README.md",
      "circle.yml",
      "config",
      "main",
      "server.go",
      "syncstorage",
      "token",
      "vendor.yml",
      "vendor",
      "version.json",
      "web"
    ]
  },
  "makefile": null,
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla-services/go-syncstorage.svg?style=svg)](https://circleci.com/gh/mozilla-services/go-syncstorage)\n\n# Mozilla Sync 1.5 Storage Server in Go\n\nGo-syncstorage is the next generation sync storage server. It was built to solve the data degradation problem with the python+mysql implementation. Logical separation of data is now physical separation of data.  In go-syncstorage each user gets their own sqlite database. Many indexes were harmed in the making of this product.\n\n## Installing and Running it\n\nThe server is distributed as a Docker container. Latest builds and releases can be found on [Dockerhub](https://hub.docker.com/r/mozilla/go-syncstorage).\n\nRunning the server is easy:\n\n```bash\n$ docker pull mozilla/go-syncstorage:latest\n$ docker run -it \\\n  -e \"PORT=8000\" \\                           [1]\n  -e \"SECRETS=secret0,secret1,secret2\" \\     [2]\n  -e \"DATA_DIR=/data\" \\                      [3]\n  -v \"/host/data/path:/data\" \\               [4]\n  mozilla/go-syncstorage\n```\n\nOnly three configurations are required: `PORT`, `SECRETS` and `DATA_DIR`.\n\n1. `PORT` - where to listen for HTTP requests\n2. `SECRETS` - CSV of secrets preshared with the [token service](https://github.com/mozilla-services/tokenserver/)\n3. `DATA_DIR` - where to save files (relative to inside the container)\n4. A volume mount so data is saved on the docker host machine\n\n## More Configuration\n\nThe server has a few knobs that can be tweaked.\n\n| Env. Var | Info |\n|---|---|\n| `HOST` | Address to listen on. Defaults to `0.0.0.0`. |\n| `PORT` | Port to listen on |\n| `DATA_DIR` | Where to save DB files. Use an absolute path. `:memory:` is valid and saves databases in RAM but recommended only for testing. |\n| `SECRETS` | Comma separated list of shared secrets. Secrets are tried in order and allows for secret rotation without downtime. |\n| `LOG_LEVEL`| Log verbosity, allowed: `fatal`,`error`,`warn`,`debug`,`info`. Default `info`. |\n| `LOG_MOZLOG` | Can be `true` or `false`. Outputs logs in [mozlog](https://github.com/mozilla-services/Dockerflow/blob/master/docs/mozlog.md) format. Default `false`.|\n| `LOG_DISABLE_HTTP` | Can be `true` or `false`. Disables logging of HTTP requests. Default `false`. |\n| `LOG_ONLY_HTTP_ERRORS` | Can be `true` or `false`. Logs only when `errno != 0` to reduce noise. Default `false`. |\n| `HOSTNAME` | Set a hostname value for mozlog output |\n| `LIMIT_MAX_REQUESTS_BYTES` | The maximum size in bytes of the overall HTTP request body that will be accepted by the server. |\n| `LIMIT_MAX_POST_BYTES` |  Maximum size of a POST request. Default: 2097152 (2MB). |\n| `LIMIT_MAX_POST_RECORDS` |  Maximum number of BSOs per POST request. Default 100. |\n| `LIMIT_MAX_TOTAL_BYTES` |  Maximum total size of a POST batch job. Default: 26,214,400 (20MB). |\n| `LIMIT_MAX_TOTAL_RECORDS` | Maximum total BSOs in a POST batch job. Default 1000. |\n| `LIMIT_MAX_BATCH_TTL` | Maximum TTL for a batch to remain uncommitted in seconds. Default 7200 (2 hours). |\n| `LIMIT_MAX_RECORD_PAYLOAD_BYTES` | Maximum bytes for a BSO payload. Default 2MB. | \n| `INFO_CACHE_SIZE` | Cache size in MB for `<uid>/info/collections` and `<uid>/info/configuration`. Default 0 (disabled) |\n| `HAWK_TIMESTAMP_MAX_SKEW` | Sets number of seconds hawk timestamps can differ from the server. Default 60. |\n\n## Advanced Configuration\n\n| Env. Var | Info |\n|---|---|\n| `POOL_NUM` | Number of DB pools. Defaults to number of CPUs.  |\n| `POOL_SIZE` | Number of open DB files per pool. Defaults to `25`.  |\n| `POOL_VACUUM_KB` | Threshold of free space in kilobytes to trigger a database vacuum. Defaults to `0` (disabled). |\n| `POOL_PURGE_MIN_HOURS\t` | Minimum hours before purging BSOs, Batches, etc for a user. Defaults to `168` (1 week) |\n| `POOL_PURGE_MAX_HOURS\t` | Max hours before purging. Defaults to `336` (2 weeks). |\n\ngo-syncstorage limits the number of open SQLite database files to keep memory usage constant. This allows a small server to handle thousands of users for a small performance hit.\n\nMultiplying `POOL_NUM x POOL_SIZE` gives the maximum number of open files. The product should to large enough so pools are not starved and have to clean up too often. A sign things are too small is when `sql: database is closed` errors appear in the logs.\n\nA low level lock is used in each pool when opening and closing files. Having a larger `POOL_NUM` decreases lock contention.\n\nWhen a pool reaches `POOL_SIZE` number of open files it will close the least recently used database. Having a larger `POOL_SIZE` reduces open/close disk IO. It also increases memory usage.\n\nTweaking these values from default won't provide significant performance gains in production. However, a `POOL_NUM=1` and `POOL_SIZE=1` is useful for testing the overhead of opening and closing databases files.\n\nThe `POOL_PURGE_MIN_HOURS` and `POOL_PURGE_MAX_HOURS` define a time range to trigger a purge job for a user. The default range is between 168 and 336 hours. This means a user will have a purge job run only once every one to two weeks. A large range spreads evens out IO load.\n\nThe `POOL_VACUUM_KB` sets the threshold before a vacuum is run. Purging of batches and BSOs free up database pages but not disk space. A vacuum will rewrite the database, defragment it and free up disk space. Depending on the number of records it can take seconds to vacuum a database.\n\n### Sqlite3 Tweaks\n\n| Env. Var | Info |\n|---|---|\n| `SQLITE3_CACHE_SIZE` | Sets sqlite's internal cache size for each open DB. Busy servers open/close the db files often so a smaller cache size may be more efficient. Follows the [PRAGMA cache_size](https://www.sqlite.org/pragma.html#pragma_cache_size) rules. Positive integers are number of pages to cache, negative numbers are KB of RAM to use for cache. Default 0 (no cache)|\n\n\n## Data Storage\n\nWhen deploying choose the EXT4 filesystem. EXT4 is an extent based filesystem and may help improve performance for magnetic storage media.\n\ngo-syncstorage gives each user gets their own sqlite database. On a production server that enough files to be a real burden for a human when troubleshooting. Thus, files are created into a directory structure like this:\n\n```\n/data-dir/\n   00/\n   01/\n   34/\n     21/\n       100001234.db\n   ...\n   99/\n```\n\n* Two levels of subdirectories, each with 100 subdirectories for total of 10,000 sub-directories.\n* The user, `100001234`, is located at `34/21/100001234.db`. The path starts at the reverse of their id. Their id is used for the actual database name.\n* Using the reverse order helps evenly balance the number of files per directory.\n\nUsing this scheme, one million users will only have 10,000 files per directory. This is a relatively low number that CLI tools like `ls` will have no trouble with. Always optimize for the proper care and feed of your sysadmins.\n\n\n## Other Releases\n\nA linux binary is also available as build artifacts from [Circle CI](https://circleci.com/gh/mozilla-services/go-syncstorage).\n\n# License\n\nSee [LICENSE](LICENSE.txt).\n"
},
{
  "name": "service-guidelines",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# isg\n"
},
{
  "name": "ailoads-syncto",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "loadtest.py",
      "syncto.tpl"
    ]
  },
  "makefile": "HERE = $(shell pwd)\nBIN = $(HERE)/venv/bin\nPYTHON = $(BIN)/python3.4\n\nINSTALL = $(BIN)/pip install\n\nSYNCTO_SERVER_URL = https://syncto.stage.mozaws.net:443\nSYNCTO_EXISTING_EMAIL =\n\n.PHONY: all test build\n\nall: build test\n\n$(PYTHON):\n\t$(shell basename $(PYTHON)) -m venv $(VTENV_OPTS) venv\n\t$(BIN)/pip install requests requests_hawk flake8\n\t$(BIN)/pip install https://github.com/mozilla/PyFxA/archive/loadtest-tools.zip\n\t$(BIN)/pip install https://github.com/tarekziade/ailoads/archive/master.zip\nbuild: $(PYTHON)\n\nloadtest.env:\n\t$(BIN)/fxa-client -c --browserid --audience https://token.stage.mozaws.net/ --prefix syncto --out loadtest.env\n\nrefresh:\n\t@rm -f loadtest.env\n\nsetup_random: refresh loadtest.env\n\nsetup_existing:\n\t$(BIN)/fxa-client --browserid --auth \"$(SYNCTO_EXISTING_EMAIL)\" --env production --out loadtest.env\n\n\ntest: build loadtest.env\n\tbash -c \"source loadtest.env && SYNCTO_SERVER_URL=$(SYNCTO_SERVER_URL) $(BIN)/ailoads -v -d 30\"\n\t$(BIN)/flake8 loadtest.py\n\ntest-heavy: build loadtest.env\n\tbash -c \"source loadtest.env && SYNCTO_SERVER_URL=$(SYNCTO_SERVER_URL) $(BIN)/ailoads -v -d 300 -u 10\"\n\nclean: refresh\n\trm -fr venv/ __pycache__/\n\ndocker-build:\n\tdocker build -t syncto/loadtest .\n\ndocker-run: loadtest.env\n\tbash -c \"source loadtest.env; docker run -e SYNCTO_DURATION=600 -e SYNCTO_NB_USERS=10 -e SYNCTO_SERVER_URL=$(SYNCTO_SERVER_URL) -e FXA_BROWSERID_ASSERTION=\\$${FXA_BROWSERID_ASSERTION} -e FXA_CLIENT_STATE=\\$${FXA_CLIENT_STATE} syncto/loadtest\"\n\nconfigure: build loadtest.env\n\t@bash syncto.tpl\n\ndocker-export:\n\tdocker save \"syncto/loadtest:latest\" | bzip2> syncto-latest.tar.bz2\n",
  "readme": "# ailoads-syncto\n\nSyncto loadtest based on ailoads\n\n## Requirements\n\n- Python 3.4\n\n\n## How to run the loadtest?\n\n### For stage\n\n    make setup_random test\n\nor for a longer one:\n\n    make setup_random test-heavy\n\n### For production\n\n    make setup_existing -e SYNCTO_EXISTING_EMAIL=test-account-email@example.com\n    make test -e SYNCTO_SERVER_URL=https://syncto.dev.mozaws.net:443\n\nor all at once:\n\n    make setup_existing test -e \\\n        SYNCTO_EXISTING_EMAIL=test-account-email@example.com \\\n        SYNCTO_SERVER_URL=https://syncto.dev.mozaws.net:443\n\n\n## How to build the docker image?\n\n    make docker-build\n\n\n## How to run the docker image?\n\n    make docker-run\n\n\n## How to clean the repository?\n\n    make clean\n"
},
{
  "name": "product-delivery-tools",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "bucketlister",
      "bucketmap.go",
      "circle.yml",
      "config.go",
      "glide.lock",
      "glide.yaml",
      "metrics",
      "mozlog",
      "post_upload",
      "vendor",
      "version.go",
      "version.json"
    ]
  },
  "makefile": null,
  "readme": "# product-delivery-tools [![Build Status](https://travis-ci.org/mozilla-services/product-delivery-tools.svg?branch=master)](https://travis-ci.org/mozilla-services/product-delivery-tools) [![Docs](https://img.shields.io/badge/godoc-reference-blue.svg)](http://godoc.org/github.com/mozilla-services/product-delivery-tools)\nTools related to product delivery\n\n## Tools\n* [post_upload](https://github.com/mozilla-services/product-delivery-tools/tree/master/post_upload)\n* [bucketlister](https://github.com/mozilla-services/product-delivery-tools/tree/master/delivery_dir_ls)\n"
},
{
  "name": "mvn-repo",
  "files": {
    "/": [
      "index.html",
      "org"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "write-good-code",
  "files": {
    "/": [
      "README.md",
      "outline.md"
    ]
  },
  "makefile": null,
  "readme": "# write-good-code\n\nA little project by [Shane Tomlinson](mailto:stomlinson@mozilla.com), [Nick Chapman](mailto:nchapman@mozilla.com), and [Peter deHaan](mailto:pdehaan@mozilla.com) outlining best practices used in Cloud Services to write good code.\n\n"
},
{
  "name": "delayed",
  "files": {
    "/": [
      "LICENSE",
      "Makefile",
      "README.md",
      "index.js",
      "package.json"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nNODE_LOCAL_BIN=./node_modules/.bin\n\ninstall:\n\t@npm install\n\n.PHONY: runserver\nrunserver:\n\t@env PORT=5000 node index.js\n",
  "readme": "Delayed Server\n==============\n\nThis repository contains a really simple node.js server which answers all\nrequests sent to it and adds a random delay to them.\n\nThis had been created to fake an interaction between two different services, do\nload testing.\n\nHow to install?\n---------------\n\nClone the delayed server and install it:\n\n    git clone https://github.com/mozilla/delayed.git\n    cd delayed && make install\n\nHow to run it?\n--------------\n\n    DELAYS=100:500 make runserver\n\n`DELAY` is an environment variable which specifies the minimum and max delay.\nThe server will add a random delay on each request.\n\nWhere to report bugs?\n---------------------\n\nFor this project, you should use github issues to report any bugs.\n\nLicense\n-------\n\nThe Loop server code is released under the terms of the\n[Mozilla Public License v2.0](http://www.mozilla.org/MPL/2.0/). See the\n`LICENSE` file at the root of the repository.\n"
},
{
  "name": "msisdn-gateway",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc",
      ".gitignore",
      ".travis.yml",
      "API.md",
      "CHANGELOG",
      "CONTRIBUTING.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "circus",
      "config",
      "loadtests",
      "locale",
      "msisdn-gateway",
      "package.json",
      "test",
      "tools"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nNODE_LOCAL_BIN = ./node_modules/.bin\nFAKE_DYNAMO = $$(which ~/.gem/ruby/*/bin/fake_dynamo fake_dynamo)\n\nTMPDIR ?= /tmp\n\n.PHONY: test\ntest: lint cover-mocha spaceleft\n\ninstall:\n\t@if [ -z \"$(FAKE_DYNAMO)\" ]; then echo \"Installing fake_dynamo...\"; gem install --user-install fake_dynamo; fi\n\t@npm install\n\n.PHONY: lint\nlint: eslint\n\nclean:\n\trm -rf .venv node_modules coverage lib-cov html-report\n\n.PHONY: cover-mocha\ncover-mocha:\n\t@$(FAKE_DYNAMO) --db $(TMPDIR)/fake_dynamo.db --pid $(TMPDIR)/fake_dynamo.pid -D > /dev/null\n\t@NODE_ENV=test $(NODE_LOCAL_BIN)/istanbul cover \\\n\t\t$(NODE_LOCAL_BIN)/_mocha -- --reporter spec test/* -t 5000; \\\n\t\tEXIT_CODE=$$?; kill `cat $(TMPDIR)/fake_dynamo.pid`; \\\n\t\trm -f $(TMPDIR)/fake_dynamo.db $(TMPDIR)/fake_dynamo.pid; \\\n\t\tsleep 2; exit $$EXIT_CODE\n\t@echo aim your browser at coverage/lcov-report/index.html for details\n\n.PHONY: eslint\neslint:\n\t@npm run lint\n\n.PHONY: mocha\nmocha:\n\t@$(FAKE_DYNAMO) --db $(TMPDIR)/fake_dynamo.db --pid $(TMPDIR)/fake_dynamo.pid -D > /dev/null\n\t@NODE_ENV=test ./node_modules/mocha/bin/mocha test/* --reporter spec -t 5000; \\\n\t\tEXIT_CODE=$$?; kill `cat $(TMPDIR)/fake_dynamo.pid`; \\\n\t\trm -f $(TMPDIR)/fake_dynamo.db $(TMPDIR)/fake_dynamo.pid; \\\n\t\tsleep 2; exit $$EXIT_CODE\n\n.PHONY: spaceleft\nspaceleft:\n\t@if which grin 2>&1 >/dev/null; \\\n\tthen \\\n\t\tgrin \" $$\" msisdn-gateway/ test/ config/; \\\n\tfi\n\n.PHONY: runserver\nrunserver:\n\t@env NODE_ENV=${NODE_ENV} PORT=5000 \\\n\t\tnode msisdn-gateway/index.js\n\n.PHONY: messages\nmessages:\n\t./node_modules/i18n-abide/node_modules/.bin/jsxgettext \\\n\t\t--join-existing \\\n\t\t-L javascript \\\n\t\t--output-dir=./locale/templates/LC_MESSAGES \\\n\t\t--from-code=utf-8 \\\n\t\t--output=messages.pot msisdn-gateway/index.js\n\tfor l in `ls ./locale | grep -v templates | grep -v README.md`; do \\\n\t\tmkdir -p locale/$$l/LC_MESSAGES/; \\\n\t\tmsginit --input=./locale/templates/LC_MESSAGES/messages.pot \\\n\t\t\t--output-file=./locale/$$l/LC_MESSAGES/messages.po \\\n\t\t\t-l $$l; \\\n\tdone\n\n.PHONY: compile-messages\ncompile-messages:\n\tmkdir -p app/i18n\n\t$(NODE_LOCAL_BIN)/compile-json locale app/i18n\n\n.PHONY: update-l10n\nupdate-l10n: update-l10n-git compile-messages\n\n.PHONY: update-l10n-git\nupdate-l10n-git:\n\t@if [ '$(NOVERIFY)' = '' ]; then \\\n\t\techo \"WARNING all update made in locale/* will be overridden by this command. CTRL-C to quit\"; \\\n\t\tread toot; \\\n\tfi\n\t@if [ ! -d $(TMPDIR)/msisdn-gateway-l10n ]; then \\\n\t\techo \"Cloning https://github.com/mozilla-services/msisdn-gateway-l10n.git\"; \\\n\t\tgit clone https://github.com/mozilla-services/msisdn-gateway-l10n.git $(TMPDIR)/msisdn-gateway-l10n; \\\n\telse \\\n\t\techo \"Updating https://github.com/mozilla-services/msisdn-gateway-l10n.git\"; \\\n\t\tcd $(TMPDIR)/msisdn-gateway-l10n; \\\n\t\t\tgit checkout master; \\\n\t\t\tgit pull origin master; \\\n\tfi\n\t@mv ./locale/README.md /tmp/README.save\n\t@echo \"Sync locales\"\n\tcp -fr $(TMPDIR)/msisdn-gateway-l10n/locale/ .\n\t@mv /tmp/README.save locale/README.md\n\n\n.PHONY: circus\ncircus:\n\tcircusd circus/msisdn-gateway.ini\n",
  "readme": "MSISDN Gateway\n==============\n\n[![Build Status](https://travis-ci.org/mozilla-services/msisdn-gateway.svg?branch=master)](https://travis-ci.org/mozilla-services/msisdn-gateway)\n\nThis is a proof of concept of an MSISDN Gateway server that takes a phone number and\nregister it using an SMS validation mechanism.\n\n[API docs](API.md)\n\n[Mailing list](https://mail.mozilla.org/listinfo/loop-services-dev)\n\n[Dummy client](https://github.com/ferjm/msisdn-verifier-client)\n\n\nRegistration process flow\n-------------------------\n\n  1. The client sends a request to ``/register``.\n\n  -- The server chooses the verification process based on MSISDN, MCC and MNC\n     codes and returns a session token and a verify endpoint.\n\n  2. The client sends a request to ``/sms/verify`` (the verify endpoint).\n\n  -- The server sends a SMS text message containing a pin code, and returns the\n     number that was used to send it (the phone number of the server, useful\n     for silent SMS catch)\n\n  3. The client sends a request to verify the pin code (to ``/sms/verify_code``)\n     with the session token and the pin code and gets back a BrowserID\n     certificate.\n  4. If needed, the client can also ask for a new pin code using the\n     ``/sms/resend_code`` URL and its sessionToken.\n  5. Finally the client can unregister itself using the ``/unregister`` URL\n     and its sessionToken.\n\n\nWhat is the needed stack?\n-------------------------\n\n<img src=\"http://www.gliffy.com/go/publish/image/5799498/L.png\" />\n\n\nHow to install?\n---------------\n\nYou will need to have redis-server installed:\n\n### Linux\n\n    apt-get install redis-server\n\n### OS X\n\nAssuming you have brew installed, use it to install redis:\n\n    brew install redis\n\nIf you need to restart it (after configuration update):\n\n    brew services restart redis\n\n### All Platforms\n\nThen clone the loop server and install its dependencies:\n\n    git clone https://github.com/mozilla-services/msisdn-gateway.git\n    cd msisdn-gateway && make install\n\n\nHow to run it?\n--------------\n\nYou can create your configuration file in `config/{NODE_ENV}.json`\n\nYou need to generate the BrowserId keys by running `./bin/generate-keypair` and\nadd them to your configuration file.\n\n`development` is the default environment.\n\n    make runserver\n\nis equivalent to:\n\n    NODE_ENV=development make runserver\n\n\nHow to run the tests?\n---------------------\n\nTo run DynamoDB tests you will need to install and run [fake_dynamo](https://github.com/ananthakumaran/fake_dynamo) or [ddbmock](https://pypi.python.org/pypi/ddbmock)\n\nFakeDynamo needs ruby, ruby-dev(el) and rubygems.\n\n    make test\n\n\nHow to update translation?\n--------------------------\n\n    make update-l10n\n\nAnd for automated scripts:\n\n    make update-l10n NOVERIFY=yes\n\n\nEstimate Redis Memory Usage\n---------------------------\n\n    usage = nbUsers * 1216 + 600000 (bytes)\n\n - For 3.5M users 4 GB\n - For 10M users 12 GB\n\nThe biggest AWS Elasticache Redis virtual machine is 68GB large so if\nwe want to handle more that 60M users we will probably want to do some\nsharding to have one redis for MSISDN validation and another one for\nhawkSession management.\n\n - 600000 bytes is the size of an empty redis-server.\n - 1216 bytes is the fixed size of a given user including:\n    - It's hawkSessionToken\n    - It's code validation related data\n    - It's MSISDN related data\n\nThe /unregister endpoint drops everything about a user.\n\n\nWhere to report bugs?\n---------------------\n\nYou should report bugs/issues or feature requests via [Github Issues](https://github.com/mozilla-services/msisdn-gateway/issues)\n\n\nLicense\n-------\n\nThe MSISDN Gateway code is released under the terms of the\n[Mozilla Public License v2.0](http://www.mozilla.org/MPL/2.0/). See the\n`LICENSE` file at the root of the repository.\n"
},
{
  "name": "messaging",
  "files": {
    "/": [
      ".travis.yml",
      "README.rst",
      "fxchat.ini",
      "fxchat",
      "fxchat_test.py",
      "requirements.txt",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "FxChat-Server\n#############\n\n.. image:: https://secure.travis-ci.org/mozilla-services/messaging.png?branch=master\n   :target: http://travis-ci.org/#!/mozilla-services/messaging\n   :alt: Travis-ci: continuous integration status.\n\nDefinition of the service is still in flux, read the wiki here:\nhttps://wiki.mozilla.org/CloudServices/FxChat\n\nTentative flow:\n===============\n\n(We assume that a keypair for each device had been generated and that the\npublic keypair had been uploaded to the server)\n\nSending messages\n----------------\n\nOverview: Each message is encrypted with an AES key and this key is sent\nencrypted to the recipients.\n\n- 1. For each message M, generate an AES Key;\n- 2. For each recipient device, do the following:\n\n    - a. ``temp_priv_key, temp_pub_key = new Keypair()``\n    - b. ``shared_secret = Diffie(recipent_device_pub_key, temp_priv_key)``\n    - c. derive ``encryption key`` and ``mac key`` from the ``shared_secret``\n\n         ``encryption_key = hash(shared_secret)``\n\n         ``mac_key = hash(encryption_key)``\n    - d. encrypted_message = encrypt(M, aes_key)\n    - e. \n        ::\n\n            recipients = [{encrypted_aes_key: encrypt(aes_key, encryption_key),\n                          msg_sig: sign(M, mac_key),\n                          msg_pub_key: temp_pub_key}]\n\n\nReceiving messages\n------------------\n\nWe received the following info:\n  * encrypted_message;\n  * encrypted_aes_key;\n  * msg_sig;\n  * msg_pub_key.\n\nWe already know our private key (recipient_private_key).\n\nGoal is to get back the AES key to decrypt the message.\n\n1. Get back the keying material.\n\n   shared_secret = Diffie(recipient_private_key, msg_pub_key)\n   encryption_key = hash(shared_secret)\n   signing_key = hash(encryption_key)\n\n2. Decrypt the AES key.\n\n   aes_key = decrypt(encrypted_aes_key, encryption_key)\n\n3. Get the message.\n\n   message = decrypt(AES, encrypted_message)\n\n4. Verify the message is valid.\n\n   sig = sign(message, signing_key)\n   sig === msg_sig\n\n\nFAQ\n===\n\nWhat do we want to store on the server for the user?\n----------------------------------------------------\n\nThe server is storing:\n\n- public keys for all the device of the system;\n- encrypted messages in a queue until they're actually retrieved;\n\nDo we want to store messages on the server, and for how long?\n-------------------------------------------------------------\n\nWe are storing all the messages on a queue on the server for some time, and\nthen we store them for some more time.\n\nWhat is the maximum delay before a message got lost?\n----------------------------------------------------\n\nWe keep messages from some time, until \n\nAre we providing forward security?\n----------------------------------\n\nWe currently aren't. If an attacker breaks one device private key, she might be\nable to read all the messages stored on the device and / or stored on the\nserver.\n\nPlans are being discussed to address that, using a ratchet and defining a new\nprivate / public key, but we're not there yet.\n\nAre we providing deniable auth?\n-------------------------------\n\nWhen a client receives a message, it discloses the signature key when ACKing\nthe message, so that anyone could have signed this message, not just the real\nperson. This is very similar to how OTR works.\n"
},
{
  "name": "statsd-with-stackdriver",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "default-config.js",
      "package.json",
      "send-fake-stats.js",
      "start-server.js"
    ]
  },
  "makefile": null,
  "readme": "statsd-with-stackdriver\n=======================\n\nThis is statsd with the stackdriver backend all in one easy to use\npackage. \n\nIt is designed to be installed on a server and listen on localhost \nfor statsd messages. \n\nRunning it\n----------\n\n* Copy `default-config.js` to `config.js` and tweak it to your environment\n* Run it with `npm start` or `node start-server config.js`\n\nLicense\n-------\n\nMPL 2.0\n"
},
{
  "name": "presence-go",
  "files": {
    "/": [
      "README.rst",
      "cmd",
      "doc",
      "src"
    ]
  },
  "makefile": null,
  "readme": "========\nPresence\n========\n\nThis is the Presence server. If you don't know what's *Presence*, present\nyour browser to https://presence.services.mozilla.com and read the slides.\n\n\n- **mcf** front end that interacts with users.\n- **postmaster** backend that interacts with 3rd party app servers.\n- **zombiekiller** script that cleans up users dead bodies.\n\n\n"
},
{
  "name": "presence-website",
  "files": {
    "/": [
      ".gitmodules",
      "Makefile",
      "README.rst",
      "favicon.ico",
      "index.html",
      "nginx.rst",
      "presence-fxapp",
      "static"
    ]
  },
  "makefile": "clean:\n\trm -f presence.zip\n\nbuild: clean\n\tgit pull\n\tcd presence-fxapp; git checkout master; git pull\n\tcd presence-fxapp/src; zip -r ../presence.zip *\n",
  "readme": "presence-website\n================\n\nMozilla Presence Website displayed at http://presence.services.mozilla.com\n\nTo update the server and the two Firefox OS apps, do the following::\n\n\tssh -i your.amazon.key.pem ubuntu@presence.services.mozilla.com\n\tcd presence-root\n\tmake build\n\nTo update the presence server app::\n\n\tssh -i your.amazon.key.pem ubuntu@presence.services.mozilla.com\n\tcd presence\n\tgit pull\n\tbin/circusctl restart presence\n\tcd ../presence-tribe-server\n\tbin/circusctl restart tribe\n\n\nTo update the presence tribe app::\n\n\tssh -i your.amazon.key.pem ubuntu@presence.services.mozilla.com\n\tcd presence-tribe-server\n\tgit pull\n\tbin/circusctl restart tribe\n\n\n"
},
{
  "name": "presence-tribe-server",
  "files": {
    "/": [
      "Makefile",
      "README.rst",
      "setup.py",
      "tribe.ini",
      "tribeserver"
    ]
  },
  "makefile": "build:\n\tvirtualenv --no-site-packages .\n\tbin/python setup.py develop\n\tbin/pip install circus\n\n",
  "readme": "Tribe Server\n------------\n\nProxies calls between the Presence Service and the Tribe Firefox OS App\n"
},
{
  "name": "presence-fxapp",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.rst",
      "index.html",
      "package.manifest",
      "src"
    ]
  },
  "makefile": "clean:\n\trm -f tribe.zip\n\nbuild: clean\n\tcd src; zip -r ../tribe.zip *\n",
  "readme": "presence-fxap\n--------------\n\nFirefox OS demo app for Presence.\n\nProvides two features: a presence daemon and a contacts app.\n\nPresence Daemon\n===============\n\nUsed to send and receive presence statuses from the Presence server.\n\n\nContacts app\n============\n\nMaintains a list of contact with their online\nstatus and let you send them instant notifications.\n\n\nFeatures:\n\n- add & remove friends in a contact list\n- see your contact online status\n- send an instant message to your contacts\n\n\n"
},
{
  "name": "presence-graffles",
  "files": {
    "/": [
      "ChannelService.graffle",
      "Overview.graffle",
      "TechnicalUserFlow.graffle",
      "UserAuthFlow.graffle",
      "presentation"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "presence-chatroom",
  "files": {
    "/": [
      "README.md",
      "boomchat",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "Boom Chat\n=========\n\nA chat room to demo presence.\n"
},
{
  "name": "ec2-instanceid-resolver",
  "files": {
    "/": [
      ".gitignore",
      "config.json.dist",
      "dns-server.js",
      "libs",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "repoze.who.plugins.hawkauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "CONTRIBUTORS.txt",
      "MANIFEST.in",
      "README.rst",
      "repoze",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "===========================\nrepoze.who.plugins.hawkauth\n===========================\n\nThis is a repoze.who plugin for Hawk Access Authentication:\n\n    https://npmjs.org/package/hawk\n\nTo access resources using Hawk Access Authentication, the client must have\nobtained a set of Hawk credentials including an id and secret key.  They use\nthese credentials to make signed requests to the server.\n\nWhen accessing a protected resource, the server will generate a 401 challenge\nresponse with the scheme \"Hawk\" as follows::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n\n    < HTTP/1.1 401 Unauthorized\n    < WWW-Authenticate: Hawk\n\nThe client will use their Hawk credentials to build a request signature and\ninclude it in the Authorization header like so::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n    > Authorization: Hawk id=\"h480djs93hd8\",\n    >                     ts=\"1336363200\",\n    >                     nonce=\"dj83hs9s\",\n    >                     mac=\"bhCQXTVyfj5cmA9uKkPFx1zeOXM=\"\n\n    < HTTP/1.1 200 OK\n    < Content-Type: text/plain\n    <\n    < For your eyes only:  secret data!\n\n\nThis plugin uses the tokenlib library for verifying Hawk credentials:\n\n    https://github.com/mozilla-services/tokenlib\n\nIf this library does not meet your needs, you can provide a custom callback\nfunction to decode the Hawk id token.\n"
},
{
  "name": "heka-raven",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "README.rst",
      "docs",
      "heka_raven",
      "required.txt",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "source"
    ]
  },
  "makefile": null,
  "readme": "==========\nheka-raven\n==========\n\nThis repository has been deprecated.  Please use `heka-py-raven\n<http://github.com/mozilla-services/heka-py-raven>`_.\n\n"
},
{
  "name": "heka-cef",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "HekaCef.spec",
      "MANIFEST.in",
      "README.rst",
      "docs",
      "heka_cef",
      "required.txt",
      "requirements.txt",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "source"
    ]
  },
  "makefile": null,
  "readme": "This repository is deprecated and has been moved to \n`heka-py-cef <http://github.com/mozilla-services/heka-py-cef>`_.\n\n\n"
},
{
  "name": "server-core",
  "files": {
    "/": [
      ".gitignore",
      ".hgignore",
      ".hgtags",
      "MANIFEST.in",
      "Makefile",
      "README",
      "RELEASE.txt",
      "Services.spec",
      "dev-reqs.txt",
      "migrations",
      "prod-reqs.txt",
      "services",
      "setup.cfg",
      "setup.py",
      "stage-reqs.txt"
    ]
  },
  "makefile": "APPNAME = server-core\nDEPS =\nVIRTUALENV = virtualenv\nPYTHON = bin/python\nNOSE = bin/nosetests -s --with-xunit\nFLAKE8 = bin/flake8\nCOVEROPTS = --cover-html --cover-html-dir=html --with-coverage --cover-package=services\nTESTS = services\nPKGS = services\nCOVERAGE = bin/coverage\nPYLINT = bin/pylint\nSERVER = dev-auth.services.mozilla.com\nSCHEME = https\nBUILDAPP = bin/buildapp\nBUILDRPMS = bin/buildrpms\nPYPI = http://pypi.python.org/simple\nPYPI2RPM = bin/pypi2rpm.py --index=$(PYPI)\nPYPIOPTIONS = -i $(PYPI)\nCHANNEL = dev\nRPM_CHANNEL = prod\nINSTALL = bin/pip install\nINSTALLOPTIONS = -U -i $(PYPI)\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n.PHONY: all build build_extras build_rpms test update\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages --distribute .\n\t$(INSTALL) Distribute\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) nose\n\t$(INSTALL) WebTest\n\t$(INSTALL) wsgi_intercept\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nupdate:\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nbuild_extras:\n\t$(INSTALL) MySQL-python\n\t$(INSTALL) recaptcha-client\n\t$(INSTALL) wsgiproxy\n\t$(INSTALL) wsgi_intercept\n\t$(INSTALL) \"python-ldap == 2.3.13\"\n\t$(INSTALL) coverage\n\t$(INSTALL) Pygments\n\ntest:\n\t$(NOSE) $(TESTS)\n\nbuild_rpms:\n\t$(BUILDRPMS) -c $(RPM_CHANNEL) $(DEPS)\n",
  "readme": "=========\nSync Core\n=========\n\nCore library that provides these features:\n\n- Config reader/writer\n- Plugin system\n- Base WSGI application for Sync servers\n- Error codes for Sync\n- Authentication back ends for Sync\n- Event registry\n\n\nMetlog\n------\n\nUse of the 'Metlog' metrics and logging system can be configured in your sync\napplication's sync.conf file. You will need to add a section [metlog_loader]\nthat looks something like this::\n\n    [metlog_loader]\n    backend = services.metrics.MetlogLoader\n    config = /path/to/this/sync.conf\n\nAnd also a [metlog] section::\n\n    [metlog]\n    sender_class = metlog.senders.zmq.ZmqPubSender\n    sender_bindstrs = tcp://localhost:5585\n\nThe [metlog] section must define a set of keys prefixed with \"sender_\".  The\nmost important one of these is the \"sender_class\" which defines the classname\nof the metlog sender. All other 'sender_' prefixed keys will be passed into the\nconstructor of the sender with the prefix stripped away.\n\nIf these are left out, the metlog client will default to passing messages to\nPython's `logging` standard library module.\n\nIn either case, the metlog client will be available from the application object\nas the `logger` attribute. If you don't have access to the application object,\nyou can get the client from Metlog's `CLIENT_HOLDER` object like so::\n\n    from metlog.holder import CLIENT_HOLDER\n    logger = CLIENT_HOLDER.default_client\n"
},
{
  "name": "gis-experiments",
  "files": {
    "/": [
      "Makefile",
      "README.rst",
      "db.py",
      "demo.png",
      "demo.py",
      "map.py",
      "utils.py"
    ]
  },
  "makefile": ".PHONY: build \n\nifndef VTENV_OPTS\nVTENV_OPTS = \"--no-site-packages\"\nendif\n\nbin/python:\n\tvirtualenv $(VTENV_OPTS) .\n\nbuild: bin/python\n\tbin/pip install numpy\n\tbin/pip install matplotlib\n\tbin/pip install shapely\n\tbin/pip install descartes\n",
  "readme": "Location demo\n=============\n\nTerms\n-----\n\n- A **signal source** is either a cell tower or a wifi access point\n\n- The **location db** contains coordinates of cell towers or wifi access points\n\n- a **measure** is a list of wifi or cell tower, each one with a signal strength\n  when possible and a unique key.\n\n- **Signal strength** is generally some ASU (arbitrary strength unit) measure like\n  \"16\" or \"92\" with a meaning dependent on the network standard\n\n- **Time of flight** is a measure of how much time it took a signal to reach the\n  user equipment, gathered from things like \"round trip time\" or \"timing advance\"\n\n- A **cell/wifi record** consists of a unique id, a lat/lon location, a network\n  standard, a maximum radius, a minimum and maximum signal strength and a maximum\n  time of flight\n\n\nThe theory\n----------\n\nTo build the location db of signal sources, we proceed as following:\n\n1/ a phone device that knows its location collects all signal sources\n   it sees around, with an asu for each.\n\n2/ a signal source that has been seen from three different locations\n   can have its location guessed using a\n   `trilateration <https://en.wikipedia.org/wiki/Trilateration>`_.\n\n3/ when a phone ask for its location, given what it sees around it,\n   we can guess it by doing a trilateration based on the signal sources\n   it sees.\n\n4/ the location db is constantly refined with new incoming data.\n\n\nDemo\n----\n\n.. image:: https://github.com/tarekziade/gis/blob/master/demo.png?raw=true\n\nTo install and run the demo ::\n\n    $ make build\n    $ bin/python demo.py\n\n"
},
{
  "name": "server-whoami",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.rst",
      "SyncWhoami.spec",
      "dev-reqs.txt",
      "loadtest",
      "prod-reqs.txt",
      "setup.py",
      "stage-reqs.txt",
      "syncwhoami",
      "upstream-deps"
    ]
  },
  "makefile": "DEPS = server-core\nVIRTUALENV = virtualenv\nPYTHON = bin/python\nNOSE = bin/nosetests -s\nTESTS = syncwhoami/tests\nBUILDAPP = bin/buildapp\nBUILDRPMS = bin/buildrpms\nBUILD_TMP = /tmp/server-storage-build.${USER}\nPYPI = http://pypi.python.org/simple\nPYPIOPTIONS = -i $(PYPI)\nCHANNEL = dev\nINSTALL = bin/pip install\nINSTALLOPTIONS = -U -i $(PYPI)\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n\n.PHONY: all build update test build_rpms\n\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --distribute --no-site-packages .\n\t$(INSTALL) Distribute\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) Nose\n\t$(INSTALL) WebTest\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\t# py-scrypt doesn't play nicely with pypi2rpm\n\t# so we can't list it in the requirements files.\n\tmkdir -p ${BUILD_TMP}\n\tcd ${BUILD_TMP}; tar -xzvf $(CURDIR)/upstream-deps/py-scrypt-0.6.0.tar.gz\n\t$(INSTALL) ${BUILD_TMP}\n\trm -rf ${BUILD_TMP}\n\nupdate:\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\ntest:\n\t$(NOSE) $(TESTS)\n\nbuild_rpms:\n\t$(BUILDRPMS) -c $(CHANNEL) $(DEPS)\n\t# py-scrypt doesn't play nicely with pypi2rpm.\n\tcd ${BUILD_TMP}; tar -xzvf $(CURDIR)/upstream-deps/py-scrypt-0.6.0.tar.gz\n\tcd ${BUILD_TMP}; python setup.py  --command-packages=pypi2rpm.command bdist_rpm2 --binary-only --name=python26-scrypt --dist-dir=$(CURDIR)/rpms\n\n",
  "readme": "Simple account-information retrieval API\n========================================\n\nThis is a simple stand-alone server that allows an authenticated user\nto retreive their userid and syncNode from the firefox sync account database.\nWe plan to use it internally for caching-proxying some authentication data\ninto AWS.\n\nIt exposes a single API endpoint:\n\n\n**GET** **/whoami**\n\n    Returns a JSON mapping of account data for the user identified in the\n    *Authorization* header.  The mapping will have the following entries:\n\n    - userid:   integer, internal numeric user identifier\n    - syncNode:  string, the user's currently-assigned sync node\n\n    Possible errors:\n\n    - 401: the provided auth credentials were not correct\n    - 503: there was an error getting the information\n\n"
},
{
  "name": "marteau-web",
  "files": {
    "/": [
      ".gitignore",
      "LICENCE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "marteau.ini",
      "marteauweb",
      "setup.py"
    ]
  },
  "makefile": ".PHONY: build\n\nifndef VTENV_OPTS\nVTENV_OPTS = \"--no-site-packages\"\nendif\n\nbin/python:\n\tvirtualenv $(VTENV_OPTS) .\n\tbin/python setup.py develop\n\nbuild: bin/python\n\ndocs: \n\tbin/python setup.py build_sphinx\n",
  "readme": "marteau-web\n===========\n\nThe Marteau Web Server"
},
{
  "name": "pyramid_srpauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.txt",
      "pyramid_srpauth",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "webapp-generator",
  "files": {
    "/": [
      "README.rst",
      "generator",
      "gunicorn.conf",
      "requirements.txt",
      "static"
    ]
  },
  "makefile": null,
  "readme": "Webapp Manifest Generator\n#########################\n\nThis is a tiny project to generate webapp.manifest files, served with the\nappropriate Content-Type, on a multidomain server.\n\nMake it run\n===========\n\nCreate a virtualenv for it::\n\n    $ virtualenv env\n    $ source env/bin/activate\n    $ pip install -r requirements.txt\n    $ python generator/web.py\n\nDeploy\n======\n\nIf you want to deploy this, you can use the following configuration:\n\nA DNS entry in bind::\n    \n    *.webapp                    IN  A      your ip\n\nA supervisor file::\n\n    [program:webapp.lolnet.org]\n    command=/path/to/venv/bin/gunicorn -c /path/to/config.conf generator.web:app\n    directory=/path/to/git/repo/\n    user=www-data\n    autostart=true\n    autorestart=true\n    redirect_stderr=True\n\n\nA gunicorn file::\n\n    backlog = 2048\n    daemon = False\n    debug = True\n    workers = 3\n    logfile = \"/home/www/logs/yourlog.gunicorn.log\n    loglevel = \"info\"\n    bind = \"unix:/home/www/yourhost.org/gunicorn.sock\"\n\nAn nginx file::\n\n   server {                                                               \n        server_name *.yourhost.org;                               \n        keepalive_timeout 5;                                           \n                                                                       \n        location /static/ {                                            \n                alias   /path/to/git/static/;           \n        }                                                                    \n                                                                             \n        location / {                                                         \n                proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; \n                proxy_set_header Host $http_host;                            \n                proxy_redirect off;                                          \n                proxy_connect_timeout 90;                                    \n                proxy_send_timeout 180;                                      \n                proxy_read_timeout 180;                                      \n                proxy_buffer_size 16k;                                       \n                proxy_buffers 8 16k;                                         \n                proxy_busy_buffers_size 32k;                                 \n                proxy_intercept_errors on;                                   \n                if (!-f $request_filename) {                                 \n                    proxy_pass http://webapp_backend;                        \n                    break;                                                   \n                }                                                            \n        }                                                                    \n    }                                                                            \n                                                                                 \n    upstream webapp_backend {                                                    \n            server unix:/path/to/the/socket.sock;               \n    }                                                                            \n"
},
{
  "name": "django-fakeauth",
  "files": {
    "/": [
      "README.rst",
      "django_fakeauth",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "Django FakeAuth\n###############\n\nThis is a fake authentication backend for django, you can use it like that (in\nyour settings)::\n\n    FAKEAUTH_TOKEN = 'superkey'\n\n    AUTHENTICATION_BACKENDS = ('django_fakeauth.FakeAuthBackend',) + AUTHENTICATION_BACKENDS\n    MIDDLEWARE_CLASSES.append('access.middleware.ACLMiddleware')\n\n    FAKEAUTH_TOKEN = os.environ.get('FAKEAUTH_TOKEN')\n\nWhen doing local development, you can also ask the system to use a specific\nuser, like this::\n\n    FAKEAUTH_BYPASS = 'username'\n"
},
{
  "name": "ha-test-web-app",
  "files": {
    "/": [
      "icon.png",
      "index.html",
      "manifest.webapp",
      "screenshot.png"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "vaurienclient",
  "files": {
    "/": [
      "MANIFEST.in",
      "README.rst",
      "setup.py",
      "vaurienclient"
    ]
  },
  "makefile": null,
  "readme": "Vaurien Client\n##############\n\nThis is a client for `vaurien <http://vaurien.rtfd.org>`_. It's a separate\nproject to avoid getting all the dependencies of vaurien when you just want to\ncontrol it.\n\nIt uses `the vaurien's APIs\n<https://vaurien.readthedocs.org/en/latest/apis.html>`_ to change the behaviors\non the proxy side.\n\n**vaurienctl** can be used to list the available behaviors, get the current one,\nor set it.\n\nHere is a quick demo::\n\n    $ vaurienctl list-behaviors\n    delay, error, hang, blackout, dummy\n\n    $ vaurienctl set-behavior blackout\n    Behavior changed to \"blackout\"\n\n    $ vaurienctl get-behavior\n    blackout\n\nUsing the marteau extension\n===========================\n\n*vaurienclient* can be plugged into the marteau mechanism, as a fixture. The\nfixture class lives in vaurienclient.ext.marteau.\n\nHere is how you can make it work with a `.marteau.yml` file::\n\n    lookup:\n        - vaurienclient.ext.marteau \n    fixtures:\n        memcache_delay:\n            class: VaurienFixture \n            arguments:\n                server: memcache\n                behavior: delay\n    vaurien-proxies:\n        memcache: http://memcache:0123\n\nThere are different sections that can be useful here:\n\n* `lookup` tells marteau to load the fixtures that are available on\n  `vaurienclient.ext.marteau`.\n* fixtures is the list of fixtures. you give them the class to Use (here the\n  `VaurienFixture` class, and some arguments.\n* And then, the last bit is the list of poxies you want to use.\n"
},
{
  "name": "metlog-raven",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "README.rst",
      "docs",
      "metlog_raven",
      "required.txt",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "source"
    ]
  },
  "makefile": null,
  "readme": "============\nmetlog-raven\n============\n\n.. image:: https://secure.travis-ci.org/mozilla-services/metlog-raven.png\n\nmetlog-raven is a plugin extension for `Metlog \n<http://github.com/mozilla-services/metlog-py>`.  metlog-raven\nprovides logging extensions to capture stacktraces and some frame\ninformation such as local variables to faciliate debugging.\n\nThe plugin acts as a thin wrapper around the Raven\n<https://github.com/dcramer/raven> library for Sentry.\n\nMore information about how Mozilla Services is using Metlog (including what is\nbeing used for a router and what endpoints are in use / planning to be used)\ncan be found on the relevant `spec page\n<https://wiki.mozilla.org/Services/Sagrada/Metlog>`_.\n\nThis version of metlog-raven must be used with :\n\n  * Raven client version 3.1.16\n  * Sentry server 5.4.2\n\nOther versions may work, but they have not been tested.\n"
},
{
  "name": "signing-service",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README",
      "dev-reqs.txt",
      "etc",
      "prod-reqs.txt",
      "setup.py",
      "signing.egg-info",
      "signing.spec",
      "signing",
      "verify.py"
    ]
  },
  "makefile": "APPNAME = signing\nDEPS =\nHERE = $(shell pwd)\nBIN = $(HERE)/bin\nVIRTUALENV = virtualenv\nNOSE = bin/nosetests -s --with-xunit\nTESTS = $(APPNAME)/tests\nPYTHON = $(BIN)/python\nBUILDAPP = $(BIN)/buildapp\nBUILDRPMS = $(BIN)/buildrpms\nPYPI = http://pypi.python.org/simple\nPYPIOPTIONS = -i $(PYPI)\nPYPI2 = http://pypi.python.org/packages\nBUILD_TMP = /tmp/signing-build.${USER}\nDOTCHANNEL := $(wildcard .channel)\nifeq ($(strip $(DOTCHANNEL)),)\n\tCHANNEL = dev\n\tRPM_CHANNEL = prod\nelse\n\tCHANNEL = `cat .channel`\n\tRPM_CHANNEL = `cat .channel`\nendif\nINSTALL = $(BIN)/pip install\nPIP_CACHE = /tmp/pip_cache\nINSTALLOPTIONS = --download-cache $(PIP_CACHE)  -U -i $(PYPI)\nRPMDIR= $(CURDIR)/rpms\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n\n.PHONY: all build test build_rpms mach\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages .\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) nose\n\t$(INSTALL) WebTest\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nupdate:\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\ntest:\n\t$(NOSE) $(APPNAME)\n\nbuild_rpms:\n\trm -rf rpms/\n\tmkdir -p rpms ${BUILD_TMP}\n\t$(BUILDRPMS) -c $(RPM_CHANNEL) $(DEPS)\n\tcd ${BUILD_TMP} && wget $(PYPI2)/source/M/M2Crypto/M2Crypto-0.21.1.tar.gz#md5=f93d8462ff7646397a9f77a2fe602d17\n\tcd ${BUILD_TMP} && tar -xzvf M2Crypto-0.21.1.tar.gz && cd M2Crypto-0.21.1 && sed -i -e 's/opensslconf\\./opensslconf-x86_64\\./' SWIG/_ec.i && sed -i -e 's/opensslconf\\./opensslconf-x86_64\\./' SWIG/_evp.i && SWIG_FEATURES=-cpperraswarn $(PYTHON) setup.py --command-packages=pypi2rpm.command bdist_rpm2 --binary-only --dist-dir=$(RPMDIR) --name=python26-m2crypto\n\trm -rf ${BUILD_TMP}/M2Crypto*\n\nmach: build build_rpms\n\tmach clean\n\tmach yum install python26 python26-setuptools\n\tcd rpms; wget http://mrepo.mozilla.org/mrepo/5-x86_64/RPMS.mozilla-services/gunicorn-0.11.2-1moz.x86_64.rpm\n\tcd rpms; wget http://mrepo.mozilla.org/mrepo/5-x86_64/RPMS.mozilla/nginx-0.7.65-4.x86_64.rpm\n\tmach yum install rpms/*\n\tmach chroot python2.6 -m signing.run\n\nclean:\n\trm -rf bin build build1 deps dist include lib lib64 man rpms\n",
  "readme": "Documentation\n=============\n\nSee https://wiki.mozilla.org/Apps/WebApplicationReceipt/SigningService"
},
{
  "name": "mopytools",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "mopytools",
      "pylintrc",
      "setup.py"
    ]
  },
  "makefile": ".PHONY: build test coverage build_rpm\n\nifndef VTENV_OPTS\nVTENV_OPTS = \"--no-site-packages\"\nendif\n\nbuild:\t\n\tvirtualenv $(VTENV_OPTS) .\n\tbin/python setup.py develop\n\ntest: bin/nosetests\n\tbin/nosetests -s mopytools\n\ncoverage: bin/coverage\n\tbin/nosetests --with-coverage --cover-html --cover-html-dir=html --cover-package=circus\n\ndocs: bin/sphinx-build\n\tSPHINXBUILD=../bin/sphinx-build $(MAKE) -C docs html $^\n\nbin/sphinx-build: bin/python\n\tbin/pip install sphinx\n\nbin/nosetests: bin/python\n\tbin/pip install nose\n\nbin/coverage: bin/python\n\tbin/pip install coverage\n",
  "readme": "=========\nMopytools\n=========\n\nMopytools provides command-line tools to build Python application that follow\nMozilla Services standard:\n\n- buildapp: builds an application inplace.\n- buildrpms: builds a collection of RPMs for the project\n\n\nThose scripts are usually driven from a project's Makefile.\n\n"
},
{
  "name": "jenkins-publisher",
  "files": {
    "/": [
      "CHANGES.rst",
      "MANIFEST.in",
      "README.rst",
      "jpub",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": ""
},
{
  "name": "puppet-cassandra",
  "files": {
    "/": [
      "README.rst",
      "manifests"
    ]
  },
  "makefile": null,
  "readme": ""
},
{
  "name": "logstash-metlog",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.rst",
      "client",
      "docs",
      "lib",
      "logrotate",
      "python",
      "rpm",
      "run_test.sh",
      "runme_tagger.sh",
      "src",
      "start"
    ],
    "/docs": [
      "Makefile",
      "make.bat",
      "source"
    ]
  },
  "makefile": "build_rpm:\n\tmkdir -p build/SOURCES\n\trm -rf build/RPMS build/logstash-metlog-*\n\ttar czf build/SOURCES/logstash-metlog.tar.gz src/logstash logrotate/bin\n\trpmbuild --define \"_topdir $$PWD/build\" -ba rpm/logstash-metlog.spec\n\tcp build/RPMS/*/*.rpm build/\n\tls -l build/logstash-metlog-*.rpm\n",
  "readme": "===============\nlogstash-metlog\n===============\n\nlogstash-metlog is a set of extensions for logstash to provide statsd,\nCEF over syslog, Sentry and JSON to HDFS capabilities.\n\nFull documentation can be found `here\n<http://logstash-metlog.rtfd.org/>`_.\n"
},
{
  "name": "repoze.who.plugins.macauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.rst",
      "repoze",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "==========================\nrepoze.who.plugins.macauth\n==========================\n\nThis is a repoze.who plugin for MAC Access Authentication:\n\n    http://tools.ietf.org/html/draft-ietf-oauth-v2-http-mac-01\n\nTo access resources using MAC Access Authentication, the client must have\nobtained a set of MAC credentials including an id and secret key.  They use\nthese credentials to make signed requests to the server.\n\nWhen accessing a protected resource, the server will generate a 401 challenge\nresponse with the scheme \"MAC\" as follows::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n\n    < HTTP/1.1 401 Unauthorized\n    < WWW-Authenticate: MAC\n\nThe client will use their MAC credentials to build a request signature and\ninclude it in the Authorization header like so::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n    > Authorization: MAC id=\"h480djs93hd8\",\n    >                    ts=\"1336363200\",\n    >                    nonce=\"dj83hs9s\",\n    >                    mac=\"bhCQXTVyfj5cmA9uKkPFx1zeOXM=\"\n\n    < HTTP/1.1 200 OK\n    < Content-Type: text/plain\n    <\n    < For your eyes only:  secret data!\n\n\nThis plugin uses the tokenlib library for verifying MAC credentials:\n\n    https://github.com/mozilla-services/tokenlib\n\nIf this library does not meet your needs, you can provide a custom callback\nfunction to decode the MAC id token.\n"
},
{
  "name": "cornice-sqla",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "README.rst",
      "cornicesqla",
      "examples",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": ""
},
{
  "name": "repoze.who.plugins.memcached",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.rst",
      "repoze",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "============================\nrepoze.who.plugins.memcached\n============================\n\nThis is a repoze.who IAuthenticator/IMetadataProvider plugin that can\ncache the results of *other* plugins using memcached.  It's useful for\nreducing load on e.g. a backend LDAP auth system.\n\nTo use it, give it the name of an authenticator and/or metadata provider\nwhose results it should wrap::\n\n    [plugin:ldap]\n    use = my.ldap.authenticator\n\n    [plugin:cached_ldap]\n    use = repoze.who.plugins.memcached\n    authenticator_name = ldap\n\n    [authenticators]\n    plugins = cached_ldap ldap;unused\n\n(The \"ldap;unused\" bit ensures that the wrapped ldap plugin still gets\nloaded, but is not used for matching any requests. Yeah, it's yuck.)\n\nTo prevent a compromise of the cache from revealing auth credentials, this\nplugin calculates a HMAC hash of the items in the incoming identity and\nuses that as the cache key.  This makes it possible to check the cache for\na match to an incoming identity, while preventing the cache keys from being \nreversed back into a valid identity.\n\nItems added to the identity by the wrapped plugin will be stored in the\ncached value and will *not* be encryped or obfuscated in any way.\n\nThe following configuration options are available:\n\n    * memcached_urls:  A list of URLs for the underlying memcached store.\n\n    * authenticator_name:  The name of an IAuthenticator plugin to wrap.\n\n    * mdprovider_name:  The name of an IMetadataProvider plugin to wrap.\n\n    * key_items:  A list of names from the identity dict that should be\n                  hashed to produce the cache key.  These items should\n                  uniquely and validly identity a user.  By default it\n                  will use all keys in the identity in sorted order.\n\n    * value_items:  A list of names from the identity dict that should be\n                    stored in the cache.  These would typically be items\n                    of metadata such as the user's email address.  By \n                    default this will include all items that the wrapped\n                    plugin adds to the identity.\n\n    * secret:  A string used when calculating the HMAC the cache keys.\n               All servers accessing a shared cache should use the same\n               secret so they produce the same set of cache keys.\n\n    * ttl:  The time for which cache entries should persist, in seconds.\n"
},
{
  "name": "metlog-node",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Makefile",
      "README.md",
      "client.js",
      "config.js",
      "docs",
      "example",
      "filters.js",
      "package.json",
      "senders",
      "tests"
    ],
    "/docs": [
      "Makefile",
      "README.rst",
      "source"
    ]
  },
  "makefile": "all:\n\tnpm link .\n\ntest:\n\tnode node_modules/jasmine-node/lib/jasmine-node/cli.js tests\n\ntest_demo:\n\tnode example/demo.js\n",
  "readme": "[![Build Status](https://secure.travis-ci.org/mozilla-services/metlog-node.png)](http://travis-ci.org/mozilla-services/metlog-node)\n\nA javascript library for generating and sending metrics logging to a metlog listener.\n\nInstalling:\n\n    Source Install:\n\n        git clone git://github.com/mozilla-services/metlog-node.git\n        npm install --dev\n\nRunning tests :\n\n    make tests\n\nFull documentation is available at http://metlog-node.rtfd.org\n"
},
{
  "name": "webtestplus",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.txt",
      "setup.py",
      "webtestplus"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "AboutSyncKey",
  "files": {
    "/": [
      "README",
      "bootstrap.js",
      "chrome.manifest",
      "content",
      "install.rdf"
    ]
  },
  "makefile": null,
  "readme": "about:synckey\n=============\n\nA small restartless add-on for Firefox to view your Sync Key at\nabout:synckey. Useful when you only have your mobile device running\nFirefox Mobile at hand and you would like to connect to your Sync\naccount.\n\n1. Install this add-on. It's restartless, so no restart required.\n2. Open a new tab and go to about:synckey. Enter the Sync Key into\n   your other device.\n3. When you're done, you probably want to uninstall the add-on again\n   to prevent easy access to your Sync Key.\n\nYou may also want to consider protecting your Firefox data on your\nphone using a Master Password using this add-on:\n\n  https://addons.mozilla.org/mobile/addon/master-password-270907/\n"
},
{
  "name": "ios-sync-client",
  "files": {
    "/": [
      "ExternalSources",
      "README.md",
      "Sources"
    ]
  },
  "makefile": null,
  "readme": "ios-sync-client\n===============\n\nA standalone iOS client for Firefox Sync.\n\nCompatibility\n-------------\n\nThis code is known to compile with Xcode 4.4.1 with the iOS 5.1 SDK under OS X 10.8.1.\n\nBuild Instructions\n------------------\n\nThe project is completely self-contained and does not depend on external sources. To build, simply clone the project and then open the Sources/SyncClient.xcodeproject project in Xcode. You should be able to just hit run to build and run in the iPhone Simulator.\n"
},
{
  "name": "shavar-list-exceptions",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "allow_list"
    ]
  },
  "makefile": null,
  "readme": "shavar-list-exceptions\n======================\nThis is the list of domains that we omit from Disconnect's upstream blocklist when generating `mozpub-track-digest256`. Lines starting with # are ignored.\n\nCommits to this repo trigger rebuild and repush of shavar-list-creation, resulting in a new blocklist in s3.\n"
},
{
  "name": "push-service",
  "files": {
    "/": [
      ".idea",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "docs",
      "mkdocs.yml"
    ],
    "/docs": [
      "assets",
      "design.md",
      "development.md",
      "history.md",
      "index.md"
    ]
  },
  "makefile": null,
  "readme": "# Welcome to the Mozilla Push Service [![Build Status](https://travis-ci.org/mozilla-services/autopush.svg?branch=master)](https://travis-ci.org/mozilla-services/autopush)\n\nMozilla Push Service is the server-side project support Push Notifications in\nthe [Firefox browser][ffx] and [Firefox OS][fxos].\n\n## Contributing to the Mozilla Push Service\n\nInterested in contributing to the development of Mozilla's Push Service?\nComplete documentation on the Push Service, History, and how to get involved\ncan be found at [the Push Service's documentation](https://mozilla-push-service.readthedocs.io/).\n\n##### Bugs List: [waffle.io/mozilla-services/push-service](https://waffle.io/mozilla-services/push-service/)\n\n## Building this Documentation\n\n```\npip install mkdocs\n```\n\nBuild:\n\n```\nmkdocs build\n```\n\nServer + File Watching:\n\n```\nmkdocs serve --dev-addr localhost:9032\n```\n\nPublishing to https://mozilla-push-service.readthedocs.io\n\n```\n# Documentation is built automatically from this repository.\ngit commit\ngit push origin/master\n```\n\n[fxos]: https://www.mozilla.org/en-US/firefox/os/\n[ffx]: https://www.mozilla.org/en-US/firefox/\n"
},
{
  "name": "push_component_rust",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "communications",
      "crypto",
      "delivery_mgr",
      "notifier",
      "storage",
      "subscriber"
    ]
  },
  "makefile": null,
  "readme": "#INACTIVE\n\nSee https://github.com/mozilla/application-services /component/push\n"
},
{
  "name": "normandy-loadtests",
  "files": {
    "/": [
      ".env.dist",
      "Dockerfile",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "api_tests.py"
    ]
  },
  "makefile": null,
  "readme": "# Normandy Load tests\n\n## Installation\nThis project uses [pipenv](https://pipenv.readthedocs.io/en/latest/) to manage dependencies.\nAfter checking out this repo, run the following commands to create the virtual environment and\ninstall the required dependencies.\n\n`pipenv install`\n\n## Configuration\n\nThis project uses environment files to store values unique to the target load testing\nenvironment. Copy the file `.env.dist` to `.env` and set the environment variables\nto their required values.\n\nThen use the following command to activate the virtual environment:\n\n`pipenv shell`\n\npipenv will automatically set any environment variables found in the `.env` file when\nyou activate the virtual environment. If you change anything in `.env` you need to\nexit the virtual environment and reenter using the following commands:\n\n`exit`\n\n`pipenv shell`\n\n## Running Load Tests.\nThe load tests were written using [Molotov](https://molotov.readthedocs.io/en/stable/)\nand can be started using the following command:\n\n`molotov -c -v d <duration> api_tests.py`\n\nwhere `<duration>` is the length of time in seconds that you want the load test to run.\n\nCheck the Molotov documentation for details on other options available to run the tests.\n\n"
},
{
  "name": "autopush-vapid-tests",
  "files": {
    "/": [
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# Autopush VAPID tests\n\nThis repository contains an automated test for verifying VAPID functionality\nworks correctly on Android devices.\n\n## Prerequisites\n\nYou need to having the following things installed:\n\n* Android Studio for your platform\n* The Android SDK installed\n* Python 2.7.15\n* an APK of a version of Firefox that supports Marionette\n\n## Pre-test Configuration\n\nOnce you have Android Studio installed, you will need to use\nthe AVD Manager to install an Android emulator for the purposes\nof testing.\n\nDuring development of these tests a \"Pixel API 22\" virtual device\nwas used.\n\nWith the emulator running you then need to use the `adb` CLI tool\nfrom the Android SDK to load your Firefox APK into the system. You\ncan use the following command:\n\n`/path/to/adb install /path/to/Firefox.apk`\n\nThis will add the APK to your virtual device.\n\nYou then need to use `adb` to forward your local TCP port of 2828\nto the virtual device so that Marionette can control the browser.\n\n`/path/to/adb forward tcp:2828 tcp:2828`\n\n## Running the tests\n\nTo run the tests use the following command:\n\n`pytest tests/`\n"
},
{
  "name": "timeseries-loadtests",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "data",
      "format_data.py",
      "opentsdb_writer.py",
      "prometheus_exporter.py",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# Purpose\n\nIn [influx-stress](https://github.com/influxdata/influx-stress) we found a tool that is able to generate a representative data set and write it to influxdb, but we failed to find suitable tools for prometheus and opentsdb.\n\nI had the idea that we could use influx-stress to generate the data set and then create scripts for writing it to prometheus and opentsdb. This is the result. Each time the prometheus or opentsdb script publishes data, it will randomly choose one of the entries from the data set.\n\nIf you are interested in testing read rather than write performance, check out other tools like [influxdb-comparisons](https://github.com/influxdata/influxdb-comparisons) or [tsbs](https://github.com/timescale/tsbs).\n\n# Preprocessing\n\nInput data was generated with\n\n```\n$ influx-stress insert --dump data --series 2000000 --points 2000000 --batch-size 1000\n```\n\nand then processed for opentsdb and prometheus with\n\n```\n$ mkdir prometheus_data\n$ mkdir opentsdb_data\n$ ./format_data.py\n```\n\nFor each database that results in 2000 files with 1000 timeseries each in the format they expect for ingestion. All of the timeseries are unique.\n\n# Running\n\n## Prometheus\n\nTo run a single prometheus exporter, just\n\n```\n$ ./prometheus_exporter.py\nStarting exporter on 8000\n```\n\nTo run more than one, specify a starting port and the number to run, e.g.\n\n```\n$ ./prometheus_exporter.py --port 8000 --num-exporters=3\nStarting exporter on 8001\nStarting exporter on 8000\nStarting exporter on 8002\n```\n\n## OpenTSDB\n\nBefore running this you must `pip install requests`\n\n```\nusage: opentsdb_writer.py [-h] [--num-writers NUM_WRITERS]\n                          [--write-interval WRITE_INTERVAL]\n                          [--num-writes NUM_WRITES] [--timeout TIMEOUT]\n                          [--verbose]\n                          server\n\nSend random sets of precalculated data to OpenTSDB\n\npositional arguments:\n  server                OpenTSDB server to write to\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --num-writers NUM_WRITERS\n                        Number of concurrent writers\n  --write-interval WRITE_INTERVAL\n                        How often to write\n  --num-writes NUM_WRITES\n                        Total number of writes to make. Default and minimum is\n                        number of writers.\n  --timeout TIMEOUT     How long to wait for a response before aborting the\n                        benchmark\n  --verbose             Output status of each request\n```\n"
},
{
  "name": "lambda-csp-reporter",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "index.js",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "# lambda-csp-reporter"
},
{
  "name": "duopull-lambda",
  "files": {
    "/": [
      "LICENSE",
      "Makefile",
      "README.md",
      "duopull.go",
      "duopull_test.go",
      "misc"
    ]
  },
  "makefile": "all: duopull\n\n# Package lambda function in zip file\npackage:\n\tdocker run -i --rm -v `pwd`:/go/src/github.com/mozilla-services/duopull-lambda \\\n\t\tgolang:1.10 \\\n\t\t/bin/bash -c 'cd /go/src/github.com/mozilla-services/duopull-lambda && make lambda'\n\n# Development target, upload package to s3\npackageupload:\n\t@if [[ -z \"$(DUOPULL_S3_BUCKET)\" ]]; then \\\n\t\techo \"set DUOPULL_S3_BUCKET in environment\"; \\\n\t\texit 1; \\\n\tfi\n\taws s3 cp duopull.zip s3://$(DUOPULL_S3_BUCKET)/duopull.zip\n\nlambda: clean dep duopull\n\tapt-get update\n\tapt-get install -y zip\n\tzip duopull.zip duopull\n\ndep:\n\tgo get ./...\n\nduopull: duopull.go\n\tgo build -ldflags=\"-s -w\" duopull.go\n\ndebugduo: duopull\n\tenv DEBUGDUO=1 ./duopull\n\nclean:\n\trm -f duopull duopull.zip\n\n.PHONY: clean dep debugduo lambda package packageupload\n",
  "readme": "# Lambda Function for Duo Log Collection\n\nThis is a Lambda function that pulls administration/authentication/telephony logs\nfrom the Duo API and pushes them into a logging pipeline via a Kinesis stream.\n\n## Basic operation\n\nThe function stores state information in an S3 bucket. The state information is essentially\ntimestamp data indicating when the last log message from the API was collected, so the\nfunction knows when to begin requesting logs from for the next period. After the function runs\nit updates the state data for the next iteration.\n\nWhen log data is read, it is written to a Kinesis stream.\n\n## Lambda Packaging\n\n`make package` can be used to package the function in a zip file. A docker container is\ntemporarily used to generate the Linux executable and archive it in the zip.\n\n## Deployment\n\nExample deployment CloudFormation templates are available in the [misc](./misc)\ndirectory. The example collects logs every 5 minutes, but a period of 15 minutes\nis more suitable for production.\n\nThe `dev` template deploys roles and resources. The `dev-lambda` template deploys the\nfunction and scheduling.\n\n### Lambda Environment\n\nThe following settings are required for deployment.\n\n#### DUOPULL_REGION\n\nShould be set to the region the function's resources exist in.\n\n#### DUOPULL_S3_BUCKET\n\nThe S3 bucket name the state should be saved and read from.\n\n#### DUOPULL_KINESIS_STREAM\n\nThe Kinesis stream name the function writes events to.\n\n#### DUOPULL_HOST\n\nThe Duo admin API host requests for logs will be made to.\n\n#### DUOPULL_IKEY\n\nThe authentication identity to be used for API requests.\n\n#### DUOPULL_SKEY\n\nThe secret key to be used for API requests.\n\n## Development\n\n### Environment\n\n#### DEBUGAWS\n\nIf the DEBUGAWS environment variable is set to `1`, the function will generate mock events and make\nrequests to `https://www.mozilla.org` instead of the actual Duo API to confirm outbound\nconnectivity from the function. In this mode the function will update the S3 state and push\nevents to Kinesis but will not make requests to the actual Duo API.\n\n#### DEBUGDUO\n\nIn this mode the function will not execute as a Lambda but will poll the Duo API for log data,\nwrite the data to stdout and exit. The DEBUGDUO environment variable should be set to `1` to\nenable this mode.\n"
},
{
  "name": "rabbitmq-docker",
  "files": {
    "/": [
      "Dockerfile.3.7",
      "README.md",
      "docker-entrypoint.sh"
    ]
  },
  "makefile": null,
  "readme": "# rabbitmq-docker\n\nthis repo contains a dockerfile for dockerized rabbitmq. it targets version 3.7, and makes some decisions for you - it uses a uid of 11002 for the rabbitmq user, so you'll want to match that on your host OS for file permissions being volume mounted in if you use restrictive permissions.\nit includes the management component and exposes the corresponding ports on top of the default ports.\n\nit's poached from https://github.com/docker-library/rabbitmq/tree/1a37166704d2ca7c386980387e81615985d5db47/3.7/debian.\n"
},
{
  "name": "sync-tps-setup",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      "Dockerfile",
      "Jenkinsfile",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# TPS automation\n\nTPS is Testing and Performance for (Firefox) Sync.  It's a test suite that lives in `mozilla-central`.  Documentation for TPS is on MDN at https://developer.mozilla.org/en-US/docs/Mozilla/Projects/TPS_Tests.\n\nThis automation is designed to run TPS from a Docker container and includes a Jenkinsfile to enable execution from a Jenkins instance.\nIt is used by the Firefox Test Engineering Team Jenkins CI system and executed once daily via cron.\n\nPLEASE NOTE:\nConfig files for the STAGE and PROD environments are stored as credentials in the Firefox Test Jenkins and \nbacked up to our team LastPass account. \n\nFor more info, contact rpapa or kthiessen\n\n# Docker \n\n## Summary\n\nThe Sync TPS tests will be downloaded from mozilla-central and executed from within a Docker container.\n\n## Building Docker\n\n```sh\ndocker build -t firefoxtesteng/sync-tps-setup .\n```\n\n## Running Docker \n\n**STAGE**\n\n```sh\nTEST_CONFIG=`cat stage-config.json` \ndocker run -e \"TEST_ENV=stage\" -e \"TEST_CONFIG=${TEST_CONFIG}\" firefoxtesteng/sync-tps-setup\n```\n\nor\n\n**PROD**\n\n```sh\nTEST_CONFIG=`cat prod-config.json`\ndocker run -e \"TEST_ENV=prod\" -e \"TEST_CONFIG=${TEST_CONFIG}\" firefoxtesteng/sync-tps-setup\n```\n\n## Running Docker via Jenkins \n\nJenkins execution is configured via the Jenkinsfile included in this repo.  You can specify all your job configurations options via this file.\n\nA few things of Note:\n\n**triggers**\n\nTo run this job on a cron schedule, set cron values here.\n\n**environment**\n\nYou will need to create a few of the environment variables in this section. They begin with: \"SYNC\\_TPS\\_\"\n\n**post**\n\nConfigure your email confirmations here\n\n**changed**\n\nTurn IRC notifications on / off here\n\n## Firefox Test Engineering Jenkins\n\n**Schedule**\n\n* STAGE:  9 UTC\n* STAGE with buffer:  10 UTC\n* PROD:   6:30 UTC\n\n"
},
{
  "name": "asr-server",
  "files": {
    "/": [
      ".gitmodules",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "apiai.mk",
      "asr-server",
      "kaldi",
      "nginx.conf"
    ]
  },
  "makefile": null,
  "readme": "# asr-server\n\n## To build\ndocker build -t asr-server .\n\n## To run\ndocker run -p 80:80 asr-server"
},
{
  "name": "pytest-bugzilla-notifier",
  "files": {
    "/": [
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "bugzilla.ini-dist",
      "pytest_bugzilla_notifier",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "pytest-bugzilla-notifier\n===================================\n\n.. image:: https://img.shields.io/badge/license-MPL%202.0-blue.svg\n   :target: https://github.com/mozilla-services/pytest-bugzilla-notifier/blob/master/LICENSE.txt\n   :alt: License\n.. image:: https://img.shields.io/pypi/v/pytest-bugzilla-notifier.svg\n   :target: https://pypi.org/project/pytest-bugzilla-notifier/\n   :alt: PyPI\n.. image:: https://img.shields.io/travis/mozilla-services/pytest-bugzilla-notifier.svg\n   :target: https://travis-ci.org/mozilla-services/pytest-bugzilla-notifier\n   :alt: Travis\n.. image:: https://img.shields.io/github/issues-raw/mozilla-services/pytest-bugzilla-notifier.svg\n   :target: https://github.com/mozilla-services/pytest-bugzilla-notifier/issues\n   :alt: Issues\n.. image:: https://pyup.io/repos/github/mozilla-services/pytest-bugzilla-notifier/shield.svg\n   :target: https://pyup.io/repos/github/mozilla-services/pytest-bugzilla-notifier/\n   :alt: Updates\n\n\nThis plugin currently has the following functionality:\n\n* posts the results of test runs to be added to existing Bugzilla tickets\n* create new tickets as part of a pytest test\n* read bugs given a bug ID\n* search for bugs given a search\n\n\n\nInstallation\n------------\n\nYou can install \"pytest-bugzilla-notifier\" via `pip`_ from `PyPI`_::\n\n    $ pip install pytest-bugzilla-notifier\n\nPre-requisites\n--------------\n\nTo use this plugin you need to have a username and password for a Bugzilla\naccount. First, you need to copy bugzilla.ini-dist to bugzilla.ini and add in\nthe BugZilla API key you will be using to access Bugzilla.\n\nReporting test runs\n-------------------\n\nYou can use the plugin to update a ticket with the results by using the following command::\n\n    $ pytest --bug=<bug ID> --config=./bugzilla.ini --bugzilla-url=<server> /path/to/tests\n\n<bug ID>\nThe ID that Bugzilla assigned to the bug you wish to have the test\nresults sent to.\n\n<server>\nThe full URL to the Bugzilla instance you wish to send test results to\n(eg https://bugzilla.mozilla.com)\n\n\nCreating new tickets\n--------------------\n\nTo create a new ticket in Bugzilla, you need to import the library using::\n\n    from pytest_bugzilla_notifier.bugzilla_rest_client import BugzillaRESTClient\n\nand then you can create bugs using code similar to this::\n\n    api_details = {\n        'bugzilla_host': '<bugzilla host you are using>',\n        'bugzilla_api_key': '<bugzilla API key>'\n    }\n    rest_client = BugzillaRESTClient(api_details)\n    bug_data = {\n        'product': 'Firefox',\n        'component': 'Developer Tools',\n        'summary': 'Test Bug',\n        'version': 'unspecified'\n    }\n    bug_id = rest_client.bug_create(bug_data)\n\nIf everything worked as expected, ``bug_id`` will contain the ID BugZilla has assigned to your ticket.\n\nReading Bug Details\n-------------------\n\nIf you know the ID for a bug, you can read in the details::\n\n    api_details = {\n        'bugzilla_host': '<bugzilla host you are using>',\n        'bugzilla_api_key': '<bugzilla API key>'\n    }\n    rest_client = BugzillaRESTClient(api_details)\n    bug_id = <bug ID>\n    response = rest_client.bug_read(bug_id)\n\n\n\nSearching For Bugs\n------------------\n\nYou can follow the outlines for `search parameters`_ and then submit your search request::\n\n    api_details = {\n        'bugzilla_host': '<bugzilla host you are using>',\n        'bugzilla_api_key': '<bugzilla API key>'\n    }\n    rest_client = BugzillaRESTClient(api_details)\n    search_details = {\n        'product': 'Firefox',\n        'component': 'Developer Tools',\n        'summary': 'Test Bug',\n    }\n    bug_id = rest_client.bug_search(search_details)\n\n\n\n\nContributing\n------------\nContributions are very welcome. Tests can be run with `tox`_, please ensure\nthat the test suite passed before submitting a pull request.\n\n\nLicense\n-------\n\nDistributed under the terms of the `Mozilla Public License 2.0`_ license, \"pytest-bugzilla-notifier\" is free and open source software.\n\n\nIssues\n------\n\nIf you encounter any problems, please `file an issue`_ along with a detailed description.\n\n.. _`Mozilla Public License 2.0`: http://mozilla.org/MPL/2.0/\n.. _`file an issue`: https://github.com/mozilla-services/pytest-bugzilla-notifier/issues\n.. _`pytest`: https://github.com/pytest-dev/pytest\n.. _`tox`: https://tox.readthedocs.io/en/latest/\n.. _`pip`: https://pypi.python.org/pypi/pip/\n.. _`PyPI`: https://pypi.python.org/pypi\n.. _`search parameters`: http://bugzilla.readthedocs.io/en/latest/api/core/v1/bug.html#search-bugs\n"
},
{
  "name": "webpagetest-settings",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# webpagetest-settings (decommissioned)\n\nThis repo and its content were migrated into the /mozilla-services/cloudops-deployment repo, in https://github.com/mozilla-services/cloudops-deployment/pull/2724.\n"
},
{
  "name": "screenshots-loadtests",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CURLING.md",
      "Dockerfile",
      "Makefile",
      "README.md",
      "SCENARIOS.md",
      "ardere.json",
      "configs",
      "exercise_images.py",
      "fake_shot.json",
      "loads-broker.tpl",
      "loadtest-list.py",
      "loadtest-read.py",
      "loadtest.py",
      "molotov.env",
      "requirements.txt",
      "sess.json",
      "setup.cfg",
      "utils.py"
    ]
  },
  "makefile": "OS := $(shell uname)\nHERE = $(shell pwd)\nPYTHON = python3\nVTENV_OPTS = --python $(PYTHON)\n\n# load env vars\ninclude molotov.env\nexport $(shell sed 's/=.*//' molotov.env)\n\nBIN = $(HERE)/venv/bin\nVENV_PIP = $(BIN)/pip3\nVENV_PYTHON = $(BIN)/python\nINSTALL = $(VENV_PIP) install\n\n.PHONY: all check-os install-elcapitan install build\n.PHONY: docker-build docker-run docker-export\n.PHONY: test test-heavy \n.PHONY: loads-config \n.PHONY: clean clean-env\n\nall: build configure\n\n\n# hack for OpenSSL problems on OS X El Captain:\n# https://github.com/phusion/passenger/issues/1630\ncheck-os:\nifeq ($(OS),Darwin)\n  ifneq ($(USER),root)\n    $(info \"clang now requires sudo, use: sudo make <target>.\")\n    $(info \"Aborting!\") && exit 1\n  endif\n  BREW_PATH_OPENSSL=$(shell brew --prefix openssl)\nendif\n\ninstall-elcapitan: check-os\n\tenv LDFLAGS=\"-L$(BREW_PATH_OPENSSL)/lib\" \\\n\t    CFLAGS=\"-I$(BREW_PATH_OPENSSL)/include\" \\\n\t    $(INSTALL) cryptography\n\n$(VENV_PYTHON):\n\tvirtualenv $(VTENV_OPTS) venv\n\ninstall:\n\t$(INSTALL) -r requirements.txt\n\nbuild: $(VENV_PYTHON) install-elcapitan install\n\n\ntest: build\n\tbash -c \"URL_SERVER=$(URL_SERVER) WEIGHT_LIST_SHOTS=$(WEIGHT_LIST_SHOTS) WEIGHT_SEARCH_SHOTS=$(WEIGHT_SEARCH_SHOTS) WEIGHT_CREATE_SHOT=$(WEIGHT_CREATE_SHOT) WEIGHT_READ_SHOT=$(WEIGHT_READ_SHOT) $(BIN)/molotov -d $(TEST_DURATION) -cx $(TEST_MODULE)\"\n\ntest-heavy: build\n\tbash -c \"URL_SERVER=$(URL_SERVER) WEIGHT_LIST_SHOTS=$(WEIGHT_LIST_SHOTS) WEIGHT_SEARCH_SHOTS=$(WEIGHT_SEARCH_SHOTS) WEIGHT_CREATE_SHOT=$(WEIGHT_CREATE_SHOT) WEIGHT_READ_SHOT=$(WEIGHT_READ_SHOT) $(BIN)/molotov -p $(TEST_PROCESSES_HEAVY) -d $(TEST_DURATION_HEAVY) -w $(TEST_CONNECTIONS_HEAVY) -cx $(TEST_MODULE)\"\n\n\ndocker-build:\n\tdocker build -t firefoxtesteng/$(PROJECT)-loadtests .\n\ndocker-run:\n\tbash -c \"docker run -e URL_SERVER=$(URL_SERVER) -e WEIGHT_LIST_SHOTS=$(WEIGHT_LIST_SHOTS) -e WEIGHT_SEARCH_SHOTS=$(WEIGHT_SEARCH_SHOTS) -e WEIGHT_CREATE_SHOT=$(WEIGHT_CREATE_SHOT) -e WEIGHT_READ_SHOT=$(WEIGHT_READ_SHOT) -e TEST_PROCESSES=$(TEST_PROCESSES) -e TEST_DURATION=$(TEST_DURATION) -e TEST_CONNECTIONS=$(TEST_CONNECTIONS) -e VERBOSE=$(VERBOSE) firefoxtesteng/$(PROJECT)-loadtests\"\n\ndocker-test: docker-run\n\ndocker-push:\n\tdocker push \"$(PROJECT)/loadtest:latest\"\n\ndocker-export:\n\tdocker save \"$(PROJECT)/loadtest:latest\" | bzip2> \"$(PROJECT)-latest.tar.bz2\"\n\n\nloads-config:\n\t@bash loads-broker.tpl\n\n\nclean: \n\t@rm -fr venv/ __pycache__/ \n\nclean-env:\n\t@cp molotov.env molotov.env.OLD\n\t@rm -f molotov.env\n\t@touch molotov.env\n",
  "readme": "# Firefox Screenshots Load Tests\n\n[![Build Status](https://travis-ci.org/mozilla-services/screenshots-loadtests.svg?branch=master)](https://travis-ci.org/mozilla-services/screenshots-loadtests)\n\nAsync [**Firefox Screenshots**](https://github.com/mozilla-services/screenshots) load tests using [**molotov**](https://github.com/loads/molotov).\n\n## Requirements:\n\n- [Python 3.5+](https://www.python.org/downloads/)\n\n## Installation:\n\n```sh\n$ virtualenv venv -p python3\n$ source ./venv/bin/activate\n$ pip install -r requirements.txt\n```\n\n## Usage:\n\n```sh\n$ molotov -h\n\n$ molotov -cvv --max-runs 1\n```\n"
},
{
  "name": "subscriber_demo",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "page",
      "pusher",
      "requirements.txt",
      "server",
      "setup.cfg",
      "setup.py",
      "test-requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "#Minimal WebPush Subscription Demo\n\nThis is the companion demo for How to Manage Web Push Subscriptions.\n\n\n## Getting Started\n\nThis demo was written on a Linux box. This means that macs may work\ntoo, but older installs of Windows might have a harder time of things.\n\nIn addition, you'll need to either serve the `./page` directory from your\nlocal web server or run `bin/server` and connect to\n`http://localhost:8200`. This is because of a\nrestriction enforced by ServiceWorkers. ServiceWorker scripts must\neither be served from a secure server (one that can run `https://` or\nfrom `localhost`)\n\n\n To get started:\n ```\ngit clone https://github.com/mozilla-services/subscriber_demo.git\ncd subscriber_demo\nvirtualenv -p python3 .\nbin/activate\npython setup.py develop\nserver\n```\n\nNow start your browser and go to the topic `page` being served.\n(Again, either this is under a server running on your local machine , or by\nrunning `bin/server` and going to http://localhost:8200 )\n\nThe page is fairly self explanatory, but basically, click on the\n***Subscribe***\nbutton, and say \"Allow\" when prompted.\n\n## Sending Messages\n\nTo send a message to all registered subscribers, use the `pusher`\ncommand:\n\n`pusher --msg \"This is a test\"`\n\nTo send a message to a specific subscriber, specify that subscriber's\nuserID (displayed on the page after registration):\n\n`pusher --msg \"This is a test\" --id='Abcd1234`\n\n"
},
{
  "name": "third-party-library-alert",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "check.py",
      "libraries.json",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# third-party-library-alert\nA tool to alert when third party libraries embedded in the FF source code go out of date.\n\n# Types of alerts\n\nThe types of library checks we can perform are below. We can add more of course, but these are the easiest ones.\n\n### Release Version\n\nWe have a README.mozilla file that says something like \"The current version is 1.2.3\", and we check against the latest released version of the library and alert when it changes from 1.2.3 to something newer.\n\n### Release Version by Commit\n\nWe have a README.mozilla file that says something like \"The current version is commit 0987654321123456789abc (Mar 31, 2016)\". Even though we identify it by commit, we only update it when the library cuts a new release. So we compare the date in the README file with the date of the latest library release and alert when a new release comes out.\n\n### Commit by Commit\n\nWe have a README.mozilla file that says something like \"The current version is commit 0987654321123456789abc (Mar 31, 2016)\". On this project; however, we don't wait for releases (or maybe upstream doesn't do releases) so we just update it ad-hoc to anew commit when we want or need to.\n\nThis one is trickier. What we do is look for new commits, and raise an alert if there is a new commit that is older than N days (where you pick N.) So for example, We're at revision 5 of libfoo, and on Mar 1 revision 6 comes out. On Mar 16th (assuming a 15 day delay) we would open a bug about Revision 6. \n\n### Commit by Commit File by File\n\nSometimes we import parts of a library that's really big. Like when we took parts of fdlibm from FreeBSD. We don't want to alert on every commit made to FreeBSD! So instead we check if there's a newer commit on the specific files we imported.  (We can do this if the list of files isn't too large - it really slows down the version check.)\n\n### Other\n\nIf you need something that isn't here, we can talk about it and figure out how hard it would be to build.\n\n# hook\n\nhttps://tools.taskcluster.net/hooks/#project-releng/misc-third-party-library-alert-service\n"
},
{
  "name": "passwords",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "elm-package.json",
      "package.json",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# Password manager\n\n\n## Setup\n\nNode and Elm v0.18 should be installed on your system. Then:\n\n```\n$ npm install\n```\n\n## Live server\n\nThis will recompile and reload your app in the browser evertyme you update a\n`.elm` file. App is available at [localhost:8000/](http://localhost:8000/).\n\n```\n$ npm run live\n```\n\nThere's also very convenient debug version with the Elm debugger:\n\n```\n$ npm run debug\n```\n\n## Deploying to gh-pages\n\n```\n$ npm run publish-to-gh-pages\n```\n\n## Tests\n\n```\n$ npm test\n```\n\nTo have tests automatically run on each file change:\n\n```\n$ npm tdd\n```\n"
},
{
  "name": "deployment-pipeline",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# deployment-pipeline"
},
{
  "name": "shavar-plugin-blocklist",
  "files": {
    "/": [
      ".gitignore",
      "CONTRIBUTING.md",
      "LICENSE-CC0",
      "LICENSE-GPL3",
      "README.md",
      "flash.txt",
      "flashallow.txt",
      "flashallowexceptions.txt",
      "flashexceptions.txt",
      "flashinfobar.txt",
      "flashsubdoc.txt",
      "flashsubdocexceptions.txt",
      "mozplugin-block.txt",
      "mozplugin2-block.txt",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# Firefox plugin blocklist\n\nFirefox uses this plugin blocklist to determine which plugin content should be prevented from loading. See the following bugs for more information:\n\n* [Firefox bug 1237198](https://bugzilla.mozilla.org/show_bug.cgi?id=1237198) for the SWF blocklist.\n* [Firefox bug 1307604](https://bugzilla.mozilla.org/show_bug.cgi?id=1307604) for the 1st-party and 3rd-party domain blocklist.\n* [Firefox bug 1369160](https://bugzilla.mozilla.org/show_bug.cgi?id=1369160) for the 1st-party infobar suppressing list.\n\n### SWF blocklist ###\n\nThis blocklist is small and covers some Flash SWF files. The criteria for adding content to the blocklist are:\n\n* Blocking the content will not be noticeable to the Firefox user.\n* It is possible to reimplement the basic functionality of the content in HTML without Flash.\n\nTo create the blocklist, we used a web crawler to analyze the SWFs on the home pages of the Alexa Top 10,000 websites. We will periodically repeat this crawl and add new entries to the list which match our criteria. We categorized SWFs as fingerprinting SWFs if they were smaller than 5x5 pixels and called Flash's `enumerateFonts()` and `ExternalInterface` APIs to obtain a list of all fonts unique to the user\u2019s machine. We categorized SWFs as \"supercookie\" SWFs if they were smaller than 5x5 pixels, called Flash\u2019s `SharedObject` API, and contained the string \"cookie\". In future versions of Firefox, we will expand the list to cover more types of plugin content.\n\nmozplugin-block.txt is a plain text list of suffix/prefix expressions as documented in Google's [Safe Browsing API Developer's Guide](https://developers.google.com/safe-browsing/v4/urls-hashing#canonicalization). Lines beginning with a `#` character are comments. The blocklist is imported into Mozilla's Shavar service and then distributed to Firefox clients (as Shavar table `mozplugin-block-digest256`).\n\n### Domain blocklist ###\n\nThese blocklists control the automatic blockling of Flash from certain domains, which can be configured to be blocked only when in a 3rd-party context (i.e., subframe from a different origin), or also when in a 1st-party context (top-level webpage or any iframe). This blocking is controlled by the option \"Block dangerous and intrusive Flash content\" in the Plugins settings.\n\nThe 3rd-party domain blocklist (flashsubdoc.txt) is based in part on one which is Copyright 2010-2016 Disconnect, Inc., to whom we are most grateful. This file is licensed under the GNU General Public License version 3 (see LICENSE-GPL3).\n\n### Infobar suppressing ###\n\nThis list contains websites that are not blocked from using Flash, but to which we'll avoid displaying the Flash infobar to the user. The user can still access the Flash content by clicking on a Flash element with a visible surface area, or by clicking on the Plugin icon in the URL bar.\n\n## LICENSE ##\n\nAll files on this repository, with the exception of flashsubdoc.txt, are licensed under the Creative Commons CC0 1.0 Universal (see LICENSE-CC0). The file flashsubdoc.txt is licensed under the GPLv3 (see LICENSE-GPL3)."
},
{
  "name": "switchboard-experiments-sync",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "config.yaml",
      "validate_and_sync_json.py"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "fxa-devmgr-server",
  "files": {
    "/": [
      ".gitignore",
      "CHANGELOG.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "development.ini",
      "devmgr",
      "doc-requirements.txt",
      "docs",
      "production.ini",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "index.rst",
      "make.bat"
    ]
  },
  "makefile": "SHELL := /bin/sh\n\nHERE = $(shell pwd)\nBIN := $(HERE)/bin\nPATH := $(BIN):$(PATH)\n\nPYTHON := $(BIN)/python\nVTENV_OPTS ?= -p `which python3.4 | head -n 1`\nVIRTUALENV = virtualenv\n\nBUILD_DIRS = bin include lib src .tox .eggs .coverage\n\n.PHONY: all build clean\n\nall: build\n\n$(PYTHON):\n\t$(VIRTUALENV) $(VTENV_OPTS) .\n\nbuild: $(PYTHON)\n\t$(PYTHON) setup.py develop\n\nclean:\n\trm -rf $(BUILD_DIRS)\n",
  "readme": "# Firefox Accounts Device Manager\n\nThe device manager associates a Firefox Account with a user's \"Foxes:\" Firefox OS, Android, iOS, and Desktop. The service allows Firefox clients to register themselves, and exposes an API for other services to query an authenticated user's devices. During registration, the device specifies its name and type, as well as a push endpoint for other services to address it. This may be extended to support individual applications on the device, which will allow services to send broadcast and multicast push messages. The device manager also provides a remote logout mechanism that allows a user to invalidate a session on a lost or stolen device.\n\nThe Firefox client (\"device\") API requires a BrowserID assertion for registration, and issues a long-lived Hawk session token once registration is complete. The client uses this session token to update its information, either when the user changes the device name, or when its push endpoint changes. The service (\"user\") API is authenticated with an OAuth bearer token, and allows relying services to retrieve all device information for a user's account.\n\nTwo examples of relying services are the Firefox Accounts content server and Find My Device. The content server could use the API to display a user's active devices, and allow her to remotely log out of each. Likewise, Find My Device can use push notifications and remote logout to ring or lock a lost device.\n\n## Installation\n\nThe server is built with [Tornado](https://tornado.readthedocs.org/), and requires Python >= 3.4. To check out a working copy of the source and install dependencies:\n\n    git clone git@github.com:kitcambridge/fxa-device-manager-server.git\n    cd fxa-device-manager-server\n    make\n\n### OS X\n\nThe server depends on the Python [cryptography](https://cryptography.io/en/latest/installation) library, which requires OpenSSL. If you're building the device manager on OS X with a custom version of OpenSSL, you'll need to set the `ARCHFLAGS` environment variable, and add your OpenSSL library path to `LDFLAGS` and `CFLAGS` before running `make build`:\n\n    export ARCHFLAGS=\"-arch x86_64\"\n    # Homebrew installs OpenSSL to `/usr/local/opt/openssl` instead of\n    # `/usr/local`.\n    export LDFLAGS=\"-L/usr/local/lib\" CFLAGS=\"-I/usr/local/include\"\n\n## License\n\nMPL2.\n"
},
{
  "name": "sync11-eol-server",
  "files": {
    "/": [
      ".gitignore",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "deployment.ini",
      "requirements.txt",
      "setup.py",
      "sync11eol"
    ]
  },
  "makefile": "VIRTUALENV = virtualenv\nNOSE = local/bin/nosetests\nPYTHON = local/bin/python\nPIP = local/bin/pip\nFLAKE8 = local/bin/flake8\nPIP_CACHE = /tmp/pip-cache.${USER}\nBUILD_TMP = /tmp/syncstorage-build.${USER}\nPYPI = https://pypi.python.org/simple\n\nINSTALL = $(PIP) install -i $(PYPI)\n\n\n.PHONY: all build test clean\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages --distribute ./local\n\t$(INSTALL) -U Distribute\n\t$(INSTALL) -U -r requirements.txt\n\t$(PYTHON) ./setup.py develop\n\n\ntest:\n\t$(INSTALL) -q nose flake8\n\t$(FLAKE8) sync11eol\n\t$(NOSE) sync11eol/tests\n\nserve:\n\t./local/bin/gunicorn --paster ./deployment.ini --worker-class gevent\n\nclean:\n\trm -rf ./local *.egg-info\n",
  "readme": "==========================\nSync1.1 End-of-Life Server\n==========================\n\nThis application implements a simple memcached-backed server to send\n\"end-of-life\" notifications to Firefox Sync clients.\n\nIt provides a tiny portion of the Sync1.1 API as documented here:\n\n    https://docs.services.mozilla.com/storage/apis-1.1.html\n\nIt allows reads from \"/info/collections\", reads/writes to \"meta/global\" and\nand \"meta/fxa_credentials\" and \"crypto/keys\", and deletes to \"/storage\".  All\nother requests are rejected with a special \"513 SERVICE EOL\" error that should\ntrigger old sync clients to show a service-deprecation message to the user.\n\nIn theory, this is enough to allow clients to show appropriate messaging\nto the user and to trigger an offer of the guided sync upgrade flow.  The\nallowed requests are designed to work around a client bug, where the EOL\nmessaging is not shown if it is \"trumped\" by an error at certain early stages\nof the sync sequence.  See the following bug for more details:\n\n    https://bugzilla.mozilla.org/show_bug.cgi?id=1017443\n\nTo build in a local virtualenv, simply do::\n\n    make build\n\nTo deploy, make sure you've got memcached running and do::\n\n    make serve\n"
},
{
  "name": "ivy-repo",
  "files": {
    "/": [
      "com.crankycoder",
      "index.html"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "msisdn-cli",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "VERSION",
      "msisdn_cli",
      "setup.py"
    ]
  },
  "makefile": "VENV := $(shell echo $${VIRTUAL_ENV-.venv})\nINSTALL_STAMP=$(VENV)/.install.stamp\n\ninstall: $(INSTALL_STAMP)\n\n$(INSTALL_STAMP): setup.py\n\tvirtualenv $(VENV)\n\t$(VENV)/bin/pip install -e ./\n\ttouch $(INSTALL_STAMP)\n\nclean:\n\trm -fr $(VENV)\n",
  "readme": "msisdn-cli\n==========\n\nThis little CLI tool let you validate both MT and MOMT flow using\nMSISDN server.\n\nTo start a MT Flow::\n\n    msisdn-cli -H https://msisdn.services.mozilla.com \\\n        -c 310 -n +1xxxxxxxxxxx\n\nTo start a MOMT Flow::\n\n    msisdn-cli -H https://msisdn.services.mozilla.com -c 310\n\nYou can also create a certificate to then try to register on a Service\nProvider such as Loop.\n\nTo do so add an --audience parameter and a --login-endpoint you can\nalso provide some --data or --json to be POST with your registration\nrequest::\n\n    msisdn-cli -H https://msisdn.services.mozilla.com -c 310 -n +1xxxxxxxxxxx \\\n               --audience https://loop.services.mozilla.com\n               --login-endpoint https://loop.services.mozilla.com/registration\n               --json '{\"simplePushURL\": \"http://httpbin.org/deny\"}'\n\nIf you want you can just display the cURL command using --dry-run::\n\n    msisdn-cli -H https://msisdn.services.mozilla.com -c 310 -n +1xxxxxxxxxxx \\\n               --audience https://loop.services.mozilla.com\n               --login-endpoint https://loop.services.mozilla.com/registration\n               --json '{\"simplePushURL\": \"http://httpbin.org/deny\"}' --dry-run\n\nThe msisdn-cli script will then build you an assertion and write\ndown a curl command to run to make sure it works.\n\nYou can also use the -v, --verbose command to display the assertion.\n\nYou should get a 200 OK status code with a Hawk-Session-Token header.\n\nIf not, here are the error messages you can get:\n\n- \"Certificate expired\": you play too long with this curl command,\n                         ask for a new certificate\n\n- \"Invalid audience\":    The Service Provider doesn't accept this audience\n                         It can be either a misconfiguration on the server or\n                         you trying the assertion to a wrong server.\n\n- \"Issuer not trusted\":  The MSISDN server that generate your certificate\n                         is not trusted on this Service Provider.\n                         It can be either a misconfiguration or\n                         you trying the assertion to a wrong server.\n\n- Something else? Please make a PR to add it here.\n\nDon't forget to use :code:`msisdn-cli -h` to get more help.\n\n\nINSTALL\n-------\n\n::\n\n    make install\n    source .venv/bin/activate\n"
},
{
  "name": "tiles-loadtest",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.md",
      "config",
      "loadtest.py",
      "requirements.txt",
      "siege",
      "test-server.js"
    ]
  },
  "makefile": "SERVER_URL = http://127.0.0.1:9999\n\n# Hackety-hack around OSX system python bustage.\n# The need for this should go away with a future osx/xcode update.\nARCHFLAGS = -Wno-error=unused-command-line-argument-hard-error-in-future\n\nINSTALL = ARCHFLAGS=$(ARCHFLAGS) ../local/bin/pip install\n\n.PHONY: build test bench\n\nbuild:\n\t$(INSTALL) pexpect\n\t$(INSTALL) https://github.com/mozilla-services/loads/archive/master.zip\n\n# Run a single test from the local machine, for sanity-checking.\ntest:\n\t./bin/loads-runner --config=./config/test.ini --server-url=$(SERVER_URL) loadtest.TilesRecordDataTest.test_justload\n\n# Run a fuller bench suite from the local machine.\nbench:\n\t./bin/loads-runner --config=./config/bench.ini --server-url=$(SERVER_URL) loadtest.TilesRecordDataTest.test_justload\n\n# Run a full bench, by submitting to broker in AWS.\nmegabench:\n\t./bin/loads-runner --config=./config/megabench.ini --user-id=$(USER) --server-url=$(SERVER_URL) loadtest.TilesRecordDataTest.test_justload\n\n",
  "readme": "About\n=====\n\nLoad testing for the Mozilla Tiles project using the [loads](https://github.com/mozilla-services/loads) \ntesting framework.\n\nInstallation\n------------\n\n1. `virtualenv .`\n1. `./bin/pip install -r requirements.txt`\n\nRunning it\n----------\n\n* `./bin/loads-runner loadtest.TilesRecordDataTest.test_click`\n\nTODO\n----\n\n* Makefile for encapsulating load testing configs / logic\n* send actual fake data to api endpoints\n* send good/bad (malformed) data to api endpoints\n* create configs to test load at: 100 req/sec, 1000 req/sec ... 100K req/sec\n"
},
{
  "name": "express-hawkauth",
  "files": {
    "/": [
      ".jshintrc",
      ".travis.yml",
      "CHANGELOG",
      "README.md",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# Hawk authentication for ExpressJS\n\n[![NPM version](https://badge.fury.io/js/express-hawkauth.svg)]\n(https://www.npmjs.org/package/express-hawkauth)\n[![Build Status](https://travis-ci.org/mozilla-services/express-hawkauth.svg?branch=master)]\n(https://travis-ci.org/mozilla-services/express-hawkauth)\n\nThis module provides an [Hawk](https://github.com/hueniverse/hawk)\nauthentication middleware for express applications.  More specifically, for\napplications which uses the connect middleware facility.\n\nHawk itself does not provide any mechanism for obtaining or transmitting the\nset of shared credentials required, but this project proposes a scheme we use\naccross mozilla-services projects.\n\n## Installation\n\n    npm install express-hawkauth\n\n## How do I plug that in my application?\n\nIn order to plug express-hawk in your application, you'll need to use it as\na middleware.\n\n    var express = require(\"express\");\n    var hawk = require(\"express-hawkauth\");\n    app = express();\n\n    var hawkMiddleware = hawk.getMiddleware({\n      hawkOptions: {},\n      getSession: function(tokenId, cb) {\n        // A function which pass to the cb the key and algorithm for the\n        // given token id. First argument of the callback is a potential\n        // error.\n        cb(null, {\n          key: \"key\",\n          algorithm: \"sha256\"\n        });\n      },\n      createSession: function(id, key, cb) {\n        // A function which stores a session for the given id and key.\n        // Argument returned is a potential error.\n        cb(null);\n      },\n      setUser: function(req, res, credentials, cb) {\n\n        // A function that uses req and res and the credentials so\n        // that it can tweak it. For instance, you can store the tokenId\n        // as the user.\n\n        req.user = credentials.id;\n      }\n    });\n\n    app.get('/hawk-enabled-endpoint', hawkMiddleware);\n\nYou can also pass a `sendError` parameter which is a function that's being\npassed the errors generated by the library. Takes (res, status, payload) as\nparameters.\n\nIf you want to only check a valid hawk session exists (without creating a new\none), just create a middleware which doesn't have any `createSession` parameter\ndefined.\n\n## What's returned to the clients\n\nIn case an hawk session is created (e.g. when createSession had been defined\nand no credentials were provided in the request, a `Hawk-Session-Token` header\nwill be set to the response, containing the session token to be derived.\n\n## How are the shared credentials shared?\n\nOkay, on to the actual details.\n\nThe server gives you a session token, that you'll need to derive to get the\nhawk credentials:\n\nDo an HKDF derivation on the given session token. You\u2019ll need to use the\nfollowing parameters::\n\n    key_material = HKDF(hawk_session, \u201c\u201d, \u2018identity.mozilla.com/picl/v1/sessionToken\u2019, 32*2);\n\nThe key material you\u2019ll get out of the HKDF need to be separated into two\nparts, the first 32 hex characters are the hawk id, and the next 32 ones are the\nhawk key:\n\n    credentials = {\n        'id': keyMaterial[0:32]\n        'key': keyMaterial[32:64]\n        'algorithm': 'sha256'\n    }\n"
},
{
  "name": "hermes",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "specs"
    ]
  },
  "makefile": null,
  "readme": "hermes\n======\n"
},
{
  "name": "msisdn-gateway-l10n",
  "files": {
    "/": [
      "README.md",
      "locale",
      "package.json",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "msisdn-gateway-l10n\n===================\n\nThis repo (abbreviated as \"L10N\" in this README) contains all translated/translatable strings for the msisdn-gateway repo (abbreviated as \"SOURCE\"). It contains an exact copy of the locales/ directory from SOURCE, and nothing else (except for this README).\n\nThe moz-verbatim translation team works with this L10N repo, pushing changes into it any time they like.\n\nOn a regular basis, somebody on the SOURCE committers team copies everything from L10N into SOURCE/locale/, reviews the changes, then commits and pushes SOURCE. This makes the new translated strings available to SOURCE users, provides a clean (single-commit) revision history, and prevents non-locale/ changes from leaking from one repo to the other. The SOURCE repo contains a script (scripts/import_locale) to perform this update.\n\nOn a (different) regular basis, a SOURCE committer will update in the other direction. This starts with running the SOURCE repo's grunt \"l10n-extract\" task, which updates the *.pot files in SOURCE. The committer then copies the *.pot files from SOURCE into a checkout of L10N. Then they run ./scripts/merge_po.sh in L10N, which updates the *.po files in L10N. Then they should commit and push to L10N. That will update the set of translatable strings for the l10n team to work on.\n"
},
{
  "name": "omxen",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.md",
      "omxen",
      "setup.py"
    ]
  },
  "makefile": "HERE = $(shell pwd)\nBIN = $(HERE)/bin\nPYTHON = $(BIN)/python\n\nINSTALL = $(BIN)/pip install --no-deps\nVTENV_OPTS ?= --distribute -p `which python2.7 python2.6 | head -n 1`\n\nBUILD_DIRS = bin build include lib lib64 man share\n\n\n.PHONY: all test docs build_extras\n\nall: build\n\n$(PYTHON):\n\tvirtualenv $(VTENV_OPTS) .\n\nbuild: $(PYTHON)\n\t$(PYTHON) setup.py develop\n\nclean:\n\trm -rf $(BUILD_DIRS)\n\ntest:\n\t$(PYTHON) setup.py test\n\nrunserver:\n\t$(BIN)/omxen\n",
  "readme": "Omxen\n-----\n\nFakes an SMS gateway.\n\n\nInstallation\n------------\n\nTo run Omxen, make sure you have python-devel, python, python-virtualenv and the usual build tools (gcc, Make)\n\nThen:\n\n    $ virtualenv .\n    $ bin/python setup.develop\n    \nTo run it:\n\n    $ bin/omxen\n    \n    \nThen visit: http://localhost:8080 to check that the service is up\n\n\n\n"
},
{
  "name": "express-logging",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      "AUTHORS",
      "CHANGELOG",
      "LICENSE",
      "Makefile",
      "README.rst",
      "demo.js",
      "index.js",
      "package.json"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nNODE_LOCAL_BIN=./node_modules/.bin\n\n.PHONY: test\ntest: lint\n\ninstall:\n\t@npm install\n\n.PHONY: lint\nlint: jshint\n\nclean:\n\trm -rf node_modules\n\n.PHONY: jshint\njshint:\n\t@$(NODE_LOCAL_BIN)/jshint index.js\n\n.PHONY: mocha\nmocha:\n\t@$(NODE_LOCAL_BIN)/mocha test.js --reporter spec\n\n.PHONY: demo\ndemo:\n\tnode demo.js\n",
  "readme": "===============\nexpress-logging\n===============\n\nexpress-logging is a middleware for express.js that displays all req/res in the console.\n\nIt can be really useful for API development.\n\nFor instance::\n\n    [14/Apr/18 13:37:16] \"GET / HTTP/1.1\" 200 219 \u2014 (2 ms)\n    [14/Apr/18 13:37:16] \"GET /favicon.ico HTTP/1.1\" 404  \u2014 (1 ms)\n    [14/Apr/18 13:37:22] \"POST /register HTTP/1.1\" 200 94 \u2014 (5 ms)\n    [14/Apr/18 13:37:23] \"GET / HTTP/1.1\" 304  \u2014 (1 ms)\n\nGreat, how do I install that?\n=============================\n\nYou have to install the middleware in your express application.\n\n.. code-block:: javascript\n\n    var express = require('express');\n    var logging = require('express-logging');\n\n    var app = express();\n    var app.use(logging);\n\nThat's all, folks!\n"
},
{
  "name": "connect-validation",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      "AUTHORS",
      "CHANGELOG",
      "LICENSE",
      "Makefile",
      "README.rst",
      "demo.js",
      "index.js",
      "package.json",
      "test.js"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nNODE_LOCAL_BIN=./node_modules/.bin\n\n.PHONY: test\ntest: lint mocha\n\ninstall:\n\t@npm install\n\n.PHONY: lint\nlint: jshint\n\nclean:\n\trm -rf node_modules\n\n.PHONY: jshint\njshint:\n\t@$(NODE_LOCAL_BIN)/jshint test.js\n\n.PHONY: mocha\nmocha:\n\t@$(NODE_LOCAL_BIN)/mocha test.js --reporter spec\n\n.PHONY: demo\ndemo:\n\tnode demo.js\n",
  "readme": "==================\nconnect-validation\n==================\n\nconnect-validation is a middleware for connect.js (so you can use it with\nexpress applications) that helps handling `400 BAD REQUEST` errors on JSON\nAPIs, in a deterministic way.\n\nSpecifically, the top-level JSON object in the response will always contain a\nkey named \"status\", which maps to a string identifying the cause of the error.\nUnexpected errors will have a status string of \u201cerror\u201d; errors expected as part\nof the protocol flow will have a specific status.\n\nErrors will have three different keys:\n\n- location is the location of the error. It can be \u201cquerystring\u201d, \u201cheader\u201d ,\n  \u201curl\u201d or \u201cbody\u201d;\n- name is the eventual name of the value that caused problem;\n- description is a description of the problem encountered.\n\nFor instance, that would be something like::\n\n    {\n      \"status\": \"errors\",\n      \"errors\": [{\"location\": \"body\",\n                  \"name\": \"version\",\n                  \"description\": \"version should be an integer\"\n                 }]\n    } \n\nGreat, how do I install that?\n=============================\n\nYou have to install the middleware in your express application.\n\n.. code-block:: javascript\n\n    var express = require('express');\n    var validationMiddleware = require('connect-validation');\n\n    var app = express();\n    var app.use(validationMiddleware);\n\n\nOnce installed, `addError` and `sendError` methods will be available on all\nResponse objects.\n\n\nHow to use it?\n==============\n\nOnce installed, you can use these two methods whenever you need to send back\nerrors to the client.  You can add as many errors as you want by using the\n`req.addError(location, name, description)` method.\n\nAt the end of the validation, use `sendError()` to build the 400 errors response.\n\n.. code-block:: javascript\n\n    app.get('/', function (req, res) {\n      if (!req.query.hasOwnProperty('id')) {\n        res.addError(\"querystring\", \"id\", \"missing; id\");\n        res.sendError();\n        return;\n      }\n    });\n\nIf you have only one error, you can use `sendError` directly.\n\n.. code-block:: javascript\n\n    app.get('/', function (req, res) {\n      if (!req.query.hasOwnProperty('id')) {\n        res.sendError(\"querystring\", \"id\", \"missing; id\");\n        return;\n      }\n    });\n\nThat's all, folks!\n"
},
{
  "name": "sync-admin-server",
  "files": {
    "/": [
      "README.md",
      "SyncNodeManager"
    ]
  },
  "makefile": null,
  "readme": "sync-admin-server\n=================\n\nadmin server scripts for managing sync storage nodes\n"
},
{
  "name": "pyramid_hawkauth",
  "files": {
    "/": [
      ".gitignore",
      ".pylintrc",
      "CHANGES.txt",
      "CONTRIBUTORS.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "pyramid_hawkauth",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": "# -*- coding: utf-8; mode: makefile-gmake -*-\n\n.DEFAULT = help\nTEST ?= .\n\n# python version to use\nPY       ?=3\n# list of python packages (folders) or modules (files) of this build\nPYOBJECTS = pyramid_hawkauth\n# folder where the python distribution takes place\nPYDIST   ?= dist\n# folder where the python intermediate build files take place\nPYBUILD  ?= build\n\nSYSTEMPYTHON = `which python$(PY) python | head -n 1`\nVIRTUALENV   = virtualenv --python=$(SYSTEMPYTHON)\nVENV_OPTS   = \"--no-site-packages\"\nTEST_FOLDER  = ./pyramid_hawkauth/tests\n\nENV     = ./local/py$(PY)\nENV_BIN = $(ENV)/bin\n\n\nPHONY += help\nhelp::\n\t@echo  'usage:'\n\t@echo\n\t@echo  '  build     - build virtualenv ($(ENV)) and install *developer mode*'\n\t@echo  '  lint      - run pylint within \"build\" (developer mode)'\n\t@echo  '  test      - run tests for all supported environments (tox)'\n\t@echo  '  dist      - build packages in \"$(PYDIST)/\"'\n\t@echo  '  publish   - upload \"$(PYDIST)/*\" files to PyPi'\n\t@echo  '  clean\t    - remove most generated files'\n\t@echo\n\t@echo  'options:'\n\t@echo\n\t@echo  '  PY=3      - python version to use (default 3)'\n\t@echo  '  TEST=.    - choose test from $(TEST_FOLDER) (default \".\" runs all)'\n\t@echo\n\t@echo  'Example; a clean and fresh build (in local/py3), run all tests (py27, py35, lint)::'\n\t@echo\n\t@echo  '  make clean build test'\n\t@echo\n\n\nPHONY += build\nbuild: $(ENV)\n\t$(ENV_BIN)/pip -v install -e .\n\n\nPHONY += lint\nlint: $(ENV)\n\t$(ENV_BIN)/pylint $(PYOBJECTS) --rcfile ./.pylintrc\n\nPHONY += test\ntest:  $(ENV)\n\t$(ENV_BIN)/tox -vv\n\n$(ENV):\n\t$(VIRTUALENV) $(VENV_OPTS) $(ENV)\n\t$(ENV_BIN)/pip install -r requirements.txt\n\n# for distribution, use python from virtualenv\nPHONY += dist\ndist:  clean-dist $(ENV)\n\t$(ENV_BIN)/python setup.py \\\n\t\tsdist -d $(PYDIST)  \\\n\t\tbdist_wheel --bdist-dir $(PYBUILD) -d $(PYDIST)\n\nPHONY += publish\npublish: dist\n\t$(ENV_BIN)/twine upload $(PYDIST)/*\n\nPHONY += clean-dist\nclean-dist:\n\trm -rf ./$(PYBUILD) ./$(PYDIST)\n\n\nPHONY += clean\nclean: clean-dist\n\trm -rf ./local ./.cache\n\trm -rf *.egg-info .coverage\n\trm -rf .eggs .tox html\n\tfind . -name '*~' -exec echo rm -f {} +\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name __pycache__ -exec rm -rf {} +\n\n# END of Makefile\n.PHONY: $(PHONY)\n",
  "readme": "================\npyramid_hawkauth\n================\n\nThis is a Pyramid authenitcation plugin for Hawk Access Authentication:\n\n    https://npmjs.org/package/hawk\n\nTo access resources using Hawk Access Authentication, the client must have\nobtained a set of Hawk credentials including an id and secret key.  They use\nthese credentials to make signed requests to the server.\n\nWhen accessing a protected resource, the server will generate a 401 challenge\nresponse with the scheme \"Hawk\" as follows::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n\n    < HTTP/1.1 401 Unauthorized\n    < WWW-Authenticate: Hawk\n\nThe client will use their Hawk credentials to build a request signature and\ninclude it in the Authorization header like so::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n    > Authorization: Hawk id=\"h480djs93hd8\",\n    >                     ts=\"1336363200\",\n    >                     nonce=\"dj83hs9s\",\n    >                     mac=\"bhCQXTVyfj5cmA9uKkPFx1zeOXM=\"\n\n    < HTTP/1.1 200 OK\n    < Content-Type: text/plain\n    <\n    < For your eyes only:  secret data!\n\n\nThis plugin uses the tokenlib library for verifying Hawk credentials:\n\n    https://github.com/mozilla-services/tokenlib\n\nIf this library does not meet your needs, you can provide a custom callback\nfunction to decode the Hawk id token.\n"
},
{
  "name": "gecko-dom-presence",
  "files": {
    "/": [
      "README.rst",
      "moz.build",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "Gecko Presence Service\n======================\n\nBuilding\n--------\n\nTo add this service in your B2G build, insert this directory in <B2G>/gecko/dom as\n\"presence\".\n\ne.g.::\n\n    $ cd B2G/gecko/dom\n    $ ln -s /path/to/repos/gecko-dom-presence presence\n\nThen, add those lines into gecko/b2g/installer/package-manifest.in ::\n\n    @BINPATH@/components/Presence.js\n    @BINPATH@/components/Presence.manifest\n    @BINPATH@/components/PresenceServiceLauncher.js\n\nUnder @BINPATH@/components/PushServiceLauncher.js\n\nRebuild & Flash::\n\n    $ cd B2G\n    $ ./build.sh\n    $ ./flash.sh\n\n\nActivation\n----------\n\nAdd a **custom-prefs.js** file inside your gaia/build directory with this content::\n\n    user_pref(\"services.presence.serverURL\", \"ws://presence.ziade.org/presence\");\n    user_pref(\"services.presence.enabled\", true);\n    user_pref(\"services.presence.connection.enabled\", true);\n    user_pref(\"services.presence.debug\", true);\n\nThis will activate the service.\n\nReset Gaia::\n\n    $ cd gaia\n    $ make reset-gaia\n\n"
},
{
  "name": "presence",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.md",
      "dpresence",
      "presence.ini",
      "setup.py"
    ]
  },
  "makefile": "build:\n\tvirtualenv --no-site-packages .\n\tbin/python setup.py develop\n\tbin/pip install circus\n\n",
  "readme": "Mozilla Presence\n================\n\nThis prototype implements https://wiki.mozilla.org/Services/Presence\n\nRead the terminology at : https://wiki.mozilla.org/Services/Presence#Terminology\n\nThe server provides:\n\n  * **Device Presence Channel**: used by *users* to send presence updates and\n    get live notifications.\n  * **Registration**: used by *AppDeveloper* to register applications\n  * **AppService Presence Channel**: used by *AppService* to get presence\n    updates and send live notifications.\n  * **Permissions**: used to *users* manage applications permissions\n\n\nDevice Presence Channel\n-----------------------\n\nThe Device Presence Channel is a web socket at *wss://server/presence*\n\nIt's a channel of communications between a phone/browser device and Mozilla\nPresence that carries live notifications from Presence and presence data to\nPresence.\n\n### Presence updates\n\nThe device can update its presence by sending a JSON mapping containing 3\nfields:\n\n  * **email** - the Persona e-mail\n  * **assertion** - a valid Persona assertion\n  * **status** - the status: 'online', 'offline' or 'unavailable'\n\nExample:\n\n\t{'email': 'tarek@mozilla.com',\n\t 'assertion': '<valid persona assertion>',\n\t 'status': 'online'}\n\nFor every presence status received, the server sends back an ACK message.\n\nExample:\n\n    {'result': 'OK'}\n\n\nIn case of an error, the server may send back an extra 'errno' field with an\nerror code (codes to be documented) and an optional 'error' message.\n\nExample:\n\n    {'result': 'KO', 'errno': 34, 'error': 'Invalid assertion'}\n\nThe user can send as many updates as it wants, but the server can ask it to slow\ndown with a specific error code (codes to be documented)\n\nThe server or the user can disconnect from the socket at any time and for any\nreason. The number of socket connections is limited to one connection per device\nand per email.\n\n### Live notifications\n\nThe Presence server can send live notifications to the device.\nWhen the server has some notifications pending for the device, they\nwill all be sent at once whenever the device is connected.\n\nLive notifications are sent as a JSON mapping containing 1 key:\n\n  * **notifications**: a list of notifications\n\nA notification is a mapping with 3 keys:\n\n  * **source**: the sender -- usually an e-mail\n  * **message**: the message\n  * **action**: the action -- can be an URL\n\nOnce the notifications are sent through the channel, they are discarded\nfrom the server.\n\n\n\nRegistration\n------------\n\nXXX\n\nAppService Presence Channel\n---------------------------\n\n\nThe AppService Channel is a web socket at *wss://server/myapps/<appid>*\n\nIt's a channel of communication between Mozilla Presence and an AppService that\ncarries presence updates for users sharing their presence with the AppService\nand live notifications from the AppService to be delivered to a PUID.\n\nXXX\n\n\nPermissions API\n---------------\n\nA user may grant an application to see their presence by performing a POST\ncall with the application unique identifier as provided by the application.\n\nThe server will keep track of this choice.\n\n\tPOST /grant/<appid>\n\n\tXXX\n\n\n"
},
{
  "name": "squid-rpm",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "SOURCES",
      "SPECS",
      "build-rpm.sh"
    ]
  },
  "makefile": null,
  "readme": "About\n-----\n\nThis captures building of our Squid 3.3.5 RPM. \n\nMuch of this work is derived from the unofficial packages\navailable [here](http://repo.ngtech.co.il/rpm/centos/6/x86_64/SRPM/).\n\nPrerequisites\n=============\n\n* [mock](http://fedoraproject.org/wiki/Projects/Mock) must be installed\n\nUsage\n=====\n\n    ./build-rpm.sh\n\nIf everything runs successfully the RPM you are looking for will be \nin the `RPM/` sub-directory.\n"
},
{
  "name": "nodejs-rpm-creator",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "create-node-rpm.sh",
      "specs"
    ]
  },
  "makefile": null,
  "readme": "# About\n\nA *simple* one line command to build a node.js RPM. \n\n## Usage and Example Output\n\n    > ./create-node-rpm.sh 0.10.21\n    2013 11 05 12:01:20 Fetching http://nodejs.org/dist/v0.10.21/node-v0.10.21.tar.gz\n    2013 11 05 12:01:39 Download OK. SHA1 sums match: b7fd2a3660635af40e3719ca0db49280d10359b2\n    2013 11 05 12:01:39 Building SRPM\n    2013 11 05 12:01:55 Building Node.js RPM\n\n## Prerequisites \n\n* [mock](http://fedoraproject.org/wiki/Projects/Mock) - builds RPM packages in a chroot\n* RHEL/SL/Centos 6.3+\n\n## Details\n\nThe `spec/` contains spec files for building specific versions of nodejs. These \nare used to generated an SRPM package (also using mock). Then the SRPM is used \nto build a node.js binary RPM. \n\nOutput goes into the `./BUILD` sub-directory (automatically created if missing). \nOnce a build has completed, simply put the RPM in `./BUILD/RPMS/` into the appropriate\nrepo for usage. \n"
},
{
  "name": "loads.js",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      "README.md",
      "loadsjs",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "# Loads.js\n\nA library for writing JavaScript loadtests that can integrate with the\n[loads](http://loads.rtfd.org) framework.\n\n## Installation\n\nLoads.js is packaged for npm; just do:\n\n```\n$ npm install /path/to/loads.js\n```\n\n## Usage\n\nXXX TODO WRITE ME\n"
},
{
  "name": "loads",
  "files": {
    "/": [
      ".coveragerc",
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "conf",
      "docs",
      "examples",
      "loads",
      "setup.py",
      "sitecustomize.py",
      "slaves.ini",
      "test-requirements.txt",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "make.bat",
      "requirements.txt",
      "source"
    ]
  },
  "makefile": "HERE = $(shell pwd)\nBIN = $(HERE)/bin\nPYTHON = $(BIN)/python\n\nINSTALL = $(BIN)/pip install --no-deps\nVTENV_OPTS ?= --distribute -p `which python2.7 python2.6 | head -n 1`\n\nBUILD_DIRS = bin build include lib lib64 man share\n\n\n.PHONY: all test docs build_extras\n\nall: build\n\n$(PYTHON):\n\tvirtualenv-2.7 $(VTENV_OPTS) .\n\nbuild: $(PYTHON)\n\tCYTHON=`pwd`/bin/cython $(BIN)/pip install -r test-requirements.txt\n\t$(PYTHON) setup.py develop\n\nbuild_extras: build\n\t$(BIN)/pip install circus\n\nclean:\n\trm -rf $(BUILD_DIRS)\n\ntest:\n\t$(BIN)/pip install tox\n\t$(BIN)/tox\n\ncoverage: build build_extras\n\t$(BIN)/pip install nose coverage circus mock flake8 paramiko boto unittest2\n\t$(BIN)/flake8 loads\n\tLONG=1 $(BIN)/nosetests -s -d -v --cover-html --cover-html-dir=html --with-coverage --cover-erase --cover-package loads loads/tests\n\nbin/sphinx-build:\n\tbin/pip install Sphinx mozilla_sphinx_theme\n\n\ndocs:  bin/sphinx-build\n\tcd docs; make html\n",
  "readme": "=====\nLoads\n=====\n\n.. warning::\n\n   This is an old version of Loads. Don't use it.\n   To get started with the new version, you can look at https://github.com/loads\n\n\n**Loads** is a framework for load testing an HTTP service.\n\n.. figure:: https://loads.readthedocs.io/en/latest/_images/logo.jpg\n   :align: right\n   :target: http://thenounproject.com/noun/riot/#icon-No15381\n\n   by Juan Pablo Bravo\n\n\n\nInstallation::\n\n    $ bin/pip install loads\n\nSee the docs at http://loads.readthedocs.io\n\n.. image:: https://secure.travis-ci.org/mozilla-services/loads.png?branch=master\n   :alt: Build Status\n   :target: https://secure.travis-ci.org/mozilla-services/loads/\n\n.. image:: https://coveralls.io/repos/mozilla-services/loads/badge.png?branch=master\n   :target: https://coveralls.io/r/mozilla-services/loads\n\n.. image:: https://pypip.in/v/loads/badge.png\n   :target: https://crate.io/packages/loads/\n\n"
},
{
  "name": "konfig",
  "files": {
    "/": [
      ".gitignore",
      ".pylintrc",
      ".travis.yml",
      "CHANGES.txt",
      "CONTRIBUTORS.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "konfig",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": "# -*- coding: utf-8; mode: makefile-gmake -*-\n\n.DEFAULT = help\nTEST ?= .\n\n# python version to use\nPY       ?=3\n# list of python packages (folders) or modules (files) of this build\nPYOBJECTS = konfig\n# folder where the python distribution takes place\nPYDIST   ?= dist\n# folder where the python intermediate build files take place\nPYBUILD  ?= build\n\nSYSTEMPYTHON = `which python$(PY) python | head -n 1`\nVIRTUALENV   = virtualenv --python=$(SYSTEMPYTHON)\nVENV_OPTS   = \"--no-site-packages\"\nTEST_FOLDER  = ./konfig/tests\n\nENV     = ./local/py$(PY)\nENV_BIN = $(ENV)/bin\n\n\nPHONY += help\nhelp::\n\t@echo  'usage:'\n\t@echo\n\t@echo  '  build     - build virtualenv ($(ENV)) and install *developer mode*'\n\t@echo  '  lint      - run pylint within \"build\" (developer mode)'\n\t@echo  '  test      - run tests for all supported environments (tox)'\n\t@echo  '  dist      - build packages in \"$(PYDIST)/\"'\n\t@echo  '  publish   - upload \"$(PYDIST)/*\" files to PyPi'\n\t@echo  '  clean\t    - remove most generated files'\n\t@echo\n\t@echo  'options:'\n\t@echo\n\t@echo  '  PY=3      - python version to use (default 3)'\n\t@echo  '  TEST=.    - choose test from $(TEST_FOLDER) (default \".\" runs all)'\n\t@echo\n\t@echo  'Example; a clean and fresh build (in local/py3), run all tests (py27, py35, lint)::'\n\t@echo\n\t@echo  '  make clean build test'\n\t@echo\n\n\nPHONY += build\nbuild: $(ENV)\n\t$(ENV_BIN)/pip -v install -e .\n\n\nPHONY += lint\nlint: $(ENV)\n\t$(ENV_BIN)/pylint $(PYOBJECTS) --rcfile ./.pylintrc\n\nPHONY += test\ntest:  $(ENV)\n\t$(ENV_BIN)/tox -vv\n\n$(ENV):\n\t$(VIRTUALENV) $(VENV_OPTS) $(ENV)\n\t$(ENV_BIN)/pip install -r requirements.txt\n\n# for distribution, use python from virtualenv\nPHONY += dist\ndist:  clean-dist $(ENV)\n\t$(ENV_BIN)/python setup.py \\\n\t\tsdist -d $(PYDIST)  \\\n\t\tbdist_wheel --bdist-dir $(PYBUILD) -d $(PYDIST)\n\nPHONY += publish\npublish: dist\n\t$(ENV_BIN)/twine upload $(PYDIST)/*\n\nPHONY += clean-dist\nclean-dist:\n\trm -rf ./$(PYBUILD) ./$(PYDIST)\n\n\nPHONY += clean\nclean: clean-dist\n\trm -rf ./local ./.cache\n\trm -rf *.egg-info .coverage\n\trm -rf .eggs .tox html\n\tfind . -name '*~' -exec echo rm -f {} +\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name __pycache__ -exec rm -rf {} +\n\n# END of Makefile\n.PHONY: $(PHONY)\n",
  "readme": "======\nKonfig\n======\n\nYet another configuration object. Compatible with the updated `configparser\n<https://pypi.python.org/pypi/configparser>`_.\n\n|travis| |master-coverage|\n\n\n.. |master-coverage| image::\n    https://coveralls.io/repos/mozilla-services/konfig/badge.svg?branch=master\n    :alt: Coverage\n    :target: https://coveralls.io/r/mozilla-services/konfig\n\n.. |travis| image:: https://travis-ci.org/mozilla-services/konfig.svg?branch=master\n    :target: https://travis-ci.org/mozilla-services/konfig\n\n\nUsage\n=====\n\n.. code-block:: python\n\n   >>> from konfig import Config\n   >>> c = Config('myconfig.ini')\n\n\nThen read `configparser's documentation\n<http://docs.python.org/3/library/configparser.html>`_ for the APIs.\n\nKonfig adds some extra APIs like **as_args()**, which will return\nthe config file as argparse compatible arguments::\n\n    >>> c.as_args()\n    ['--other-stuff', '10', '--httpd', '--statsd-endpoint', 'http://ok']\n\n\nFor automatic filtering, you can also pass an argparse parser object\nto **scan_args()**. I will iterate over the arguments you've defined in the\nparser and look for them in the config file, then return a list of args\nlike **as_args()**. You can then use this list directly\nwith **parser.parse_args()** - or complete it with sys.argv or whatever.\n\n    >>> import argparse\n    >>> parser = argparse.ArgumentParser()\n    >>> parser.add_argument('--log-level', dest='loglevel')\n    >>> parser.add_argument('--log-output', dest='logoutput')\n    >>> parser.add_argument('--daemon', dest='daemonize', action='store_true')\n\n    >>> config = Config('myconfig.ini')\n    >>> args_from_config = config.scan_args(parser)\n\n    >>> parser.parse_args(args=sys.argv[1:]+args_from_config)\n\n\nSyntax Definition\n=================\n\nThe configuration file is a ini-based file. (See\nhttp://en.wikipedia.org/wiki/INI_file for more details.) Variables name can be\nassigned values, and grouped into sections. A line that starts with \"#\" is\ncommented out. Empty lines are also removed.\n\nExample:\n\n.. code-block:: ini\n\n  [section1]\n  # comment\n  name = value\n  name2 = \"other value\"\n\n  [section2]\n  foo = bar\n\nIni readers in Python, PHP and other languages understand this syntax.\nAlthough, there are subtle differences in the way they interpret values and in\nparticular if/how they convert them.\n\nValues conversion\n=================\n\nHere are a set of rules for converting values:\n\n- If value is quoted with \" chars, it's a string. This notation is useful to\n  include \"=\" characters in the value. In case the value contains a \" character,\n  it must be escaped with a \"\\\" character.\n\n- When the value is composed of digits and optionally prefixed by \"-\", it's\n  tentatively converted to an integer or a long depending on the language. If the\n  number exceeds the range available in the language, it's left as a string.\n\n- If the value is \"true\" or \"false\", it's converted to a boolean, or 0 and 1\n  when the language does not have a boolean type.\n\n- A value can be an environment variable : \"${VAR}\" is replaced by the value of\n  VAR if found in the environment. If the variable is not found, an error must be\n  raised.\n\n- A value can contains multiple lines. When read, lines are converted into a\n  sequence of values. Each new line for a multiple lines value must start with a\n  least one space or tab character.\n\nExamples:\n\n.. code-block:: ini\n\n  [section1]\n  # comment\n  a_flag = True\n  a_number = 1\n  a_string = \"other=value\"\n  another_string = other value\n  a_list = one\n           two\n           three\n  user = ${USERNAME}\n\n\nExtending a file\n================\n\nAn INI file can extend another file. For this, a \"DEFAULT\" section must contain\nan \"extends\" variable that can point to one or several INI files which will be\nmerged to the current file by adding new sections and values.\n\nIf the file pointed in \"extends\" contains section/variable names that already\nexist in the original file, they will not override existing ones.\n\nHere's an example: you have a public config file and want to keep some database\npasswords private. You can have those password in a separate file.\n\npublic.ini:\n\n.. code-block:: ini\n\n  [database]\n  user = tarek\n  password = PUBLIC\n\n  [section2]\n  foo = baz\n  bas = bar\n\n\nAnd then in private.ini:\n\n.. code-block:: ini\n\n  [DEFAULT]\n  extends = public.ini\n\n  [database]\n  password = secret\n\nNow if you use *private.ini* you will get:\n\n.. code-block:: ini\n\n  [database]\n  user = tarek\n  password = secret\n\n  [section2]\n  foo = baz\n  bas = bar\n\n\n\nTo point several files, the multi-line notation can be used:\n\n.. code-block:: ini\n\n  [DEFAULT]\n  extends = public1.ini\n            public2.ini\n\n\nWhen several files are provided, they are processed sequentially. So if the\nfirst one has a value that is also present in the second, the second one will\nbe ignored. This means that the configuration goes from the most specialized to\nthe most common.\n\nOverride mode\n=============\n\nIf you want to extend a file and have existing values overridden,\nyou can use \"overrides\" instead of \"extends\".\n\nHere's an example.  file2.ini:\n\n.. code-block:: ini\n\n  [section1]\n  name2 = \"other value\"\n\n  [section2]\n  foo = baz\n  bas = bar\n\n\nfile1.ini:\n\n.. code-block:: ini\n\n  [DEFAULT]\n  overrides = file2.ini\n\n  [section2]\n  foo = bar\n\n\nResult if you use *file1.ini*:\n\n.. code-block:: ini\n\n  [section1]\n  name2 = \"other value\"\n\n  [section2]\n  foo = baz\n  bas = bar\n\nIn *section2*, notice that *foo* is now *baz*.\n\n"
},
{
  "name": "heka-build",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CMakeLists.txt",
      "LICENSE.txt",
      "Makefile",
      "README.rst",
      "bin",
      "build.bat",
      "cmake",
      "package_deps.txt",
      "resources",
      "sample",
      "scripts"
    ]
  },
  "makefile": "APPNAME = hekad\nDEPS =\nHERE = $(shell pwd)\nBIN = $(HERE)/bin\n\nHGBIN = $(HERE)/pythonVE/bin/hg\n\nCHECK_GOROOT_CMD = python scripts/check_goroot.py\nGOROOT = $(shell $(CHECK_GOROOT_CMD))\nifeq ($(GOROOT),)\n    $(error \"Can't find working Go installation. Either install Go 1.1 or greater, or set GOROOT if you already have a working Go 1.1 or greater installation in a non-standard location\")\nendif\n\nGOBIN = $(GOROOT)/bin/go\nGOCMD = GOROOT=$(GOROOT) GOPATH=$(HERE) $(GOBIN)\nGOPATH = $GOPATH:$(HERE)\n\n\nifeq ($(MAKECMDGOALS),test-bench)\n\tBENCH = -bench .\nendif\n\n.PHONY: all build test clean-env clean gospec moz-plugins check_goroot\n.SILENT: test\n\nall: build\n\nclean-go:\n\trm -rf bin/go build\n\nclean-src:\n\trm -rf src/*\n\nclean-heka:\n\trm -f bin/hekad\n\nclean-all: clean-go clean-src clean-heka\n\nclean: clean-heka\n\n$(HERE)/heka-docs:\n\tgit clone https://github.com/mozilla-services/heka-docs.git && \\\n\tcd heka-docs && \\\n\tgit submodule update --init --recursive\n\n$(HERE)/virtualenv:\n\tcurl -O https://pypi.python.org/packages/source/v/virtualenv/virtualenv-1.9.1.tar.gz\n\ttar xzvf virtualenv-1.9.1.tar.gz\n\tmv virtualenv-1.9.1 virtualenv\n\n$(HERE)/pythonVE: $(HERE)/virtualenv\n\tcd virtualenv && \\\n\tpython virtualenv.py ../pythonVE\n\n$(HERE)/pythonVE/bin/sphinx-build: $(HERE)/pythonVE\n\tpythonVE/bin/pip install Sphinx\n\ndocs: $(HERE)/heka-docs $(HERE)/pythonVE/bin/sphinx-build bin/hekad\n\tcd heka-docs && \\\n\t\tmake html SPHINXBUILD=$(HERE)/pythonVE/bin/sphinx-build\n\tcd src/github.com/mozilla-services/heka/docs && \\\n\t\tmake html SPHINXBUILD=$(HERE)/pythonVE/bin/sphinx-build && \\\n\t\tmake man SPHINXBUILD=$(HERE)/pythonVE/bin/sphinx-build\n\nsandbox: heka-source\n\tmkdir -p release\n\tcd release && cmake .. && make\n\nsrc/github.com/mozilla-services/heka/README.md:\n\tmkdir -p src/github.com/mozilla-services\n\tcd src/github.com/mozilla-services && \\\n\t\tgit clone https://github.com/mozilla-services/heka.git && \\\n\t\tcd heka && \\\n\t\tgit submodule update --init --recursive\n\nheka-source: src/github.com/mozilla-services/heka/README.md\n\nbin/hekad: pluginloader heka-source $(HERE)/pythonVE\n\tGOPATH=$GOPATH PATH=\"$(HERE)/pythonVE/bin:$(PATH)\" python scripts/update_deps.py package_deps.txt\n\t@cd src && \\\n\t\t$(GOCMD) install -ldflags=\"-linkmode=external\"  github.com/mozilla-services/heka/cmd/hekad\n\nhekad: sandbox bin/hekad\n\nbin/flood:\n\t$(GOCMD) install github.com/mozilla-services/heka/cmd/flood\n\nflood: bin/flood\n\nbin/sbmgr:\n\t$(GOCMD) install github.com/mozilla-services/heka/cmd/sbmgr\n\nsbmgr: bin/sbmgr\n\nbin/sbmgrload:\n\t$(GOCMD) install github.com/mozilla-services/heka/cmd/sbmgrload\n\nsbmgrload: bin/sbmgrload\n\nsrc/github.com/mozilla-services/heka-mozsvc-plugins/README.md:\n\tmkdir -p src/github.com/mozilla-services\n\tcd src/github.com/mozilla-services && \\\n\t\tgit clone https://github.com/mozilla-services/heka-mozsvc-plugins.git\n\nsource: sandbox pluginloader heka-source $(HERE)/pythonVE\n\tGOPATH=$GOPATH PATH=\"$(HERE)/pythonVE/bin:$(PATH)\" python scripts/update_deps.py package_deps.txt\n\nmoz-plugins-source: src/github.com/mozilla-services/heka-mozsvc-plugins/README.md\n\nsrc/github.com/crankycoder/g2s:\n\t$(GOCMD) get github.com/crankycoder/g2s\n\ng2s: src/github.com/crankycoder/g2s\n\nmoz-plugins: $(GOBIN) g2s moz-plugins-source\n\t./scripts/register_mozsvc_plugins.py\n\nbuild: hekad\n\nsrc/code.google.com/p/gomock/gomock:\n\t$(GOCMD) get code.google.com/p/gomock/gomock\n\nbin/mockgen:\n\t$(GOCMD) install code.google.com/p/gomock/mockgen\n\ngomock: src/code.google.com/p/gomock/gomock bin/mockgen\n\nsrc/github.com/rafrombrc/gospec/src/gospec:\n\t$(GOCMD) get github.com/rafrombrc/gospec/src/gospec\n\ngospec: src/github.com/rafrombrc/gospec/src/gospec\n\ntest: hekad gomock gospec\n\t$(GOCMD) test -i github.com/mozilla-services/heka/pipeline\n\t$(GOCMD) test -i github.com/mozilla-services/heka/cmd/hekad\n\t$(GOCMD) test -ldflags=\"-linkmode=external\" github.com/mozilla-services/heka/cmd/hekad\n\t$(GOCMD) test -ldflags=\"-linkmode=external\" $(BENCH) github.com/mozilla-services/heka/pipeline\n\t$(GOCMD) test -ldflags=\"-linkmode=external\" $(BENCH) github.com/mozilla-services/heka/message\n\t$(GOCMD) test -ldflags=\"-linkmode=external\" $(BENCH) github.com/mozilla-services/heka/sandbox/lua\n\ntest-bench: test\n\ntest-all: test\n\t$(GOCMD) test -i github.com/mozilla-services/heka-mozsvc-plugins\n\t$(GOCMD) test github.com/mozilla-services/heka-mozsvc-plugins\n\npluginloader: heka-source\n\t./scripts/setup_pluginloader.py\n\nrpms: moz-plugins build docs sbmgr flood\n\t./scripts/make_pkgs.sh rpm\n\ndebs: moz-plugins build docs sbmgr flood\n\t./scripts/make_pkgs.sh deb\n\ntarballs: moz-plugins docs sbmgr flood\n\t./scripts/make_pkgs.sh tarball\n\nosx: build docs\n\tmkdir -p osxproto/bin\n\tmkdir -p osxproto/share/man/man1\n\tmkdir -p osxproto/share/man/man5\n\tcp bin/hekad osxproto/bin/\n\tcp src/github.com/mozilla-services/heka/docs/build/man/*.1 osxproto/share/man/man1/\n\tcp src/github.com/mozilla-services/heka/docs/build/man/*.5 osxproto/share/man/man5/\n\ndev: heka-source\n\tcd src/github.com/mozilla-services/heka && \\\n\tgit config remote.origin.url https://github.com/mozilla-services/heka.git && \\\n\tgit checkout dev && \\\n\tgit submodule update --init --recursive; \\\n\tcd ../../../..; \\\n\tif [ -e src/github.com/mozilla-services/heka-mozsvc-plugins ]; \\\n\tthen \\\n\t    cd src/github.com/mozilla-services/heka-mozsvc-plugins && \\\n\t    git config remote.origin.url https://github.com/mozilla-services/heka-mozsvc-plugins.git && \\\n\t    git checkout dev && \\\n\t    git submodule update --init --recursive; \\\n\tfi\n\tif [ -e heka-docs ]; \\\n\tthen \\\n\t\tcd heka-docs && \\\n\t\tgit config remote.origin.url https://github.com/mozilla-services/heka-docs.git && \\\n\t\tgit checkout dev && \\\n\t\tgit submodule update --init --recursive; \\\n\tfi\n\ndev-ssh: heka-source\n\tcd src/github.com/mozilla-services/heka && \\\n\tgit config remote.origin.url git@github.com:mozilla-services/heka.git && \\\n\tgit checkout dev && \\\n\tgit submodule update --init --recursive; \\\n\tcd ../../../..; \\\n\tif [ -e src/github.com/mozilla-services/heka-mozsvc-plugins ]; \\\n\tthen \\\n\t    cd src/github.com/mozilla-services/heka-mozsvc-plugins && \\\n\t    git config remote.origin.url git@github.com:mozilla-services/heka-mozsvc-plugins.git && \\\n\t    git checkout dev && \\\n\t    git submodule update --init --recursive; \\\n\tfi\n\tif [ -e heka-docs ]; \\\n\tthen \\\n\t\tcd heka-docs && \\\n\t\tgit config remote.origin.url git@github.com:mozilla-services/heka-docs.git && \\\n\t\tgit checkout dev && \\\n\t\tgit submodule update --init --recursive; \\\n\tfi\n\nundev: heka-source\n\tcd src/github.com/mozilla-services/heka && \\\n\tgit config remote.origin.url https://github.com/mozilla-services/heka.git && \\\n\tgit checkout master; \\\n\tcd ../../../..; \\\n\tif [ -e src/github.com/mozilla-services/heka-mozsvc-plugins ]; \\\n\tthen \\\n\t    cd src/github.com/mozilla-services/heka-mozsvc-plugins && \\\n\t    git config remote.origin.url https://github.com/mozilla-services/heka-mozsvc-plugins.git && \\\n\t    git checkout master; \\\n\tfi\n\tif [ -e heka-docs ]; \\\n\tthen \\\n\t\tcd heka-docs && \\\n\t\tgit config remote.origin.url https://github.com/mozilla-services/heka-docs.git && \\\n\t\tgit checkout master; \\\n\tfi\n\nFORCE:\n",
  "readme": "Heka-build\n==========\n\nThis repo is obsolete. Heka's build is now driven entirely by cmake,\nand is contained entirely within the core Heka repository at\nhttps://github.com/mozilla-services/heka. Install instructions are\nat http://hekad.readthedocs.org/en/latest/installing.html.\n"
},
{
  "name": "heka-mozsvc-plugins",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "all_specs_test.go",
      "cloudwatch.go",
      "cloudwatch_test.go",
      "docs",
      "filters.go",
      "outputs.go",
      "sentry.go",
      "sentry_test.go",
      "statsd.go",
      "statsd_test.go",
      "syslog_test.go",
      "syslogwriter.go",
      "testsupport",
      "update_mocks.sh"
    ],
    "/docs": [
      "Makefile",
      "README.rst",
      "conf.py",
      "config.rst",
      "index.rst",
      "make.bat"
    ]
  },
  "makefile": null,
  "readme": "heka-mozsvc-plugins\n===================\n\nSet of heka plugins in use by Mozilla Services"
},
{
  "name": "heka",
  "files": {
    "/": [
      ".dockerignore",
      ".gitattributes",
      ".gitignore",
      ".gitmodules",
      ".travis.yml",
      "CHANGES.txt",
      "CMakeLists.txt",
      "CPackConfig.cmake",
      "Dockerfile",
      "LICENSE.txt",
      "README.md",
      "build.bat",
      "build.sh",
      "client",
      "cmake",
      "cmd",
      "coverage.txt",
      "dasher",
      "docker",
      "docs",
      "env.bat",
      "env.sh",
      "examples",
      "logstreamer",
      "message",
      "packaging",
      "pipeline",
      "plugins",
      "ringbuf",
      "rpm",
      "sandbox"
    ],
    "/docs": [
      "Makefile",
      "make.bat",
      "source"
    ]
  },
  "makefile": null,
  "readme": "This project is deprecated. Please see [this email](https://mail.mozilla.org/pipermail/heka/2016-May/001059.html) for more details.\n\n# Heka\n\nData Acquisition and Processing Made Easy\n\nHeka is a tool for collecting and collating data from a number of different\nsources, performing \"in-flight\" processing of collected data, and delivering\nthe results to any number of destinations for further analysis.\n\nHeka is written in [Go](http://golang.org/), but Heka plugins can be written\nin either Go or [Lua](http://lua.org). The easiest way to compile Heka is by\nsourcing (see below) the build script in the root directory of the project,\nwhich will set up a Go environment, verify the prerequisites, and install all\nrequired dependencies. The build process also provides a mechanism for easily\nintegrating external plug-in packages into the generated `hekad`. For more\ndetails and additional installation options see\n[Installing](https://hekad.readthedocs.io/en/latest/installing.html).\n\nWARNING: YOU MUST *SOURCE* THE BUILD SCRIPT (i.e. `source build.sh`) TO\n         BUILD HEKA. Setting up the Go build environment requires changes to\n         the shell environment, if you simply execute the script (i.e.\n         `./build.sh`) these changes will not be made.\n         \nResources:\n* Heka project docs: https://hekad.readthedocs.io/\n* GoDoc package docs: http://godoc.org/github.com/mozilla-services/heka\n* Mailing list: https://mail.mozilla.org/listinfo/heka\n* IRC: #heka on irc.mozilla.org\n"
},
{
  "name": "marteau",
  "files": {
    "/": [
      ".gitignore",
      ".marteau.yml",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "marteau",
      "setup.py",
      "tools"
    ]
  },
  "makefile": ".PHONY: build\n\nifndef VTENV_OPTS\nVTENV_OPTS = \"--no-site-packages\"\nendif\n\nbin/python:\n\tvirtualenv $(VTENV_OPTS) .\n\tbin/python setup.py develop\n\nbuild: bin/python\n\n",
  "readme": "Marteau - Continuous load testing\n=================================\n\nMarteau uses Funkload to run a load test against a project and\nbuild a report.\n\nThe doc is located at http://marteau.readthedocs.org\n"
},
{
  "name": "pyramid_macauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.rst",
      "pyramid_macauth",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "===============\npyramid_macauth\n===============\n\nThis is a Pyramid authenitcation plugin for MAC Access Authentication:\n\n    http://tools.ietf.org/html/draft-ietf-oauth-v2-http-mac-01\n\nTo access resources using MAC Access Authentication, the client must have\nobtained a set of MAC credentials including an id and secret key.  They use\nthese credentials to make signed requests to the server.\n\nWhen accessing a protected resource, the server will generate a 401 challenge\nresponse with the scheme \"MAC\" as follows::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n\n    < HTTP/1.1 401 Unauthorized\n    < WWW-Authenticate: MAC\n\nThe client will use their MAC credentials to build a request signature and\ninclude it in the Authorization header like so::\n\n    > GET /protected_resource HTTP/1.1\n    > Host: example.com\n    > Authorization: MAC id=\"h480djs93hd8\",\n    >                    ts=\"1336363200\",\n    >                    nonce=\"dj83hs9s\",\n    >                    mac=\"bhCQXTVyfj5cmA9uKkPFx1zeOXM=\"\n\n    < HTTP/1.1 200 OK\n    < Content-Type: text/plain\n    <\n    < For your eyes only:  secret data!\n\n\nThis plugin uses the tokenlib library for verifying MAC credentials:\n\n    https://github.com/mozilla-services/tokenlib\n\nIf this library does not meet your needs, you can provide a custom callback\nfunction to decode the MAC id token.\n"
},
{
  "name": "flemmard",
  "files": {
    "/": [
      "CHANGES.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "docs",
      "flemmard",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "make.bat",
      "source"
    ]
  },
  "makefile": ".PHONY: docs build test\n\nifndef VTENV_OPTS\nVTENV_OPTS = \"--no-site-packages\"\nendif\n\nbuild:\t\n\tvirtualenv $(VTENV_OPTS) .\n\tbin/python setup.py develop\n\ntest: bin/nosetests\n\tbin/nosetests -s circus\n\ndocs: bin/sphinx-build\n\tSPHINXBUILD=../bin/sphinx-build $(MAKE) -C docs html $^\n\nbin/sphinx-build: bin/python\n\tbin/pip install sphinx\n\nbin/nosetests: bin/python\n\tbin/pip install nose\n",
  "readme": "Flemmard\n--------\n\nFlemmard let you drive a Jenkins server from the command line.\n\nThe console script will let you:\n\n- build jobs at given tags\n- download artifacts\n- create new jobs using templates\n- list jobs and their status\n"
},
{
  "name": "metlog-cef",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "MANIFEST.in",
      "MetlogCef.spec",
      "README.rst",
      "docs",
      "metlog_cef",
      "required.txt",
      "requirements.txt",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "source"
    ]
  },
  "makefile": null,
  "readme": "==========\nmetlog-cef\n==========\n\nmetlog-cef is a plugin extension for `Metlog \n<http://github.com/mozilla-services/metlog-py>`.  metlog-cef\nprovides an extension to log send CEF messages to a logstash server.\n\nMore information about how Mozilla Services is using Metlog (including what is\nbeing used for a router and what endpoints are in use / planning to be used)\ncan be found on the relevant `spec page\n<https://wiki.mozilla.org/Services/Sagrada/Metlog>`_.\n"
},
{
  "name": "vagrant-centos",
  "files": {
    "/": [
      ".gitignore",
      "README.rst",
      "Vagrantfile",
      "files",
      "local_repo",
      "manifests",
      "modules"
    ]
  },
  "makefile": null,
  "readme": "====================\nVagrant CentOS Setup\n====================\n\nThis vagrant build is to create a CentOS VM that has the mozilla-services repo's enabled for\na build environment aiming to be quite similar to production Mozilla Service machines (though\nnot exactly as it doesn't use the same service-ops puppet scripts).\n\nInstalling\n==========\n\n**Note**: VirtualBox 4.1.x currently `seems to have a nasty kernel panic issue with Lion <https://www.virtualbox.org/ticket/9359>`_\n, use the second link provided in 2.1 to install the previous version which is stable in OSX Lion.\n\n**VPN Note**: Using the Mozilla repo's requires access to the Mozilla MPT VPN. The connection must be\nactive before provisioning the machine. If there's intermittent RPM repo failures, add a line to your\n/etc/hosts file like so::\n\n    63.245.209.182 mrepo.mozilla.org\n\n1. Install Vagrant: http://downloads.vagrantup.com/tags/v1.0.1\n\n2. Install Virtualbox (**do not install this in OSX Lion**): http://www.virtualbox.org/wiki/Downloads\n\n2. Install Virtualbox (**use this in Lion**): https://www.virtualbox.org/wiki/Download_Old_Builds_4_0\n\n3. Install the box VM used::\n\n       $ vagrant box add centos-60-x86_64 http://dl.dropbox.com/u/1627760/centos-6.0-x86_64.box\n\n4. Run the following::\n\n       $ mkdir myproj\n       $ curl --silent https://nodeload.github.com/mozilla-services/vagrant-centos/tarball/master | tar zxv --directory=myproj --strip-components=1\n       $ cd myproj\n       $ vim manifests/default.pp  # Edit as needed\n       $ vagrant up\n\n"
},
{
  "name": "wimms",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.rst",
      "MANIFEST.in",
      "README.rst",
      "setup.py",
      "wimms"
    ]
  },
  "makefile": null,
  "readme": "=====\nWIMMS\n=====\n\n**WIMMS** stands for *Where Is My Mozilla Service ?*.\n\nThis library implements a SQL backend for a database that\nlists for a list of service, (user, node, service) tuples.\n\nIt's used by Mozilla's Token Server.\n\nTo see where WIMMS fits in the whole architecture, see:\n\n.. image:: http://ziade.org/token-org.png\n\nTests\n-----\n\nTo run the tests for mysql, you need to install a mysqlserver and export the \n'WIMMS_MYSQLURI' variable properly. Something like this::\n\n    $ export WIMMS_MYSQLURI=\"mysql://user:pass@host/db\"\n"
},
{
  "name": "stokenserver",
  "files": {
    "/": [
      ".gitignore",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "RELEASE.txt",
      "dev-reqs.txt",
      "etc",
      "ezgevent.py",
      "prod-reqs.txt",
      "setup.py",
      "stokenserver.spec",
      "stokenserver"
    ]
  },
  "makefile": "APPNAME = stokenserver\nDEPS =\nVIRTUALENV = virtualenv\nPYTHON = $(CURDIR)/bin/python\nNOSE = bin/nosetests -s --with-xunit\nFLAKE8 = bin/flake8\nCOVEROPTS = --cover-html --cover-html-dir=html --with-coverage --cover-package=appsync\nTESTS = stokenserver\nPKGS = stokenserver\nCOVERAGE = bin/coverage\nPYLINT = bin/pylint\nSERVER = dev-auth.services.mozilla.com\nSCHEME = https\nPYPI = http://pypi.python.org/simple\nPYPI2 = http://pypi.python.org/packages\nPYPI2RPM = bin/pypi2rpm.py --index=$(PYPI)\nPYPIOPTIONS = -i $(PYPI)\nCHANNEL = dev\nRPM_CHANNEL = dev\nPIP_CACHE = /tmp/pip-cache.${USER}\nBUILD_TMP = /tmp/stoken-build.${USER}\nINSTALL = bin/pip install --download-cache=$(PIP_CACHE)\nBUILDAPP = bin/buildapp --download-cache=$(PIP_CACHE)\nBUILDRPMS = bin/buildrpms --download-cache=$(PIP_CACHE)\nINSTALLOPTIONS = -U -i $(PYPI)\nTIMEOUT = 300\nDURATION = 30\nCYCLES = 5:10:20\nHOST = http://localhost:5000\nBIN = ../bin\nRPMDIR= $(CURDIR)/rpms\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n.PHONY: all build build_rpms test update\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages --distribute .\n\t$(INSTALL) Distribute\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) nose\n\t$(INSTALL) WebTest\n\t$(INSTALL) wsgi_intercept\n\t$(INSTALL) https://github.com/mozilla-services/wimms/zipball/master\n\tbin/easy_install `bin/python ezgevent.py`\n\t$(BUILDAPP) -t $(TIMEOUT) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nupdate:\n\t$(BUILDAPP) -t $(TIMEOUT) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\ntest:\n\t$(NOSE) $(TESTS)\n\nbuild_rpms:\n\tmkdir -p ${BUILD_TMP}\n\tcd ${BUILD_TMP} && wget http://pypi.python.org/packages/source/g/gevent/gevent-0.13.7.tar.gz\n\tbin/pypi2rpm.py ${BUILD_TMP}/gevent-0.13.7.tar.gz\n\trm ${BUILD_TMP}/gevent-0.13.7.tar.gz\n\t$(BUILDRPMS) -t $(TIMEOUT) -c $(RPM_CHANNEL) $(DEPS)\n\nmock: build build_rpms\n\tmock init\n\tmock --install python26 python26-setuptools\n\tcd rpms; wget http://mrepo.mozilla.org/mrepo/5-x86_64/RPMS.mozilla-services/gunicorn-0.11.2-1moz.x86_64.rpm\n\tcd rpms; wget http://mrepo.mozilla.org/mrepo/5-x86_64/RPMS.mozilla/nginx-0.7.65-4.x86_64.rpm\n\tmock --install rpms/*\n\tmock --chroot \"python2.6 -m stokenserver.run\"\n\nclean:\n\trm -rf bin lib include local docs/build\n",
  "readme": ""
},
{
  "name": "tokenlib",
  "files": {
    "/": [
      ".gitignore",
      ".pylintrc",
      "CHANGES.txt",
      "CONTRIBUTORS.rst",
      "LICENSE.txt",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tokenlib",
      "tox.ini"
    ]
  },
  "makefile": "# -*- coding: utf-8; mode: makefile-gmake -*-\n\n.DEFAULT = help\nTEST ?= .\n\n# python version to use\nPY       ?=3\n# list of python packages (folders) or modules (files) of this build\nPYOBJECTS = tokenlib\n# folder where the python distribution takes place\nPYDIST   ?= dist\n# folder where the python intermediate build files take place\nPYBUILD  ?= build\n\nSYSTEMPYTHON = `which python$(PY) python | head -n 1`\nVIRTUALENV   = virtualenv --python=$(SYSTEMPYTHON)\nVENV_OPTS    = \"--no-site-packages\"\nTEST_FOLDER  = ./tokenlib/tests\n\nENV     = ./local/py$(PY)\nENV_BIN = $(ENV)/bin\n\n\nPHONY += help\nhelp::\n\t@echo  'usage:'\n\t@echo\n\t@echo  '  build     - build virtualenv ($(ENV)) and install *developer mode*'\n\t@echo  '  lint      - run pylint within \"build\" (developer mode)'\n\t@echo  '  test      - run tests for all supported environments (tox)'\n\t@echo  '  dist      - build packages in \"$(PYDIST)/\"'\n\t@echo  '  publish   - upload \"$(PYDIST)/*\" files to PyPi'\n\t@echo  '  clean\t    - remove most generated files'\n\t@echo\n\t@echo  'options:'\n\t@echo\n\t@echo  '  PY=3      - python version to use (default 3)'\n\t@echo  '  TEST=.    - choose test from $(TEST_FOLDER) (default \".\" runs all)'\n\t@echo\n\t@echo  'Example; a clean and fresh build (in local/py3), run all tests (py27, py35, lint)::'\n\t@echo\n\t@echo  '  make clean build test'\n\t@echo\n\n\nPHONY += build\nbuild: $(ENV)\n\t$(ENV_BIN)/pip -v install -e .\n\n\nPHONY += lint\nlint: $(ENV)\n\t$(ENV_BIN)/pylint $(PYOBJECTS) --rcfile ./.pylintrc\n\nPHONY += test\ntest:  $(ENV)\n\t$(ENV_BIN)/tox -vv\n\n$(ENV):\n\t$(VIRTUALENV) $(VENV_OPTS) $(ENV)\n\t$(ENV_BIN)/pip install -r requirements.txt\n\n# for distribution, use python from virtualenv\nPHONY += dist\ndist:  clean-dist $(ENV)\n\t$(ENV_BIN)/python setup.py \\\n\t\tsdist -d $(PYDIST)  \\\n\t\tbdist_wheel --bdist-dir $(PYBUILD) -d $(PYDIST)\n\nPHONY += publish\npublish: dist\n\t$(ENV_BIN)/twine upload $(PYDIST)/*\n\nPHONY += clean-dist\nclean-dist:\n\trm -rf ./$(PYBUILD) ./$(PYDIST)\n\n\nPHONY += clean\nclean: clean-dist\n\trm -rf ./local ./.cache\n\trm -rf *.egg-info .coverage\n\trm -rf .eggs .tox html\n\tfind . -name '*~' -exec echo rm -f {} +\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name __pycache__ -exec rm -rf {} +\n\n# END of Makefile\n.PHONY: $(PHONY)\n",
  "readme": "========\ntokenlib\n========\n\nThis is generic support library for doing token-based authentication.  You\nmight use it to build a login system using bearer tokens, two-legged oauth, or\nMAC Access authentication.\n\nGiven a server-side master secret, you can serialize a dict of data into\nan opaque, unforgeable authentication token::\n\n   >>> token = tokenlib.make_token({\"userid\": 42}, secret=\"I_LIKE_UNICORNS\")\n   >>> print token\n   eyJzYWx0IjogImY0NTU5NCIsICJleHBpcmVzIjogMTMyOTg3NTI2Ny4xNDQ5MzUsICJ1c2VyaWQiOiA0Mn0miXCe4NQQtXTE8NXSGcsL6dzSuQ==\n\nLater, you can use the same secret to verify the token and extract the\nembedded data::\n\n    >>> data = tokenlib.parse_token(token, secret=\"I_LIKE_UNICORNS\")\n    >>> print data\n    {u'userid': 42, u'expires': 1329875384.073159, u'salt': u'1c033f'}\n\nNotice that the data includes an expiry time.  If you try to parse an expired\ntoken, it will fail::\n\n    >>> # Use now=XXX to simulate a time in the future.\n    >>> tokenlib.parse_token(token, secret=\"I_LIKE_UNICORNS\", now=9999999999)\n    Traceback (most recent call last):\n    ...\n    ValueError: token has expired\n\nLikewise, it will fail if the token was constructed with a non-matching secret\nkey::\n\n    >>> tokenlib.parse_token(token, secret=\"I_HATE_UNICORNS\")\n    Traceback (most recent call last):\n    ...\n    ValueError: token has invalid signature\n\nEach token also has an associated \"token secret\".  This is a secret key that\ncan be shared with the consumer of the token to enable authentication schemes\nsuch as MAC Access Authentication of Two-Legged OAuth::\n\n    >>> key = tokenlib.get_token_secret(token, secret=\"I_LIKE_UNICORNS\")\n    >>> print key\n    EZslG8yEYTGyDvBjRnxGipL5Kd8=\n\nFor applications that are using the same settings over and over again, you\nwill probably want to create a TokenManager object rather than using the\nmodule-level convenience functions::\n\n    >>> manager = tokenlib.TokenManager(secret=\"I_LIKE_UNICORNS\")\n    >>> data = manager.parse_token(token)\n    >>> print data\n    {u'userid': 42, u'expires': 1329875384.073159, u'salt': u'1c033f'}\n\nThis will let you customize e.g. the token expiry timeout or hash module\nwithout repeating the settings in each call.\n"
},
{
  "name": "server-full",
  "files": {
    "/": [
      ".hgignore",
      ".hgtags",
      "CENTOS.txt",
      "MANIFEST.in",
      "Makefile",
      "README",
      "_build.py",
      "build.py",
      "dev-reqs.txt",
      "development.ini",
      "docs",
      "etc",
      "prod-reqs.txt",
      "pylintrc",
      "setup.cfg",
      "setup.py",
      "stage-reqs.txt",
      "stdeb.cfg",
      "sync.wsgi",
      "syncserver",
      "tests.ini",
      "tests",
      "tests_ldap.ini",
      "tests_memcached.ini",
      "tests_memcachedldap.ini",
      "tests_mysql.ini",
      "tools"
    ],
    "/docs": [
      "Makefile",
      "make.bat",
      "source"
    ]
  },
  "makefile": "DEPS = server-core,server-reg,server-storage\nVIRTUALENV = virtualenv\nPYTHON = bin/python\nNOSE = bin/nosetests -s --with-xunit\nTESTS = deps/server-core/services/tests deps/server-reg/syncreg/tests deps/server-storage/syncstorage/tests\nSERVER = dev-auth.services.mozilla.com\nSCHEME = https\nBUILDAPP = bin/buildapp\nBUILDRPMS = bin/buildrpms\nPYPI = http://pypi.python.org/simple\nPYPIOPTIONS = -i $(PYPI)\nCHANNEL = prod\nINSTALL = bin/pip install\nINSTALLOPTIONS = -U -i $(PYPI)\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\tINSTALLOPTIONS += --install-option=\"--allow-hosts=$(HOST)\"\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n\n.PHONY: all build update test build_rpms\n\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages .\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) Nose\n\t$(INSTALL) WebTest\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nupdate:\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\ntest:\n\t$(NOSE) $(TESTS)\n\nbuild_rpms:\n\t$(BUILDRPMS) -c $(CHANNEL) $(DEPS)\n",
  "readme": "= Sync =\n\nThe Python Sync Server is composed of four parts:\n\n* ''server-core'': library containing various helpers. \n* ''server-storage'': the storage server. \n* ''server-reg'': the user registration/management server.\n* ''server-full'': a server that reunites the server and the user servers so\nyou can run everything in a single box.\n\n\n= Running The Sync Server =\n\nPlease refer to the online documentation at http://docs.services.mozilla.com/howtos/run-sync.html\n\n== More documentation, Feedback ==\n\n- Dev guide: http://docs.services.mozilla.com\n- IRC channel: #sync. See http://irc.mozilla.org/\n- Mailing list: https://mail.mozilla.org/admin/services-dev\n"
},
{
  "name": "mozilla-style",
  "files": {
    "/": [
      "README.md",
      "java"
    ]
  },
  "makefile": null,
  "readme": "# About\n\nThis repository contains files that define and help enforce Mozilla's\ncoding styles and best practices.\n\nThe repository is organized by programming language.\n\nThe content in this repository is currently unofficial. It is hoped that\nthe repository eventually be migrated to somewhere more authoritative,\nhosted under mozilla.org.\n"
},
{
  "name": "walint",
  "files": {
    "/": [
      "CHANGES.rst",
      "MANIFEST.in",
      "README.rst",
      "docs",
      "setup.py",
      "walint"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "configuration.rst",
      "default_controllers.rst",
      "index.rst",
      "make.bat",
      "writing_controllers.rst"
    ]
  },
  "makefile": null,
  "readme": "WALint\n######\n\nWALint (Web App Lint) is a tool for testing your web services against a set\nof common HTTP metrics. The main goal is to test that your application does not\nfail on some weird cases, and that it complies with the HTTP specification.\n\nThe documentation is available at http://packages.python.org/walint/\n"
},
{
  "name": "rpms",
  "files": {
    "/": [
      "README",
      "python-2.7.2-1.spec"
    ]
  },
  "makefile": null,
  "readme": "Spec collections for building Third-party RPMS\n"
},
{
  "name": "SetupSync",
  "files": {
    "/": [
      ".classpath",
      ".gitignore",
      ".project",
      "AndroidManifest.xml",
      "build.xml",
      "proguard.cfg",
      "project.properties",
      "res",
      "src"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "zktools",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "COPYRIGHT.txt",
      "LICENSE.txt",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "docs",
      "setup.cfg",
      "setup.py",
      "sw",
      "zktools",
      "zoo.cfg"
    ],
    "/docs": [
      "Makefile",
      "api.rst",
      "api",
      "changelog.rst",
      "conf.py",
      "glossary.rst",
      "index.rst",
      "make.bat"
    ]
  },
  "makefile": "APPNAME = zktools\nHERE = $(shell pwd)\nBIN = $(HERE)/bin\nNOSE = $(BIN)/nosetests -s --with-xunit\nPYTHON = $(BIN)/python\nZOOKEEPER = $(BIN)/zookeeper\nZOOKEEPER_VERSION = 3.4.3\nSW = sw\nBUILD_DIRS = bin build deps include lib lib64 man\n\n.PHONY: all zookeeper\n.SILENT: lib python pip $(ZOOKEEPER) zookeeper\n\nall: build\n\n$(BIN)/python:\n\tpython $(SW)/virtualenv.py --distribute . >/dev/null 2>&1\n\trm distribute-0.6.*.tar.gz\n\n$(BIN)/pip: $(BIN)/python\n\n$(ZOOKEEPER):\n\t@echo \"Installing Zookeeper\"\n\tmkdir -p bin\n\tcd bin && \\\n\tcurl --silent http://apache.osuosl.org/zookeeper/zookeeper-$(ZOOKEEPER_VERSION)/zookeeper-$(ZOOKEEPER_VERSION).tar.gz | tar -zx\n\tmv bin/zookeeper-$(ZOOKEEPER_VERSION) bin/zookeeper\n\tcd bin/zookeeper && ant compile >/dev/null 2>&1\n\tcd bin/zookeeper/src/c && \\\n\t./configure >/dev/null 2>&1 && \\\n\tmake >/dev/null 2>&1\n\tcd bin/zookeeper/src/contrib/zkpython && \\\n\tmv build.xml old_build.xml && \\\n\tcat old_build.xml | sed 's|executable=\"python\"|executable=\"../../../../../bin/python\"|g' > build.xml && \\\n\tant install >/dev/null 2>&1\n\tcd bin/zookeeper/bin && \\\n\tmv zkServer.sh old_zkServer.sh && \\\n\tcat old_zkServer.sh | sed 's|    $$JAVA \"-Dzoo|    exec $$JAVA \"-Dzoo|g' > zkServer.sh && \\\n\tchmod a+x zkServer.sh\n\tcp zoo.cfg bin/zookeeper/conf/\n\t@echo \"Finished installing Zookeeper\"\n\nzookeeper: \t$(ZOOKEEPER)\n\nclean-env:\n\trm -rf $(BUILD_DIRS)\n\nclean-cassandra:\n\trm -rf cassandra bin/cassandra\n\nclean-nginx:\n\trm -rf bin/nginx\n\nclean-zookeeper:\n\trm -rf zookeeper bin/zookeeper\n\nclean: clean-env\n\nbuild: $(BIN)/python zookeeper\n\t$(PYTHON) setup.py develop\n\t$(BIN)/pip install nose\n\t$(BIN)/pip install Mock\n\ntest:\n\t$(BIN)/zookeeper/bin/zkServer.sh start $(HERE)/zoo.cfg\n\t$(NOSE) -v --with-coverage --cover-package=$(APPNAME) --cover-inclusive $(APPNAME)\n\t$(BIN)/zookeeper/bin/zkServer.sh stop $(HERE)/zoo.cfg\n",
  "readme": "===============\nZookeeper Tools\n===============\n\n``zktools`` is a package of tools implementing higher level constructs using\n`Apache Zookeeper`_.\n\nIt currently provides:\n\n* ``Configuration`` - Zookeeper Configuration Helpers\n  to store and load configuration information stored\n  in Zookeeper nodes.\n* ``Locks`` - A Zookeeper lock with support for\n  non-blocking acquire, modeled on Python's Lock objects that also includes a\n  `Revocable Shared Locks with Freaking Laser Beams` described in the\n  `Zookeeper Recipe's\n  <http://zookeeper.apache.org/doc/current/recipes.html#sc_recoverableSharedLocks>`_.\n\nSee `the full docs`_ for more  information.\n\nLicense\n=======\n\n``zktools`` is offered under the MPL license.\n\nAuthors\n=======\n\n``zktools`` is made available by the `Mozilla Foundation`.\n\n.. _Apache Zookeeper: http://zookeeper.apache.org/\n.. _the full docs: http://zktools.rtfd.org/\n"
},
{
  "name": "ldappool",
  "files": {
    "/": [
      "README.rst"
    ]
  },
  "makefile": null,
  "readme": "ldappool\n========\n\nThe project is now part of OpenStack, see https://git.openstack.org//cgit/openstack/ldappool\n\n"
},
{
  "name": "keyretrieval",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "Makefile",
      "README.txt",
      "etc",
      "keyretrieval.spec",
      "keyretrieval",
      "setup.cfg",
      "setup.py"
    ]
  },
  "makefile": "APPNAME = keyretrieval\nDEPS =\nHERE = $(shell pwd)\nBIN = $(HERE)/bin\nVIRTUALENV = virtualenv\nNOSE = bin/nosetests -s --with-xunit\nTESTS = $(APPNAME)/tests\nPYTHON = $(BIN)/python\nBUILDAPP = $(BIN)/buildapp\nBUILDRPMS = $(BIN)/buildrpms\nPYPI = http://pypi.python.org/simple\nPYPIOPTIONS = -i $(PYPI)\nDOTCHANNEL := $(wildcard .channel)\nifeq ($(strip $(DOTCHANNEL)),)\n\tCHANNEL = dev\n\tRPM_CHANNEL = prod\nelse\n\tCHANNEL = `cat .channel`\n\tRPM_CHANNEL = `cat .channel`\nendif\nINSTALL = $(BIN)/pip install\nPIP_CACHE = /tmp/pip_cache\nINSTALLOPTIONS = --download-cache $(PIP_CACHE)  -U -i $(PYPI)\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n\n.PHONY: all build test build_rpms mach\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages .\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) nose\n\t$(INSTALL) WebTest\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nupdate:\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\ntest:\n\t$(NOSE) $(APPNAME)\n\nbuild_rpms:\n\trm -rf rpms/\n\t$(BUILDRPMS) -c $(RPM_CHANNEL) $(DEPS)\n\nmach: build build_rpms\n\tmach clean\n\tmach yum install python26 python26-setuptools\n\tcd rpms; wget http://mrepo.mozilla.org/mrepo/5-x86_64/RPMS.mozilla-services/gunicorn-0.11.2-1moz.x86_64.rpm\n\tcd rpms; wget http://mrepo.mozilla.org/mrepo/5-x86_64/RPMS.mozilla/nginx-0.7.65-4.x86_64.rpm\n\tmach yum install rpms/*\n\tmach chroot python2.6 -m keyretrieval.run\n",
  "readme": null
},
{
  "name": "pyramid_digestauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.txt",
      "pyramid_digestauth",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "repoze.who.plugins.digestauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.rst",
      "repoze",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "=============================\nrepoze.who.plugins.digestauth\n=============================\n\nThis is repoze.who plugin implementing HTTP's Digest Access Authentication\nas per RFC-2617:\n\n    http://tools.ietf.org/html/rfc2617\n\nIt provides good support for the protocol as it is typically used in the\nwild:\n\n    * both qop=\"auth\" and qop=\"auth-int\" modes\n    * compatability mode for legacy clients\n    * client nonce-count checking\n    * next-nonce generation via the Authentication-Info header\n\nThe following features of the protocol are rarely supported by HTTP clients\nand thus have not yet been implemented:\n\n    * MD5-sess, or any hash algorithm other than MD5\n    * mutual authentication via the Authentication-Info header\n\n\nConfiguration\n=============\n\nConfiguration of the digest-auth plugin can be done from the standard \nrepoze.who config file like so::\n\n    [plugin:digestauth]\n    use = repoze.who.plugins.digestauth:make_plugin\n    realm = MyRealm\n    get_pwdhash = mymodule:get_pwdhash\n\nThe following configuration options are available:\n\n    * realm:  the realm string; included verbatim in the challenge header\n    * domain:  the domain string; included verbatim in the challenge header\n    * qop:  the desired quality of protection (\"auth\" or \"auth-int\")  \n    * get_password:  dotted name of a callback to get the user's password\n    * get_pwdhash:  dotted name of a callback to get the user's password hash\n    * nonce_manager:  dotted name of a class to use for nonce management\n\n\nAuthentication\n==============\n\nTo authenticate a user via Digest Auth, this plugin needs access to either\ntheir raw password or their \"password hash\", which is the MD5 digest of their\nusername, password and authentication realm::\n\n    def calculate_pwdhash(username, password, realm):\n        return md5(\"%s:%s:%s\" % (username, realm, password)).hexdigest()\n\nYou must provide the callback function \"get_password\" or \"get_pwdhash\" to\nthe DigestAuthPlugin.\n\n\nNonce Management\n================\n\nThe security of Digest Access Authentication depends crucially on the secure\ngeneration and managent of cryptographic nonces.  In order to prevent replay\nattacks the server must reject requests that have a repeated nonce.\n\nThe details of nonce management have been extracted into a separate interface,\ndefined by the repoze.who.plugins.digestauth.noncemanager:NonceManager class.\nThe default implementation uses HMAC-signed tokens and an in-memory cache of\nrecently seen nonce counts.  If you have more particular needs you might like\nto implement your own NonceManager subclass.\n"
},
{
  "name": "pyramid_whoauth",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.rst",
      "pyramid_whoauth",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "===============\npyramid_whoauth\n===============\n\nAn authentication policy for Pyramid that uses the repoze.who v2 API.\n\n\nOverview\n========\n\nThis plugin allows you to configure a repoze.who authentication stack as a\npyramid authentication policy.  It takes a repoze.who API factory and turns\nit into an pyramid IAuthenticationPolicy::\n\n    from repoze.who.config import make_api_factory_with_config\n\n    api_factory = make_api_factory_with_config(global_conf, \"etc/who.ini\")\n    authn_policy = WhoAuthenticationPolicy(api_factory)\n    config.set_authentication_policy(authn_policy)\n\nThis will load the repoze.who configuration from the specified config file\nand hook it into Pyramid.\n\nThe advantage of using pyramid_whoauth instead of the repoze.who middleware\nis that authentication is only performed when your application explicitly\nrequests it using e.g. pyramid's authenticated_userid() function.\n\nFor convenience, you can also specify all of the repoze.who configuration\nsettings as part of your paster deployment settings.  For example, you\nmight have the following::\n\n    [app:pyramidapp]\n    use = egg:mypyramidapp\n\n    who.plugin.basicauth.use = repoze.who.plugins.basicauth:make_plugin\n    who.plugin.basicauth.realm = MyRealm\n\n    who.plugin.authtkt.use = repoze.who.plugins.auth_tkt:make_plugin\n    who.plugin.authtkt.secret = Oh So Secret!\n\n    who.identifiers.plugins = authtkt basicauth\n    who.authenticators.plugins = authtkt basicauth\n    who.challengers.plugins = basicauth\n\nThis configures repoze.who to use the \"basicauth\" and \"auth_tkt\" plugins,\nusing pyramid's dotted-settings style rather than the repoze.who config file.\nThen it is a simple matter of including the pyramid_whoauth module into your\nconfigurator::\n\n    config.include(\"pyramid_whoauth\")\n\nIn addition to configuring the repoze.who API factory from the given settings,\nthis will also set up some extra conveniences for your application:\n\n    * a forbidden view that challenges for credentials via repoze.who\n    * a login view that authenticates any credentials submitted via POST\n    * a logout view that sends forget headers when accessed\n    * a tween that calls the repoze.who \"remember\" method for each response\n\n"
},
{
  "name": "mozservices",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.txt",
      "mozsvc",
      "requirements.txt",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "metlog-py",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "MANIFEST.in",
      "MetlogPy.spec",
      "README.rst",
      "docs",
      "metlog",
      "optional.txt",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "api",
      "conf.py",
      "config.rst",
      "getstarted.rst",
      "index.rst"
    ]
  },
  "makefile": null,
  "readme": "This project has been superceded by \n`heka-py https://github.com/mozilla-services/heka-py` and \n`hekad https://github.com/mozilla-services/heka`.\n\nThis repository is no longer being actively maintained.\n\n=========\nmetlog-py\n=========\n\n.. image:: https://secure.travis-ci.org/mozilla-services/metlog-py.png\n\nmetlog-py is a Python client for the \"Metlog\" system of application logging and\nmetrics gathering developed by the `Mozilla Services\n<https://wiki.mozilla.org/Services>`_ team. The Metlog system is meant to make\nlife easier for application developers with regard to generating and sending\nlogging and analytics data to various destinations. It achieves this goal (we\nhope!) by separating the concerns of message generation from those of message\ndelivery and analysis. Front end application code no longer has to deal\ndirectly with separate back end client libraries, or even know what back end\ndata storage and processing tools are in use. Instead, a message is labeled\nwith a type (and possibly other metadata) and handed to the Metlog system,\nwhich then handles ultimate message delivery.\n\nThe Metlog system consists of three pieces:\n\ngenerator\n  This is the application that will be generating the data that is to be sent\n  into the system.\n\nrouter\n  This is the initial recipient of the messages that the generator will be\n  sending. Typically, a metlog router deserializes the messages it receives,\n  examines them, and decides based on the message metadata or contents which\n  endpoint(s) to which the message should be delivered.\n\nendpoints\n  Different types of messages lend themselves to different types of\n  presentation, processing, and analytics. The router has the ability to\n  deliver messages of various types to destinations that are appropriate for\n  handling those message types. For example, simple log messages might be\n  output to a log file, while counter timer info is delivered to a `statsd\n  <https://github.com/etsy/statsd>`_ server, and Python exception information\n  is sent to a `Sentry <https://github.com/dcramer/sentry>`_ server.\n\nThe metlog-py library you are currently reading about is a client library meant\nto be used by Python-based generator applications. It provides a means for\nthose apps to insert messages into the system for delivery to the router and,\nultimately, one or more endpoints.\n\nMore information about how Mozilla Services is using Metlog (including what is\nbeing used for a router and what endpoints are in use / planning to be used)\ncan be found on the relevant `spec page\n<https://wiki.mozilla.org/Services/Sagrada/Metlog>`_.\n"
},
{
  "name": "redbarrel",
  "files": {
    "/": [
      ".gitignore",
      "AUTHORS.txt",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "dev-reqs.txt",
      "doc",
      "prod-reqs.txt",
      "redbarrel",
      "setup.py",
      "stage-reqs.txt"
    ]
  },
  "makefile": "APPNAME = redbarrel\nDEPS =\nVIRTUALENV = virtualenv\nPYTHON = bin/python\nEZ = bin/easy_install\nNOSE = bin/nosetests -s --with-xunit\nFLAKE8 = bin/flake8\nCOVEROPTS = --cover-html --cover-html-dir=html --with-coverage --cover-package=redbarrel\nTESTS = redbarrel\nPKGS = redbarrel\nCOVERAGE = bin/coverage\nPYLINT = bin/pylint\nSERVER = dev-auth.services.mozilla.com\nSCHEME = https\nBUILDAPP = bin/buildapp\nBUILDRPMS = bin/buildrpms\nPYPI = http://pypi.python.org/simple\nPYPI2RPM = bin/pypi2rpm.py --index=$(PYPI)\nPYPIOPTIONS = -i $(PYPI)\nCHANNEL = dev\nRPM_CHANNEL = prod\nINSTALL = bin/pip install\nINSTALLOPTIONS = -U -i $(PYPI)\n\nifdef PYPIEXTRAS\n\tPYPIOPTIONS += -e $(PYPIEXTRAS)\n\tINSTALLOPTIONS += -f $(PYPIEXTRAS)\nendif\n\nifdef PYPISTRICT\n\tPYPIOPTIONS += -s\n\tifdef PYPIEXTRAS\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1] + ',' + urlparse.urlparse('$(PYPIEXTRAS)')[1]\"`\n\n\telse\n\t\tHOST = `python -c \"import urlparse; print urlparse.urlparse('$(PYPI)')[1]\"`\n\tendif\n\nendif\n\nINSTALL += $(INSTALLOPTIONS)\n\n\n\n.PHONY: all build coverage test\n\nall:\tbuild\n\nbuild:\n\t$(VIRTUALENV) --no-site-packages --distribute .\n\t$(INSTALL) MoPyTools\n\t$(INSTALL) nose\n\t$(INSTALL) WebTest\n\t$(INSTALL) wsgi_intercept\n\t$(BUILDAPP) -c $(CHANNEL) $(PYPIOPTIONS) $(DEPS)\n\nbuild_rpms:\n\t$(BUILDRPMS) -c $(RPM_CHANNEL) $(DEPS)\n\ntest:\n\t$(NOSE) $(TESTS)\n\ncoverage:\n\trm -rf html\n\t- $(NOSE) $(COVEROPTS) $(TESTS)\n",
  "readme": "RedBarrel\n=========\n\nWeb builder and execution environment for Services server applications.\n\n\nredbarrel\n    |\n  pistil : web server that runs the system\n\n    |\n    | -- appbuilder : a tool capable of creation an application online \n    |       |\n    |      ----------\n    |      |        |\n    |   iPython    Web\n    |\n    | -- modules: a list of Python and JS modules\n    |      |\n    |    vmodule: a sandboxed Python or JS Module\n    | \n    | -- applications\n            |\n           app1 : a wsgi app that runs with the DSL under a /root\n            |\n          mapper: creates a mapper using Routes and an AST, given a root\n            |\n          -------\n          |     |\n         ast   hook : responsible for the execution for one web service\n                      pre/post calls, sandboxed call\n\n\n"
},
{
  "name": "socorro-crash-utils",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "download_crashdata.py",
      "download_dumps.py",
      "parse_crashdata.py",
      "socorro"
    ]
  },
  "makefile": null,
  "readme": "This repository contains modules and tools for interacting with Socorro crash\ndata.\n\nYou will find some Python modules in socorro/.\n\nIn the root directory are the following scripts:\n\n* parse_crashdata.py - Swiss army knife of the package. This reads crash data\n  (either from daily CSV fails, a directory containing .json files, or stdin),\n  filters the data (filters supplied by arguments), and then outputs whatever\n  analysis you tell it to. Run with --help for complete usage info.\n\n* download_crashdata.py - Convenience script for downloading the daily CSV\n  crash dumps. It defaults to hit Mozilla's server and will download the\n  last 30 days of crashes.\n\n* download_dumps.py - Retrieves detailed dumps for individual crash UUIDs,\n  which are specified on stdin. It simply opens an HTTP connection to\n  Mozilla's socorro instance, fetches individual dumps, and writes them to\n  a directory. These files can later be analyzed using parse_crashdata.py\n\n##Example Workflow\n\nSay you want to analyze crashes from the last week:\n\n    # Download the past 7 days worth of crash dumps\n    $ ./download_crashdata.py --days=7 ~/tmp/crashdata\n\nThen, you want to filter the crashes based on a signature:\n\n    # find all crashes where 'memcpy' appears in the signature\n    # .gz files are automatically recognized!\n    $ ./parse_crashdata.py --signature=memcpy ~/tmp/crashdata/*.gz\n\n    # The above will print a list of crash UUIDs by default. You probably want\n    # to do something useful with these UUIDs:\n    $ ./parse_crashdata.py --signature=memcpy ~/tmp/crashdata/*.gz > ~/tmp/memcpy_ids\n\nNow, let's dig deeper into crashes. First, we need to fetch the detailed crash\nrecords for the crashes of interest:\n\n    $ ./download_dumps.py ~/tmp/dumps < ~/tmp/memcpy_ids\n\nNow, we perform some analysis of the detailed records:\n\n    $ ./parse_crashdata.py --json-dir ~/tmp/dumps --print-frame-counts\n\n"
},
{
  "name": "sync-spray",
  "files": {
    "/": [
      "README.md",
      "data",
      "doc",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "sync-spray is an add-on for Firefox that generates \"random\" data and\neffectively stress tests the Sync component.\n\nsync-spray works by surfing the web automatically.\n\nStart by going to about:sync-spray. Then, enter a seed URI in the text box\nand hit the button. A new window with multiple tabs will be opened and the\nbrowser will start surfing the web automatically.\n\nWhen every page has finished loading, the extension will find a random anchor\nto an http URI and will follow it. If it cannot find one, it will go back\nin the history.\n\nThere are many known issues, particularly with things getting stuck. This can\nhappen when a download is started, etc.\n\nOnce you start automatic browsing, it continues indefinitely. You can stop\nautomatic browsing by closing the new window. But, you probably want to exit\nthe entire process. Also, you probably want to run this in a new profile so it\ndoesn't \"pollute\" a profile you care about.\n"
},
{
  "name": "firefox-send-tab-to-device",
  "files": {
    "/": [
      ".travis.yml",
      "README.md",
      "doc",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "This is the addon-sendtab add-on.  It contains:\n\n* A program (lib/main.js).\n* A few tests.\n* Some meager documentation.\n\n## Now Obsolete\n\nThis addon has been superseded by the [Synced\nTabs](https://wiki.mozilla.org/QA/Synced_Tabs) feature of Firefox Sync (since\n[bug 677372](https://bugzilla.mozilla.org/show_bug.cgi?id=677372) was fixed in\nFirefox 50).  A tab can be sent to a device by enabling \"Open Tabs\" in the\n\"Sync Across All Devices\" section of the \"Sync\" preferences, then\nright-clicking on the tab to send and choosing the \"Send Tab to Device\"\noption.\n"
},
{
  "name": "syncorro",
  "files": {
    "/": [
      "README",
      "bootstrap.js",
      "chrome.manifest",
      "content",
      "install.rdf",
      "locale",
      "modules",
      "skin"
    ]
  },
  "makefile": null,
  "readme": "Syncorro\n========\n\nWhat is it?\n-----------\n\nThis addons prototypes automatic error reporting for Firefox Sync, including some of the related UI bits and pieces.\n\nRight now, it supports error reports being generated, saved to disk, and uploaded to an Elastic Search instance where they're indexed and can be queried. Some of the chrome tab UI is in place, but not all of it is hooked up and finalized. The related error notifications are missing for now.\n\nSee https://wiki.mozilla.org/Services/Sync/FxSync/Syncorro for full details.\n\n\nHow can I try it?\n-----------------\n\n1. Install Elastic Search on your local machine. This is as simple as downloading the latest version from http://elasticsearch.org, extracing the archive file, and running `bin/elasticsearch`.\n\n2. Install this add-on on a recent Firefox nightly. Best is to use a separate throw-away profile with a throw-away Sync account.\n\n3. Sync.\n\n4. Go to about:syncorro and view the reports. For now every sync creates and submits a report. The data you see here is what gets saved in your local Firefox profile.\n\n5. To query the Elastic Search server where the reports are submitted, you can use Elastic Search's JSON-based query language. Queries are submitted via HTTP.\n\nFor instance, to query all reports that were submitted since a certain date and time, use the following command (with an appropriate value for the timestamp, of course):\n\n  curl -XGET 'http://localhost:9200/syncorro/report/_search' \\\n       -d '{\"query\":{\"range\":{\"timestamp\":{\"from\": 1314145430231}}}}'\n\nTo query based on the Sync version, enter this:\n\n  curl -XGET 'http://localhost:9200/syncorro/report/_search' \\\n       -d '{\"query\":{\"term\":{\"sync.version\": \"1.11.0\"}}}'\n\n(These examples use the `curl` program which is a commonly available command-line HTTP client.)\n\n\nWhere to send feedback?\n-----------------------\n\nYour feedback is most welcome. Please send it to services-dev@mozilla.org or philikon@mozilla.com.\n"
},
{
  "name": "fx-sync-esse-delendam",
  "files": {
    "/": [
      "bootstrap.js",
      "install.rdf"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "areweasyncyet.rs",
  "files": {
    "/": [
      "CNAME",
      "fonts",
      "index.html",
      "main.js",
      "style.css"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "serde_dynamodb",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "appveyor.yml",
      "example",
      "serde_dynamodb",
      "serde_dynamodb_derive"
    ]
  },
  "makefile": null,
  "readme": "#  serde_dynamodb [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) [![Build Status](https://travis-ci.org/mockersf/serde_dynamodb.svg?branch=master)](https://travis-ci.org/mockersf/serde_dynamodb) [![Build status](https://ci.appveyor.com/api/projects/status/jnckw7ll01cberis?svg=true)](https://ci.appveyor.com/project/mockersf/serde-dynamodb) [![Coverage Status](https://coveralls.io/repos/github/mockersf/serde_dynamodb/badge.svg?branch=master)](https://coveralls.io/github/mockersf/serde_dynamodb?branch=master) [![Realease Doc](https://docs.rs/serde_dynamodb/badge.svg)](https://docs.rs/serde_dynamodb) [![Crate](https://img.shields.io/crates/v/serde_dynamodb.svg)](https://crates.io/crates/serde_dynamodb)\n\nLibrary to de/serialize an object to an `HashMap` of `AttributeValue`s used by [rusoto_dynamodb](https://crates.io/crates/rusoto_dynamodb) to manipulate objects saved in dynamodb using [serde](https://serde.rs)\n\nThe API docs for the master branch are published [here](https://mockersf.github.io/serde_dynamodb/).\n\n## Example\n\n```rust\n#[derive(Serialize, Deserialize)]\nstruct Todo {\n    id: uuid::Uuid,\n    title: &'static str,\n    done: bool,\n}\n\nlet todo = Todo {\n    id: uuid::Uuid::new_v4(),\n    title: \"publish crate\",\n    done: false,\n};\n\nlet put_item = PutItemInput {\n    item: serde_dynamodb::to_hashmap(&todo).unwrap(),\n    table_name: \"todos\".to_string(),\n    ..Default::default()\n};\n\nlet client = DynamoDbClient::simple(Region::UsEast1);\nclient.put_item(&put_item).unwrap();\n```\n\n"
},
{
  "name": "ev-checker",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "Dockerfile",
      "EVCheckerTrustDomain.cpp",
      "EVCheckerTrustDomain.h",
      "Makefile",
      "README.md",
      "Util.cpp",
      "Util.h",
      "ev-checker.cpp",
      "index.css",
      "index.html",
      "pkix-import",
      "pkix",
      "server.js",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nCC=clang++\nCFLAGS=-Ipkix/include -I/usr/include/nspr4 -I/usr/include/nspr -I/usr/local/include/nspr \\\n\t-I/usr/include/nss3 -I/usr/local/include/nss -I/usr/include/nss \\\n\t-g -Wall -c -std=c++11\nLDFLAGS=-lnss3 -lnssutil3 -lnspr4 -lplc4 -lcurl -Wl,-L/usr/local/lib\nSOURCES=pkix/lib/pkixbuild.cpp pkix/lib/pkixcert.cpp pkix/lib/pkixcheck.cpp \\\n\tpkix/lib/pkixder.cpp pkix/lib/pkixnames.cpp pkix/lib/pkixnss.cpp \\\n\tpkix/lib/pkixocsp.cpp pkix/lib/pkixresult.cpp pkix/lib/pkixtime.cpp \\\n\tpkix/lib/pkixverify.cpp ev-checker.cpp \\\n\tEVCheckerTrustDomain.cpp Util.cpp\nOBJECTS=$(SOURCES:.cpp=.o)\nEXECUTABLE=ev-checker\n\nall: $(SOURCES) $(EXECUTABLE)\n\n$(EXECUTABLE): $(OBJECTS)\n\t$(CC) $(LDFLAGS) $(OBJECTS) -o $@\n\n.cpp.o:\n\t$(CC) $(CFLAGS) $< -o $@\n\ndeps-ubuntu:\n\tsudo apt -y install clang make libcurl4-nss-dev libnspr4-dev libnss3-dev gnutls-bin\n\ndeps-rhel:\n\tsudo dnf install -y clang make nss-devel nspr-devel libcurl-devel gnutls-utils\n\nclean:\n\trm -f $(EXECUTABLE) $(OBJECTS) test/*.pem test/*.key test/*.req \\\n\ttest/run-tests.sh\n",
  "readme": "# ev-checker #\n******\n\n## What ##\n`ev-checker` is a standalone command-line utility for determining if a given EV\npolicy fulfills the requirements of Mozilla's Root CA program and may thus be\nenabled.\n\n## How ##\n`ev-checker` depends on the libraries\n[NSS](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS) and\n[NSPR](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSPR). It\nadditionally makes use of\n[mozilla::pkix](https://wiki.mozilla.org/SecurityEngineering/Certificate_Verification).\nSince mozilla::pkix has not been released as a stand-alone library yet, this\nproject imports a snapshot of the implementation. (See the file `pkix-import`.)\n`ev-checker` implements a `mozilla::pkix::TrustDomain` and uses\n`mozilla::pkix::BuildCertChain` to determine if a given EV policy meets the\nrequirements to be enabled in Firefox.\n\n## Example ##\nFirst, compile with `make`. There is no guarantee of portability, so feel free\nto file issues if this does not work as expected.\n\nThen, given the file `cert-chain.pem`, the dotted OID of the EV policy, and a\nhostname to validate against, run `ev-checker` like so:\n\n`./ev-checker -c cert-chain.pem -o dotted.OID -h hostname`\n\n`-c` specifies the file containing a sequence of PEM-encoded certificates. The\nfirst certificate is the end-entity certificate intended to be tested for EV\ntreatment. The last certificate is the root certificate that is authoritative\nfor the given EV policy. Any certificates in between are intermediate\ncertificates.\n\nIf run with the flag `-d` and a description of the EV OID, `ev-checker` will\noutput a blob of text that must be added to\n[security/certverifier/ExtendedValidation.cpp](https://dxr.mozilla.org/mozilla-central/source/security/certverifier/ExtendedValidation.cpp)\nin the mozilla-central tree for Firefox to consider this a valid EV policy.\nIt will also validate the end-entity certificate. If it succeeds, the EV policy\nis ready to be enabled. If not, something needs to be fixed.\nHopefully `ev-checker` emitted a helpful error message pointing to the problem.\n\n```bash\n$ ev-checker -c chain.pem -o 2.16.840.1.114412.2.1 -d \"Digicert EV OID\" -h addons.mozilla.org\n\n// CN=DigiCert High Assurance EV Root CA,OU=www.digicert.com,O=DigiCert Inc,C=US\n\"2.16.840.1.114412.2.1\",\n\"Digicert EV OID\",\nSEC_OID_UNKNOWN,\n{ 0x74, 0x31, 0xE5, 0xF4, 0xC3, 0xC1, 0xCE, 0x46, 0x90, 0x77, 0x4F,\n  0x0B, 0x61, 0xE0, 0x54, 0x40, 0x88, 0x3B, 0xA9, 0xA0, 0x1E, 0xD0,\n  0x0B, 0xA6, 0xAB, 0xD7, 0x80, 0x6E, 0xD3, 0xB1, 0x18, 0xCF },\n\"MGwxCzAJBgNVBAYTAlVTMRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsT\"\n\"EHd3dy5kaWdpY2VydC5jb20xKzApBgNVBAMTIkRpZ2lDZXJ0IEhpZ2ggQXNzdXJh\"\n\"bmNlIEVWIFJvb3QgQ0E=\",\n\"AqxcJmoLQJuPC3nyrkYldw==\",\nSuccess!\n```\n\n## TODO Items ##\n* Do OCSP fetching\n* Other policy issues\n* More helpful error messages\n"
},
{
  "name": "rusoto",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      ".semaphoreci",
      ".travis.yml",
      "AWS-CREDENTIALS.md",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Cargo.toml",
      "LICENSE",
      "MAINTAINER_GUIDELINES.md",
      "README.md",
      "RELEASING.md",
      "appveyor.yml",
      "assets",
      "clippy.toml",
      "helpers",
      "integration_tests",
      "mock",
      "rusoto",
      "service_crategen"
    ]
  },
  "makefile": null,
  "readme": "# ![Rusoto](./assets/logo-wide.png)\n\n<table>\n    <tr>\n        <td><strong>Linux / OS X</strong></td>\n        <td><a href=\"https://travis-ci.org/rusoto/rusoto\" title=\"Travis Build Status\"><img src=\"https://travis-ci.org/rusoto/rusoto.svg?branch=master\" alt=\"travis-badge\"></img></a></td>\n    </tr>\n    <tr>\n        <td><strong>Windows</strong></td>\n        <td><a href=\"https://ci.appveyor.com/project/matthewkmayer/rusoto/branch/master\" title=\"Appveyor Build Status\"><img src=\"https://ci.appveyor.com/api/projects/status/o83ruaeu7xft0ru5/branch/master?svg=true\" alt=\"appveyor-badge\"></img></a></td>\n    </tr>\n    <tr>\n        <td><strong>Ceph and Minio support</strong></td>\n        <td><a href='https://semaphoreci.com/matthewkmayer/rusoto'> <img src='https://semaphoreci.com/api/v1/matthewkmayer/rusoto/branches/master/badge.svg' alt='Build Status'></a></td>\n    </tr>\n    <tr>\n        <td colspan=\"2\">\n            <a href=\"https://rusoto.github.io/rusoto/\" title=\"API Docs\"><img src=\"https://img.shields.io/badge/API-docs-blue.svg\" alt=\"api-docs-badge\"></img></a>\n            <a href=\"https://crates.io/crates/rusoto_core\" title=\"Crates.io\"><img src=\"https://img.shields.io/crates/v/rusoto_core.svg\" alt=\"crates-io\"></img></a>\n            <a href=\"#license\" title=\"License: MIT\"><img src=\"https://img.shields.io/badge/license-MIT-blue.svg\" alt=\"license-badge\"></img></a>\n        </td>\n    </tr>\n</table>\n\n**Rusoto is an AWS SDK for Rust**\n\n---\n\nYou may be looking for:\n\n* [An overview of Rusoto][rusoto-overview]\n* [AWS services supported by Rusoto][supported-aws-services]\n* [API documentation][api-documentation]\n* [Getting help with Rusoto][rusoto-help]\n\n## Requirements\n\nRust 1.23.0 or later is required.\n\nOn Linux, OpenSSL is required.\n\n## Installation\n\nRusoto is available on [crates.io](https://crates.io/crates/rusoto_core).\nTo use Rusoto in your Rust program built with Cargo, add it as a dependency and `rusoto_$SERVICENAME` for any supported AWS service you want to use.\n\nFor example, to include only S3 and SQS:\n\n``` toml\n[dependencies]\nrusoto_core = \"0.32.0\"\nrusoto_sqs = \"0.32.0\"\nrusoto_s3 = \"0.32.0\"\n```\n\n## Migration notes\n\nBreaking changes and migration details are documented at [https://rusoto.org/migrations.html](https://rusoto.org/migrations.html).\n\n## Usage\n\nRusoto has a crate for each AWS service, containing Rust types for that service's API.\nA full list of these services can be found [here][supported-aws-services].\nAll other public types are reexported to the crate root.\nConsult the rustdoc documentation for full details by running `cargo doc` or visiting the online [documentation](https://rusoto.github.io/rusoto/rusoto/index.html) for the latest crates.io release.\n\nA simple example of using Rusoto's DynamoDB API to list the names of all tables in a database:\n\n```rust\nextern crate rusoto_core;\nextern crate rusoto_dynamodb;\n\nuse rusoto_core::Region;\nuse rusoto_dynamodb::{DynamoDb, DynamoDbClient, ListTablesInput};\n\nfn main() {\n    let client = DynamoDbClient::simple(Region::UsEast1);\n    let list_tables_input: ListTablesInput = Default::default();\n\n    match client.list_tables(list_tables_input).sync() {\n        Ok(output) => {\n            match output.table_names {\n                Some(table_name_list) => {\n                    println!(\"Tables in database:\");\n\n                    for table_name in table_name_list {\n                        println!(\"{}\", table_name);\n                    }\n                },\n                None => println!(\"No tables in database!\"),\n            }\n        },\n        Err(error) => {\n            println!(\"Error: {:?}\", error);\n        },\n    }\n}\n```\n\n### Credentials\n\nFor more information on Rusoto's use of AWS credentials such as priority and refreshing, see [AWS Credentials](AWS-CREDENTIALS.md).\n\n## Semantic versioning\n\nRusoto complies with [semantic versioning 2.0.0](http://semver.org/).\nUntil reaching 1.0.0 the API is to be considered unstable.\nSee [Cargo.toml](Cargo.toml) or [rusoto on crates.io](https://crates.io/crates/rusoto_core) for current version.\n\n## Releases\n\nInformation on release schedules and procedures are in [RELEASING](RELEASING.md).\n\n## Contributing\n\nSee [CONTRIBUTING](CONTRIBUTING.md).\n\n## Supported OSs and Rust versions\n\nLinux, OSX and Windows are supported and tested via TravisCI and Appveyor.\n\nRust stable is supported.  Older versions of Rust are supported and tested via TravisCI.  The minimum Rust version is\nincremented when it becomes inconvenient to support older versions.  The current minimum version of Rust supported can\nbe found in [.travis.yml](.travis.yml).  If a version number is not specified in the `rust` section, only the named versions\nlisted are supported.  This should be stable, beta and nightly.\n\n## License\n\nRusoto is distributed under the terms of the MIT license.\n\nSee [LICENSE][license] for details.\n\n[api-documentation]: https://rusoto.github.io/rusoto/rusoto/ \"API documentation\"\n[license]: https://github.com/rusoto/rusoto/blob/master/LICENSE \"MIT License\"\n[rusoto-help]: https://www.rusoto.org/help.html \"Getting help with Rusoto\"\n[rusoto-overview]: https://www.rusoto.org/ \"Rusoto overview\"\n[supported-aws-services]: https://www.rusoto.org/supported-aws-services.html \"List of AWS services supported by Rusoto\"\n"
},
{
  "name": "certificate-revocation-analysis",
  "files": {
    "/": [
      "Censys_data_export.png",
      "LICENSE",
      "README.md",
      "build_filter",
      "get_CRL_revocations",
      "get_OCSP_revocations"
    ]
  },
  "makefile": null,
  "readme": "This collection of tools is designed to assemble a cascading\nbloom filter containing all TLS certificate revocations, as described\nin this [CRLite paper.](http://www.ccs.neu.edu/home/cbw/static/pdf/larisch-oakland17.pdf)\n\nThese tools were built from scratch, using the original CRLite research code as a design reference and closely following the documentation in their paper. \n\n## Dependancies\n1. A [Censys](https://censys.io) Researcher Account (for downloading certificates)\n2. **About 3 terabytes of space to store certificates and associated data**\n3. Node\n4. Python 2 & Python 3 (default is to use Python 2 except when explicitly noted)\n5. Aria2c (or wget or Curl)\n6. pyopenssl (at least version 16.1.0)\n7. Lots of patience, as many of the scripts take several hours even with multiprocessing\n\n## Instructions\n### Part A: Obtaining all NSS-trusted Certificates\n1. After obtaining a researcher account on [Censys](https://censys.io),\nperform the following Data export query to collect all valid NSS-trusted certificates.\nBe sure to request the results in JSON format, and select the \"nested\" option to\nprevent collisions when flattening the data entries. The compression option is also\nrecommended (see screenshot below). **If you just want to try the tools on a small sample\nsubset of certificates (no Censys account required), use [this file](https://drive.google.com/open?id=0B_ImpEaqYaA8djd2NkxLNFdEdE0) instead and\nskip to step 3.**\n\n```\nSELECT parsed.*\nFROM certificates.certificates\nWHERE validation.nss.valid = TRUE\n```\n\n![Screenshot](Censys_data_export.png \"Screenshot\")\n\n2. Download the exported certificates, which will be provided in several hundred\nfiles. The recommended method is to copy-paste the provided download URLs into\na file on your target machine, then use `wget -i URL_FILE` to download all\nof the certificate files.\n\n3. Unzip the certificate files and place their contents in a single, unified file.\nUnzip with `gzip -u *.gz`, then unify the files with `cat *.json > certificates.json`.\nYou can then delete all files except for `certificates.json`. (If you're using\nthe sample file, then just unzip it and rename it as `certificates.json`).\n\n### Part B: Determining CRL Revocations\n0. Set `get_CRL_revocations` as the working directory. This folder contains all scripts for Part B.\n\n1. Extract the CRL distribution points by running `python extract_crls.py`. This\nscript will output three files: a file of all certificates which have listed CRLs(`../certs_using_crl.json`),\na file of all certificates which do not list a CRL(`../certs_without_crl.json`),\nand a list of all CRL distribution points (`CRL_servers`).\n\n2. Sort and eliminate duplicate entries in `CRL_servers` using the command\n`sort -u CRL_servers > CRL_servers_final`. You can compare your `CRL_servers_final`\nto [my reference CRL list](https://drive.google.com/file/d/0B_ImpEaqYaA8MGRMSTh1cVJVdmM/view?usp=sharing)\nto see that the replication results are similar up to this point.\n\n3. Download all of the CRLs listed in `CRL_servers_final`. First create a new subdirectory `raw_CRLs`, set it as the working directory, then run `aria2c -i ../CRL_servers_final -j 16`.\n\n\n4. Set the working directory back one level up (to `get_CRL_revocations` again).\nCreate a catalogue, or \"megaCRL,\" of all revocations with `python3 build_megaCRL.py`\nscript **(note that this must use python3 and pyopenssl version 16.1.0 and above)**.\nThis will output `megaCRL`, which contains all revocation serial numbers\norganized by CRL.\n\n5. Use `python count_serials.py` to see the total number of revocation serials that are\ncontained in the megaCRL. You can compare your results against mine by using the\nsame script on [my reference megaCRL file](https://drive.google.com/file/d/0B_ImpEaqYaA8Y0YxRzhsZ09UX0E/view?usp=sharing).\n\n6. Make a new subdirectory `revokedCRLCerts`, then match the revocation serial numbers to known certificates using `python build_CRL_revoked.py`.\nThis script uses multiprocessing to get around the I/O bottleneck,\nand you may need to adjust the number of \"worker\" processes to get optimal\nspeed on your machine. Each worker has a dedicated output file, so after the script you\nwill need to combine each output file into a single, final result using\n`cat revokedCRLCerts/certs* > ../final_CRL_revoked.json`.\n\n7. Count the number of actual revoked certificates using `wc -l final_CRL_revoked.json`.\n\n### Part C: Determining OCSP Revocations\n0. Set `get_OCSP_revocations` as the working directory. This folder contains all scripts for Part C. Make a subdirectory called `OCSP_revoked`.\n\n1. Use `python build_OCSP_revoked.py` to determine all Let's Encrypt revocations.\nThis tooling replicates the process of the CRLite authors, and I believe they made this\ndesign choice to only include OCSP for Let's Encrypt based off the statistic that the\nvast majority of OCSP-only certificates are issued by them. After the script completes,\ncombine the results of each worker into a final output file with\n`cat OCSP_revoked/certs* > ../final_OCSP_revoked.json`.\n\n### Part D: Building The Filter\n0. Set `build_filter` as the working directory. This folder contains all scripts for Part D.\nMake subdirectories `final_unrevoked` and `final_revoked`.\n\n1. Use `python build_final_sets.py` to convert the data created from the steps above into a single\nset of all revoked certificates and all valid certificates. This script uses multiprocessing,\nso after running the script you will need to use `cat final_unrevoked/*.json > ../final_unrevoked.json`\nand `cat final_revoked/*.json > ../final_revoked.json` to combine the results of the individual\nworkers into a single file. You can see how your results match against mine by comparing\nagainst [this file](https://drive.google.com/file/d/0B_ImpEaqYaA8eHVlTnJ4cW9lclk/view?usp=sharing).\n\n2. Use the command `node ./build_filer.js --max_old_space_size=32768 > filter` to assemble\nthe final filter. Be sure to change the `REVOKED` and `UNREVOKED` constants to reflect\naccurately. **(acknowledgements to James Larisch for the build_filter.js code)**\n\n## Acknowledgements \nThanks to Eric Rescorla, J.C. Jones, James Larisch, the CRLite research team and the Mozilla Cryptography Engineering team.\n"
},
{
  "name": "webextensions-examples",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.json",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "annotate-page",
      "apply-css",
      "beastify",
      "bookmark-it",
      "borderify",
      "build",
      "chill-out",
      "commands",
      "context-menu-copy-link-with-types",
      "contextual-identities",
      "cookie-bg-picker",
      "devtools-panels",
      "discogs-search",
      "dynamic-theme",
      "embedded-webextension-bootstrapped",
      "embedded-webextension-overlay",
      "embedded-webextension-sdk",
      "emoji-substitution",
      "eslint-example",
      "examples.json",
      "export-helpers",
      "favourite-colour",
      "find-across-tabs",
      "firefox-code-search",
      "forget-it",
      "google-userinfo",
      "history-deleter",
      "http-response",
      "imagify",
      "latest-download",
      "list-cookies",
      "menu-demo",
      "menu-labelled-open",
      "mocha-client-tests",
      "native-messaging",
      "navigation-stats",
      "notify-link-clicks-i18n",
      "open-my-page-button",
      "package.json",
      "page-to-extension-messaging",
      "permissions",
      "private-browsing-theme",
      "proxy-blocker",
      "quicknote",
      "react-es6-popup",
      "runtime-examples",
      "selection-to-clipboard",
      "session-state",
      "store-collected-images",
      "stored-credentials",
      "tabs-tabs-tabs",
      "theme-integrated-sidebar",
      "theme-switcher",
      "themes",
      "top-sites",
      "user-agent-rewriter",
      "user-script",
      "webpack-modules",
      "window-manipulator"
    ]
  },
  "makefile": null,
  "readme": "# webextensions-examples [![Build Status](https://travis-ci.org/mdn/webextensions-examples.svg?branch=master)](https://travis-ci.org/mdn/webextensions-examples)\n\n[https://github.com/mdn/webextensions-examples](https://github.com/mdn/webextensions-examples)\n\nMaintained by the [MDN team at Mozilla](https://wiki.mozilla.org/MDN).\n\nWebExtensions are a way to write browser extensions: that is, programs\ninstalled inside a web browser that modify the behaviour of the browser or\nof web pages loaded by the browser. They are built on a set of\ncross-browser APIs, so WebExtensions written for Google Chrome or Opera will\nin most cases run in Firefox or Edge too.\n\nThe \"webextensions-examples\" repository is a collection of simple but complete\nand installable WebExtensions. You can use the examples to see how to use the\nWebExtensions APIs, and as a starting point for your own WebExtensions.\n\nFor an index of all the examples, see the [\"Example extensions\" page on MDN](https://developer.mozilla.org/Add-ons/WebExtensions/Examples).\n\nThe examples are made available under the\n[Mozilla Public License 2.0](https://www.mozilla.org/en-US/MPL/2.0/).\n\n## How to use \"webextensions-examples\"\n\nTo use the repository, first clone it.\n\nEach example is in its own top-level directory. Install an example in your\nfavourite web browser ([installation instructions](#installing-an-example) are below),\nand see how it works. Each example has its own short README explaining what\nit does.\n\nTo find your way around a WebExtension's internal structure, have a look at the\n[Anatomy of a WebExtension](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Anatomy_of_a_WebExtension)\npage on MDN.\n\nTo use these examples in Firefox, you should use the most recent release\nof Firefox. Some examples work with earlier releases.\n\nA few examples rely on APIs that are currently only available in pre-release\nversions of Firefox. Where this is the case, the example should declare\nthe minimum version that it needs in the `strict_min_version` part of the\n[applications key](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/manifest.json/applications)\nin its manifest.json file.\n\n## Installing an example\n\nThere are a couple ways to try out the example extensions in this repository.\n\n1. Open Firefox and load `about:debugging` in the URL bar. Click the\n   [Load Temporary Add-on](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Temporary_Installation_in_Firefox)\n   button and select the `manifest.json` file within the\n   directory of an example extension you'd like to install.\n   Here is a [video](https://www.youtube.com/watch?v=cer9EUKegG4)\n   that demonstrates how to do this.\n2. Install the\n   [web-ext](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext)\n   tool, change into the directory of the example extension\n   you'd like to install, and type `web-ext run`. This will launch Firefox and\n   install the extension automatically. This tool gives you some\n   additional development features such as\n   [automatic reloading](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext#Automatic_extension_reloading).\n\n## Learn more\n\nTo learn more about developing WebExtensions, see the\n[WebExtensions documentation on MDN](https://developer.mozilla.org/en-US/Add-ons/WebExtensions)\nfor getting started guides, tutorials, and full API reference docs.\n\n## Problems?\n\nIf you find a problem, please [file a bug](https://github.com/mdn/webextensions-examples/issues/new).\n\nIf you need help, email the [dev-addons mailing list](https://mail.mozilla.org/listinfo/dev-addons) or contact the WebExtensions team in the #webextensions IRC channel on irc.mozilla.org.\n\n## Contributing\n\nWe welcome contributions, whether they are whole new examples, new features,\nbug fixes, or translations of localizable strings into new languages. Please\nsee the [CONTRIBUTING.md](https://github.com/mdn/webextensions-examples/blob/master/CONTRIBUTING.md) file for more details.\n"
},
{
  "name": "services-quality-dashboard",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "Gemfile",
      "Gemfile.lock",
      "README.md",
      "VERSION",
      "assets",
      "config.ru",
      "dashboards",
      "jobs",
      "monitor.god",
      "npm-debug.log",
      "public",
      "widgets"
    ]
  },
  "makefile": null,
  "readme": "# Build it yourself\n## Using Ruby/Dashing directly (while developing)\n```\nbundle install\ndashing start\n```\nVisit localhost:3030 to view\n\n## Using Dockerfile (testing before deployment)\n\nBuild the image and run it:\n```\n$ docker build -t dashboard .\n$ docker run -t -i -d -P -p 80:80 dashboard\n```\nIf you're running docker directly on your host linux then you should see the dashboard on localhost, if you're on windows or osx machine and running docker in a VM then the dashboard will be running on your docker-environment host's IP. You can find your docker host IP by checking docker-machine's env vars, ex. with:\n```\n$ echo $DOCKER_HOST\ntcp://192.168.99.100:2376\n```\nyou would point your browser to http://192.168.99.100 or similar.\n\n# To pull and run the latest 'stable' docker image\n```\n$ docker run -t -i -d -P -p 80:80 mozservicesqa/dashboard\n```\nVisiting localhost or your docker-env IP as above. This docker image is also deployed to http://dashboard.stage.mozaws.net\n"
},
{
  "name": "maxscale-docker",
  "files": {
    "/": [
      "Dockerfile",
      "README.md",
      "circle.yml"
    ]
  },
  "makefile": null,
  "readme": "maxscale-docker\n===============\n\nThis project is a Docker container for MaxScale. \n\n[![](https://imagelayers.io/badge/asosso/maxscale:latest.svg)](https://imagelayers.io/?images=asosso/maxscale:latest 'ImageLayers') [![Docker Repository on Quay](https://quay.io/repository/asosso/maxscale/status \"Docker Repository on Quay\")](https://quay.io/repository/asosso/maxscale)\n\nBase [docker image](http://www.docker.io) to run a [MaxScale](https://mariadb.com/products/mariadb-maxscale) server\n\n    MariaDB MaxScale is an open-source, database-centric proxy that works with MariaDB Enterprise, MariaDB Enterprise Cluster, MariaDB 5.5, MariaDB 10 and Oracle MySQL\u00ae. \n    It\u2019s pluggable architecture is designed to increase flexibility and aid customization. Built upon a lightweight, high-speed networking core designed to facilitate throughput.\n    MariaDB MaxScale runs between the client application and the database cluster offering connection and statement-based load balancing. \n    MariaDB MaxScale allows scaling of an organization's database infrastructure while keeping the needs of DBAs, Developers and Data Architects in mind.\n\n## Getting the container\n\nThe container is very small and available on the Docker Index:\n\n    docker pull asosso/maxscale\n\n## Using the container\n\nJust trying out MaxScale.\n\nIf you just want to run a single instance of MaxScale server to try out its functionality:\n\n    docker run -d asosso/maxscale\n\n## Build the container\n\nTo create the image `asosso/maxscale`, execute the following command on the maxscale-docker folder:\n\n    docker build -t asosso/maxscale .\n\n## Thanks\n\n* [MaxScale](https://github.com/mariadb-corporation/MaxScale) - for its MySQL Proxy\n* [@MassimilianoPinto](https://github.com/MassimilianoPinto) - for his collaboration\n\n## Contribute\n\nContributions are welcome.\n\n1. Fork it\n2. Create your feature branch (`git checkout -b my-new-feature`)\n3. Commit your changes (`git commit -am 'Add some feature'`)\n4. Push to the branch (`git push origin my-new-feature`)\n5. Create new Pull Request\n\n## License\n\nCopyright 2015 Andrea Sosso\nLicensed under the MIT License"
},
{
  "name": "pytest-testrail",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "MANIFEST.ini",
      "Makefile",
      "README.md",
      "README.rst",
      "pytest_testrail",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": "\ndefine HELP\n\nThis is the pytest testrail project Makefile.\n\nUsage:\n\nmake requirements - Install dependencies\nmake coverage     - Run coverage analysis\nmake lint         - Run static analysis\nmake test         - Run static analysis, tests with coverage\nmake quicktest    - Run tests without coverage\nmake cleantest    - Run tests cleaning tox environment first\nmake clean        - Remove generated files\nendef\n\nexport HELP\n\n\n.PHONY: all clean help lint quicktest requirements test\n\n\nall help:\n\t@echo \"$$HELP\"\n\n\nlint:\n\tflake8 pytest_testrail | tee pytest_testrail.txt\n\nrequirements: .requirements.txt\n\n.requirements.txt: requirements/*.txt\n\tpip install -r requirements/base.txt\n\tpip freeze > $@\n\nREADME.rst: README.md\n\tpandoc --from=markdown --to=rst --output=README.rst README.md\n\ntest: coverage lint\n\ttox\n\ncoverage:\n\ttox -e coverage\n\nclean:\n\trm -rf .cache .coverage .tox pytests_py*-test.xml pytest_testrail.egg-info pytest_testrail.txt pytests_coverage.xml\n\tfind . -name '*.pyc' -delete\n",
  "readme": "pytest-testrail\n===============\n\n|Build Status|\n\nThis is a pytest plugin for creating testruns based on pytest markers.\nThe results of the collected tests will also be updated against the\ntestrun in TestRail.\n\nInstallation\n------------\n\n::\n\n    pip install pytest-testrail\n\nConfiguration\n-------------\n\nAdd a marker to the tests that will be picked up to be added to the run.\n\n::\n\n    from pytest_testrail.plugin import testrail\n\n    @testrail('C1234', 'C5678')\n    def test_foo():\n        # test code goes here\n\nSettings file template cfg:\n\n::\n\n    [API]\n    url = https://yoururl.testrail.net/\n    email = user@email.com\n    password = password\n\n    [TESTRUN]\n    assignedto_id = 1\n    project_id = 1\n    suite_id = 1\n\nUsage\n-----\n\n::\n\n    py.test --testrail=<settings file>.cfg\n\nThis will create a test run in TestRail, add all marked tests to run.\nOnce the all tests are finished they will be updated in TestRail.\n\n::\n\n    --tr_name='My Test Run'\n\nTestruns can be named using the above flag, if this is not set a\ngenerated one will be used. ' Automation Run \"timestamp\" '\n\n::\n\n    --no-ssl-cert-check\n\nThis flag can be used prevent checking for a valid SSL certificate on\nTestRail host.\n\n.. |Build Status| image:: https://travis-ci.org/allankilpatrick/pytest-testrail.svg?branch=master\n   :target: https://travis-ci.org/allankilpatrick/pytest-testrail\n"
},
{
  "name": "msisdn-verifier-client",
  "files": {
    "/": [
      "Makefile",
      "README.md",
      "css",
      "index.html",
      "js",
      "manifest.webapp"
    ]
  },
  "makefile": "serve:\n\tpython -m SimpleHTTPServer 8000\n",
  "readme": "msisdn-verifier-client\n======================\n\nhttp://mozilla-services.github.io/msisdn-verifier-client/\n"
},
{
  "name": "transmutator",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "AUTHORS",
      "CHANGELOG",
      "INSTALL",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "VERSION",
      "etc",
      "setup.py",
      "tests",
      "transmutator"
    ]
  },
  "makefile": "# Makefile for development.\n# See INSTALL and docs/dev.txt for details.\nVENV := $(shell echo $${VIRTUAL_ENV-var/venv})\nROOT_DIR = $(shell pwd)\nBIN_DIR = $(VENV)/bin\nPYTHON=$(BIN_DIR)/python\nPIP = $(BIN_DIR)/pip\nNOSE = $(BIN_DIR)/nosetests\nPROJECT = $(shell $(PYTHON) -c \"import setup; print setup.NAME\")\n\ninstall: develop\n\ndevelop:\n\t@mkdir -p $(ROOT_DIR)/var\n\tvirtualenv $(VENV)\n\t$(PYTHON) setup.py develop\n\t$(PIP) install nose rednose docutils coverage sphinxcontrib-testbuild flake8\n\nclean:\n\tfind $(ROOT_DIR)/ -name \"*.pyc\" -delete\n\tfind $(ROOT_DIR)/ -name \".noseids\" -delete\n\n\ndistclean: clean\n\trm -rf $(ROOT_DIR)/*.egg-info\n\n\ntest: test-app test-pep8\n\n\ntest-app:\n\t@mkdir -p $(ROOT_DIR)/var/test/\n\t$(NOSE) -c $(ROOT_DIR)/etc/nose.cfg tests $(PROJECT)\n\tmv $(ROOT_DIR)/.coverage $(ROOT_DIR)/var/test/app.coverage\n\n\ntest-pep8:\n\t$(BIN_DIR)/flake8 $(PROJECT)\n\n\ntest-documentation:\n\t$(NOSE) -c $(ROOT_DIR)/etc/nose.cfg sphinxcontrib.testbuild.tests\n\n\ndocumentation: sphinx-apidoc sphinx-html\n\n\n# Remove auto-generated API documentation files.\n# Files will be restored during sphinx-build, if \"autosummary_generate\" option\n# is set to True in Sphinx configuration file.\nsphinx-apidoc-clean:\n\tfind docs/api/ -type f \\! -name \"index.txt\" -delete\n\n\nsphinx-apidoc: sphinx-apidoc-clean\n\t$(BIN_DIR)/sphinx-apidoc --output-dir $(ROOT_DIR)/docs/api/ --suffix txt $(PROJECT)\n\n\nsphinx-html:\n\tif [ ! -d docs/_static ]; then mkdir docs/_static; fi\n\tmake --directory=docs clean html doctest\n\n\nrelease:\n\t$(PIP) install zest.releaser\n\t$(BIN_DIR)/fullrelease\n",
  "readme": "############\nTransmutator\n############\n\nTransmutator is a general purpose migration framework. \nIt focuses on automating actions you perform to upgrade (or downgrade) a\nproduct.\n\n.. warning::\n\n   This project is experimental. At this stage, it just describes concepts.\n   Perhaps the concepts are implemented by some existing tools.\n\nA typical migration for a web service could be:\n\n* ask admin for confirmation\n* enable maintenance page\n* stop frontends\n* backup data\n* update configuration\n* provision machines (upgrade software)\n* migrate databases\n* restart frontends\n* run smoketests\n* disable maintenance page.\n\n\n***********************\nUpgrades are migrations\n***********************\n\nProvisioning is not enough to manage upgrades.\nDatabase migrations are just a part of the upgrade procedure.\nWe need migration scripts and workflows.\n\n\n*****************\nMigration scripts\n*****************\n\n* Recipes are libraries that provide classes with forward() and backward()\n  methods.\n* Dispatchers manage lists of actions to run and watch/notify stop conditions.\n* Migration scripts load, configure and run recipes.\n* Collecter grabs the list of unapplied migration scripts.\n\nMigration scripts are small shell scripts that accept standardized arguments:\nthey \"forward\" by default, they optionally implement \"--backward\". The\nlanguage in which scripts are written does not matter.\n\nThere is two kind of migration scripts:\n\n* atomic mutations, migrations that focus on one thing;\n* orchestration, migrations that focus on running smaller scripts.\n\nAn upgrade from a release to another is typically encapsulated in an\norchestration-type script, which itself groups atomic-type scripts.\n\n\n*******************\nMigration workflows\n*******************\n\nAutomating everything is hard, sometimes impossible, sometimes unwanted.\nA migration procedure is a workflow: it passes from one state to another via\ntransitions. Most transitions can be automated, but some may require human\ninteraction.\n\nExample of human interactions:\n\n* setup SSH keys\n* update configuration where defaults are not suitable\n* review and confirm some actions\n* perform actions that have not been automated yet\n\n\n********************************\nIterative deployment development\n********************************\n\nWhen you start a project, you do not want to spend days to get the perfect\ndeployment workflow. In fact, you usually cannot even get a suitable deployment\nworkflow at first. Partly because you do not know how to deploy things you\nhave not developed yet. Partly because you want to focus on proof of concepts,\nwhere automated deployment is not top priority.\n\nTransmutator allows you to setup interactive workflows, where you can tell the\nuser to perform actions you have not automated yet.\n\n\n********************************\nRemote-control multiple machines\n********************************\n\nOn distributed architectures, you have to orchestrate migrations on multiple\nmachines. Transmutator runs high-level migration scripts that use your favorite\nremote-control tools, such as fabric or salt.\n\n\n****************\nFrom DEV to PROD\n****************\n\nMigrations are part of the development process. Several developers can\ncontribute to migrations, concurrently. Transmutator is made to reproduce\nmigrations over every environments, from DEV to PROD.\nThe differences between DEV (tends to be monolithic) and PROD (tends to be\ndistributed) are managed via configuration. Transmutator supposes you manage\narchitecture as configuration.\n\n\n***************************\nFeatures / workflows / demo\n***************************\n\nAt last, here is implementation... `transmutator` tries to implement the\nconcepts above. It is a proof of concept. If you feel something is going wrong,\nplease tell us ;)\n\n.. note:: This is kind of a plan for some tests.\n\n.. note:: All features are not be implemented yet.\n\n* `transmutator` provides ``transmute`` command.\n\n* ``transmute`` without arguments runs mutations \"forward\".\n\n* ``transmute`` reads mutations in `mutations directory`: ``mutations`` folder\n  in current directory (pwd).\n\n* here is a sample \"mutations\" folder tree:\n\n  .. code:: text\n\n     mutations\n     \u251c\u2500\u2500 0001_hello_world.py\n     \u251c\u2500\u2500 0040_1234.sh\n     \u251c\u2500\u2500 1.2\n     \u2502\u00a0\u00a0 \u2514\u2500\u2500 0093_print_version.sh\n     \u251c\u2500\u2500 1.3\n     \u2502\u00a0\u00a0 \u2514\u2500\u2500 0060_print_version.sh\n     \u251c\u2500\u2500 development\n     \u2502\u00a0\u00a0 \u2514\u2500\u2500 0077_refactoring.py\n     \u2514\u2500\u2500 recurrent\n         \u2514\u2500\u2500 0050_syncdb.sh\n\n* A mutation file must be executable. Else, it is ignored.\n\n* All mutation scripts/binaries implement the `mutation interface`:\n\n  * no arguments means \"forward\"\n  * accept ``--backward`` argument to run \"backward\" instead of \"forward\"\n  * that's all for now. Later, additional options such as ``help`` may be\n    added.\n\n* Mutations can be grouped by \"release/version\". In the example above:\n\n  * ``0001_hello_world.py`` and ``0040_1234.sh`` have \"no release\".\n  * ``1.2/0093_print_version.sh`` has release \"1.2\"\n  * ``1.3/0060_print_version.sh`` has release \"1.3\"\n  * mutations in ``development/`` have not been released yet, their content\n    may change during developement.\n  * mutations in ``recurrent/`` are special kind of mutations, they are to\n    be executed for every release.\n\n* Mutations are executed in order:\n\n  * first ordering criteria is \"release/version\" groups:\n\n    * ``1.2/0093_print_version.sh`` is executed before\n      ``1.3/0060_print_version.sh``\n\n    * mutations in ``development/`` are executed at the end. \"development\" is\n      a special release, the latest.\n\n    * mutations in ``recurrent/`` are considered part of every release, so\n      they are run for each release.\n    \n  * then, in a release, mutations are sorted by filename:\n\n    * ``0001_hello_world.py`` is executed before ``0040_1234.sh``\n\n    * ``recurrent/0050_syncdb.sh`` is executed before\n      ``1.3/0060_print_version.sh``\n\n* Once mutations have been executed, they are not executed again. Except\n  recurrent and in-development mutations:\n\n  * recurrent mutations are executed (forward) for each release\n  * in-development mutations are always executed. But they are run \"backward\"\n    then \"forward\" (undo/redo).\n"
},
{
  "name": "awsbox",
  "files": {
    "/": [
      ".awsbox.json",
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "ChangeLog",
      "LICENSE",
      "README.md",
      "awsbox.js",
      "defaultImages.json",
      "doc",
      "lib",
      "package.json",
      "tests",
      "users"
    ]
  },
  "makefile": null,
  "readme": "# A Cloudops fork of Awsbox\n\nKey differences: \n\n* matches closer with what cloudops uses to deploy to production\n* uses our custom EC2 AMI and node.js deployment procedures\n* works as close as possible to [mozilla/awsbox](https://github.com/mozilla/awsbox)\n* etc. etc..\n\n## A Lightweight DIY PaaS for Amazon Web Services\n\nAmazon Web Services (AWS) \"Elastic Compute Cloud\" provides low cost, instant\non VMs that can be used to deploy any kind of service.  AWS also provides a\nfull set of APIs that make it possible to programatically allocate servers.\nFinally, AWS offers the ability to create \"template\" instances (Amazon Machine\nImages) that are VM snapshots.\n\n*The problem:* For small scale nodejs projects, there's a lot of\nadministrative boiler plate work that one must to set up a machine.\nYou must install web server software, set up security policies and network\naccess, copy up your keypair, determine how you'll deploy your software on the\nnew VM, etc.\n\n\"Platform as a service\" providers like heroku make most of these decisions for\nyou, providing a demand spun \"vm-like\" thing that you can deploy code on by\nadhering to a couple conventions and `git pushing`.  Where heroku breaks down\nis in *generativity* - you are limited to doing things that heroku has thought\nof, and when you want to do something custom (install a new native software\nlibrary, run an experimental database for which you cannot find a third party\nhosted provider) - you are screwed.\n\nAlso, heroku is relatively expensive.  The moment you want to run two\nprocesses, you're paying 0.05$/hr for that process vs. on aws where\nyou can purchase a \"micro\" instance for 0.02$/hr for the whole VM.\nThe final area of expense is in \"add-ons\" - service providers that offer\nthings like hosted databases, email sending, etc.  A small scale database\ncan cost another .015$/hr.\n\nBut Wait!  What about [nodejitsu]?  Well, probably use them: they're\nawesome, smart, admirably share their work, have a free service for\nnon-commercial deployments, and *just work* for most apps.  But\nsometimes you might want full control.  That you?  Read on...  (NOTE:\nawsbox is *built* on lots of nodejistu stuffs).\n\n  [nodejitsu]: http://nodejitsu.com/\n\nSo what we maybe want is the convenience of Nodejitsu and Heroku, and the\npricing and freedom of a raw amazon image...\n\n*The solution:* **awsbox** is a set of nodejs scripts, a command line utility,\nand a template image (AMI).  Together it allows you to deploy a new server\nfrom the command line that is pre-configured to run your Node.JS service.\n\n## Features\n\n  * **nodejs focused** - While other stacks could be supported in the future,\n    awsbox is laser focused on node.js to start.\n  * **full root access** - awsbox just gets you started, after that you can do\n    Whatever You Want.\n  * **magic ssh key config** - Your SSH key will be copied up and installed for you.\n  * **git push support** - After you provision a vm, it's pre-configured so you can\n    push to deploy\n  * **multi-region support** - awsbox base AMIs are published in every region AWS\n    supports, so you can deploy anywhere.\n  * **command line or programmatic usage** - type at it, or script it.\n  * **OS level user isolation** - all deployed code is run with user permissions under\n    a single account.\n  * **HTTP forwarding with custom 503 page** - [nginx] is pre-configured to forward\n    requests to your nodejs process bound to a local port.\n  * **SSL support** - By default your process runs with a self-signed cert.  Enabling\n    SSL support is as easy as copying up a private key and certificate in [PEM] format.\n  * **WebSocket support** - AWSBOX fully supports WebSockets, via [socket.io] or otherwise.\n  * **Route53 support** - manage your DNS from the command line, and have DNS set up for\n    your boxes at creation time.\n\n  [nginx]: https://nginx.org\n  [PEM]: http://en.wikipedia.org/wiki/X.509\n  [socket.io]: http://socket.io\n\n## Get Started\n\nStart by working through [the tutorial].  Then have a look at the [Hello World] sample app.\nAnd after that, check out [the documentation] in this repository.\n\n  [the tutorial]: https://github.com/mozilla/awsbox/blob/master/doc/TUTORIAL.md\n  [Hello World]: https://github.com/lloyd/awsbox-helloworld\n  [the documentation]: https://github.com/mozilla/awsbox/tree/master/doc\n"
},
{
  "name": "retools",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "COPYRIGHT.txt",
      "LICENSE.txt",
      "README.rst",
      "docs",
      "retools",
      "setup.cfg",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "api.rst",
      "api",
      "caching.rst",
      "changelog.rst",
      "conf.py",
      "index.rst"
    ]
  },
  "makefile": null,
  "readme": "=====================\nRedis Tools (retools)\n=====================\n\n``retools`` is a package of Redis tools. It's aim is to provide a variety of\nRedis backed Python tools that are always 100% unit tested, fast, efficient,\nand utilize the capabilities of Redis.\n\nCurrent tools in ``retools``:\n\n* Caching\n* Global Lock\n\nOn the horizon for future implementation:\n\n* A worker/job processing system similar to Celery but based on how Ruby's\n  Resque system works.\n\n.. image:: https://secure.travis-ci.org/bbangert/retools.png?branch=master\n   :alt: Build Status\n   :target: https://secure.travis-ci.org/bbangert/retools\n\n\nCaching\n=======\n\nA high performance caching system that can act as a drop-in replacement for\nBeaker's caching. Unlike Beaker's caching, this utilizes Redis for distributed\nwrite-locking dogpile prevention. It also collects hit/miss cache statistics\nalong with recording what regions are used by which functions and arguments.\n\nExample::\n    \n    from retools.cache import CacheRegion, cache_region, invalidate_function\n    \n    CacheRegion.add_region('short_term', expires=3600)\n    \n    @cache_region('short_term')\n    def slow_function(*search_terms):\n        # Do a bunch of work\n        return results\n    \n    my_results = slow_function('bunny')\n    \n    # Invalidate the cache for 'bunny'\n    invalidate_function(slow_function, [], 'bunny')\n\n\nDifferences from Beaker\n-----------------------\n\nUnlike Beaker's caching system, this is built strictly for Redis. As such, it\nadds several features that Beaker doesn't possess:\n\n* A distributed write-lock so that only one writer updates the cache at a time\n  across a cluster.\n* Hit/Miss cache statistics to give you insight into what caches are less\n  effectively utilized (and may need either higher expiration times, or just\n  not very worthwhile to cache).\n* Very small, compact code-base with 100% unit test coverage.\n\n\nLocking\n=======\n\nA Redis based lock implemented as a Python context manager, based on `Chris\nLamb's example\n<http://chris-lamb.co.uk/2010/06/07/distributing-locking-python-and-redis/>`_.\n\nExample::\n    \n    from retools.lock import Lock\n    \n    with Lock('a_key', expires=60, timeout=10):\n        # do something that should only be done one at a time\n\n\nLicense\n=======\n\n``retools`` is offered under the MIT license.\n\n\nAuthors\n=======\n\n``retools`` is made available by `Ben Bangert`.\n"
},
{
  "name": "services-central",
  "files": {
    "/": [
      ".gdbinit",
      ".gitignore",
      ".hgignore",
      ".hgtags",
      "AUTHORS",
      "Android.mk",
      "CLOBBER",
      "LEGAL",
      "LICENSE",
      "Makefile.in",
      "README.txt",
      "accessible",
      "aclocal.m4",
      "addon-sdk",
      "allmakefiles.sh",
      "b2g",
      "browser",
      "build",
      "caps",
      "chrome",
      "client.mk",
      "client.py",
      "config",
      "configure.in",
      "content",
      "db",
      "dbm",
      "docshell",
      "dom",
      "editor",
      "embedding",
      "extensions",
      "gfx",
      "hal",
      "image",
      "intl",
      "ipc",
      "js",
      "layout",
      "mach",
      "media",
      "memory",
      "mfbt",
      "mobile",
      "modules",
      "mozglue",
      "mozilla-config.h.in",
      "netwerk",
      "nsprpub",
      "other-licenses",
      "parser",
      "probes",
      "profile",
      "python",
      "rdf",
      "security",
      "services",
      "startupcache",
      "storage",
      "testing",
      "toolkit",
      "tools",
      "uriloader",
      "view",
      "webapprt",
      "widget",
      "xpcom",
      "xpfe",
      "xulrunner"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "metlog-hive",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.rst",
      "Makefile",
      "README.rst",
      "build.xml",
      "ivy.xml",
      "ivysettings.xml",
      "rpm",
      "src",
      "test"
    ]
  },
  "makefile": "# For some reason, ant sometimes needs the CLASSPATH set and won't just use what is in ~/.ant/lib\n\ndefault: build_rpm\n\ndist/lib/MetlogHive.jar:\n\tCLASSPATH=~/.ant/lib/ivy-2.3.0-rc1.jar ant\n\nbuild_rpm: \tdist/lib/MetlogHive.jar\n\tmkdir -p build/SOURCES\n\trm -rf build/RPMS build/metlog-hive-*\n\ttar czf build/SOURCES/metlog-hive.tar.gz dist/lib\n\trpmbuild --define \"_topdir $$PWD/build\" -ba rpm/metlog-hive.spec\n\tcp build/RPMS/*/*.rpm build/\n\tls -l build/metlog-hive*.rpm\n\nclean:\n\trm -rf build dist\n",
  "readme": "===========\nmetlog-hive\n===========\n\nmetlog-hive is a set of extensions for Apache Hive to provide richer query support\nfor JSON structures that Metlog produces. \n\nCurrently we provide a User Defined Table Function (UDTF)\n`exjson_tuple` that supports indexing into a JSON structure at an\narbitrary depth with support for both dictionaries and list\nstructures.  This is meant to be a drop in replacement for calls to\nthe standard Hive `json_tuple` UDTF.\n\nMore information about how Mozilla Services is using Metlog (including what is\nbeing used for a router and what endpoints are in use / planning to be used)\ncan be found on the relevant `spec page\n<https://wiki.mozilla.org/Services/Sagrada/Metlog>`_.\n\n\nSyntax\n------\n\nExJSONTuple expects (json_blob, json_path1, json_path2, ... , json_pathN)\n\nEach `json_path` is a dotted path notation string and can also use square bracket syntax to address items arrays.\n\nSome examples should clear this up.\n\nHere's a sample JSON blob that has been formatted so that it's easier to see what's going on ::\n\n       {\"severity\":6,\n        \"timestamp\":\"2012-07-18T21:16:45.572517\",\n        \"metlog_hostname\":\"aitc1.web.mtv1.dev.svc.mozilla.com\",\n        \"fields\":{\"headers\":{\"path\":\"/1.0/11225/devices/\",\n                \"host\":\"dev-aitc10.services.mozilla.com:443\",\n                \"some_list\": [5,10,15],\n                \"User-Agent\":\"\"},\n        \"threadlocal\":{}},\n        \"metlog_pid\":26482,\n        \"logger\":\"\",\n        \"type\":\"wsgi\",\n        \"payload\":\"\",\n        \"env_version\":\"0.8\"}\n\n\nWe can address the top level keys in this blob using paths like \"severity\", \"timestamp\" and \"metlog_hostname\". \nTo address values in the `fields` section, you just have to use dot notation.\n\n        ========================= =============================================\n        Path                      Value\n        ------------------------- ---------------------------------------------\n        severity                       6  \n        timestamp                      \"2012-07-18T21:16:45.572517\"\n        metlog_hostname                \"aitc1.web.mtv1.dev.svc.mozilla.com\"\n        fields.headers.path            \"/1.0/11225/devices/\"\n        fields.host                    \"dev-aitc10.services.mozilla.com:443\"\n        fields.some_list[2]            15\n        ========================= =============================================\n\n\nGetting started with JSON in Hive\n---------------------------------\n\nFor full documentation on using Hive, please refer to the \ndocumentation at:\n\n    http://hive.apache.org/\n\nUsing the exjson_tuple is fairly straight forwrad.  The basic steps\nare :\n \n\n 1. copy the MetlogHive.jar file into the HDFS cluster if it doesn't\n    already exist. We suggest putting the JAR file into /metlog/lib\n 2. create a table in hive where you want to process your JSON data if\n    one doesn't already exist\n 3. Get your JSON data onto the HDFS cluster\n 4. load the data from HDFS into the your Hive table\n 5. enable the exjson_tuple function in Hive with a \"create temporary\n    function\" call\n 6. run your query\n\n\n1. Create the DFS directory and put the MetlogHive.jar file into HDFS ::\n\n    $ hadoop dfs -mkdir /metlog/lib\n    $ hadoop dfs -put MetlogHive.jar /metlog/lib\n\n2. Create the Hive table if it doesn't already exist ::\n\n    $ hive \n    Hive history file=/tmp/vagrant/hive_job_log_vagrant_201208091816_1680191485.txt\n    hive> create table metlog_json(json STRING);\n    OK\n    Time taken: 15.484 seconds\n\n3.  Store JSON onto HDFS ::\n    \n    $ hadoop dfs -put metrics_hdfs.log /var/log/aitc\n\n4.  Load the data into Hive and do a quick test of the data ::\n        \n        $ hive\n        hive> LOAD DATA INPATH '/var/log/aitc/metrics_hdfs.log' overwrite into table metlog_json;\n        Loading data to table default.metlog_json\n        Deleted hdfs://localhost/user/hive/warehouse/metlog_json\n        OK\n        Time taken: 14.087 seconds\n        hive> select * from metlog_json limit 5;\n        OK\n        {\"severity\":6,\"timestamp\":\"2012-07-18T21:13:25.687518\",\"metlog_hostname\":\"aitc1.web.mtv1.dev.svc.mozilla.com\",\"fields\":{\"headers\":{\"path\":\"/1.0/11224/apps/\",\"host\":\"dev-aitc5.services.mozilla.com:443\",\"User-Agent\":\"\"},\"threadlocal\":{}},\"metlog_pid\":26482,\"logger\":\"\",\"type\":\"wsgi\",\"payload\":\"\",\"env_version\":\"0.8\"}\n        {\"severity\":6,\"timestamp\":\"2012-07-18T21:13:26.140094\",\"metlog_hostname\":\"aitc1.web.mtv1.dev.svc.mozilla.com\",\"fields\":{\"headers\":{\"path\":\"/1.0/11224/apps/\",\"host\":\"dev-aitc5.services.mozilla.com:443\",\"User-Agent\":\"\"},\"threadlocal\":{}},\"metlog_pid\":26483,\"logger\":\"\",\"type\":\"wsgi\",\"payload\":\"\",\"env_version\":\"0.8\"}\n        {\"severity\":6,\"timestamp\":\"2012-07-18T21:13:26.400760\",\"metlog_hostname\":\"aitc1.web.mtv1.dev.svc.mozilla.com\",\"fields\":{\"headers\":{\"path\":\"/1.0/11224/devices/\",\"host\":\"dev-aitc5.services.mozilla.com:443\",\"User-Agent\":\"\"},\"threadlocal\":{}},\"metlog_pid\":26482,\"logger\":\"\",\"type\":\"wsgi\",\"payload\":\"\",\"env_version\":\"0.8\"}\n        {\"severity\":6,\"timestamp\":\"2012-07-18T21:13:26.555777\",\"metlog_hostname\":\"aitc1.web.mtv1.dev.svc.mozilla.com\",\"fields\":{\"headers\":{\"path\":\"/1.0/11224/devices/\",\"host\":\"dev-aitc5.services.mozilla.com:443\",\"User-Agent\":\"\"},\"threadlocal\":{}},\"metlog_pid\":26482,\"logger\":\"\",\"type\":\"wsgi\",\"payload\":\"\",\"env_version\":\"0.8\"}\n        {\"severity\":6,\"timestamp\":\"2012-07-18T21:13:27.018063\",\"metlog_hostname\":\"aitc1.web.mtv1.dev.svc.mozilla.com\",\"fields\":{\"headers\":{\"path\":\"/1.0/11224/apps/Mnw_2ofOKGhIpXSYLd0LfHSH-BY\",\"host\":\"dev-aitc5.services.mozilla.com:443\",\"User-Agent\":\"\"},\"threadlocal\":{}},\"metlog_pid\":26482,\"logger\":\"\",\"type\":\"wsgi\",\"payload\":\"\",\"env_version\":\"0.8\"}\n        Time taken: 1.458 seconds\n\n5. Enable the JAR in Hive and bind the `exjuson_tuple` name to the\n   class \n\n   ::\n\n       hive> add jar hdfs:///metlog/lib/MetlogHive.jar;\n       converting to local hdfs:///metlog/lib/MetlogHive.jar\n       Added /tmp/vagrant/hive_resources/MetlogHive.jar to class path\n       Added resource: /tmp/vagrant/hive_resources/MetlogHive.jar\n       hive> create temporary function exjson_tuple as 'org.mozilla.services.json.ExJSONTuple';\n       OK\n       Time taken: 0.658 seconds\n       hive> \n   \n6.  You should now be able to run a simple query. ::\n\n        hive> select f1, f2, f3 from metlog_json m lateral view exjson_tuple(m.json, \"fields.headers.host\", \"severity\", \"fields.headers.path\") b as f1, f2, f3 limit 5;\n        Total MapReduce jobs = 1\n        Launching Job 1 out of 1\n        Number of reduce tasks is set to 0 since there's no reduce operator\n        Starting Job = job_201208091751_0001, Tracking URL = http://localhost:50030/jobdetails.jsp?jobid=job_201208091751_0001\n        Kill Command = /usr/lib/hadoop/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201208091751_0001\n        2012-08-09 19:02:14,179 Stage-1 map = 0%,  reduce = 0%\n        2012-08-09 19:02:20,245 Stage-1 map = 100%,  reduce = 0%\n        2012-08-09 19:02:24,278 Stage-1 map = 100%,  reduce = 100%\n        Ended Job = job_201208091751_0001\n        OK\n        dev-aitc5.services.mozilla.com:443     6    /1.0/11224/apps/\n        dev-aitc5.services.mozilla.com:443     6    /1.0/11224/apps/\n        dev-aitc5.services.mozilla.com:443     6    /1.0/11224/devices/\n        dev-aitc5.services.mozilla.com:443     6    /1.0/11224/devices/\n        dev-aitc5.services.mozilla.com:443     6    /1.0/11224/apps/Mnw_2ofOKGhIpXSYLd0LfHSH-BY\n        Time taken: 32.534 seconds\n        hive> \n\n\nBuilding the plugin\n-------------------\n\nThis plugin has been built with the following tools.  Lower versions of each package may work, but have not been tested.\n\n    * Java SDK.  (>= 1.6.0.33) \n    * Apache Ant (>= 1.8.2)\n    * Apache Ivy (>= 2.3.0-rc1)\n    * Ant-JUnit  (>= 1.8.0)\n\nAssuming your Ivy and JUnit jar files are located in ~/.ant/lib, ant\nshould be able to run all targets in the build.xml file.  Your\n~/.ant/lib should look something like this ::\n\n    ~ > ls -l ~/.ant/lib\n    total 2872\n    -rw-r--r--  1 victorng  staff   1.2M 16 Apr 00:02 ivy-2.3.0-rc1.jar\n    -rw-r--r--  1 victorng  staff   247K 30 Sep  2011 ant-junit.jar\n\n    ~ > \n\nBuilding a JAR file ::\n\n    $ ant\n\nRunning tests ::\n\n    $ ant test\n\nGenerating the Javadoc ::\n\n    $ ant javadoc\n"
},
{
  "name": "product-announcements-test-addon",
  "files": {
    "/": [
      ".gitignore",
      "README",
      "bootstrap.js",
      "build.sh",
      "build_beta.sh",
      "config_build.sh",
      "install.rdf",
      "jni.jsm"
    ]
  },
  "makefile": null,
  "readme": "Restartless native Fennec add-on to set testing prefs for product announcements feature.\n\nBased on cscott/skeleton-addon-fxandroid.\n\nWhat to do:\n\n* Edit config_build.sh to set ANDROID_APP_ID to the product you're testing (e.g., org.mozilla.fennec).\n* Edit bootstrap.js, setting ANNO_URL to the server URL and ANNO_INTERVAL to the check interval in milliseconds.\n* Run build.sh with your device connected and reachable over ADB.\n\nAsk rnewman if you have other questions.\n"
},
{
  "name": "FunkLoad",
  "files": {
    "/": [
      ".gitignore",
      "BUILDOUT.txt",
      "CHANGES.txt",
      "INSTALL.txt",
      "LICENSE.txt",
      "Makefile",
      "README.rst",
      "README.txt",
      "THANKS",
      "TODO.txt",
      "bootstrap.py",
      "buildout.cfg",
      "contrib",
      "doc",
      "ez_setup.py",
      "minimal.cfg",
      "scripts",
      "setup.py",
      "src"
    ]
  },
  "makefile": "# FunkLoad Makefile\n# $Id: $\n#\n.PHONY: build pkg sdist egg install clean rpm\n\nTARGET := gateway:/opt/public-dev/funkload\n\n# use TAG=a for alpha, b for beta, rc for release candidate\nifdef TAG\n\tPKGTAG := egg_info --tag-build=$(TAG) --tag-date\nelse\n    PKGTAG :=\nendif\n\n\nbuild:\n\tpython setup.py $(PKGTAG) build\n\ntest:\n\tcd src/funkload/tests && fl-run-test -v --doctest test_Install.py\n\npkg: sdist egg\n\nsdist:\n\tpython setup.py $(PKGTAG) sdist\n\negg:\n\t-python2.4 setup.py $(PKGTAG) bdist_egg\n\t-python2.5 setup.py $(PKGTAG) bdist_egg\n\t-python2.6 setup.py $(PKGTAG) bdist_egg\n\t-python2.7 setup.py $(PKGTAG) bdist_egg\n\ndistrib:\n\t-scp dist/funkload-*.tar.gz $(TARGET)/snapshots\n\t-scp dist/funkload-*.egg $(TARGET)/snapshots\n\ninstall:\n\tpython setup.py $(PKGTAG) install\n\nregister:\n\t-python2.6 setup.py register sdist bdist_egg upload\n\t-python2.7 setup.py register bdist_egg upload\n\t-python2.5 setup.py register bdist_egg upload\n\n\nuninstall:\n\t-easy_install -m funkload\n\t-rm -rf /usr/lib/python2.3/site-packages/funkload*\n\t-rm -rf /usr/lib/python2.4/site-packages/funkload*\n\t-rm -rf /usr/lib/python2.5/site-packages/funkload*\n\t-rm -rf /usr/lib/python2.6/dist-packages/funkload*\n\t-rm -rf /usr/local/lib/python2.6/dist-packages/funkload*\n\t-rm -rf /usr/local/funkload/\n\t-rm -f /usr/local/bin/fl-*\n\t-rm -f /usr/bin/fl-*\n\nclean:\n\tfind . \"(\" -name \"*~\" -or  -name \".#*\" -or  -name \"#*#\" -or -name \"*.pyc\" \")\" -print0 | xargs -0 rm -f\n\trm -rf ./build ./dist ./MANIFEST ./funkload.egg-info\n",
  "readme": "Introduction\n==============\n\nFunkLoad_ is a functional and load web tester, written in Python, whose\nmain use cases are:\n\n* Functional testing of web projects, and thus regression testing as well.\n\n* Performance testing: by loading the web application and monitoring\n  your servers it helps you to pinpoint bottlenecks, giving a detailed\n  report of performance measurement.\n\n* Load testing tool to expose bugs that do not surface in cursory testing,\n  like volume testing or longevity testing.\n\n* Stress testing tool to overwhelm the web application resources and test\n  the application recoverability.\n\n* Writing web agents by scripting any web repetitive task.\n\nFeatures\n---------\n\nMain FunkLoad_ features are:\n\n* Functional test are pure Python scripts using the pyUnit_ framework\n  like normal unit test. Python enable complex scenarios to handle\n  real world applications.\n\n* Truly emulates a web browser (single-threaded) using an enhanced\n  Richard Jones' webunit_:\n\n  - get/post/put/delete support\n  - post any kind of content type like ``application/xml``\n  - DAV support\n  - basic authentication support\n  - file upload and multipart/form-data submission\n  - cookies support\n  - referrer support\n  - accept gzip content encoding\n  - https support\n  - https with ssl/tls by providing a private key and certificate (PEM\n    formatted)\n  - http_proxy support\n  - fetching css, javascript and images\n  - emulating a browser cache\n\n* Advanced test runner with many command-line options:\n\n  - set the target server url\n  - display the fetched page in real time in your browser\n  - debug mode to display http headers\n  - check performance of a single page (or set of pages) inside a test\n  - green/red color mode\n  - select or exclude tests cases using a regex\n  - support normal pyUnit_ test\n  - support doctest_ from a plain text file or embedded in python\n    docstring\n\n* Turn a functional test into a load test: just by invoking the bench\n  runner you can identify scalability and performance problems. If\n  needed the bench can distributed over a group of worker machines.\n\n* Detailed bench reports in ReST, HTML, Org-mode_, PDF (using\n  LaTeX/PDF Org-mode export) containing:\n\n  - the bench configuration\n  - tests, pages, requests stats and charts\n  - the requets that took the most time\n  - monitoring one or many servers cpu usage, load average,\n    memory/swap usage and network traffic charts\n  - an http error summary list\n\n* Differential reports to compare 2 bench reports giving a quick\n  overview of scalability and velocity changes.\n\n* Trend reports to view the performance evolution with multiple\n  reports.\n\n* Easy test customization using a configuration file or command line\n  options.\n\n* Easy test creation using embeded TCPWatch_ as proxy recorder, so you\n  can use your web browser and produce a FunkLoad_ test automatically,\n  including file upload or any ajax call.\n\n* Provides web assertion helpers to check expected results in responses.\n\n* Provides helpers to retrieve contents in responses page using DOM.\n\n* Easy to install (EasyInstall_).\n\n* Comes with examples look at the demo_ folder.\n\n* Successfully tested with dozen of differents web servers: PHP,\n  python, Java...\n\nLicense\n----------\n\nFunkLoad_ is free software distributed under the `GNU GPL`_ license.\n\n\\(C) Copyright 2005-2011 Nuxeo SAS (http://nuxeo.com).\n\nThis program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or (at\nyour option) any later version.\n\nThis program is distributed in the hope that it will be useful, but\nWITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nGeneral Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program; if not, write to the Free Software\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA\n02110-1301, USA.\n\n\n.. _FunkLoad: http://funkload.nuxeo.org/\n.. _Org-mode: http://orgmode.org/\n.. _TCPWatch: http://hathawaymix.org/Software/TCPWatch/\n.. _webunit: http://mechanicalcat.net/tech/webunit/\n.. _pyUnit: http://pyunit.sourceforge.net/\n.. _API: api/index.html\n.. _Nuxeo: http://www.nuxeo.com/\n.. _`python cheese shop`: http://www.python.org/pypi/funkload/\n.. _EasyInstall: http://peak.telecommunity.com/DevCenter/EasyInstall\n.. _`GNU GPL`: http://www.gnu.org/licenses/licenses.html#GPL\n.. _doctest: http://docs.python.org/lib/module-doctest.html\n.. _demo: https://github.com/nuxeo/FunkLoad/tree/master/src/funkload/demo/\n\n.. Local Variables:\n.. mode: rst\n.. End:\n.. vim: set filetype=rst:\n"
},
{
  "name": "gevent-zeromq",
  "files": {
    "/": [
      ".gitignore",
      "AUTHORS",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "examples",
      "gevent_zeromq",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "=============\ngevent-zeromq\n=============\n\nWrapper of pyzmq to make it compatible with gevent. \u00d8MQ socket operations that\nwould normally block the current thread will only block the current greenlet.\n\nInspired by Ben Ford's work on \u00d8MQ support in eventlet.\n\nRequirements\n------------\n\nRequires pyzmq>=2.1.0\n\nBefore version 2.1.0 (of both \u00d8MQ and pyzmq) the underlying file descriptors\nthat this library utilizes were not available.\n\n\nUsage\n-----\n\nInstead of importing zmq directly, do so in the following manner:\n\n..\n    \n    from gevent_zeromq import zmq\n\n\nAny calls that would have blocked the current thread will now only block the\ncurrent green thread.\n\n\nAbout\n-----\n\nThis compatibility is accomplished by ensuring the nonblocking flag is set\nbefore any blocking operation and the \u00d8MQ file descriptor is polled internally\nto trigger needed events.\n\nWill build with cython if available. In my simple nonscientific test this\nresulted in an almost 50% speedup in a local 1-1 PUB SUB sending of 100,000\n1K messages in a single tight loop.\n\nThere are plans to further the integration with both gevent and pyzmq via\ncython for speed.\n\n\nLicense\n-------\nSee LICENSE (New BSD)\n"
},
{
  "name": "metlog-psutils",
  "files": {
    "/": [
      ".gitignore",
      "MANIFEST.in",
      "MetlogPsutils.spec",
      "README.rst",
      "docs",
      "metlog_psutils",
      "requirements.txt",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "source"
    ]
  },
  "makefile": null,
  "readme": "==============\nmetlog-psutils\n==============\n\nmetlog-psutils is a plugin extension for `Metlog \n<http://github.com/mozilla-services/metlog-py>`.  metlog-psutils\nprovides details about network connections, memory usage, cpu usage\nand even some thread details.\n\nThis plugin works best on Linux.  Running the plugin under OSX will\nskip some functionality and will require root privileges.\n\nMore information about how Mozilla Services is using Metlog (including what is\nbeing used for a router and what endpoints are in use / planning to be used)\ncan be found on the relevant `spec page\n<https://wiki.mozilla.org/Services/Sagrada/Metlog>`_.\n"
},
{
  "name": "ultrajson",
  "files": {
    "/": [
      "MANIFEST.in",
      "README",
      "lib",
      "python",
      "setup.py",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "UltraJSON is a fast and extendable JSON encoder and decoder written in pure C\n\nPython bindings are available as the module ujson (through easy_install / pypi):\nhttp://pypi.python.org/pypi/ujson/\n\n\nInstallation instructions:\n\n1. Build and install ujson Python extension (requires root)\nGo to <root>/python\nType: python setup.py build install  \n\n2. Run tests (as needed)\nType: python tests.py\n\nBenchmarks:\n\n64-bit benchmarks Linux\nPython 2.7.1+ (r271:86832, Apr 11 2011, 18:13:53)\nOS Version: Ubuntu 11.04\nSystem Type: x64-based PC\nProcessor: Intel(R) Core(TM) i5-2300 CPU @ 2.80GHz\nTotal Physical Memory: 4096 MB\n\npy-yajl 0.3.5 => http://rtyler.github.com/py-yajl\nsimplejson 2.1.2 => http://simplejson.readthedocs.org/en/latest/index.html\ncjson 1.0.5 => http://pypi.python.org/pypi/python-cjson\n\nNote: Some of the string benchmarks are unfair as much of the final performance depends on to what extent the individual JSON encoders/decoders implement the JSON specification and Unicode standard.\nAt the time of writing py-yayl and cjson did not fully comply to the JSON specification and Unicode standard.\n\nArray with 256 utf-8 strings:\n*yajl  encode      : 5132.81010 calls/sec\nujson encode      : 2793.22602 calls/sec\nsimplejson encode : 1609.20858 calls/sec\ncjson encode      : 139.65933 calls/sec\n*) Output not ASCII but UTF-8 (default escaping behaviour not implemented)\n\nujson decode      : 1404.18567 calls/sec\ncjson decode      : 814.54848 calls/sec\nyajl decode       : 594.28917 calls/sec\nsimplejson decode : 347.25104 calls/sec\n\n\nMedium complex object:\nujson encode      : 18757.01101 calls/sec\nyajl  encode      : 6315.14030 calls/sec\nsimplejson encode : 5542.03928 calls/sec\ncjson encode      : 4651.59072 calls/sec\n\nujson decode      : 10759.69649 calls/sec\nsimplejson decode : 8148.35221 calls/sec\ncjson decode      : 7931.04387 calls/sec\nyajl decode       : 5887.38201 calls/sec\n\nArray with 256 strings:\nujson encode      : 40427.72790 calls/sec\nyajl  encode      : 23158.54336 calls/sec\nsimplejson encode : 22544.31270 calls/sec\ncjson encode      : 12726.16278 calls/sec\n\n*simplejson decode : 28077.25572 calls/sec\nujson decode      : 26161.35785 calls/sec\ncjson decode      : 25661.96211 calls/sec\nyajl decode       : 13565.59414 calls/sec\n\n*) Specific decoder advantage from 8-bit strings (character encoding not preserved)\n\nArray with 256 doubles:\ncjson encode      : 10890.30488 calls/sec\nyajl  encode      : 6335.60567 calls/sec\nujson encode      : 6035.47694 calls/sec\nsimplejson encode : 4153.79495 calls/sec\n\nujson decode      : 27824.38869 calls/sec\nsimplejson decode : 12599.16997 calls/sec\nyajl decode       : 11014.36990 calls/sec\ncjson decode      : 8082.23784 calls/sec\n\nArray with 256 True values:\nujson encode      : 189406.12793 calls/sec\nyajl  encode      : 97727.04396 calls/sec\ncjson encode      : 73106.77801 calls/sec\nsimplejson encode : 50481.90233 calls/sec\n\nujson decode      : 148107.72025 calls/sec\nsimplejson decode : 125620.84065 calls/sec\ncjson decode      : 76946.51962 calls/sec\nyajl decode       : 73256.86109 calls/sec\n\nArray with 256 dict{string, int} pairs:\nujson encode      : 26391.91864 calls/sec\nyajl  encode      : 12496.90579 calls/sec\nsimplejson encode : 6294.72548 calls/sec\ncjson encode      : 5044.07534 calls/sec\n\nujson decode      : 16069.25600 calls/sec\ncjson decode      : 13145.57687 calls/sec\nsimplejson decode : 10464.16719 calls/sec\nyajl decode       : 8713.18458 calls/sec\n\nDict with 256 arrays with 256 dict{string, int} pairs:\nujson encode      : 93.33057 calls/sec\nyajl  encode      : 47.37864 calls/sec\ncjson encode      : 19.17972 calls/sec\nsimplejson encode : 18.89625 calls/sec\n\nujson decode      : 42.76765 calls/sec\ncjson decode      : 33.52049 calls/sec\nsimplejson decode : 25.83548 calls/sec\nyajl decode       : 24.70140 calls/sec\n\n\n\n32-bit benchmarks Windows\nPython 2.6.6 (r266:84297, Aug 24 2010, 18:46:32) [MSC v.1500 32 bit (Intel)]\nOS Version: 6.1.7601 Service Pack 1 Build 7601\nSystem Type: x64-based PC\nProcessor: Intel(R) Core(TM)2 Quad CPU Q9550 @ 2.83GHz 2.83 GHz\nTotal Physical Memory: 8191 MB\n\nArray with 256 utf-8 strings:\nujson encode      : 1191.98175 calls/sec\nsimplejson encode : 1013.98279 calls/sec\ncjson encode      : 1040.66063 calls/sec\n\nujson decode      : 1215.66875 calls/sec\ncjson decode      : 493.30484 calls/sec\nsimplejson decode : 269.85512 calls/sec\n\nMedium complex object:\nujson encode      : 10307.63723 calls/sec\nsimplejson encode : 2534.94769 calls/sec\ncjson encode      : 2047.95118 calls/sec\n\nujson decode      : 7274.10026 calls/sec\ncjson decode      : 3575.39307 calls/sec\nsimplejson decode : 3565.51252 calls/sec\n\nArray with 256 strings:\nujson encode      : 21348.25210 calls/sec\nsimplejson encode : 15736.74638 calls/sec\ncjson encode      : 6371.26334 calls/sec\n\nujson decode      : 26050.25316 calls/sec\ncjson decode      : 16468.88215 calls/sec\nsimplejson decode : 21115.75770 calls/sec\n\nArray with 256 doubles:\nujson encode      : 26975.49110 calls/sec\nsimplejson encode : 2046.29746 calls/sec\ncjson encode      : 2133.56594 calls/sec\n\nujson decode      : 28430.33722 calls/sec\ncjson decode      : 4114.36400 calls/sec\nsimplejson decode : 4419.08507 calls/sec\n\nArray with 256 True values:\nujson encode      : 89846.12897 calls/sec\nsimplejson encode : 34288.36862 calls/sec\ncjson encode      : 47168.35849 calls/sec\n\nujson decode      : 99423.47549 calls/sec\ncjson decode      : 58795.91460 calls/sec\nsimplejson decode : 76296.14699 calls/sec\n\nArray with 256 dict{string, int} pairs:\nujson encode      : 14776.41614 calls/sec\nsimplejson encode : 3876.86634 calls/sec\ncjson encode      : 3050.65343 calls/sec\n\nujson decode      : 12934.39432 calls/sec\ncjson decode      : 7993.04345 calls/sec\nsimplejson decode : 7152.09475 calls/sec\n\nHere is the benchmark run from a 32bit CentOS 5.6 (Python 2.4) machine:\n\nArray with 256 utf-8 strings:\nujson encode : 1453.30891 calls/sec\nsimplejson encode : 658.31181 calls/sec\ncjson encode : 62.18416 calls/sec\n\nujson decode : 1016.58767 calls/sec\ncjson decode : 455.28550 calls/sec\nsimplejson decode : 124.20439 calls/sec\n\nMedium complex object:\nujson encode : 6010.21634 calls/sec\nsimplejson encode : 1418.77823 calls/sec\ncjson encode : 1252.92530 calls/sec\n\nujson decode : 4637.52630 calls/sec\ncjson decode : 3444.13604 calls/sec\nsimplejson decode : 2166.18641 calls/sec\n\nArray with 256 strings:\nujson encode : 12252.28889 calls/sec\nsimplejson encode : 9351.67532 calls/sec\ncjson encode : 7786.13697 calls/sec\n\nujson decode : 10951.17394 calls/sec\ncjson decode : 15971.02425 calls/sec\nsimplejson decode : 6796.77480 calls/sec\n\nArray with 256 doubles:\nujson encode : 16300.61218 calls/sec\nsimplejson encode : 1613.39428 calls/sec\ncjson encode : 2035.58937 calls/sec\n\nujson decode : 17301.00746 calls/sec\ncjson decode : 5785.33627 calls/sec\nsimplejson decode : 6199.49364 calls/sec\n\nArray with 256 True values:\nujson encode : 72618.15350 calls/sec\nsimplejson encode : 18707.57593 calls/sec\ncjson encode : 24150.26201 calls/sec\n\nujson decode : 53650.94162 calls/sec\ncjson decode : 48069.53050 calls/sec\nsimplejson decode : 47098.40293 calls/sec\n\nArray with 256 dict{string, int} pairs:\nujson encode : 8811.85922 calls/sec\nsimplejson encode : 2756.91262 calls/sec\ncjson encode : 1758.26962 calls/sec\n\nujson decode : 6490.36358 calls/sec\ncjson decode : 6330.77263 calls/sec\nsimplejson decode : 4161.97048 calls/sec\n\nDict with 256 arrays with 256 dict{string, int} pairs:\nujson encode : 31.08834 calls/sec\nsimplejson encode : 10.41434 calls/sec\ncjson encode : 6.93790 calls/sec\n\nujson decode : 19.81373 calls/sec\ncjson decode : 20.31727 calls/sec\nsimplejson decode : 15.05690 calls/sec\n\n\nSee (python/benchmark.py) for further information.\n\n\n"
},
{
  "name": "PyVEP",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "MANIFEST.in",
      "README.rst",
      "setup.py",
      "vep"
    ]
  },
  "makefile": null,
  "readme": "=======================================================\nPyVEP: a python library for the Verified Email Protocol\n=======================================================\n\nThis is a python client library for the Verified Email Protocol, a.k.a\nMozilla's BrowserID project.  See here for details:\n\n    https://wiki.mozilla.org/Identity/Verified_Email_Protocol\n\nAnd see here for how to integrate it into your website:\n\n    https://browserid.org/\n\nTo just get something stable and working, it's currently recommended that you\nuse the browserid.org remote verifier service to check your assertions. Do\nso like this::\n\n    >>> verifier = vep.RemoteVerifier()\n    >>> data = verifier.verify(BROWSERIDASSERTION, \"http://mysite.com\")\n    >>> print data[\"email\"]\n    \"test@example.com\"\n\n\nFor improved performance, or if you just want to live on the bleeding edge,\nyou can perform verification locally like so::\n\n    >>> verifier = vep.LocalVerifier()\n    >>> data = verifier.verify(BROWSERIDASSERTION, \"http://mysite.com\")\n    >>> print data[\"email\"]\n    \"test@example.com\"\n\nAs the Verified Email Protocol gets locked down more firmly, using the local\nverifier will become the preferred method of checking VEP identity assertions.\n"
}
]

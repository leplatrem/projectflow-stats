[
{
  "name": "glean",
  "files": {
    "/": [
      ".buildconfig.yml",
      ".cargo",
      ".circleci",
      ".coveragerc",
      ".detekt.yml",
      ".dictionary",
      ".flake8",
      ".flake8rc",
      ".github",
      ".gitignore",
      ".swiftlint.yml",
      ".taskcluster.yml",
      ".yamllint",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "DEPENDENCIES.md",
      "LICENSE",
      "Makefile",
      "README.iOS.md",
      "README.md",
      "about.toml",
      "bin",
      "build-scripts",
      "build.gradle",
      "deny.toml",
      "docs",
      "glean-core",
      "glean.1.schema.json",
      "gradle-plugin",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "samples",
      "settings.gradle",
      "setup.py",
      "taskcluster",
      "tools",
      "xcconfig"
    ],
    "/docs": [
      "dev",
      "shared",
      "user"
    ],
    "/.github": [
      "CODEOWNERS",
      "dependabot.yml",
      "workflows"
    ],
    "/.circleci": [
      "config.yml",
      "jazzy.yml"
    ]
  },
  "makefile": ".PHONY: help\nhelp:\n\t@grep -E '^[0-9a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | \\\n\t  sort | \\\n\t  awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-30s\\033[0m %s\\n\", $$1, $$2}'\n\nGLEAN_PYENV := $(shell python3 -c \"import sys; print('glean-core/python/.venv' + '.'.join(str(x) for x in sys.version_info[:2]))\")\nGLEAN_PYDEPS := ${GLEAN_PYDEPS}\n# Read the `GLEAN_BUILD_VARIANT` variable, default to debug.\n# If set it is passed as a flag to cargo, so we prefix it with `--`\nifeq ($(GLEAN_BUILD_VARIANT),)\nGLEAN_BUILD_PROFILE :=\nelse ifeq ($(GLEAN_BUILD_VARIANT),debug)\n# `--debug` is invalid and `--profile debug` is unstable.\nGLEAN_BUILD_PROFILE :=\nelse\nGLEAN_BUILD_PROFILE := --$(GLEAN_BUILD_VARIANT)\nendif\n\n# Setup environments\n\npython-setup: $(GLEAN_PYENV)/bin/python3 ## Setup a Python virtual environment\n\t@:\n\n$(GLEAN_PYENV)/bin/python3:\n\tpython3 -m venv $(GLEAN_PYENV)\n\t$(GLEAN_PYENV)/bin/pip install --upgrade pip wheel setuptools\n\t$(GLEAN_PYENV)/bin/pip install -r glean-core/python/requirements_dev.txt\n\tsh -c \"if [ \\\"$(GLEAN_PYDEPS)\\\" = \\\"min\\\" ]; then \\\n\t\t$(GLEAN_PYENV)/bin/pip install requirements-builder; \\\n\t\t$(GLEAN_PYENV)/bin/requirements-builder --level=min glean-core/python/setup.py > min_requirements.txt; \\\n\t\t$(GLEAN_PYENV)/bin/pip install -r min_requirements.txt; \\\n\tfi\"\n\n# All builds\n\nbuild: build-rust\n\nbuild-rust: ## Build all Rust code\n\tcargo build --all $(GLEAN_BUILD_PROFILE) $(addprefix --target ,$(GLEAN_BUILD_TARGET))\n\nbuild-kotlin: ## Build all Kotlin code\n\t./gradlew build -x test\n\nbuild-swift: ## Build all Swift code\n\tbin/run-ios-build.sh\n\nbuild-apk: build-kotlin ## Build an apk of the Glean sample app\n\t./gradlew glean-sample-app:build glean-sample-app:assembleAndroidTest\n\nbuild-python: python-setup ## Build the Python bindings\n\t$(GLEAN_PYENV)/bin/python3 glean-core/python/setup.py build install\n\nbindgen-python: glean-core/python/glean/_uniffi.py  # Generate the uniffi wrapper code manually\n\nglean-core/python/glean/_uniffi.py: glean-core/src/glean.udl\n\tcargo uniffi-bindgen generate $< --language python --out-dir target\n\tcp target/glean.py $@\n\n.PHONY: build build-rust build-kotlin build-swift build-apk build-python bindgen-python\n\n# All tests\n\ntest: test-rust\n\ntest-rust: ## Run Rust tests for glean-core and glean-ffi\n\tcargo test --all $(addprefix --target ,$(GLEAN_BUILD_TARGET))\n\ntest-rust-with-logs: ## Run all Rust tests with debug logging and single-threaded\n\tRUST_LOG=glean_core=debug cargo test --all -- --nocapture --test-threads=1 $(addprefix --target ,$(GLEAN_BUILD_TARGET))\n\ntest-kotlin: ## Run all Kotlin tests\n\t./gradlew :glean:testDebugUnitTest\n\ntest-swift: ## Run all Swift tests\n\tbin/run-ios-tests.sh\n\ntest-android-sample: build-apk ## Run the Android UI tests on the sample app\n\t./gradlew :glean-sample-app:connectedAndroidTest\n\ntest-ios-sample: ## Run the iOS UI tests on the sample app\n\tbin/run-ios-sample-app-test.sh\n\ntest-python: build-python ## Run all Python tests\n\t$(GLEAN_PYENV)/bin/py.test glean-core/python/tests $(PYTEST_ARGS)\n\n.PHONY: test test-rust test-rust-with-logs test-kotlin test-swift test-ios-sample\n\n# Benchmarks\n\nbench-rust: ## Run Rust benchmarks\n\tcargo bench -p benchmark $(addprefix --target ,$(GLEAN_BUILD_TARGET))\n\n.PHONY: bench-rust\n\n# Linting\n\nlint-rust: ## Run cargo-clippy to lint Rust code\n\tcargo clippy --all --all-targets --all-features -- -D warnings -A unknown-lints\n\nlint-kotlin: ## Run ktlint to lint Kotlin code\n\t./gradlew lint ktlint detekt\n\nlint-swift: ## Run swiftlint to lint Swift code\n\tswiftlint --strict\n\nlint-yaml: ## Run yamllint to lint YAML files\n\tyamllint glean-core .circleci\n\nshellcheck: ## Run shellcheck against important shell scripts\n\tshellcheck glean-core/ios/sdk_generator.sh\n\tshellcheck bin/check-artifact.sh\n\nlint-python: python-setup ## Run flake8 and black to lint Python code\n\t$(GLEAN_PYENV)/bin/python3 -m flake8 glean-core/python/glean glean-core/python/tests\n\t$(GLEAN_PYENV)/bin/python3 -m black --check --exclude \\(\\.venv.\\*\\)\\|\\(.eggs\\)\\|_uniffi.py glean-core/python/glean glean-core/python/tests\n\t$(GLEAN_PYENV)/bin/python3 -m mypy glean-core/python/glean\n\n.PHONY: lint-rust lint-kotlin lint-swift lint-yaml\n\n# Formatting\n\nfmt-rust: ## Format all Rust code\n\tcargo fmt --all\n\nfmt-python: python-setup ## Run black to format Python code\n\t$(GLEAN_PYENV)/bin/python3 -m black --exclude \\(\\.venv.\\*\\)\\|\\(.eggs\\)\\|_uniffi.py glean-core/python/glean glean-core/python/tests\n\n.PHONY: fmt-rust fmt-python\n\n# Docs\n\ndocs: rust-docs ## Build the Rust API documentation\n\nrust-docs: ## Build the Rust documentation\n\tbin/build-rust-docs.sh\n\nswift-docs: ## Build the Swift documentation\n\tbin/build-swift-docs.sh\n\npython-docs: build-python ## Build the Python documentation\n\t$(GLEAN_PYENV)/bin/python3 -m pdoc --html glean --force -o build/docs/python --config show_type_annotations=True\n\n.PHONY: docs rust-docs swift-docs\n\nmetrics-docs: python-setup ## Build the internal metrics documentation\n\t$(GLEAN_PYENV)/bin/pip install glean_parser==6.1.1\n\t$(GLEAN_PYENV)/bin/glean_parser translate --allow-reserved \\\n\t\t -f markdown \\\n\t\t -o ./docs/user/user/collected-metrics \\\n\t\t glean-core/metrics.yaml glean-core/pings.yaml glean-core/android/metrics.yaml\n\n\t\t cat ./docs/user/_includes/glean-js-redirect-collected-metrics.md ./docs/user/user/collected-metrics/metrics.md > ./docs/user/user/collected-metrics/metrics.tmp.md\n\t\t mv ./docs/user/user/collected-metrics/metrics.tmp.md ./docs/user/user/collected-metrics/metrics.md\n\nlinkcheck: docs linkcheck-raw  ## Run link-checker on the generated docs\n\nlinkcheck-raw:\n\t# Requires https://www.npmjs.com/package/link-checker\n\tlink-checker \\\n\t\tbuild/docs \\\n    --disable-external true \\\n    --allow-hash-href true \\\n    --file-ignore \"swift/.*\" \\\n    --file-ignore \"python/.*\" \\\n    --file-ignore \"javadoc/.*\" \\\n    --file-ignore \"docs/.*\" \\\n    --url-ignore \".*/swift/.*\" \\\n    --url-ignore \".*/python/.*\" \\\n    --url-ignore \".*/javadoc/.*\" \\\n    --url-ignore \".*/docs/glean_.*\" \\\n    --url-ignore \".*/docs/glean/.*\"\n.PHONY: linkcheck linkcheck-raw\n\nspellcheck: ## Spellcheck the docs\n\t# Requires http://aspell.net/\n\tbin/spellcheck.sh\n\n# Utilities\n\nandroid-emulator: ## Start the Android emulator with a predefined image\n\t$(ANDROID_HOME)/emulator/emulator -avd Nexus_5X_API_P -netdelay none -netspeed full\n.PHONY: android-emulator\n\nrust-coverage: export CARGO_INCREMENTAL=0\nrust-coverage: export RUSTFLAGS=-Zprofile -Ccodegen-units=1 -Cinline-threshold=0 -Clink-dead-code -Coverflow-checks=off -Zno-landing-pads\nrust-coverage: export RUSTUP_TOOLCHAIN=nightly\nrust-coverage: ## Generate code coverage information for Rust code\n\t# Expects a Rust nightly toolchain to be available.\n\t# Expects grcov and genhtml to be available in $PATH.\n\tcargo build --verbose $(addprefix --target ,$(GLEAN_BUILD_TARGET))\n\tcargo test --verbose $(addprefix --target ,$(GLEAN_BUILD_TARGET))\n\tzip -0 ccov.zip `find . \\( -name \"glean*.gc*\" \\) -print`\n\tgrcov ccov.zip -s . -t lcov --llvm --branch --ignore-not-existing --ignore \"/*\" --ignore \"glean-core/ffi/*\" -o lcov.info\n\tgenhtml -o report/ --show-details --highlight --ignore-errors source --legend lcov.info\n.PHONY: rust-coverage\n\npython-coverage: build-python ## Generate a code coverage report for Python\n\tGLEAN_COVERAGE=1 $(GLEAN_PYENV)/bin/python3 -m coverage run --parallel-mode -m pytest\n\t$(GLEAN_PYENV)/bin/python3 -m coverage combine\n\t$(GLEAN_PYENV)/bin/python3 -m coverage html\n.PHONY: python-coverage\n",
  "readme": "# Glean SDK\n\n![Glean logo](docs/user/glean.jpeg)\n\n[![glean-core on crates.io](https://img.shields.io/crates/v/glean-core.svg)](https://crates.io/crates/glean-core)\n[![License: MPL-2.0](https://img.shields.io/crates/l/glean-core)](https://github.com/mozilla/glean/blob/main/LICENSE)\n[![The Glean SDK book](https://img.shields.io/badge/Docs-Glean%20SDK-brightgreen)][book]\n[![Build Status](https://img.shields.io/circleci/build/github/mozilla/glean/main)](https://circleci.com/gh/mozilla/glean)\n\n## Documentation\n\nAll documentation is available online:\n\n## [The Glean SDK Book][book]\n\n## Overview\n\nRefer to the documentation for [using][book] and [developing][devbook] the Glean SDK.\n\nFor an overview of Glean beyond just the SDK, see the [section in the Firefox data docs](https://docs.telemetry.mozilla.org/concepts/glean/glean.html).\n\nThe code in this repository is organized as follows:\n\n* [./glean-core/](glean-core) contains the source for the low-level Rust library.\n* [./glean-core/ffi](glean-core/ffi) contains the mapping into a C FFI.\n* [./glean-core/android](glean-core/android) contains the Kotlin bindings for use by Android applications.\n* [./glean-core/ios](glean-core/ios) contains the Swift bindings for use by iOS applications.\n* [./glean-core/python](glean-core/python) contains Python bindings.\n\n**Note: The Glean SDK requires at least [Rust 1.57.0](https://blog.rust-lang.org/2021/12/02/Rust-1.57.0.html). Older versions are untested.**\n\n## Contact\n\nTo contact us you can:\n\n* Find us in the [#glean channel on chat.mozilla.org](https://chat.mozilla.org/#/room/#glean:mozilla.org).\n* To report issues or request changes, file a bug in [Bugzilla in Data Platform & Tools :: Glean: SDK][newbugzilla].\n* Send an email to *glean-team@mozilla.com*.\n* The Glean Core team is: *:dexter*, *:janerik*, *:travis_*, *:chutten*, *:brizental*.\n\n## Credits\n\nThe [Glean logo artwork](https://dianaciufo.wordpress.com/2019/10/11/glean-graphic-identity-for-mozilla-firefox/) was contributed by [Diana Ciufo](https://dianaciufo.wordpress.com/).\nIt's licensed under MPL.\n\n### Alumni contributors\n\n* Georg Fritzsche\n* Michael Droettboom\n\nSee the full list of contributors for:\n\n* the [Glean SDK](https://github.com/mozilla/glean/graphs/contributors)\n* the [Glean.js SDK](https://github.com/mozilla/glean.js/graphs/contributors)\n* the [glean_parser](https://github.com/mozilla/glean_parser/graphs/contributors)\n\n## License\n\n    This Source Code Form is subject to the terms of the Mozilla Public\n    License, v. 2.0. If a copy of the MPL was not distributed with this\n    file, You can obtain one at http://mozilla.org/MPL/2.0/\n\n\n[newbugzilla]: https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=nobody%40mozilla.org&bug_ignored=0&bug_severity=normal&bug_status=NEW&bug_type=defect&cf_fx_iteration=---&cf_fx_points=---&cf_status_firefox100=---&cf_status_firefox101=---&cf_status_firefox99=---&cf_status_firefox_esr91=---&cf_tracking_firefox100=---&cf_tracking_firefox101=---&cf_tracking_firefox99=---&cf_tracking_firefox_esr91=---&component=Glean%3A%20SDK&contenttypemethod=list&contenttypeselection=text%2Fplain&defined_groups=1&filed_via=standard_form&flag_type-4=X&flag_type-607=X&flag_type-721=X&flag_type-737=X&flag_type-799=X&flag_type-800=X&flag_type-803=X&flag_type-936=X&flag_type-947=X&form_name=enter_bug&maketemplate=Remember%20values%20as%20bookmarkable%20template&op_sys=Unspecified&priority=P3&product=Data%20Platform%20and%20Tools&rep_platform=Unspecified&status_whiteboard=%5Bglean-sdk%3Am%3F%5D&target_milestone=---&version=unspecified\n[book]: https://mozilla.github.io/glean/\n[devbook]: https://mozilla.github.io/glean/dev/\n[rustdoc]: https://mozilla.github.io/glean/docs/index.html\n[ktdoc]: https://mozilla.github.io/glean/javadoc/glean/index.html\n"
},
{
  "name": "code-review",
  "files": {
    "/": [
      ".dockerignore",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "backend",
      "bot",
      "docs",
      "events",
      "frontend",
      "integration",
      "tools"
    ],
    "/docs": [
      "README.md",
      "analysis_format.md",
      "architecture.drawio",
      "architecture.md",
      "architecture.png",
      "ci-cd",
      "configuration.md",
      "debugging.md",
      "new_repository.md",
      "phabricator.md",
      "phabricator.mermaid",
      "phabricator.png",
      "projects",
      "publication.md",
      "trigger.md"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Code Review\n\nThe **Code Review Bot** aims to give early feedback to Mozilla developers about their patches. We automate code analyzers and publish detected issues on Phabricator as soon as possible, and for all revisions.\n\nThis project has 4 parts:\n\n* `bot` is a Python script running as a Taskcluster task, reporting issues found in analyzer tasks,\n* `backend` is a Django web API used to store issues detected by the bot,\n* `frontend` is an administration frontend (in Vue.js) displaying detailed information about analyses and issues,\n* `events` is a Python distributed application running in Heroku that receives Phabricator notifications and triggers Try pushes,\n\n:blue_book: Documentation is available in this repository [in the docs folder](docs/README.md). A good starting point is the [architecture description](docs/architecture.md).\n\n:loudspeaker: You can contact the code review bot's developers [on Matrix](https://chat.mozilla.org/#/room/#code-review-bot:mozilla.org) or on Slack in #code-review-bot.\n"
},
{
  "name": "jira-bugzilla-integration",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".pre-commit-config.yaml",
      ".secrets.baseline",
      ".yamllint",
      "LICENSE",
      "Makefile",
      "README.md",
      "config",
      "docker-compose.yaml",
      "infra",
      "poetry.lock",
      "pyproject.toml",
      "src",
      "tests",
      "version.json"
    ],
    "/.github": [
      "CODEOWNERS",
      "workflows"
    ]
  },
  "makefile": "# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\n_UID ?= 10001\n_GID ?= 10001\n\n.PHONY: help\nhelp:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@echo \"JBI make rules:\"\n\t@echo \"\"\n\t@echo \"  build   - build docker containers\"\n\t@echo \"  lint    - lint check for code\"\n\t@echo \"  start   - run the API service\"\n\t@echo \"\"\n\t@echo \"  test        - run test suite\"\n\t@echo \"  shell       - open a shell in the web container\"\n\t@echo \"  test-shell  - open a shell in test environment\"\n\t@echo \"\"\n\t@echo \"  generate    - create json file from TEMPLATE\"\n\t@echo \"\"\n\t@echo \"  help    - see this text\"\n\n\n.PHONY: build\nbuild:\n\tdocker-compose -f ./docker-compose.yaml -f ./tests/infra/docker-compose.test.yaml build \\\n\t\t--build-arg userid=${_UID} --build-arg groupid=${_GID}\n\n.PHONY: lint\nlint:\n\tdocker-compose -f ./docker-compose.yaml -f ./tests/infra/docker-compose.lint.yaml build \\\n\t\t--build-arg userid=${_UID} --build-arg groupid=${_GID} lint\n\n\n.PHONY: shell\nshell:\n\tdocker-compose -f ./docker-compose.yaml run web\n\n.PHONY: start\nstart:\n\tdocker-compose up\n\n.PHONY: test\ntest:\n\tdocker-compose -f ./docker-compose.yaml -f ./tests/infra/docker-compose.test.yaml run tests\nifneq (1, ${MK_KEEP_DOCKER_UP})\n\t# Due to https://github.com/docker/compose/issues/2791 we have to explicitly\n\t# rm all running containers\n\tdocker-compose down\nendif\n\n.PHONY: test-shell\ntest-shell:\n\tdocker-compose -f ./docker-compose.yaml -f ./tests/infra/docker-compose.test.yaml run web\n",
  "readme": "[![Build Docker image](https://github.com/mozilla/jira-bugzilla-integration/actions/workflows/build-image.yaml/badge.svg)](https://github.com/mozilla/jira-bugzilla-integration/actions/workflows/build-image.yaml)\n[![Run tests](https://github.com/mozilla/jira-bugzilla-integration/actions/workflows/test-build.yaml/badge.svg)](https://github.com/mozilla/jira-bugzilla-integration/actions/workflows/test-build.yaml)\n[![pre-commit](https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white)](https://github.com/pre-commit/pre-commit)\n\n# Jira Bugzilla Integration (JBI)\nSystem to sync Bugzilla bugs to Jira issues.\n\n### Caveats\n- The system accepts webhook events from Bugzilla\n- Bugs' whiteboard tags are used to determine if they should be synchronized or ignored\n- The events are transformed into Jira issues\n- The system sets the `see_also` field of the Bugzilla bug with the URL to the Jira issue\n\n## Action Configuration\nThe system reads the action configuration from a YAML file, one per environment. Each entry controls the synchronization between Bugzilla tickets with Jira issues.\n\n\nBelow is a full example of an action configuration:\n```yaml\n    action: src.jbi.whiteboard_actions.default\n    allow_private: false\n    contact: [example@allizom.com]\n    description: example configuration\n    enabled: true\n    parameters:\n      jira_project_key: EXMPL\n      whiteboard_tag: example\n```\n\nA bit more about the different fields...\n- `action` (optional)\n    - string\n    - default: [src.jbi.whiteboard_actions.default](src/jbi/whiteboard_actions/default.py)\n    - The specified Python module must be available in the `PYTHONPATH`\n- `allow_private` (optional)\n    - bool [true, false]\n    - default: false\n    - If false bugs that are not public will not be synchronized. Note that in order to synchronize\n      private bugs the bugzilla user that JBI runs as must be in the security groups that are making\n      the bug private.\n- `contact`\n    - list of strings\n    - If an issue arises with the workflow, communication will be established with these contacts\n    - Please enter the contact information for one or more stakeholders\n- `description`\n    - string\n    - Please enter a description; for example, team name or project use-case.\n- `enabled` (optional)\n    - bool [true, false]\n    - default: false\n    - If false, matching events will not be synchronized\n- `parameters` (optional)\n    - dict\n    - default: {}\n    - The parameters will be validated to ensure the selected action accepts the specified values\n    - The [default action](src/jbi/whiteboard_actions/default.py) expects both the `whiteboard_tag` and `jira_project_key` fields\n\n\n[View 'nonprod'  configurations here.](config/config.nonprod.yaml)\n\n[View 'prod' configurations here.](config/config.prod.yaml)\n\n\n## Default with assignee and status action\nThe `src.jbi.whiteboard_actions.default_with_assignee_and_status` action adds some additional\nfeatures on top of the default.\n\nIt will attempt to assign the Jira issue the same person as the bug is assigned to. This relies on\nthe user using the same email address in both Bugzilla and Jira. If the user does not exist in Jira\nthen the assignee is cleared from the Jira issue.\n\nThe action supports setting the Jira issues's status when the Bugzilla status and resolution change.\nThis is defined using a mapping on a per-project basis configured in the `status_map` field of the\n`parameters` field.\n\nAn example configuration:\n```yaml\n    action: src.jbi.whiteboard_actions.default_with_assignee_and_status\n    contact: [example@allizom.com]\n    description: example configuration\n    enabled: true\n    parameters:\n      jira_project_key: EXMPL\n      whiteboard_tag: example\n      status_map:\n        NEW: \"In Progress\"\n        FIXED: \"Closed\"\n```\n\nIn this case if the bug changes to the NEW status the action will attempt to set the linked Jira\nissue status to \"In Progress\". If the bug changes to RESOLVED FIXED it will attempt to set the\nlinked Jira issue status to \"Closed\". If the bug changes to a status not listed in `status_map` then\nno change will be made to the Jira issue.\n\n### Custom Actions\nIf you're looking for a unique capability for your team's data flow, you can add your own python methods and functionality[...read more here.](src/jbi/whiteboard_actions/README.md)\n\n\n## Diagram Overview\n\n``` mermaid\ngraph TD\n    subgraph bugzilla services\n        A[Bugzilla] -.-|bugzilla event| B[(Webhook Queue)]\n        B --- C[Webhook Push Service]\n    end\n    D --> |create/update/delete issue| E[Jira]\n    D<-->|read bug| A\n    D -->|update see_also| A\n    subgraph jira-bugzilla-integration\n        C -.->|post /bugzilla_webhook| D{JBI}\n        F[\"config.{ENV}.yaml\"] ---| read actions config| D\n    end\n```\n\n\n## Deployment\n\nSoftware and configuration are deployed automatically:\n\n- on NONPROD when a pull-request is merged\n- on PROD when a tag is pushed\n\n| Env     | Base URL                                       |\n|---------|------------------------------------------------|\n| Nonprod | https://stage.jbi.nonprod.cloudops.mozgcp.net/ |\n| Prod    | https://jbi.services.mozilla.com/              |\n\nIn order to view the configured Jira and Bugzilla, check the root URL:\n\n```\nGET /\n\n{\n    \"configuration\": {\n        \"bugzilla_base_url\": \"https://bugzilla-dev.allizom.org\",\n        \"jira_base_url\": \"https://mozit-test.atlassian.net/\"\n    },\n    \"description\": \"JBI v2 Platform\",\n    \"documentation\": \"/docs\",\n    \"title\": \"Jira Bugzilla Integration (JBI)\",\n    \"version\": \"2.0.1\"\n}\n```\n\nIn order to verify that a certain commit was deployed, check that the Github Actions executed successfully on the commit, and use the *Version* endpoint:\n\n```\nGET /__version__\n\n{\n  \"commit\": \"1ea792a733d704e0094fe6065ee64b2a3435f280\",\n  \"version\": \"refs/tags/v2.0.1\",\n  \"image_tag\": \"v2.0.1\",\n  \"source\": \"https://github.com/mozilla/jira-bugzilla-integration\",\n  \"build\": \"https://github.com/mozilla/jira-bugzilla-integration/actions/runs/2315380477\"\n}\n```\n\nIn order to verify that a certain action is configured correctly and enabled, use the *Powered By JBI* endpoint: [https://${SERVER}/powered_by_jbi](https://jbi.services.mozilla.com/powered_by_jbi)\n\nFor the list of configured whiteboard tags:\n\n```\nGET /whiteboard_tags/\n{\n    \"addons\": {\n        \"action\": \"src.jbi.whiteboard_actions.default\",\n        \"contact\": \"tbd\",\n        \"description\": \"Addons whiteboard tag for AMO Team\",\n        \"enabled\": true,\n        \"parameters\": {\n            \"jira_project_key\": \"WEBEXT\",\n            \"whiteboard_tag\": \"addons\"\n        }\n    },\n    ...\n}\n```\n"
},
{
  "name": "mozregression",
  "files": {
    "/": [
      ".coveragerc",
      ".coveralls.yml",
      ".github",
      ".gitignore",
      ".linter-files",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "appveyor.yml",
      "bin",
      "docs",
      "gui",
      "mozregression",
      "pyproject.toml",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tests"
    ],
    "/docs": [
      "Gemfile",
      "README.md",
      "_config.yml",
      "_data",
      "_includes",
      "_layouts",
      "_posts",
      "_sass",
      "contribute.md",
      "css",
      "documentation",
      "feed.xml",
      "images",
      "index.md",
      "install.md",
      "js",
      "quickstart.md"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# mozregression\n\nmozregression is an interactive regression rangefinder for quickly tracking down the source of bugs in Mozilla nightly and integration builds.\n\nYou can start using mozregression today:\n\n- [start with our installation guide](https://mozilla.github.io/mozregression/install.html), then\n- take a look at [our Quick Start document](https://mozilla.github.io/mozregression/quickstart.html).\n\n## Status\n\n[![Latest Version](https://img.shields.io/pypi/v/mozregression.svg)](https://pypi.python.org/pypi/mozregression/)\n[![License](https://img.shields.io/pypi/l/mozregression.svg)](https://pypi.python.org/pypi/mozregression/)\n\nBuild status:\n\n- Linux:\n  [![Linux Build Status](https://travis-ci.org/mozilla/mozregression.svg?branch=master)](https://travis-ci.org/mozilla/mozregression)\n  [![Coverage Status](https://img.shields.io/coveralls/mozilla/mozregression.svg)](https://coveralls.io/r/mozilla/mozregression)\n- Windows: [![Windows Build status](https://ci.appveyor.com/api/projects/status/bcg7t1pt2bahggdr?svg=true)](https://ci.appveyor.com/project/wlach/mozregression/branch/master)\n\nFor more information see:\n\nhttps://mozilla.github.io/mozregression/\n\n## Contact\n\nYou can chat with the mozregression developers on Mozilla's instance of [Matrix](https://chat.mozilla.org/#/room/#mozregression:mozilla.org): https://chat.mozilla.org/#/room/#mozregression:mozilla.org\n\n## Issue Tracking\n\nFound a problem with mozregression? Have a feature request? We track bugs [on bugzilla](https://bugzilla.mozilla.org/buglist.cgi?quicksearch=product%3ATesting%20component%3Amozregression&list_id=14890897).\nYou can file a new bug [here](https://bugzilla.mozilla.org/enter_bug.cgi?product=Testing&component=mozregression).\n\n## Building And Developing mozregression\n\nWant to hack on mozregression ? Cool!\n\n### Installing dependencies\n\nTo make setup more deterministic, we have provided requirements files to use a known-working\nset of python dependencies. From your mozregression checkout, you can install these inside\na virtual development environment.\n\nAfter checking out the mozregression repository from GitHub, this is a two step process:\n\n1. Be sure you are using Python 3.6 or above: earlier versions are not supported (if you\n   are not sure, run `python --version` or `python3 --version` on the command line).\n\n2. From inside your mozregression checkout, create a virtual environment, activate it, and install the dependencies. The instructions are slightly different depending on whether you are using Windows or Linux/MacOS.\n\n\nOn Windows:\n\n```bash\npython3 -m venv venv\nvenv\\Scripts\\activate\npip install -r requirements\\requirements-3.9-Windows.txt\npip install -e .\n```\n\nOn Linux:\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements/requirements-3.9-Linux.txt\npip install -e .\n```\n\nOn macOS:\n\n```bash\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements/requirements-3.9-macOS.txt\npip install -e .\n```\n\nNOTE: You should replace the Python version with the one that matches with the virtual environment.\n\n### Hacking on mozregression\n\nAfter running the above commands, you should be able to run the command-line version of\nmozregression as normal (e.g. `mozregression --help`) inside the virtual environment. If\nyou wish to try running the GUI, use the provided helper script:\n\n```bash\npython gui/build.py run\n```\n\nTo run the unit tests for the console version:\n\n```bash\npytest tests\n```\n\nFor the GUI version:\n\n```bash\npython gui/build.py test\n```\n\nBefore submitting a pull request, please lint your code for errors and formatting (we use [black](https://black.readthedocs.io/en/stable/), [flake8](https://flake8.pycqa.org/en/latest/) and [isort](https://isort.readthedocs.io/en/latest/))\n\n```bash\n./bin/lint-check.sh\n```\n\nIf it turns up errors, try using the `lint-fix.sh` script to fix any errors which can be addressed automatically:\n\n```bash\n./bin/lint-fix.sh\n```\n\n### Making a release\n\nThis is currently a multi-step process:\n\n1. Create a new GitHub release and give it a tag name identical to the version number you want (e.g. `4.0.20`).\n   CI should automatically upload new versions of the GUI applications to the release.\n2. Fetch the tag locally and make a new release. The recommended workflow is something like this (this assumes you have [twine](https://pypi.org/project/twine/) installed):\n\n```bash\ngit fetch origin  # assumes origin is mozilla/mozregression\ngit checkout 4.0.20\npython3 setup.py sdist bdist_wheel\npython3 -m twine upload dist/*\n```\n"
},
{
  "name": "gecko-dev",
  "files": {
    "/": [
      ".arcconfig",
      ".babel-eslint.rc.js",
      ".cargo",
      ".clang-format",
      ".clang-format-ignore",
      ".cron.yml",
      ".eslintignore",
      ".eslintrc.js",
      ".flake8",
      ".git-blame-ignore-revs",
      ".gitattributes",
      ".gitignore",
      ".hg-annotate-ignore-revs",
      ".hg-format-source",
      ".hgignore",
      ".hgtags",
      ".lando.ini",
      ".lldbinit",
      ".mailmap",
      ".prettierignore",
      ".prettierrc",
      ".taskcluster.yml",
      ".trackerignore",
      ".vscode",
      ".yamllint",
      ".ycm_extra_conf.py",
      "AUTHORS",
      "CLOBBER",
      "Cargo.lock",
      "Cargo.toml",
      "GNUmakefile",
      "LICENSE",
      "Makefile.in",
      "README.txt",
      "accessible",
      "aclocal.m4",
      "browser",
      "build.gradle",
      "build",
      "caps",
      "chrome",
      "client.mk",
      "client.py",
      "config",
      "configure.in",
      "configure.py",
      "devtools",
      "docs",
      "docshell",
      "dom",
      "editor",
      "extensions",
      "gfx",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "hal",
      "image",
      "intl",
      "ipc",
      "js",
      "layout",
      "mach",
      "mach.cmd",
      "mach.ps1",
      "media",
      "memory",
      "mfbt",
      "mobile",
      "modules",
      "moz.build",
      "moz.configure",
      "mozglue",
      "mozilla-config.h.in",
      "netwerk",
      "nsprpub",
      "old-configure.in",
      "other-licenses",
      "package-lock.json",
      "package.json",
      "parser",
      "python",
      "remote",
      "security",
      "services",
      "servo",
      "settings.gradle",
      "startupcache",
      "storage",
      "substitute-local-geckoview.gradle",
      "supply-chain",
      "taskcluster",
      "test.mozbuild",
      "testing",
      "third_party",
      "toolkit",
      "tools",
      "uriloader",
      "view",
      "widget",
      "xpcom",
      "xpfe"
    ],
    "/docs": [
      "_addons",
      "_static",
      "_templates",
      "bug-mgmt",
      "code-quality",
      "conf.py",
      "config.yml",
      "contributing",
      "crash-reporting",
      "gtest",
      "index.rst",
      "jsdoc.json",
      "metrics",
      "nspr",
      "performance",
      "setup",
      "testing-rust-code",
      "writing-rust-code"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "django-product-details",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "product_details",
      "requirements.txt",
      "runtests.py",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini",
      "updatejson.py"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "Mozilla Product Details for Django\n==================================\n\n|GHACI| |PyPI|\n\n**Mozilla Product Details** is a\n`library <http://viewvc.svn.mozilla.org/vc/libs/product-details/README?view=markup>`__\ncontaining information about the latest versions, localizations, etc. of\n`Mozilla <http://www.mozilla.org>`__ products (most notably Firefox,\nFirefox for mobile, and Thunderbird).\n\nFrom the original `README\nfile <http://viewvc.svn.mozilla.org/vc/libs/product-details/README?view=markup>`__:\n\n::\n\n    This library holds information about the current builds of Firefox and\n    Thunderbird that Mozilla ships including:\n\n    - Latest version numbers for all builds\n    - English and Native names for all languages we support\n\nThis is a `Django <http://www.djangoproject.com/>`__ app allowing this\ndata to be used in Django projects. A Django management command can be\nused as a cron job or called manually to keep the data in sync with\nMozilla.\n\nWhy?\n----\n\nThe `data source <http://svn.mozilla.org/libs/product-details/>`__ of\nMozilla Product Details is a PHP library kept on the Mozilla SVN server,\nand was originally written so it could be included into PHP projects via\nan `SVN external <http://svnbook.red-bean.com/en/1.0/ch07s03.html>`__. A\nsimple ``svn up`` would fetch the latest data when it became available.\n\nIn the meantime, the Product Details library received an additional JSON\nfeed, allowing non-PHP projects to consume the data. If, however, the\nconsumer is not kept in SVN like the library is, there is no easy way to\nkeep the data up to date.\n\nFor Django projects, this app solves that problem.\n\nGetting Started\n---------------\n\nInstalling\n~~~~~~~~~~\n\nInstall this library using ``pip``:\n\n::\n\n    pip install django-mozilla-product-details\n\n... or by downloading the ``product_details`` directory and dropping it\ninto your django project.\n\nAdd ``product_details`` to your ``INSTALLED_APPS`` to enable the\nmanagement commands.\n\nConfiguration\n~~~~~~~~~~~~~\n\nNo configuration should be necessary. However, you can add the\nfollowing settings to your ``settings.py`` file if you disagree with the\ndefaults:\n\n-  ``PROD_DETAILS_URL`` defaults to the JSON directory on the Mozilla\n   SVN server. If you have a secondary mirror at hand, or you want this\n   tool to download completely unrelated JSON files from somewhere else,\n   adjust this setting. Include a trailing slash.\n-  ``PROD_DETAILS_DIR`` is the target directory for the JSON files. It\n   needs to be writable by the user that'll execute the management\n   command, and readable by the user running the Django project.\n   Defaults to: ``.../install_dir_of_this_app/product_details/json/``\n   (only for use with ``PDFileStorage`` backend (see below)).\n\nYou can further decide where the JSON data should be stored by using\na storage backend class. There are 2 provided in the app currently, but\nit should be easy to create a subclass of\n``product_details.storage.ProductDetailsStorage`` and store them wherever\nyou like. The two provided are for the filesystem (the default) and\nthe database. To configure which backend it uses set the following:\n\n-  ``PROD_DETAILS_STORAGE`` a string of the dotted path to a storage\n   class (like in MIDDLEWARE_CLASSES). Available classes included with\n   the app are ``product_details.storage.PDFileStorage`` (default) and\n   ``product_details.storage.PDDatabaseStorage``. To use the database\n   storage class you should run migrations (./manage.py migrate) which\n   will create the database table required to store the data and populate\n   the table with the JSON data included with the library (or the data\n   in the configured data directory). You can then keep the data updated\n   via the ``update_product_details`` management command just like normal.\n\nThis app uses Django's cache framework to store the product data so that\nthe data can be updated on the site without requiring a server restart.\nThe following settings will allow you to control how this works.\n\n-  ``PROD_DETAILS_CACHE_NAME`` defaults to the cache in your ``CACHES``\n   setting called ``default`` (django provides an in-memory cache here\n   by default). If you provide a name of a cache configured in the\n   Django configuration ``CACHES``, it will use that cache to store the\n   file data instead.\n-  ``PROD_DETAILS_CACHE_TIMEOUT`` If set to an integer, it represents\n   the number of seconds the cached data should be kept per file.\n   Defaults to 12 hours.\n\nUpdating the feed\n~~~~~~~~~~~~~~~~~\n\nTo update the data, execute this:\n\n::\n\n    ./manage.py update_product_details\n\nYou want to run this once manually after installing the app. To\nperiodically pull in new data, you can make this a cron job.\n\n**Note:** Please be considerate of the server when adding a cron job.\nThe data does not change often enough to warrant an update every minute\nor so. Most applications will run perfectly fine if you pull new data\nonce a day or even less frequently. When in doubt, contact the author of\nthis library.\n\nUsing the data\n~~~~~~~~~~~~~~\n\nTo use the data, just import the library:\n\n::\n\n    from product_details import product_details\n\nThe library turns all imported JSON files automatically into Python\nobjects. The contents are perhaps best inspected using\n`IPython <http://ipython.scipy.org/>`__.\n\nVersion Compare\n---------------\n\nProduct details comes with an implementation of version comparison code\nfor Mozilla-style product versions. Use it like this:\n\n::\n\n    >>> from product_details.version_compare import Version\n    >>> v1 = Version('4.0b10')\n    >>> v2 = Version('4.0b10pre')\n    >>> v1 < v2\n    False\n\nThe second useful part of the version compare code is generating a list\nof unique versions, sorted by their release date, like this:\n\n::\n\n    >>> from product_details import product_details\n    >>> from product_details.version_compare import version_list\n    >>> version_list(product_details.firefox_history_development_releases)\n    ['3.6.4', '3.6.3', '3.6', '3.6b5', '3.6b4', '3.6b3', '3.6b2', ... ]\n\nCaveats / Known Issues\n----------------------\n\n1. While the management task will not overwrite existing files if the\n   server returns bogus data (i.e., an empty document or unparseable\n   JSON data), this library will also *never delete* a JSON file that\n   was completely removed from the server. This is unlikely to happen\n   very often, though.\n2. You don't want to ``import product_details`` in ``settings.py`` as\n   that would cause an import loop (since product\\_details itself\n   imports ``django.conf.settings``). However, if you must, you can\n   lazily wrap the import like this, mitigating the problem:\n\n   ::\n\n       from django.utils.functional import lazy\n\n       MY_LANGUAGES = ('en-US', 'de')\n       class LazyLangs(list):\n           def __new__(self):\n               from product_details import product_details\n               return [(lang.lower(), product_details.languages[lang]['native'])\n                       for lang in MY_LANGUAGES]\n       LANGUAGES = lazy(LazyLangs, list)()\n3. Using product_details before Django has finished initializing, e.g. in your\n   app's ``__init__.py`` it may raise a\n   ``django.core.exceptions.AppRegistryNotReady`` exception. The lazy loading\n   example from above should help you overcome this issue.\n\nDevelopment\n-----------\n\nPatches are welcome.\n\nTo run tests, install ``tox`` and run ``tox`` from the project root.\nThis will run the tests in Python 3.7, 3.8 and 3.9 against\nvarious appropriate Django versions. If you don't have ``tox`` and/or all the\nversions of Python available, install ``nose``, ``mock``, ``requests``,\n``responses`` and ``Django`` (see ``tox.ini``'s ``deps``) and run the\ntests in your current Python version by running ``./runtests.py``.\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/django-mozilla-product-details.svg\n   :target: https://pypi.python.org/pypi/django-mozilla-product-details\n\n.. |GHACI| image:: https://github.com/mozilla/django-product-details/actions/workflows/ci.yml/badge.svg\n   :target: https://github.com/mozilla/django-product-details/actions\n\nReleasing\n---------\n\n1. Update the version number in ``product_details/__init__.py``.\n2. Add an entry to the change log in the README file.\n3. Tag the commit where you changed the above with the version number: e.g. ``0.14.1``.\n4. Push the commit and tag to the github repo.\n5. Github will build and release the package to PyPI.\n\nChange Log\n----------\n\n1.0.3 - 2022-03-08\n~~~~~~~~~~~~~~~~~~\n\n- Previous release(s) did not contain product-details data. This release does include a copy of the data.\n\n1.0.2 - 2022-01-31\n~~~~~~~~~~~~~~~~~~\n\n- Move CI to Github Actions. Thanks stevejalim!\n\n1.0.1 - 2022-01-13\n~~~~~~~~~~~~~~~~~~\n\n- Updates to be able to handle Firefox versions over 100. Thanks robhudson!\n\n1.0.0 - 2022-01-07\n~~~~~~~~~~~~~~~~~~\n\n- Drop Python 2 support.\n- Covert codebase to use black formatting.\n- Update the tox testing configuration to add new Django and Python releases.\n\nThanks to stevejalim and tasos for these improvements.\n\n0.14.1 - 2019-06-03\n~~~~~~~~~~~~~~~~~~~\n\n- Add back last-modified data for directory lists in the data to avoid migration failure.\n\n0.14 - 2019-05-28\n~~~~~~~~~~~~~~~~~\n\n- Remove the last-modified check for directory lists. Fixes #72. Thanks pmac!\n\n0.13.1 - 2019-03-03\n~~~~~~~~~~~~~~~~~~~\n\n- Tweak a migration to make Django 2+ under Python 3 happy. Fixes #68. Thanks peterbe!\n\n0.13 - 2017-08-30\n~~~~~~~~~~~~~~~~~~~\n\n- Lazily load the storage class to avoid import issues in Django 1.9+. Thanks Giorgos!\n\n0.12.1 - 2016-08-18\n~~~~~~~~~~~~~~~~~~~\n\n- Add --database option to management command to allow data to be updated\n  in a configured database other than \"default\".\n\n0.12 - 2016-07-29\n~~~~~~~~~~~~~~~~~\n\n- Update caching strategy to cache all files in a single cache entry. The file contents\n  are interdependent, so caching separately caused errors when timeouts were staggered.\n- Change the default data URL to https://product-details.mozilla.org/1.0/\n  (`bug 1282494 <https://bugzil.la/1282494>`__).\n\n0.11.1 - 2016-04-08\n~~~~~~~~~~~~~~~~~~~\n\n- Include updated JSON data in the release. A problem with deployment in Travis resulted in 0.11\n  failing to include the data.\n\n0.11 - 2016-04-08\n~~~~~~~~~~~~~~~~~\n\n- Wrap the update of JSON data in a transaction when using the database storage backend\n  (`bug 1254664 <https://bugzil.la/1254664>`__).\n- Avoid caching empty data (`bug 1254664 <https://bugzil.la/1254664>`__).\n\nThanks to jgmize for both of these improvements!\n\n0.10 - 2016-01-25\n~~~~~~~~~~~~~~~~~\n\n- Use requests lib to fetch remote data for reliability and better Py3k compatibility.\n- Update management command to avoid Django 1.9 deprecation warnings. Django 1.8 is now the minimum supported version.\n\nThanks to Osmose for both of these improvements!\n\n0.9 - 2015-12-28\n~~~~~~~~~~~~~~~~\n\n- Support for Python 3 and 2 simultaneously! Also provide a universal wheel package.\n- Support for Django 1.9. Thanks Osmose!\n\n0.8.2 - 2015-12-22\n~~~~~~~~~~~~~~~~~~\n\n- Use HTTPS by default to fetch JSON data. Thanks jvehent!\n- Fix product_details.last_update property. It's been broken since 0.8. Thanks for the report diox!\n\n0.8.1 - 2015-10-07\n~~~~~~~~~~~~~~~~~~\n\n- Add a data migration that will import the included JSON file data into the database\n  table upon creation.\n\n0.8 - 2015-09-30\n~~~~~~~~~~~~~~~~\n\n- Add configurable json data file storage backends.\n- Add filesystem and database backends.\n\n0.7.1 - 2015-06-15\n~~~~~~~~~~~~~~~~~~\n\n-  Do not cache a file miss.\n-  Catch an attempt to parse a non-JSON or corrupt file.\n\n0.7 - 2015-05-22\n~~~~~~~~~~~~~~~~\n\n-  Use the Django cache framework to store product data, allowing data to be\n   updated without a server restart.\n-  Add and update tests, setup tox for testing across Python and Django versions,\n   and setup Travis for CI.\n\n0.6 - 2015-05-08\n~~~~~~~~~~~~~~~~\n\n-  Initial PyPI release. Prior to this it was released and installed via github.\n"
},
{
  "name": "addons-linter",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc",
      ".github",
      ".gitignore",
      ".npmignore",
      ".npmrc",
      ".nvmrc",
      ".prettierignore",
      ".prettierrc",
      ".spelling",
      "LICENSE",
      "README.md",
      "bin",
      "codecov.yml",
      "config.js",
      "docs",
      "jest.config.js",
      "jest.integration.config.js",
      "locale",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "scripts",
      "src",
      "tests",
      "tmp",
      "webpack.config.js",
      "webpack.l10n.config.babel.js"
    ],
    "/docs": [
      ".nojekyll",
      "github-markdown.css",
      "import-firefox-schema.md",
      "index.html",
      "rules.md",
      "rules.tmpl",
      "third-party-libraries.md",
      "types.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "PULL_REQUEST_TEMPLATE.md",
      "stale.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/addons-linter.svg?style=svg)](https://circleci.com/gh/mozilla/addons-linter) [![codecov](https://codecov.io/gh/mozilla/addons-linter/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/addons-linter) [![Dependency Status](https://david-dm.org/mozilla/addons-linter.svg)](https://david-dm.org/mozilla/addons-linter) [![devDependency Status](https://david-dm.org/mozilla/addons-linter/dev-status.svg)](https://david-dm.org/mozilla/addons-linter#info=devDependencies) [![npm version](https://badge.fury.io/js/addons-linter.svg)](https://badge.fury.io/js/addons-linter)\n\n# addons-linter\n\nThe Add-ons Linter is being used by [web-ext](https://github.com/mozilla/web-ext/) and [addons.mozilla.org](https://github.com/mozilla/addons-server/) to lint [WebExtensions](https://developer.mozilla.org/en-US/Add-ons/WebExtensions).\n\nIt can also be used as a standalone binary and library.\n\nYou can find more information about the linter and it's implemented rules in our [documentation](https://mozilla.github.io/addons-linter/).\n\n## Usage\n\n### Command Line\n\nYou need Node.js to use the add-ons linter.\n\nTo validate your add-on locally, install the linter from [npm](http://nodejs.org/):\n\n```sh\n# Install globally so you can use the linter from any directory on\n# your machine.\nnpm install -g addons-linter\n```\n\nAfter installation, run the linter and direct it to your add-on file:\n\n```sh\naddons-linter my-addon.zip\n```\n\nAlternatively you can point it at a directory:\n\n```sh\naddons-linter my-addon/src/\n```\n\nThe addons-linter will check your add-on and show you errors, warnings, and friendly messages for your add-on. If you want more info on the options you can enable/disable for the command-line app, use the `--help` option:\n\n```sh\naddons-linter --help\n```\n\n#### Privileged extensions\n\nThe addons-linter can lint privileged extensions **only** when the `--privileged` option is passed to it. This option changes the behavior of the linter to:\n\n1. emit errors when the input file (or directory) is a regular extension (i.e. the extension does not use privileged features)\n2. hide messages related to privileged features (e.g., permissions and properties) when the input file (or directory) is a privileged extension\n\n### Linter API Usage\n\nYou can use the linter directly as a library to integrate it better into your development process.\n\n```js\nimport linter from 'addons-linter';\n\nconst sourceDir = process.cwd();\n\nconst linter = linter.createInstance({\n  config: {\n    // This mimics the first command line argument from yargs,\n    // which should be the directory to the extension.\n    _: [sourceDir],\n    logLevel: process.env.VERBOSE ? 'debug' : 'fatal',\n    stack: Boolean(process.env.VERBOSE),\n    pretty: false,\n    warningsAsErrors: false,\n    metadata: false,\n    output: 'none',\n    boring: false,\n    selfHosted: false,\n    // Lint only the selected files\n    //   scanFile: ['path/...', ...]\n    //\n    // Exclude files:\n    shouldScanFile: (fileName) => true,\n  },\n  // This prevent the linter to exit the nodejs application\n  runAsBinary: false,\n});\n\nlinter.run()\n  .then((linterResults) => ...)\n  .catch((err) => console.error(\"addons-linter failure: \", err));\n```\n\n`linter.output` is composed by the following properties (the same of the 'json' report type):\n\n```js\n{\n  metadata: {...},\n  summary: {\n    error, notice, warning,\n  },\n  scanFile,\n  count,\n  error: [{\n    type: \"error\",\n    code, message, description,\n    column, file, line\n  }, ...],\n  warning: [...],\n  notice: [...]\n}\n```\n\n## Development\n\nIf you'd like to help us develop the addons-linter, that's great! It's pretty easy to get started, you just need Node.js installed on your machine.\n\n### Quick Start\n\nIf you have Node.js installed, here's the quick start to getting your development dependencies installed and running the tests\n\n```sh\ngit clone https://github.com/mozilla/addons-linter.git\ncd addons-linter\nnpm install\n# Build the project.\nnpm run build\n# Run the test-suite and watch for changes. Use `npm run test-once` to\n# just run it once.\nnpm run test\n```\n\nYou can also build the addons-linter binary to test your changes.\n\n```sh\nnpm run build\n# Now run it against your add-on. Please note that for every change\n# in the linter itself you'll have to re-build the linter.\nbin/addons-linter my-addon.zip\n```\n\n### Required Node version\n\naddons-linter requires Node.js v12 or greater. Have a look at our `.circleci/config.yml` file which Node.js versions we officially test.\n\nUsing nvm is probably the easiest way to manage multiple Node versions side by side. See [nvm on GitHub](https://github.com/creationix/nvm) for more details.\n\n### Install dependencies\n\nInstall dependencies with npm:\n\n```sh\nnpm install\n```\n\nDependencies are automatically kept up-to-date using [renovatebot](https://renovatebot.com/).\n\n#### npm scripts\n\n| Script                          | Description                                                                      |\n| ------------------------------- | -------------------------------------------------------------------------------- |\n| npm test                        | Runs the tests (watches for changes)                                             |\n| npm [run] build                 | Builds the lib (used by CI)                                                      |\n| npm run test-coverage           | Runs the tests with coverage (watches for changes)                               |\n| npm run test-once               | Runs the tests once                                                              |\n| npm run lint                    | Runs ESLint                                                                      |\n| npm run test-coverage-once      | Runs the tests once with coverage                                                |\n| npm run test-integration-linter | Runs our integration test-suite                                                  |\n| npm run prettier                | Automatically format the whole code-base with Prettier                           |\n| npm run prettier-ci             | Run [Prettier][] and fail if some code has been changed without being formatted  |\n| npm run prettier-dev            | Automatically compare and format modified source files against the master branch |\n\n### Building\n\nYou can run `npm run build` to build the library.\n\nOnce you build the library you can use the CLI in `bin/addons-linter`.\n\n### Testing\n\nRun `npm test`. This will watch for file-changes and re-runs the test suite.\n\n#### Coverage\n\nWe're looking to maintain coverage at 100%. Use the coverage data in the test output to work out what lines aren't covered and ensure they're covered.\n\n#### Assertions and testing APIs\n\nWe are using using Sinon for assertions, mocks, stubs and more [see the Sinon docs for the API available](http://sinonjs.org/).\n\n[Jest](https://facebook.github.io/jest/) is being used as a test-runner but also provides helpful tools. Please make sure you read their documentation for more details.\n\n### Logging\n\nWe use [pino](https://github.com/pinojs/pino) for logging:\n\n- By default logging is off (level is set to 'fatal') .\n- Logging in tests can be enabled using an env var e.g: `LOG_LEVEL=debug jest test`\n- Logging on the CLI can be enabled with `--log-level [level]`.\n\n### Prettier\n\nWe use [Prettier](https://prettier.io/) to automatically format our JavaScript code and stop all the on-going debates over styles. As a developer, you have to run it (with `npm run prettier-dev`) before submitting a Pull Request.\n\n### L10n extraction\n\nThe localization process is very similar to [how we do it for addons-frontend](https://addons-frontend.readthedocs.io/en/latest/i18n/#updating-locales): locales are always updated on the `master` branch, any PR that changes or introduces new localized strings should be merged on `master` first.\n\nIn order to update the locales (when new localized strings are added to the codebase), run the following script from the `master` branch. This script automates _all_ the steps described in the addons-frontend docs, without any confirmation step.\n\n```\n./scripts/run-l10n-extraction\n```\n\n## Architecture\n\nIn a nutshell the way the linter works is to take an add-on package, extract the metadata from the xpi (zip) format and then process the files it finds through various content scanners.\n\nWe are heavily relying on [ESLint](https://eslint.org/) for JavaScript linting, [cheerio](https://github.com/cheeriojs/cheerio) for HTML parsing as well as [fluent.js](https://github.com/projectfluent/fluent.js) for parsing language packs.\n\n### Scanners\n\nEach file-type has a scanner. For example: CSS files use `CSSScanner`; JavaScript files use `JavaScriptScanner`. Each scanner looks at relevant files and passes each file through a parser which then hands off to a set of rules that look for specific things.\n\n### Rules\n\nRules get exported via a single function in a single file. A rule can have private functions it uses internally, but rule code should not depend on another rule file and each rule file should export one rule.\n\nEach rule function is passed data from the scanner in order to carry out the specific checks for that rule it returns a list of objects which are then made into message objects and are passed to the Collector.\n\n### Collector\n\nThe Collector is an in-memory store for all validation message objects \"collected\" as the contents of the package are processed.\n\n### Messages\n\nEach message has a code which is also its key. It has a message which is a short outline of what the message represents, and a description which is more detail into why that message was logged. The type of the message is set as messages are added so that if necessary the same message could be an error _or_ a warning for example.\n\n### Output\n\nLastly when the processing is complete the linter will output the collected data as text or JSON.\n\n## Deploys\n\nWe deploy to npm automatically using Circle CI. To release a new version, increment the version in `package.json` and create a PR. Make sure your version number conforms to the [semver][] format eg: `0.2.1`.\n\nAfter merging the PR, [create a new release][new release] with the same tag name as your new version. Once the build passes it will deploy. Magic! \u2728\n\n## Dispensary\n\nAs of November 2021, [dispensary](https://github.com/mozilla/dispensary) has been merged into this project and a CLI is available by running `./scripts/dispensary`.\n\n### Libraries updates\n\nThis is the (manual) process to update the \"dispensary\" libraries:\n\n1. Open `src/dispensary/libraries.json`\n2. Open the release pages of each library. Here is a list:\n\n   - https://github.com/angular/angular.js/releases\n   - https://github.com/jashkenas/backbone/releases\n   - https://github.com/twbs/bootstrap/releases\n   - https://download.dojotoolkit.org/\n   - https://github.com/cure53/DOMPurify/releases\n   - https://github.com/jquery/jquery/releases\n   - https://github.com/jquery/jquery-ui/releases\n   - https://github.com/moment/moment/releases\n   - https://github.com/mootools/mootools-core/releases\n   - http://prototypejs.org/\n   - https://github.com/facebook/react/releases\n   - https://github.com/jashkenas/underscore/releases\n   - https://github.com/mozilla/webextension-polyfill/releases\n\n3. On each page, check whether there are newer release versions than what is in `src/dispensary/libraries.json`. Note that some libraries, like react, support several versions, so we need to check each \"branch\".\n4. For major upgrades, take a quick look at the code changes\n5. Add new versions to `src/dispensary/libraries.json`\n6. Run `npm run update-hashes`\n7. Commit the changes in `src/dispensary/libraries.json`and `src/dispensary/hashes.txt`\n8. Open a Pull Request\n\nNote: `hashes.txt` will be embedded into the addons-linter bundle.\n\n[new release]: https://github.com/mozilla/addons-linter/releases/new\n[semver]: http://semver.org/\n[prettier]: https://prettier.io/\n"
},
{
  "name": "fxa-content-server-l10n",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Gruntfile.js",
      "README.md",
      "grunttasks",
      "locale",
      "package-lock.json",
      "package.json",
      "scripts"
    ],
    "/.github": [
      "CODEOWNERS",
      "linter_config.yml",
      "requirements.txt",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# fxa-content-server-l10n\n\n| Group | Results |\n|----|----|\n| Tests | [![Tests](https://github.com/mozilla/fxa-content-server-l10n/actions/workflows/test.yaml/badge.svg)](https://github.com/mozilla/fxa-content-server-l10n/actions/workflows/test.yaml) |\n| L10n Linters | [![FxA](https://github.com/mozilla-l10n/mozl10n-linter/actions/workflows/fxa.yaml/badge.svg)](https://github.com/mozilla-l10n/mozl10n-linter/actions/workflows/fxa.yaml)<br>[![FxA Gettext](https://github.com/mozilla-l10n/mozl10n-linter/actions/workflows/fxa_gettext.yaml/badge.svg)](https://github.com/mozilla-l10n/mozl10n-linter/actions/workflows/fxa_gettext.yaml) |\n\nThis repo (abbreviated as \"L10N\" in this README) contains all translated/translatable strings for all of the FxA servers (fxa-content-server, fxa-auth-server, etc.). The FxA repository is abbreviated as \"SOURCE\" in this document.\n\nThe string localization is managed in [Pontoon](https://pontoon.mozilla.org/projects/firefox-accounts/) and it pushes changes anytime it likes.\n\nOn a regular basis (currently once a week) a [cron job runs](https://github.com/mozilla/fxa-content-server-l10n/blob/master/.github/workflows/l10n_extract.yaml) to extract all the strings from SOURCE and open a PR to merge them into this repository. Someone from the localization team will review that PR for any strings that are confusing to localize and, if there aren't any problems, will merge it. At that point Pontoon sees the changes and strings can be localized from the Pontoon interface.\n\nThe extraction process can also be triggered manually from [here](https://github.com/mozilla/fxa-content-server-l10n/actions/workflows/l10n_extract.yaml).\n\nA new copy of this repository is checked out every time a deploy happens so deployed sites have the latest strings.\n\n## Submitting Translations\n\nPlease find your locale on [Pontoon - Firefox Accounts](https://pontoon.mozilla.org/projects/firefox-accounts/).\nFollow the instructions on those sites to submit your translations.\n"
},
{
  "name": "bedrock",
  "files": {
    "/": [
      ".adr-dir",
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".editorconfig",
      ".env-dist",
      ".eslintignore",
      ".eslintrc.js",
      ".git-blame-ignore-revs",
      ".github",
      ".gitignore",
      ".gitlab-ci.yml",
      ".pre-commit-config.yaml",
      ".prettierignore",
      ".prettierrc.json",
      ".readthedocs.yaml",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "Procfile",
      "README.md",
      "app.json",
      "bedrock",
      "bin",
      "codecov.yml",
      "contribute.json",
      "data",
      "docker-compose.yml",
      "docker",
      "docs",
      "etc",
      "gcp",
      "glean",
      "heroku.yml",
      "l10n-pocket",
      "l10n",
      "lib",
      "license_header",
      "manage.py",
      "media",
      "newrelic.ini",
      "package-lock.json",
      "package.json",
      "pyproject.toml",
      "requirements",
      "root_files",
      "scripts",
      "setup.cfg",
      "setup.py",
      "tests",
      "webpack.config.js",
      "webpack.static.config.js",
      "wsgi"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "_templates",
      "abtest.rst",
      "analytics.rst",
      "architectural-decisions.rst",
      "architecture",
      "banners.rst",
      "browser-support.rst",
      "build-github.zsh",
      "coding.rst",
      "conf.py",
      "content-cards.rst",
      "contentful.rst",
      "contribute.rst",
      "download-buttons.rst",
      "firefox-accounts.rst",
      "firefox-stub-attribution.rst",
      "funnelcake.rst",
      "images",
      "index.rst",
      "install.rst",
      "l10n.rst",
      "newsletters.rst",
      "pipeline.rst",
      "redirects.rst",
      "send-to-device.rst",
      "sitemap.rst",
      "testing.rst",
      "uitour.rst",
      "vpn-affiliate-attribution.rst"
    ],
    "/.github": [
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE.md",
      "dependabot.yml",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "DC_CI = \"bin/docker-compose.sh\"\nDC = $(shell which docker-compose)\nDOCKER = $(shell which docker)\nTEST_DOMAIN = www.mozilla.org\nPOCKET_MODE = Pocket\n\nall: help\n\nhelp:\n\t@echo \"Please use \\`make <target>' where <target> is one of\"\n\t@echo \"  build                      - build docker images for dev\"\n\t@echo \"  run                        - docker-compose up the entire system for dev\"\n\t@echo \"  stop                       - stop all docker containers\"\n\t@echo \"  kill                       - kill all docker containers (more forceful than stop)\"\n\t@echo \"  pull                       - pull the latest production images from Docker Hub\"\n\t@echo \"  run-shell                  - open a bash shell in a fresh container\"\n\t@echo \"  shell                      - open a bash shell in the running app\"\n\t@echo \"  djshell                    - start the Django Python shell in the running app\"\n\t@echo \"  fresh-data                 - pull the latest database and update all external data\"\n\t@echo \"  clean                      - remove all build, test, coverage and Python artifacts\"\n\t@echo \"  rebuild                    - force a rebuild of all of the docker images\"\n\t@echo \"  lint                       - check style with Flake8, ESlint, Stylelint, and Prettier\"\n\t@echo \"  format                     - format front-end code using Stylelint and Prettier\"\n\t@echo \"  test                       - run tests against local files\"\n\t@echo \"  test-image                 - run tests against files in docker image\"\n\t@echo \"  test-cdn                   - run CDN tests against TEST_DOMAIN\"\n\t@echo \"  docs                       - generate Sphinx HTML documentation with server and live reload using Docker\"\n\t@echo \"  livedocs                   - generate Sphinx HTML documentation with server and live reload\"\n\t@echo \"  build-docs                 - generate Sphinx HTML documentation using Docker\"\n\t@echo \"  build-ci                   - build docker images for use in our CI pipeline\"\n\t@echo \"  test-ci                    - run tests against files in docker image built by CI\"\n\t@echo \"  compile-requirements       - update Python requirements files using pip-compile\"\n\t@echo \"  check-requirements         - get a report on stale/old Python dependencies in use\"\n\t@echo \"  install-local-python-deps  - install Python dependencies for local development\"\n\n.env:\n\t@if [ ! -f .env ]; then \\\n\t\techo \"Copying .env-dist to .env...\"; \\\n\t\tcp .env-dist .env; \\\n\tfi\n\n.docker-build:\n\t${MAKE} build\n\n.docker-build-pull:\n\t${MAKE} pull\n\nbuild: .docker-build-pull\n\t${DC} build --pull app assets\n\ttouch .docker-build\n\nbuild-prod: .docker-build-pull\n\t${DC} build --pull release\n\npull: .env\n\t-GIT_COMMIT= ${DC} pull release app assets builder app-base\n\ttouch .docker-build-pull\n\nrebuild: clean build\n\n# Run in Mozorg-only mode, using Bedrock to serve ONLY Mozorg pages\nrun: .docker-build-pull\n\t${DC} up assets app\n\nrun-prod: .docker-build-pull\n\t${DC} up release-local\n\n# Run in Pocket-only mode, using Bedrock to serve ONLY Pocket pages _at the root path_\nrun-pocket: .docker-build-pull\n\t-SITE_MODE=${POCKET_MODE} ${DC} up assets app\n\nrun-pocket-prod: .docker-build-pull\n\t-SITE_MODE=${POCKET_MODE} ${DC} up release-local\n\nstop:\n\t${DC} stop\n\nkill:\n\t${DC} kill\n\nfresh-data:\n\t${DC} exec app bin/sync-all.sh\n\nrun-shell:\n\t${DC} run --rm app bash\n\nshell:\n\t${DC} exec app bash\n\ndjshell:\n\t${DC} exec app python manage.py shell_plus\n\nclean:\n#\tpython related things\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -rf {} +\n#\ttest related things\n\t-rm -f .coverage\n#\tdocs files\n\t-rm -rf docs/_build/\n#\tstatic files\n\t-rm -rf static_build/\n#\tstate files\n\t-rm -f .docker-build*\n# clean untracked files & directories\n\tgit clean -d -f\n\nlint: .docker-build-pull\n\t${DC} run test flake8\n\t${DC} run assets npm run lint\n\nformat: .docker-build-pull\n\t${DC} run assets npm run format\n\t${DC} run app black .\n\ntest: .docker-build-pull\n\t${DC} run --rm test\n\ntest-cdn: .docker-build-pull test_infra/fixtures/tls.json\n\t${DC} run test py.test --base-url https://${TEST_DOMAIN} test_infra\n\ntest-image: .docker-build\n\t${DC} run test-image\n\ndocs: .docker-build-pull\n\t${DC} up docs\n\nbuild-docs: .docker-build-pull\n\t${DC} run app make -C docs/ clean html\n\nlivedocs:\n\t${MAKE} -C docs/ clean livehtml\n\ntest_infra/fixtures/tls.json:\n\t${DOCKER} run -it --rm jumanjiman/ssllabs-scan:latest --quiet https://${TEST_DOMAIN}/en-US/ > \"test_infra/fixtures/tls.json\"\n\n###############\n# For use in CI\n###############\n.docker-build-ci:\n\t${MAKE} build-ci\n\nbuild-ci: .docker-build-pull\n\t${DC_CI} build --pull release\n#\ttag intermediate images using cache\n\t${DC_CI} build app builder assets app-base\n\ttouch .docker-build-ci\n\ntest-ci: .docker-build-ci\n\t${DC_CI} run test-image\n\n#########################\n# Requirements management\n#########################\n\ncompile-requirements: .docker-build-pull\n\t${DC} run --rm compile-requirements\n\ncheck-requirements: .docker-build-pull\n\t${DC} run --rm test pip list -o\n\n######################################################\n# For use in local-machine development (not in Docker)\n######################################################\n\ninstall-local-python-deps:\n\t# Dev requirements are a superset of prod requirements\n\tpip install -r requirements/dev.txt\n\n.PHONY: all clean build pull docs livedocs build-docs lint run stop kill run-shell shell test test-image rebuild build-ci test-ci fresh-data djshell run-prod run-pocket run-pocket-prod build-prod test-cdn compile-requirements check-requirements install-local-python-deps\n",
  "readme": "Bedrock\n=======\n\n*Bedrock* is the code name of [mozilla.org][mozilla]. It is as shiny,\nawesome, and open source as always. Perhaps even a little more.\n\n[mozilla]: https://www.mozilla.org/\n\n[![Circle CI](https://circleci.com/gh/mozilla/bedrock.svg?style=svg)](https://circleci.com/gh/mozilla/bedrock)\n[![What's deployed on dev,stage,prod?](https://img.shields.io/badge/whatsdeployed-dev,stage,prod-green.svg)](https://whatsdeployed.io/s/RuO/mozilla/bedrock)\n\nDocs\n----\n\nBedrock is a [Django][django] project. Check out the [django docs][dj-docs] for\ngeneral technical documentation. In addition, there are project-specific\n[bedrock docs][br-docs].\n\n[django]: https://www.djangoproject.com/\n[dj-docs]: https://docs.djangoproject.com/\n[br-docs]: http://bedrock.readthedocs.org/\n\nContributing\n------------\n\nPatches are welcome! Feel free to fork and contribute to [this project][gh-bedrock] on\nGitHub. If you find a problem and wish to report it, please [file\na bug][github-issue].\n\nLooking for a good first bug to work on? Take a look at our [contributing doc][contributing]\nto get started.\n\n[gh-bedrock]: https://github.com/mozilla/bedrock\n[github-issue]: https://github.com/mozilla/bedrock/issues/new?template=bug_report.md\n[contributing]: https://github.com/mozilla/bedrock/blob/main/.github/CONTRIBUTING.md\n\nCode of Conduct\n---------------\n\nThis repository is governed by Mozilla's [Community Participation Guidelines][participation]\nand [Developer Etiquette Guidelines][etiquette].\n\n[participation]: https://github.com/mozilla/bedrock/blob/main/CODE_OF_CONDUCT.md\n[etiquette]: https://bugzilla.mozilla.org/page.cgi?id=etiquette.html\n\nLicense\n-------\n\nThis software is licensed under the [MPL version 2.0][MPL]. For more\ninformation, read this repository's [LICENSE][LICENSE].\n\n[MPL]: https://www.mozilla.org/MPL/\n[LICENSE]: https://github.com/mozilla/bedrock/blob/main/LICENSE\n\n![](http://i.imgur.com/ElotJSI.jpg)\n\ncredit [@designerham](https://github.com/designerham)\n"
},
{
  "name": "relman-auto-nag",
  "files": {
    "/": [
      ".coveragerc",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.rst",
      "alembic.ini",
      "auto_nag",
      "db",
      "extra",
      "requirements-dev.txt",
      "requirements-test.txt",
      "requirements.txt",
      "run_close_intermittents.sh",
      "run_code_freeze_week.sh",
      "run_survey_sec_bugs.sh",
      "runauto_nag_common.sh",
      "runauto_nag_daily.sh",
      "runauto_nag_hourly.sh",
      "templates",
      "tox.ini",
      "update_people.sh"
    ],
    "/.github": [
      "dependabot.yml",
      "pull_request_template.md"
    ]
  },
  "makefile": null,
  "readme": ".. image:: https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/relman-auto-nag/master/badge.svg\n    :target: https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/relman-auto-nag/master/latest\n.. image:: https://coveralls.io/repos/github/mozilla/relman-auto-nag/badge.svg\n    :target: https://coveralls.io/github/mozilla/relman-auto-nag\n\n\nThis tool is used by Mozilla release management to send emails to the Firefox developers. It will query the bugzilla.mozilla.org database and send emails to Mozilla developers and their managers (if Mozilla staff).\n\nThe tool will also notify release managers about potential issues in bugzilla and autofix some categories of issues.\n\nThe list of checkers is documented on the Mozilla wiki:\nhttps://wiki.mozilla.org/Release_Management/autonag\n\n\nThis package currently uses Mozilla's `Bugzilla REST API <https://wiki.mozilla.org/Bugzilla:REST_API>`_, and optionally the Mozilla IAM `phonebook <https://github.com/mozilla-iam/cis/blob/master/docs/PersonAPI.md>`_ (to access bug assignees' managers & Mozilla email addresses).\n\n\nInstallation\n------------\n\n#. Check out the code::\n\n    git clone git://github.com/mozilla/relman-auto-nag.git\n\n#. (optional) Create your virtualenv using virtualenvwrapper::\n\n    virtualenv -p python3 venv\n    source venv/bin/activate\n\n#. Install pip::\n\n    easy_install pip\n\n#. Install the dependencies for Python 3 too::\n\n    pip3 install -r requirements.txt\n\n\nTo run it into production, you will need the full list of employees + managers.\n\nAutomated Nagging Script\n------------------------\n\nBefore running:\n\n1. The LDAP + SMTP infos are used to send emails\n2. Need to generate an API key from bugzilla admin ( https://bugzilla.mozilla.org/userprefs.cgi?tab=apikey )\n3. Should generate an API key from Phabricator ( https://phabricator.services.mozilla.com/settings/user )\n4. The IAM secrets are used to generate a dump of phonebook, which is required for some scripts (employees can request them by `filing a bug in the SSO: Requests component <https://bugzilla.mozilla.org/enter_bug.cgi?product=Infrastructure%20%26%20Operations&component=SSO%3A%20Requests>`_ )\n5. The private entry contains URLs for private calendar in ICS format:\n\n.. code-block:: json\n\n    # in scripts/configs/config.json\n    {\n      \"ldap_username\": \"xxx@xxxx.xxx\",\n      \"ldap_password\": \"xxxxxxxxxxxxxx\",\n      \"smtp_server\": \"smtp.xxx.xxx\",\n      \"smtp_port\": 314,\n      \"smtp_ssl\": true,\n      \"bz_api_key\": \"xxxxxxxxxxxxxx\",\n      \"phab_api_key\": \"xxxxxxxxxxxxxx\",\n      \"iam_client_secret\": \"xxxxxxxxxxxxxx\",\n      \"iam_client_id\": \"xxxxxxxxxxxxxx\",\n      \"private\":\n      {\n        \"Core::General\": \"https://...\"\n      }\n    }\n\nDo a dryrun::\n    python -m auto_nag.scripts.stalled -d\n\nThere is a ton of scripts in auto_nag/scripts/ so you should be able to find some good examples.\n\nSetting up 'Round Robin' triage rotations\n-----------------------------------------\n\nOne use case for this tool is managing triage of multiple components across a team of multiple people.\n\nTo set up a new Round Robin rotation, a manager or team lead should create a Google Calendar with the rotation of triagers.\n\nThen the administrators will need to create a configuration file:\n\n.. code-block:: json\n\n    # in scripts/configs/<name of rotation>_round_robin.json\n    {\n        \"fallback\": \"<Name of manager or lead>\",\n        \"components\":\n        {\n            \"Product::Component\": \"default\",\n            \"Product::Component\": \"default\",\n            \u2026\n        },\n        \"default\":\n        {\n            \"calendar\": \"private://<Name of calendar>\"\n        }\n    }\n\nThe person requesting the round robin schedule must provide the URL of the calendar's `.ics` file.\n\nIn the calendar, the summary of the events must be the full name (eventually prefixed with text between square brackets) of triage owner as it appears in Phonebook, e.g. `[Gfx Triage] Foo Bar` or just `Foo Bar`.\n\nAnd then you just have to add an entry in `auto_nag/scripts/config/tools.json <https://github.com/mozilla/relman-auto-nag/blob/333ec164ba5c3ceebf3c39cf84196fa35c667b1b/auto_nag/scripts/configs/tools.json#L2>`_ in the round-robin section.\n\nOnce everything is set-up you can make a PR similar too https://github.com/mozilla/relman-auto-nag/pull/858/files\n\nRunning on a server\n-------------------\n\nThis needs to run on a private server because it will have login for smtp and bugzilla key so it can't currently be shared access.\n\nCronjob::\n\n  00 14 * * 1-5 $HOME/run_autonags_daily.sh &> /tmp/autonag.log\n  15 */1 * * * $HOME/runauto_nag_hourly.sh &> /tmp/autonag-hour.log\n"
},
{
  "name": "addons-release-tests",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      ".pre-commit-config.yaml",
      "Dockerfile",
      "README.md",
      "api",
      "dev.json",
      "img",
      "pages",
      "prod.json",
      "pytest.ini",
      "regions",
      "requirements.dev.txt",
      "requirements.txt",
      "sample-addons",
      "scripts",
      "setup.cfg",
      "stage.json",
      "tests",
      "translations.json"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Release Tests for the [Mozilla Addons Website][amo].\n\n## Scope\nThis is an Automation project handled mostly by the AMO QA team. Its goal is to reduce the size of our manual test suites that need to be executed weekly on the [AMO staging][stage] environment\nbefore a new AMO prod release. The current tests are focused mostly on the frontend site, covering areas such as search, homepage UI, add-on installations and \nother UI elements. The test suites are grouped in individual test files to match the site area they are focused on. \n\nAs the project will continue to grow, test areas will be extended to also cover the [developer hub pages][devhub].\n\n## Prerequisites\nYou'll need to have the following programs installed in your system:\n- [Python 3][python]\n- [geckodriver][geckodriver]\n  - if you extract the geckodriver in your main Python directory you can call the driver at runtime from the command line\n  - on a Windows machine, python is usually installed in `C:\\Users\\AppData\\Local\\Programs\\Python`\n- [Docker for Windows][docker]\n  \n\n## How to run the tests locally\n### Clone the repository\n\nYou'll need to clone this repo using Git. If you do not know how to clone a GitHub\nrepository, check out this [help page][git-clone] from GitHub.\n\nIf you think you would like to contribute to the tests by writing or maintaining\nthem in the future, it would be a good idea to create a fork of this repository\nfirst, and then clone that. GitHub also has great instructions for\n[forking a repository][git-fork].\n\nInstall the dependencies listed in `requirements.txt`:\n  - navigate to the project root directory and run `pip install -r requirements.txt`\n\n### Running tests in the foreground\nThese tests are meant to be run against the [AMO staging][stage] environment. We use [pytest][pytest] as our test runner.\nIf you want to see the tests being run on your local machine you can do this simply by \nnavigating to the project directory and running the following command:\n```\npytest --driver Firefox --variables stage.json --variables users.json\n```\nYou can also run tests from one single test file by specifying the file name:\n\n```\npytest test_search.py --driver Firefox --variables stage.json --variables users.json\n```\nOr you can run a specific test name:\n\n```\npytest test_search.py::test_name_of_choice --driver Firefox --variables stage.json --variables users.json\n```\n\n- _note that you need to have all the requirements installed for this to work_\n- _we are using pytest `--variables` as a tool to store reusable test data_\n- _make sure that you have a [Nightly][nightly] version installed on your machine if you want the tests to launch in the foreground_\n\n\n\n### Running tests on selenium-standalone with Docker and PowerShell\n\nBefore starting, make sure that Docker is up and running and you have switched to Wndows continers.\n- _to make the container switch, click on the Docker icon in the system tray and select \"Switch to Windows continaers\"_\n\n1. Build the selenium-standalone image based on the Dockerfile instructions:\n```\ndocker image build -t firefox-standalone:latest .\n```\n- _note that the process can take a while; you will know that the image was successfully built when docker exits without any errors in the build logs_\n\n2. Once the image is built successfully you can start a container based on it:\n```\ndocker run -p 4444:4444 --shm-size 2g --rm firefox-standalone:latest\n```\n- _the contianer is successfully initialized if you see `Selenium Server is up and running on port 4444` as the last entry_\n- _you can also load `localhost:4444` in your browser and make sure you see the Selenium-standalone homepage_\n\n3. To run the tests inside the selenium-standalone container, you need to point `pytest` to `port 4444`:\n```\npytest test_name.py --driver Remote --port 4444 --capability browserName firefox --variables stage.json\n```\n- _we use `--driver Remote` and `--port 4444` because we want to tell our tests to run against the Selenium-standalone server inside our container_\n- _the tests will run headless (the browser should not open). If the browser opens, your set-up might not be correct_\n\n\n### Adding a test\n\nThe tests are written in Python using a POM, or Page Object Model. The plugin we use for this is called [pypom][pypom]. Please read the documentation there for good examples\non how to use the Page Object Model when writing tests.\n\nThe pytest plugin that we use for running tests has a number of advanced command\nline options available too. The full documentation for the plugin can be found [here][pytest-selenium].\n\n\n### Mobile and Desktop testing\n\nIf you would like to add or edit tests please consider that these are run on both a mobile resolution and a desktop resolution. The mobile resolution is ```738x414 (iPhone 7+)```, the desktop resolution is: ```1920x1080```. Your tests should be able to work on both.\n\n\n### Debugging a failure\n\nWhether a test passes or fails will result in a HTML report being created. This report will have detailed information of the test run and if a test does fail, it will provide geckodriver logs, terminal logs, as well as a screenshot of the browser when the test failed. \nWe use a pytest plugin called [pytest-html][pytest-html] to create this report. The report can be found within the project directory and is named `ui-test.html`. It should be viewed within a browser.\n\n[amo]: https://addons.mozilla.org\n[stage]: https://addons.allizom.org\n[python]: https://www.python.org/downloads/\n[docker]: https://www.docker.com/products/docker-desktop\n[addons-frontend]: https://github.com/mozilla/addons-frontend/\n[addons-server]: https://github.com/mozilla/addons-server\n[addons-server-docs]: https://addons-server.readthedocs.io/en/latest/topics/install/docker.html\n[addons-server-selenium-testing]: https://addons-server.readthedocs.io/en/latest/topics/development/testing.html#selenium-integration-tests\n[flake8]: http://flake8.pycqa.org/en/latest/\n[git-clone]: https://help.github.com/articles/cloning-a-repository/\n[git-fork]: https://help.github.com/articles/fork-a-repo/\n[geckodriver]: https://github.com/mozilla/geckodriver/releases\n[pypom]: http://pypom.readthedocs.io/en/latest/\n[pytest]: https://docs.pytest.org/en/latest/\n[pytest-html]: https://github.com/pytest-dev/pytest-html\n[pytest-selenium]: http://pytest-selenium.readthedocs.org/\n[ReadTheDocs]: https://addons-server.readthedocs.io/en/latest/topics/development/testing.html#selenium-integration-tests\n[Selenium]: http://selenium-python.readthedocs.io/index.html\n[selenium-api]: http://selenium-python.readthedocs.io/locating-elements.html\n[nightly]: https://www.mozilla.org/en-US/firefox/channel/desktop/\n[devhub]: https://addons.allizom.org/developers/\n"
},
{
  "name": "addons-pm",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".env",
      ".eslintrc",
      ".gitignore",
      ".prettierignore",
      ".prettierrc",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "Procfile",
      "README.md",
      "bin",
      "components",
      "config",
      "jest.config.js",
      "jest.setup.js",
      "jsconfig.json",
      "lib",
      "next.config.js",
      "package.json",
      "pages",
      "public",
      "renovate.json",
      "styles",
      "tests",
      "yarn.lock"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "## Add-ons Project Manager\n\n[![CircleCI](https://circleci.com/gh/mozilla/addons-pm.svg?style=svg)](https://circleci.com/gh/mozilla/addons-pm) [![codecov](https://codecov.io/gh/mozilla/addons-pm/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/addons-pm)\n\nThis app is a view on the org level projects specific to add-ons.\n\n## Development Requirements\n\n- Uses node LTS\n- Uses yarn\n\n### Installation and start-up\n\n- `yarn install`\n- `yarn dev`\n\n`yarn dev` will start the Next.js development environment.\n\n### Environment Variables\n\nThe server requires setting some required environment variables. To do this create a `.env.local` file in the root of your checkout (Note: `.env.local` files are .gitignored) and add the following:\n\n#### GH_TOKEN\n\n```yaml\nGH_TOKEN=this-should-be-a-personal-access-token\n```\n\nYou can generate a personal access token token here: https://github.com/settings/tokens and you'll need the following scopes:\n\n```\npublic_repo, read:org\n```\n\n#### BZ_USERS\n\nFor needinfos to work the BZ_USERS env var should contain nicknames and Bugzilla users.\n\n```yaml\nBZ_USERS={\"name\": \"bz-email@example.com\", \"name2\": \"bz-email@example.com\"}\n```\n\n### Deployment\n\nThe current method used to deploy to Heroku is via git. To do that you'll need to setup the relevant branches and then, as long as you have rights to the apps in Heroku, you'll be able to do a release by pushing to the relevant remote repo.\n\nPushing to the a remote repo will start the deployment process and you'll get feedback in the terminal. For more details on this process see: https://devcenter.heroku.com/articles/git\n\n#### Requirements\n\nInstall the [heroku CLI](https://devcenter.heroku.com/articles/heroku-cli).\n\nAdd heroku git repos:\n\n```sh\ngit remote add staging https://git.heroku.com/addons-pm-staging.git\ngit remote add production https://git.heroku.com/addons-pm.git\n```\n\n#### Pushing to stage\n\nDouble check you're on the revision you want to deploy.\n\n```sh\nheroku login\ngit push staging\n```\n\n#### Pushing to prod\n\nDouble check you're currently on the revision you want to deploy.\n\n```sh\nheroku login\ngit push production\n```\n"
},
{
  "name": "addons-frontend",
  "files": {
    "/": [
      ".ackrc",
      ".browserslistrc",
      ".circleci",
      ".dockerignore",
      ".eslintignore",
      ".eslintrc",
      ".flowconfig",
      ".github",
      ".gitignore",
      ".husky",
      ".ignore",
      ".lintstagedrc",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      ".pyup.yml",
      ".stylelintrc",
      ".yarnrc",
      "Dockerfile",
      "LICENSE.txt",
      "README.md",
      "babel.config.js",
      "bin",
      "codecov.yml",
      "config",
      "dist",
      "docker",
      "docs",
      "flow",
      "jest.config.js",
      "locale",
      "mkdocs.yml",
      "package.json",
      "renovate.json",
      "src",
      "tests",
      "tox.ini",
      "version.json",
      "webpack-common.js",
      "webpack.blog-utils.config.babel.js",
      "webpack.dev.config.babel.js",
      "webpack.l10n.config.babel.js",
      "webpack.prod.config.babel.js",
      "yarn.lock"
    ],
    "/docs": [
      "adding-a-page.md",
      "caching.md",
      "css.md",
      "fenix.md",
      "fonts.md",
      "i18n.md",
      "index.md",
      "moz-addon-manager.md",
      "testing.md",
      "using-the-staging-sites-api.md",
      "windows.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "PULL_REQUEST_TEMPLATE.md",
      "stale.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Addons-frontend \ud83d\udd25\n\n[![Code of Conduct](https://img.shields.io/badge/%E2%9D%A4-code%20of%20conduct-blue.svg)](https://github.com/mozilla/addons-frontend/blob/master/.github/CODE_OF_CONDUCT.md) [![CircleCI](https://circleci.com/gh/mozilla/addons-frontend.svg?style=svg)](https://circleci.com/gh/mozilla/addons-frontend) [![codecov](https://codecov.io/gh/mozilla/addons-frontend/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/addons-frontend) [![Documentation](https://readthedocs.org/projects/addons-frontend/badge/?version=latest)](http://addons-frontend.readthedocs.io/en/latest/)\n\nFront-end infrastructure and code to complement [mozilla/addons-server](https://github.com/mozilla/addons-server).\n\n## Security Bug Reports\n\nThis code and its associated production website are included in Mozilla\u2019s web and services [bug bounty program]. If you find a security vulnerability, please submit it via the process outlined in the program and [FAQ pages]. Further technical details about this application are available from the [Bug Bounty Onramp page].\n\nPlease submit all security-related bugs through Bugzilla using the [web security bug form].\n\nNever submit security-related bugs through a Github Issue or by email.\n\n[bug bounty program]: https://www.mozilla.org/en-US/security/web-bug-bounty/\n[faq pages]: https://www.mozilla.org/en-US/security/bug-bounty/faq-webapp/\n[bug bounty onramp page]: https://wiki.mozilla.org/Security/BugBountyOnramp/\n[web security bug form]: https://bugzilla.mozilla.org/form.web.bounty\n\n## Requirements\n\n- You need [Node](https://nodejs.org/en/) 14.x which is the current [LTS](https://github.com/nodejs/Release) (long term support) release.\n- Install [yarn](https://yarnpkg.com/en/) to manage dependencies and run scripts.\n\nThe easiest way to manage multiple node versions in development is to use [nvm](https://github.com/nvm-sh/nvm).\n\n## Get started\n\nIf you are on Windows, please make sure to follow [windows guidelines](docs/windows.md#windows) too.\n\n- type `yarn` to install all dependencies\n- type `yarn amo:stage` to start a local server that connects to a hosted staging server\n\n## Development commands\n\nHere are some commands you can run:\n\n| Command | Description |\n| --- | --- |\n| yarn amo:olympia | Start the dev server/proxy (for amo) using data from a local addons-server environment. |\n| yarn amo:dev | Start the dev server/proxy (for amo) using data from the dev server (https://addons-dev.allizom.org/) |\n| yarn amo:dev-https | Same as `amo:dev` but with HTTPS, available at: https://example.com:3000/. [Read about setting up this environment](docs/moz-addon-manager.md#developing-with-a-local-https-server-recommended) |\n| yarn amo:stage | Start the dev server/proxy (for amo) using data from the staging server (https://addons.allizom.org/) |\n| yarn build | Build the app. |\n| yarn build-ci | Run the `build` and `bundlewatch` npm scripts. |\n| yarn bundlewatch | Run [bundlewatch][] to check the generated AMO bundle sizes. [Building AMO is required first](#building-and-running-services). |\n| yarn flow | Run Flow. By default this checks for errors and exits |\n| yarn flow:check | Explicitly check for Flow errors and exit |\n| yarn flow:dev | Continuously check for Flow errors |\n| yarn eslint | Lint the JS |\n| yarn start-func-test-server | Start a Docker container for functional tests |\n| yarn stylelint | Lint the SCSS |\n| yarn lint | Run all the JS + SCSS linters |\n| yarn prettier | Run [Prettier][] to automatically format the entire codebase |\n| yarn prettier-dev | Run [Pretty-Quick][] to automatically compare and format modified source files against the master branch |\n| yarn prettier-ci | Run [Prettier][] and fail if some code has been changed without being formatted |\n| yarn version-check | Check you have the required dependencies |\n| yarn test | Run all tests (Enters [jest][] in `--watch` mode) |\n| yarn test-coverage | Run all tests and generate code coverage report (Enters [jest][] in `--watch` mode) |\n| yarn test-coverage-once | Run all tests, generate code coverage report, then exit |\n| yarn test-once | Run all tests, run all JS + SCSS linters, then exit |\n| yarn test-ci | Run all continuous integration checks. This is only meant to run on TravisCI. |\n\n### Running tests\n\nYou can enter the interactive [jest][] mode by typing `yarn test`. This is the easiest way to develop new features.\n\nHere are a few tips:\n\n- When you start `yarn test`, you can switch to your code editor and begin adding test files or changing existing code. As you save each file, [jest][] will only run tests related to the code you change.\n- If you had typed `a` when you first started then [jest][] will continue to run the full suite even when you change specific files. Type `o` to switch back to the mode of only running tests related to the files you are changing.\n- Sometimes running tests related to your file changes is slow. In these cases, you can type `p` or `t` to filter tests by name while you working fixing a specific test suite. [More info](https://github.com/jest-community/jest-watch-typeahead).\n- If you see something like `Error watching file for changes: EMFILE` on Mac OS then `brew install watchman` might fix it. See https://github.com/facebook/jest/issues/1767\n\n#### Run a subset of the tests\n\nBy default, `yarn test` will only run a subset of tests that relate to the code you are working on.\n\nTo explicitly run a subset of tests, you can type `t` or `p` which are explained in the [jest][] watch usage.\n\nAlternatively, you can start the test runner with a [specific file or regular expression](https://facebook.github.io/jest/docs/en/cli.html#jest-regexfortestfiles), like:\n\n```\nyarn test tests/unit/amo/components/TestAddon.js\n```\n\n#### Run all tests\n\nIf you want to run all tests and exit, type:\n\n```\nyarn test-once\n```\n\n### Eslint\n\nAs you run tests you will see a report of Eslint errors at the end of the test output:\n\n    yarn test\n\nIf you would like to run tests without Eslint checks, set an environment variable:\n\n    NO_ESLINT=1 yarn test\n\n### Flow\n\nThere is limited support for using [Flow](https://flowtype.org/) to validate the intention of our program.\n\nAs you run tests you will see a report of Flow errors at the end of the test output:\n\n    yarn test\n\nIf you would like to run tests without Flow checks, set an environment variable:\n\n    NO_FLOW=1 yarn test\n\nTo only check for Flow issues during development while you edit files, run:\n\n    yarn flow:dev\n\nIf you are new to working with Flow, here are some tips:\n\n- Check out the [getting started](https://flow.org/en/docs/getting-started/) guide.\n- Read through the [web-ext guide](https://github.com/mozilla/web-ext/blob/master/CONTRIBUTING.md#check-for-flow-errors) for hints on how to solve common Flow errors.\n\nTo add flow coverage to a source file, put a `/* @flow */` comment at the top. The more source files you can opt into Flow, the better.\n\nHere is our Flow manifesto:\n\n- We use Flow to **declare the intention of our code** and help others refactor it with confidence. Flow also makes it easier to catch mistakes before spending hours in a debugger trying to find out what happened.\n- Avoid magic [Flow declarations](https://flowtype.org/en/docs/config/libs/) for any _internal_ code. Just declare a [type alias](https://flowtype.org/en/docs/types/aliases/) next to the code where it's used and [export/import](https://flow.org/en/docs/types/modules/) it like any other object.\n- Never import a real JS object just to reference its type. Make a type alias and import that instead.\n- Never add more type annotations than you need. Flow is really good at inferring types from standard JS code; it will tell you when you need to add explicit annotations.\n- When a function like `getAllAddons` takes object arguments, call its type object `GetAllAddonsParams`. Example:\n\n```js\ntype GetAllAddonsParams = {|\n  categoryId: number,\n|};\n\nfunction getAllAddons({ categoryId }: GetAllAddonsParams = {}) {\n  ...\n}\n```\n\n- Use [Exact object types](https://flowtype.org/en/docs/types/objects/#toc-exact-object-types) via the pipe syntax (`{| key: ... |}`) when possible. Sometimes the spread operator triggers an error like 'Inexact type is incompatible with exact type' but that's a [bug](https://github.com/facebook/flow/issues/2405). You can use the `Exact<T>` workaround from [`src/amo/types/util`](https://github.com/mozilla/addons-frontend/blob/master/src/amo/types/util.js) if you have to. This is meant as a working replacement for [\\$Exact<T>](https://flow.org/en/docs/types/utilities/#toc-exact).\n- Add a type hint for components wrapped in HOCs (higher order components) so that Flow can validate calls to the component. We need to add a hint because we don't yet have decent type coverage for all the HOCs we rely on. Here is an example:\n\n```js\n// Imagine this is something like components/ConfirmButton/index.js\nimport { compose } from 'redux';\nimport * as React from 'react';\n\n// This expresses externally used props, i.e. to validate how the app would use <ConfirmButton />\ntype Props = {|\n  prompt?: string | null,\n|};\n\n// This expresses internally used props, such as i18n which is injected by translate()\ntype InternalProps = {|\n  ...Props,\n  i18n: I18nType,\n|};\n\nexport class ConfirmButtonBase extends React.Component<InternalProps> {\n  render() {\n    const prompt = this.props.prompt || this.props.i18n.gettext('Confirm');\n    return <button>{prompt}</button>;\n  }\n}\n\n// This provides a type hint for the final component with its external props.\n// The i18n prop is not in external props because it is injected by translate() for internal use only.\nconst ConfirmButton: React.ComponentType<Props> = compose(translate())(\n  ConfirmButtonBase,\n);\n\nexport default ConfirmButton;\n```\n\n- Try to avoid loose types like `Object` or `any` but feel free to use them if you are spending too much time declaring types that depend on other types that depend on other types, and so on.\n- You can add a `$FlowFixMe` comment to skip a Flow check if you run into a bug or if you hit something that's making you bang your head on the keyboard. If it's something you think is unfixable then use `$FlowIgnore` instead. Please explain your rationale in the comment and link to a GitHub issue if possible.\n- If you're stumped on why some Flow annotations aren't working, try using the `yarn flow type-at-pos ...` command to trace which types are being applied to the code. See `yarn flow -- --help type-at-pos` for details.\n\n### Prettier\n\nWe use [Prettier][] to automatically format our JavaScript code and stop all the on-going debates over styles.\n\n### Pre-commit hook\n\nWe use [husky](https://github.com/typicode/husky) and [lint-staged](https://github.com/okonet/lint-staged) to configure and run a pre-commit hook whenever you add a commit locally using Git. This hook will automatically run `Prettier` on your code, and will also check it for `eslint` and `flow` errors. If any errors exist, you will see a message similar to:\n\n`\u2714 Reverting to original state because of errors...`\n\nwhich will be followed by the details of the error(s). The last line of output will say:\n\n`husky > pre-commit hook failed (add --no-verify to bypass)`\n\nAs per this message, you can bypass this hook by adding `--no-verify` to your `git commit` command, but we do not recommend doing this. Instead, you should fix any errors reported and then run `git commit` again, and it should succeed.\n\n### Code coverage\n\nTo see a report of code coverage, type:\n\n```\nyarn test-coverage-once\n```\n\nThis will print a table of files showing the percentage of code coverage. The uncovered lines will be shown in the right column but you can open the full report in a browser:\n\n```\nopen coverage/lcov-report/index.html\n```\n\n### Running AMO for local development\n\nA proxy server is provided for running the AMO app with the API on the same host as the frontend. This mimics our production setup.\n\nStart developing against a hosted API like this:\n\n```\nyarn amo:dev\n```\n\nThis configures the proxy to use `https://addons-dev.allizom.org` for API data. This command is the most common way to develop new frontend features. See the table of commands up above for similar ways to run the server.\n\nTo use a [local API server running in Docker](https://addons-server.readthedocs.io/en/latest/topics/install/index.html), you can use the `yarn amo` command. However, this is currently not working. See [issue-7196][].\n\nAuthentication will work when initiated from addons-frontend and will persist to addons-server but it will not work when logging in from an addons-server page. See [mozilla/addons-server#4684](https://github.com/mozilla/addons-server/issues/4684) for more information on fixing this.\n\n### Local configuration\n\nIf you need to override any settings while running `yarn amo`, `yarn amo:dev`, or `yarn amo:stage`, first create a local config file named exactly like this:\n\n    touch config/local-development.js\n\nMake any config changes. For example:\n\n```javascript\nmodule.exports = {\n  trackingEnabled: true,\n};\n```\n\nRestart the server to see it take affect.\n\nConsult the [config file loading order docs](https://github.com/lorenwest/node-config/wiki/Configuration-Files#file-load-order) to learn more about how configuration is applied.\n\n### Configuring an Android device for local development\n\nIf you want to access your local server on an Android device you will need to change a few settings. Let's say your local machine is accessible on your network at the IP address `10.0.0.1`. You could start your server like this:\n\n```\nAPI_HOST=http://10.0.0.1:3000 \\\n    SERVER_HOST=10.0.0.1 \\\n    WEBPACK_SERVER_HOST=10.0.0.1 \\\n    yarn amo:dev\n```\n\nOn your Android device, you could then access the development site at `http://10.0.0.1:3000`.\n\n**NOTE**: At this time, it is not possible to sign in with this configuration because the Firefox Accounts client redirects to `localhost:3000`. You may be able to try a different approach by editing `/etc/hosts` on your device so that `localhost` points to your development machine but this has not been fully tested.\n\n### Disabling CSP for local development\n\nWhen developing locally with a webpack server, the randomly generated asset URL will fail our Content Security Policy (CSP) and clutter your console with errors. You can turn off all CSP errors by settings CSP to `false` in any local config file, such as `local-development-amo.js`. Example:\n\n```javascript\nmodule.exports = {\n  CSP: false,\n};\n```\n\n### Working on the documentation\n\nThe documentation you are reading right now lives inside the source repository as [Github flavored Markdown](https://guides.github.com/features/mastering-markdown/#GitHub-flavored-markdown). When you make changes to these files you can create a pull request to preview them or, better yet, you can use [grip](https://github.com/joeyespo/grip) to preview the changes locally. After installing `grip`, run it from the source directory like this:\n\n```\ngrip .\n```\n\nOpen its `localhost` URL and you will see the rendered `README.md` file. As you make edits, it will update automatically.\n\n### Building and running services\n\nThe following are scripts that are used in deployment - you generally won't need unless you're testing something related to deployment or builds.\n\nThe env vars are:\n\n- `NODE_ENV`: the node environment, e.g. `production` or `development`\n- `NODE_CONFIG_ENV`: the name of the configuration to load, e.g., `dev`, `stage`, `prod`\n\n| Script     | Description                                    |\n| ---------- | ---------------------------------------------- |\n| yarn start | Starts the express server (requires env vars)  |\n| yarn build | Builds the libs (all apps) (requires env vars) |\n\n**Example:** Building and running a production instance of the app:\n\n```\nNODE_ENV=production NODE_CONFIG_ENV=prod yarn build\nNODE_ENV=production NODE_CONFIG_ENV=prod yarn start\n```\n\n#### Running builds locally\n\n**To run the app locally in production mode you'll need to create a config file for local production builds.** Production builds can be built for different environments: `dev`, `stage` and `prod` (controlled by the `NODE_CONFIG_ENV` env var), but only one extra config file is needed for these environments to run locally.\n\nRename the file named `config/local.js.dist` to `config/local.js`. After this, re-build and restart using `yarn build` and `yarn start` as documented above. If you have used `127.0.0.1` before with a different configuration, be sure to clear your cookies. The application should be available at: http://127.0.0.1:4000/.\n\n**NOTE**: At this time, it's not possible to sign in using this approach.\n\n## What version is deployed?\n\nYou can check to see what commit of `addons-frontend` is deployed, which A/B experiments are running, or which feature flags are enabled by making a request like this:\n\n```\ncurl https://addons-dev.allizom.org/__frontend_version__\n{\n    \"build\": \"https://circleci.com/gh/mozilla/addons-frontend/10333\",\n    \"commit\": \"47edfa6f24e333897b25516c587f504e294e8fa9\",\n    \"experiments\": {\n        \"homeHero\": true\n    },\n    \"feature_flags\": {\n        \"enableFeatureAMInstallButton\": true,\n        \"enableFeatureStaticThemes\": true\n    },\n    \"source\": \"https://github.com/mozilla/addons-frontend\",\n    \"version\": \"\"\n}\n```\n\nThis will return a 415 response if a `version.json` file doesn't exist in the root directory. This file is typically generated by the deploy process.\n\nFor consistency with monitoring scripts, the same data can be retrieved at this URL:\n\n```\ncurl https://addons-dev.allizom.org/__version__\n```\n\n:bulb: You can install the [amo-info extension](https://addons.mozilla.org/en-US/firefox/addon/amo-info/) to easily view this information.\n\n## Addons Frontend Blog Utils\n\nThis project also contains code to build a library named `addons-frontend-blog-utils` and offers the following commands:\n\n- `yarn build:blog-utils-dev`: build the library, start a watcher to rebuild the library on change and serve a development page at http://127.0.0.1:11000\n- `yarn build:blog-utils-prod`: build the library in production mode\n\nThis library is exclusively designed to work with [addons-blog][].\n\n### Release process\n\nIn order to publish a new version of `addons-frontend-blog-utils`, a special tag has to be pushed to the main repository. The tag name must start with `blog-utils-` and usually contains the version number. This can be automated using the following command:\n\n```\nnpm version [major|minor|patch]\n```\n\nIssuing this command from the `master` branch will update the version in the `package.json`, create a commit and create a tag. Push both this commit and the tag to the main repository.\n\n**Note:** When a new `addons-frontend-blog-utils` release is merged in [addons-blog][], you should publish a new version of the WordPress theme. Please follow [these instructions in the addons-blog repository][addons-blog-wp-theme].\n\n## Core technologies\n\n- Based on Redux + React\n- Code written in ES2015+\n- Universal rendering via node\n- Unit tests with high coverage (aiming for 100%)\n\n[bundlewatch]: https://github.com/bundlewatch/bundlewatch\n[jest]: https://jestjs.io/docs/en/getting-started.html\n[prettier]: https://prettier.io/\n[addons-blog]: https://github.com/mozilla/addons-blog\n[issue-7196]: https://github.com/mozilla/addons-frontend/issues/7196\n[addons-blog-wp-theme]: https://github.com/mozilla/addons-blog#about-the-wordpress-theme\n"
},
{
  "name": "dump_syms",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "src",
      "test_data"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# dump_syms\n\n[![Task Status](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/dump_syms/master/badge.svg)](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/dump_syms/master/latest)\n[![codecov](https://codecov.io/gh/calixteman/dump_syms/branch/master/graph/badge.svg)](https://codecov.io/gh/calixteman/dump_syms)\n\ndump_syms is a command-line utility for parsing the debugging information the\ncompiler provides (whether as DWARF or STABS sections in an ELF file or as\nstand-alone PDB files) and writing that information back out in the Breakpad\nsymbol file format.\n\n\n# Usage\n\nUse dump_syms:\n\n    dump_syms [FLAGS] [OPTIONS] <filenames>...\n    \nfor help:\n\n    dump_syms --help\n\n\n# Development\n\nTo build:\n\n    cargo build\n    \nTo run tests:\n\n    cargo test\n"
},
{
  "name": "rust-parsepatch",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE-MPL-2.0",
      "README.md",
      "src",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# rust-parsepatch\n\n[![Build Status](https://travis-ci.org/mozilla/rust-parsepatch.svg?branch=master)](https://travis-ci.org/mozilla/rust-parsepatch)\n[![codecov](https://codecov.io/gh/mozilla/rust-parsepatch/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/rust-parsepatch)\n[![Crates.io](https://img.shields.io/crates/v/parsepatch.svg)](https://crates.io/crates/parsepatch)\n\n## License\n\nPublished under the MPL 2.0 license.\n\n"
},
{
  "name": "releases-comm-central",
  "files": {
    "/": [
      ".arcconfig",
      ".clang-format-ignore",
      ".cron.yml",
      ".eslintignore",
      ".eslintrc.js",
      ".gecko_rev.yml",
      ".hg-annotate-ignore-revs",
      ".hgignore",
      ".hgtags",
      ".prettierignore",
      ".taskcluster.yml",
      ".yamllint",
      ".ycm_extra_conf.py",
      "AUTHORS",
      "README.md",
      "build",
      "calendar",
      "chat",
      "mail",
      "mailnews",
      "moz.build",
      "other-licenses",
      "python",
      "suite",
      "taskcluster",
      "testing",
      "third_party",
      "tools"
    ]
  },
  "makefile": null,
  "readme": "# Thunderbird\nThunderbird is a powerful and customizable open source email client with lots of users. It is based on the same platform that Firefox uses.\n\n## Getting Started\nThis README will try and give you the basics that you need to get started, more comprehensive documentation is available on the [Thunderbird Developer Website](https://developer.thunderbird.net).\n\n### Mozilla Code Base\nThunderbird is built on the Mozilla platform, the same base that Firefox is built from. As such the two projects share a lot of code and much of the documentation for one will apply, in many ways, to the other.\n\nIn order to be able to build Thunderbird - you will need the mozilla-central repository as well as the comm-central repository (where this README lives). Check out our [Getting Started documentation](https://developer.thunderbird.net/thunderbird-development/getting-started) for instructions on how and where to get the source code.\n\n### mozilla-central vs. comm-central\n\nThe mozilla-central repostitory contains the Firefox codebase and all of the platform code. The comm-central repository is added as a subdirectory \"comm/\" under mozilla-central. This contains the code for Thunderbird.\n\n## Building Thunderbird\n\n### Build Prerequisites\n\nThis README assumes that you already have the prerequisite software required to build Thunderbird. If you have not already done so, please complete the instructions for your operating system and then continue following this guide:\n\n- [Windows Build Prerequisites](https://developer.thunderbird.net/thunderbird-development/building-thunderbird/windows-build-prerequisites)\n- [Linux Build Prerequisites](https://developer.thunderbird.net/thunderbird-development/building-thunderbird/linux-build-prerequisites)\n- [macOS Build Prerequisites](https://developer.thunderbird.net/thunderbird-development/building-thunderbird/macos-build-prerequisites)\n\n### Build Configuration\n\nTo build Thunderbird, you need to create a file named `mozconfig` (can also be `.mozconfig`) to the root directory of the mozilla-central checkout that contains the option `comm/mail` enabled. You can create a file with this line by doing this in the root source directory:\n\n```text\necho 'ac_add_options --enable-application=comm/mail' > mozconfig\n```\n\n**If you omit this line, the build system will build Firefox instead**. Other build configuration options can be added to this file, although it's **strongly recommended** that you only use options that you fully understand. For example, to create a debug build instead of a release build, that file would also contain the line:\n\n```text\nac_add_options --enable-debug\n```\n\n_Each of these ac\\_add\\_options entries needs to be on its own line._\n\nFor more on configuration options, see the page [Configuring build options](https://developer.mozilla.org/en/Configuring_Build_Options). Note that if you use an MOZ\\_OBJDIR it cannot be a sibling folder to the root source directory. Use an absolute path to be sure!\n\n### Building\n\n**Before you start**, make sure that the version you checked out is not busted. For `hg` tip, you should see green Bs on [https://treeherder.mozilla.org/\\#/jobs?repo=comm-central](https://treeherder.mozilla.org/#/jobs?repo=comm-central)\n\nTo start the build, cd into the root source directory, and run:\n\n```text\n./mach build\n```\n\nmach is our command-line tool to streamline common developer tasks. See the [mach](https://developer.mozilla.org/en-US/docs/Mozilla/Developer_guide/mach) article for more.\n\nBuilding can take a significant amount of time, depending on your system, OS, and chosen build options. Linux builds on a fast box may take under _15 minutes_, but Windows builds on a slow box may take _several hours_.\n\n### Make Your Build Faster\n\nFollow this guide to rely on `ccache` and other [Tips for making builds faster](../getting-started.md).\n\n## Running Thunderbird\n\nTo run your build, you can use:\n\n```text\n./mach run\n```\n\nThere are various command line parameters you can add, e.g. to specify a profile, such as: -no-remote -P testing --purgecaches\n\nVarious temporary files, libraries, and the Thunderbird executable will be found in your object directory \\(under `comm-central/`\\), which is prefixed with `obj-`. The exact name depends on your system and OS. For example, a Mac user may get an object directory name of `obj-x86_64-apple-darwin10.7.3/`.\n\nThe Thunderbird executable in particular, and its dependencies are located under the `dist/bin` folder under the object directory. To run the executable from your `comm-central` working directory:\n\n* Windows: `obj-.../dist/bin/thunderbird.exe`\n* Linux: `obj-.../dist/bin/thunderbird`\n* macOS: `obj-.../dist/Daily.app/Contents/MacOS/thunderbird`\n\n## Update and Build Again\n\nTo pull down the latest changes, in the mozilla directory run the following commands:\n\n```text\nhg pull -u\ncd comm\nhg pull -u\ncd ..\n```\n\nor to do it via one command:\n\n```text\nhg pull -u && cd comm && hg pull -u\n```\n\nThe just run the `./mach build` command detailed in the [Building](./#building)instructions above. This will only recompile files that changed, but it may still take a long time.\n\n## Rebuilding\n\nTo build after changes you can simply run:\n\n```text\n./mach build\n```\n\n### Rebuilding Specific Parts\n\nIf you have made many changes, but only want to rebuild specific parts, you may run the following commands.\n\n#### C or C++ Files:\n\n```text\n./mach build binaries\n```\n\n#### JavaScript or XUL Files \\(Windows Only\\):\n\n```text\n./mach build path/to/dir\n```\n\n\nReplace `path/to/dir` with the directory with the files changed.\n\nThis is the tricky bit since you need to specify the directory that installs the files, which may be a parent directory of the changed file's directory. For example, to just rebuild the Lightning calendar extension:\n\n```text\n./mach build comm/calendar/lightning\n```\n\n\n## Contributing\n\n### Getting Plugged into the Community\n\nWe have a complete listing of the ways in which you can get involved with Thunderbird [on our website](https://thunderbird.net/get-involved). Below are some quick references from that page that you can use if you are looking to contribute to Thunderbird core right away.\n\n#### Mailing Lists\n\nIf you want to participate in discussions about Thunderbird development, there are two main mailing lists you want to join.\n\n1. [**TB-Planning**](https://wiki.mozilla.org/Thunderbird/tb-planning)**:** This mailing list is higher level topics like: the future of Thunderbird, potential features, and changes that you would like to see happen. It is also used to discuss a variety of broader issues around community and governance of the project.\n2. [**Maildev**](http://lists.thunderbird.net/mailman/listinfo/maildev_lists.thunderbird.net)**:** A moderated mailing list for discussing engineering plans for Thunderbird. It is a place where you can raise questions and ideas for core Thunderbird development.\n\n#### IRC\n\nIf you want to ask questions about how to hack on Thunderbird, the IRC channel you want to join is [\\#maildev on irc.mozilla.org](irc://irc.mozilla.org/maildev).\n\n### Report a Bug and Request Features\n\n### [Bugzilla](https://bugzilla.mozilla.org/enter_bug.cgi?product=Thunderbird)\n\nThunderbird uses bugzilla for reporting and tracking bugs as well as enhancement requests. If you want to become a contributor to Thunderbird, you will need an account on Bugzilla.\n\n### Fixing a Bug and Submitting Patches\n\nAll the issues, bugs, work in progress patches, or updates related to Thunderbird, are listed on Bugzilla and are properly organized per **Product**, **Component**, and **Status**. For instance you can see how they are listed by looking at [recent bugs for Thunderbird](https://bugzilla.mozilla.org/buglist.cgi?query_format=advanced&product=Thunderbird&bug_status=UNCONFIRMED&bug_severity=blocker&bug_severity=critical&bug_severity=major&bug_severity=normal&bug_severity=minor&bug_severity=trivial&chfieldfrom=-30d&chfield=%5BBug%20creation%5D&list_id=14706087).\n\n#### Create a Bugzilla account\n\nCreating an account is necessary in order to submit patches, leave comments, and interact with any other aspect of Bugzilla. If you're currently using an `IRC` username in the `#maildev` channel, we recommend saving your profile name with the current format `Firstname Lastname (:username)` in order to be easily searchable and allow the Thunderbird team to offer better support.\n\n#### Find a Bug\n\nUse the [Advanced Search](https://bugzilla.mozilla.org/query.cgi?format=advanced) section to find bugs you want to take care of, and be sure that the bug doesn't currently have any user listed as _Assignee_ and the _Status_ is set to `NEW`. You can see a list of \"easy\" bugs for beginners [via this query](https://bugzilla.mozilla.org/buglist.cgi?bug_status=NEW&classification=Client%20Software&classification=Developer%20Infrastructure&classification=Components&classification=Server%20Software&classification=Other&f1=status_whiteboard&o1=allwordssubstr&product=Calendar&product=Chat%20Core&product=MailNews%20Core&product=Thunderbird&resolution=---&v1=good%20first%20bug&list_id=14884036). However, we assume you came here to fix your \"pet hate\" bug, so you already likely have a bug to work with.\n\n#### Search for Code References\n\nMaking sense of the **Thunderbird** source code, and knowing where to look, will take some time. The code base is pretty big and if you never worked with `XBL` or `Custom Elements` it can be overwhelming at first. We recommend using our code search engine, [Searchfox](https://searchfox.org/comm-central/source/), to inspect the source code and find snippets and references to help you out while investigating a bug.\n\n#### Mercurial Workflow\n\nMercurial is pretty flexible in terms of allowing you to write your own code and keep it separate from the main code base. You can use Mercurial Bookmarks or Mercurial Queues for managing your work. We have guides created for [bookmarks](https://developer.thunderbird.net/contributing/fixing-a-bug/using-mercurial-bookmarks) and [queues](https://developer.thunderbird.net/contributing/fixing-a-bug/using-mercurial-queues) on our developer website. While some find Mercurial Queues easier to work with, support for them is being deprecated in various Mozilla tools.\n\nOnce you finished taking care of your favorite bug and using Mercurial to commit and export your patch, you can upload it to Bugzilla for review.\n\n#### Upload a Patch\n\nOpen your patch file in your code editor and be sure it includes all your code changes, and your name and commit message at the top. You can see an example of a patch for this [README here](https://bug1547325.bmoattachments.org/attachment.cgi?id=9093146).\n\nIf everything looks good, you can access the selected bug in Bugzilla and click on the **Attach File** link located above the first comment.\n\n#### Ask for a Review\n\nWhen uploading a patch to Bugzilla, you can request a review from the user who opened the bug or another developer. Simply select the `?` in the dropdown selector in the _review_ option of the **Flags** section. An input field will appear which will allow you to type the name or username of the user you want to review your patch. You can see an example of [a patch on Bugzilla here](https://bugzilla.mozilla.org/show_bug.cgi?id=1547325#c1).\n"
},
{
  "name": "cargo-vet",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "book",
      "registry.toml",
      "src",
      "tests"
    ],
    "/.github": [
      "dependabot.yaml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# cargo-vet\r\n\r\n[![crates.io](https://img.shields.io/crates/v/cargo-vet.svg)](https://crates.io/crates/cargo-vet)\r\n![Rust CI](https://github.com/mozilla/cargo-vet/workflows/Rust%20CI/badge.svg?branch=main)\r\n\r\n> Note: cargo-vet is under heavy development and not quite ready for general use. Feel free to give it a try you're feeling adventerous \u2014 feedback is most welcome!\r\n\r\nThe `cargo vet` subcommand is a tool to help projects ensure that third-party Rust dependencies have been audited by a trusted source. It strives to be lightweight and easy to integrate.\r\n\r\nMore details available in the [book](https://mozilla.github.io/cargo-vet/).\r\n\r\n## License\r\n\r\nLicensed under either of\r\n\r\n * Apache License, Version 2.0, ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)\r\n * MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)\r\n\r\nat your option.\r\n\r\n### Contribution\r\n\r\nUnless you explicitly state otherwise, any contribution intentionally submitted\r\nfor inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any\r\nadditional terms or conditions.\r\n"
},
{
  "name": "pdf.js",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintignore",
      ".eslintrc",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".gitpod.Dockerfile",
      ".gitpod.yml",
      ".mailmap",
      ".prettierrc",
      ".stylelintignore",
      ".stylelintrc",
      "AUTHORS",
      "CODE_OF_CONDUCT.md",
      "EXPORT",
      "LICENSE",
      "README.md",
      "docs",
      "examples",
      "extensions",
      "external",
      "gulpfile.js",
      "l10n",
      "package-lock.json",
      "package.json",
      "pdfjs.config",
      "src",
      "systemjs.config.js",
      "test",
      "web"
    ],
    "/docs": [
      "config.json",
      "contents",
      "plugins",
      "templates"
    ],
    "/.github": [
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# PDF.js [![Build Status](https://github.com/mozilla/pdf.js/workflows/CI/badge.svg?branch=master)](https://github.com/mozilla/pdf.js/actions?query=workflow%3ACI+branch%3Amaster)\n\n[PDF.js](https://mozilla.github.io/pdf.js/) is a Portable Document Format (PDF) viewer that is built with HTML5.\n\nPDF.js is community-driven and supported by Mozilla. Our goal is to\ncreate a general-purpose, web standards-based platform for parsing and\nrendering PDFs.\n\n## Contributing\n\nPDF.js is an open source project and always looking for more contributors. To\nget involved, visit:\n\n+ [Issue Reporting Guide](https://github.com/mozilla/pdf.js/blob/master/.github/CONTRIBUTING.md)\n+ [Code Contribution Guide](https://github.com/mozilla/pdf.js/wiki/Contributing)\n+ [Frequently Asked Questions](https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions)\n+ [Good Beginner Bugs](https://github.com/mozilla/pdf.js/issues?direction=desc&labels=good-beginner-bug&page=1&sort=created&state=open)\n+ [Projects](https://github.com/mozilla/pdf.js/projects)\n\nFeel free to stop by our [Matrix room](https://chat.mozilla.org/#/room/#pdfjs:mozilla.org) for questions or guidance.\n\n## Getting Started\n\n### Online demo\n\nPlease note that the \"Modern browsers\" version assumes native support for\nfeatures such as `async`/`await`, optional chaining, nullish coalescing,\nand private `class` fields/methods.\n\n+ Modern browsers: https://mozilla.github.io/pdf.js/web/viewer.html\n\n+ Older browsers: https://mozilla.github.io/pdf.js/legacy/web/viewer.html\n\n### Browser Extensions\n\n#### Firefox\n\nPDF.js is built into version 19+ of Firefox.\n\n#### Chrome\n\n+ The official extension for Chrome can be installed from the [Chrome Web Store](https://chrome.google.com/webstore/detail/pdf-viewer/oemmndcbldboiebfnladdacbdfmadadm).\n*This extension is maintained by [@Rob--W](https://github.com/Rob--W).*\n+ Build Your Own - Get the code as explained below and issue `gulp chromium`. Then open\nChrome, go to `Tools > Extension` and load the (unpackaged) extension from the\ndirectory `build/chromium`.\n\n## Getting the Code\n\nTo get a local copy of the current code, clone it using git:\n\n    $ git clone https://github.com/mozilla/pdf.js.git\n    $ cd pdf.js\n\nNext, install Node.js via the [official package](https://nodejs.org) or via\n[nvm](https://github.com/creationix/nvm). You need to install the gulp package\nglobally (see also [gulp's getting started](https://github.com/gulpjs/gulp/blob/master/docs/getting-started.md#getting-started)):\n\n    $ npm install -g gulp-cli\n\nIf everything worked out, install all dependencies for PDF.js:\n\n    $ npm install\n\nFinally, you need to start a local web server as some browsers do not allow opening\nPDF files using a `file://` URL. Run:\n\n    $ gulp server\n\nand then you can open:\n\n+ http://localhost:8888/web/viewer.html\n\nPlease keep in mind that this requires a modern and fully up-to-date browser; refer to [Building PDF.js](https://github.com/mozilla/pdf.js/blob/master/README.md#building-pdfjs) for non-development usage of the PDF.js library.\n\nIt is also possible to view all test PDF files on the right side by opening:\n\n+ http://localhost:8888/test/pdfs/?frame\n\n## Building PDF.js\n\nIn order to bundle all `src/` files into two production scripts and build the generic\nviewer, run:\n\n    $ gulp generic\n\nIf you need to support older browsers, run:\n\n    $ gulp generic-legacy\n\nThis will generate `pdf.js` and `pdf.worker.js` in the `build/generic/build/` directory (respectively `build/generic-legacy/build/`).\nBoth scripts are needed but only `pdf.js` needs to be included since `pdf.worker.js` will\nbe loaded by `pdf.js`. The PDF.js files are large and should be minified for production.\n\n## Using PDF.js in a web application\n\nTo use PDF.js in a web application you can choose to use a pre-built version of the library\nor to build it from source. We supply pre-built versions for usage with NPM and Bower under\nthe `pdfjs-dist` name. For more information and examples please refer to the\n[wiki page](https://github.com/mozilla/pdf.js/wiki/Setup-pdf.js-in-a-website) on this subject.\n\n## Including via a CDN\n\nPDF.js is hosted on several free CDNs:\n - https://www.jsdelivr.com/package/npm/pdfjs-dist\n - https://cdnjs.com/libraries/pdf.js\n - https://unpkg.com/pdfjs-dist/\n\n## Learning\n\nYou can play with the PDF.js API directly from your browser using the live demos below:\n\n+ [Interactive examples](https://mozilla.github.io/pdf.js/examples/index.html#interactive-examples)\n\nMore examples can be found in the [examples folder](https://github.com/mozilla/pdf.js/tree/master/examples/). Some of them are using the pdfjs-dist package, which can be built and installed in this repo directory via `gulp dist-install` command.\n\nFor an introduction to the PDF.js code, check out the presentation by our\ncontributor Julian Viereck:\n\n+ https://www.youtube.com/watch?v=Iv15UY-4Fg8\n\nMore learning resources can be found at:\n\n+ https://github.com/mozilla/pdf.js/wiki/Additional-Learning-Resources\n\nThe API documentation can be found at:\n\n+ https://mozilla.github.io/pdf.js/api/\n\n## Questions\n\nCheck out our FAQs and get answers to common questions:\n\n+ https://github.com/mozilla/pdf.js/wiki/Frequently-Asked-Questions\n\nTalk to us on Matrix:\n\n+ https://chat.mozilla.org/#/room/#pdfjs:mozilla.org\n\nFile an issue:\n\n+ https://github.com/mozilla/pdf.js/issues/new\n\nFollow us on Twitter: @pdfjs\n\n+ https://twitter.com/pdfjs\n"
},
{
  "name": "glean.js",
  "files": {
    "/": [
      ".circleci",
      ".dictionary",
      ".github",
      ".gitignore",
      "ARCHITECTURE.md",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "automation",
      "bin",
      "docs",
      "glean",
      "samples"
    ],
    "/docs": [
      "README.md",
      "_assets",
      "guides",
      "implementation",
      "reference"
    ],
    "/.github": [
      "PULL_REQUEST_TEMPLATE.md",
      "auto_assign.yml",
      "dependabot.yml",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Glean.js\n\n![Glean logo](https://mozilla.github.io/glean/book/glean.jpeg)\n\n`Glean.js` is a modern approach for a Telemetry library aimed at JavaScript environments. It is part of the [Glean project](https://docs.telemetry.mozilla.org/concepts/glean/glean.html).\n\n## Documentation\n\nAll documentation is available online:\n\n### [The Glean Book](https://mozilla.github.io/glean/)\n\nIf you are using Glean.js in your project, this is the place to go. \nIt contains user guides on getting started and other topics, as well as reference to Glean APIs.\n\n### [Glean.js developer documentation](https://github.com/mozilla/glean.js/tree/main/docs)\n\nIf you want to contribute to the Glean.js code base this is the place to go.\n\n### [Glean.js internal reference](https://mozilla.github.io/glean.js/)\n\nIf you are looking for detailed documentation on the Glean APIs (internal or external) this is the place to go.\n\n_This documentation is auto generated from the doc comments throughout the Glean.js code base and\nthe Glean book should be preferred over this documentation whenever possible._\n\n## Contact\n\nTo contact us you can:\n\n* Find us in the [#glean channel on chat.mozilla.org](https://chat.mozilla.org/#/room/#glean:mozilla.org).\n* To report issues or request changes, file a bug in [Bugzilla in Data Platform & Tools :: Glean SDK](https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=nobody%40mozilla.org&bug_ignored=0&bug_severity=normal&bug_status=NEW&bug_type=defect&cf_fx_iteration=---&cf_fx_points=---&cf_status_firefox100=---&cf_status_firefox101=---&cf_status_firefox99=---&cf_status_firefox_esr91=---&cf_tracking_firefox100=---&cf_tracking_firefox101=---&cf_tracking_firefox99=---&cf_tracking_firefox_esr91=---&component=Glean%3A%20SDK&contenttypemethod=list&contenttypeselection=text%2Fplain&defined_groups=1&filed_via=standard_form&flag_type-4=X&flag_type-607=X&flag_type-721=X&flag_type-737=X&flag_type-799=X&flag_type-800=X&flag_type-803=X&flag_type-936=X&flag_type-947=X&form_name=enter_bug&maketemplate=Remember%20values%20as%20bookmarkable%20template&op_sys=Unspecified&priority=P3&product=Data%20Platform%20and%20Tools&rep_platform=Unspecified&status_whiteboard=%5Bglean-sdk%3Am%3F%5D%5Bglean-js%5D&target_milestone=---&version=unspecified).\n* Send an email to *glean-team@mozilla.com*.\n* The Glean SDKs team is: *:dexter*, *:janerik*, *:travis_*, *:chutten*.\n\n## Credits\n\nThe [Glean logo artwork](https://dianaciufo.wordpress.com/2019/10/11/glean-graphic-identity-for-mozilla-firefox/) was contributed by [Diana Ciufo](https://dianaciufo.wordpress.com/).\nIt's licensed under MPL.\n\n## License\n\nThis Source Code Form is subject to the terms of the Mozilla Public\nLicense, v. 2.0. If a copy of the MPL was not distributed with this\nfile, You can obtain one at http://mozilla.org/MPL/2.0/\n"
},
{
  "name": "addons-server",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".jshintrc",
      ".prettierignore",
      ".prettierrc",
      ".readthedocs.yml",
      "Dockerfile",
      "Dockerfile.deploy",
      "Dockerfile.perftests",
      "LICENSE",
      "Makefile",
      "Makefile-docker",
      "Makefile-os",
      "README.rst",
      "babel.cfg",
      "babeljs.cfg",
      "conftest.py",
      "contribute.json",
      "docker-compose.override.yml",
      "docker-compose.private.yml",
      "docker-compose.yml",
      "docker",
      "docs",
      "jest.config.js",
      "locale",
      "logs",
      "manage.py",
      "package-lock.json",
      "package.json",
      "private",
      "pyproject.toml",
      "renovate.json",
      "requirements",
      "scripts",
      "services",
      "settings.py",
      "settings_test.py",
      "setup.cfg",
      "setup.py",
      "src",
      "static",
      "storage",
      "tests",
      "version.json"
    ],
    "/docs": [
      "Makefile",
      "README.rst",
      "__init__.py",
      "_intersphinx",
      "_static",
      "_templates",
      "build-github.zsh",
      "conf.py",
      "extensions",
      "index.rst",
      "screenshots",
      "settings",
      "topics",
      "watcher.py"
    ],
    "/.github": [
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "ISSUE_TEMPLATE.md",
      "PULL_REQUEST_TEMPLATE.md",
      "SECURITY.md",
      "dependabot.yml",
      "stale.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "IN_DOCKER = $(wildcard /addons-server-docker-container)\n\nifneq ($(IN_DOCKER),)\n\tSUB_MAKEFILE = Makefile-docker\nelse\n\tSUB_MAKEFILE = Makefile-os\nendif\n\ninclude $(SUB_MAKEFILE)\n\n\nhelp:\n\t@echo \"Please use 'make <target>' where <target> is one of the following commands.\\n\"\n\t@$(MAKE) help_submake --no-print-directory\n\t@echo \"\\nCheck the Makefile to know exactly what each target is doing.\"\n\n# You probably want to put new commands in Makefile-docker rather than here -\n# unless they operate on multiple containers or are host-os specific, then\n# Makefile-os instead.\n",
  "readme": ".. image:: https://img.shields.io/badge/%E2%9D%A4-code%20of%20conduct-blue.svg\n    :target: https://github.com/mozilla/addons-server/blob/master/.github/CODE_OF_CONDUCT.md\n    :alt: Code of conduct\n\n.. image:: https://circleci.com/gh/mozilla/addons-server.svg?style=svg\n    :target: https://circleci.com/gh/mozilla/addons-server\n\n\nAddons-Server\n=============\n\nWelcome to the Addons Server repository!  Please feel free to visit the web page of the current project hosted on `addons.mozilla.org`_. If you want to install it follow our guide located in `install docs`_.  We'd love your help!  You can come talk to us on `Matrix #amo:mozilla.org`_ if you have any questions.\n\nPlease report bugs here: https://github.com/mozilla/addons/issues or https://github.com/mozilla/addons-server/issues\nYou can access the AMO dev environment at https://addons-dev.allizom.org/ and the AMO stage environment at https://addons.allizom.org/\n\n\n.. _`addons.mozilla.org`: https://addons.mozilla.org\n.. _`install docs`: https://addons-server.readthedocs.io/en/latest/topics/install/docker.html\n.. _`Matrix #amo:mozilla.org`: https://chat.mozilla.org/#/room/#amo:mozilla.org\n\n\n.. marker-for-security-bug-inclusion-do-not-remove\n\nSecurity Bug Reports\n--------------------\n\nThis code and its associated production web page are included in the Mozilla\u2019s web and services `bug bounty program`_. If you find a security vulnerability, please submit it via the process outlined in the program and `FAQ pages`_. Further technical details about this application are available from the `Bug Bounty Onramp page`_.\n\nPlease submit all security-related bugs through Bugzilla using the `web security bug form`_. Never submit security-related bugs through a Github Issue or by email.\n\n.. _bug bounty program: https://www.mozilla.org/en-US/security/web-bug-bounty/\n.. _FAQ pages: https://www.mozilla.org/en-US/security/bug-bounty/faq-webapp/\n.. _Bug Bounty Onramp page: https://wiki.mozilla.org/Security/BugBountyOnramp/\n.. _web security bug form: https://bugzilla.mozilla.org/form.web.bounty\n"
},
{
  "name": "treeherder",
  "files": {
    "/": [
      ".circleci",
      ".codecov.yml",
      ".eslintrc.js",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".markdownlint.json",
      ".markdownlintignore",
      ".pre-commit-config.yaml",
      ".prettierignore",
      ".prettierrc.js",
      ".readthedocs.yml",
      ".renovaterc",
      ".yarnrc",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "README.md",
      "babel.config.json",
      "bin",
      "deployment",
      "docker-compose.yml",
      "docker",
      "docs",
      "initialize_data.sh",
      "jest-puppeteer.config.js",
      "jest.config.js",
      "manage.py",
      "misc",
      "mkdocs.yml",
      "newrelic.ini",
      "package.json",
      "poetry.lock",
      "pylintrc",
      "pyproject.toml",
      "requirements",
      "schemas",
      "setup.cfg",
      "tests",
      "tox.ini",
      "treeherder",
      "ui",
      "version.json",
      "webpack.config.js",
      "yarn.lock"
    ],
    "/docs": [
      "__init__.py",
      "accessibility.md",
      "accessing_data.md",
      "backend_tasks.md",
      "code_style.md",
      "common_tasks.md",
      "data_cycling.md",
      "index.md",
      "infrastructure",
      "installation.md",
      "pulseload.md",
      "submitting_data.md",
      "testcases.md",
      "testing.md"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Treeherder\n\n[![What's Deployed](https://img.shields.io/badge/whatsdeployed-prototype,stage,prod-green.svg)](https://whatsdeployed.io/s/BIY/Mozilla/Treeherder)\n[![Build Status](https://travis-ci.org/mozilla/treeherder.png?branch=master)](https://app.circleci.com/pipelines/github/mozilla/treeherder)\n[![Node dependencies Status](https://david-dm.org/mozilla/treeherder/status.svg)](https://david-dm.org/mozilla/treeherder)\n[![Node devDependencies Status](https://david-dm.org/mozilla/treeherder/dev-status.svg)](https://david-dm.org/mozilla/treeherder?type=dev)\n[![Documentation Status](https://readthedocs.org/projects/treeherder/badge/?version=latest)](https://treeherder.readthedocs.io/?badge=latest)\n[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n\n## Description\n\n[Treeherder](https://treeherder.mozilla.org) is a reporting dashboard for Mozilla checkins. It allows users to see the results of automatic builds and their respective tests. The Treeherder service manages the etl layer for data ingestion, web services, and the data model behind Treeherder.\n\n## Instances\n\nTreeherder exists on two instances: [staging](https://treeherder.allizom.org) for pre-deployment validation, and [production](https://treeherder.mozilla.org) for actual use.\n\n## Installation\n\nThe steps to run Treeherder are provided [here](https://treeherder.readthedocs.io/installation.html).\n\nThe steps to run only the UI are provided [here](https://treeherder.readthedocs.io/installation.html#ui-development).\n\n## Links\n\nVisit our project tracking Wiki [here](https://wiki.mozilla.org/EngineeringProductivity/Projects/Treeherder).\n\nFor other setup and configuration, visit our\u00a0readthedocs\u00a0page [here](https://treeherder.readthedocs.io).\n\nFile any bugs you may encounter [here](https://bugzilla.mozilla.org/enter_bug.cgi?product=Tree+Management&component=Treeherder).\n\n## Contributing\n\nEveryone is welcome to contribute!\n\nIf a bug is not assigned to someone, you can request the bug be assigned to you. You should ask the component owner with your request (\"Request information\" in Bugzilla and mention in Github).\n\nIf you do not receive a response within 2-3 days, you can follow up in the **#treeherder** matrix channel.\n\nAfter addressing the issue, make sure [every test passes](https://treeherder.readthedocs.io/testing.html) before sending a pull request.\n\nWe also recommend setting an `upstream` remote that points to the [Mozilla's Github repo](https://github.com/mozilla/treeherder.git), in addition to `origin` that points to your fork. You should then frequently use `git rebase upstream` rather than merging from your fork to keep your branch current. There are less conflicts this way and the git history is cleaner.\n\n## Sending a Pull Request\n\nWe receive contributions from both Bugzilla and Github. We have some specifications to keep track of them:\n\n1. If your bug comes from **[Bugzilla](https://bugzilla.mozilla.org/query.cgi?query_format=advanced&product=Tree+Management&f1=component&o1=substring&v1=Treeherder&resolution=---)**\n\n    After addressing the issue, please send a pull request to this repository, with the **Bugzilla's number ID** in the **title**, so that our bot attaches your patch to the corresponding Bugzilla bug.\n\n    `\"Bug xxxxxx - [title of the bug or brief explanation]\"`\n\n    For example: \"Bug 123456 - Fix scrolling behavior in Perfherder\"\n\n2. If your bug comes from **Github**\n\n    In the **description** of the pull request, please mention the **issue number**. That can be done by typing #[issue's number].\n\n    For example: \"This pull request fixes #5135\".\n\n    Github automatically links both issue and pull request to one another.\n"
},
{
  "name": "uniffi-rs",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "docker",
      "docs",
      "examples",
      "fixtures",
      "release.toml",
      "rust-toolchain.toml",
      "uniffi",
      "uniffi_bindgen",
      "uniffi_build",
      "uniffi_macros",
      "uniffi_meta",
      "uniffi_testing",
      "weedle2"
    ],
    "/docs": [
      "adr",
      "contributing.md",
      "diplomat-and-macros.md",
      "manual",
      "policies",
      "release-process.md"
    ],
    "/.github": [
      "CODEOWNERS"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# UniFFI - a multi-language bindings generator for Rust\n\nUniFFI is a toolkit for building cross-platform software components in Rust.\n\nBy writing your core business logic in Rust and describing its interface in a special\n[interface definition file](https://mozilla.github.io/uniffi-rs/udl_file_spec.html),\nyou can use UniFFI to help you:\n\n* Compile your Rust code into a shared library for use on different target platforms.\n* Generate bindings to load and use the library from different target languages.\n\nFor example, UniFFI is currently used in the [mozilla/application-services](https://github.com/mozilla/application-services)\nproject to build browser storage and syncing functionality for Firefox mobile browsers. Core functionality is\nwritten once in Rust, and auto-generated bindings allow that functionality to be called from both Kotlin (for Android apps)\nand Swift (for iOS apps).\n\nCurrently supported \"foreign\" languages include Kotlin, Swift, Python and Ruby.\nIn general, we'd welcome contributions of other foreign bindings, but we recommend you contact the team first -\nsee [the contributing section below](#contributing).\n\n## User Guide\n\nYou can read more about using the tool in [**the UniFFI user guide**](https://mozilla.github.io/uniffi-rs/).\n\nPlease be aware that UniFFI is being developed concurrently with its initial consumers, so it is changing rapidly and there\nare a number of sharp edges to the user experience. Still, we consider is developed enough for production use in Mozilla\nproducts and we welcome any feedback you may have about making it more broadly useful.\n\n### Etymology and Pronunciation\n\n\u02c8ju\u02d0n\u026afa\u026a. Pronounced to rhyme with \"unify\".\n\nA portmanteau word that also puns with \"unify\", to signify the joining of one codebase accessed from many languages.\n\nuni - [Latin \u016bni-, from \u016bnus, one]\nFFI - [Abbreviation, Foreign Function Interface]\n\n## Alternative tools\n\nOther tools we know of which try and solve a similarly shaped problem are:\n\n* [Diplomat](https://github.com/rust-diplomat/diplomat/) - see our [writeup of\n  the different approach taken by that tool](docs/diplomat-and-macros.md)\n\n(Please open a PR if you think other tools should be listed!)\n\n## Contributing\n\nIf this tool sounds interesting to you, please help us develop it! You can:\n\n* View the [contributor guidelines](./docs/contributing.md).\n* File or work on [issues](https://github.com/mozilla/uniffi-rs/issues) here in GitHub.\n* Join discussions in the [#uniffi:mozilla.org](https://matrix.to/#/#uniffi:mozilla.org)\n  room on Matrix.\n\n## Code of Conduct\n\nThis project is governed by Mozilla's [Community Participation Guidelines](./CODE_OF_CONDUCT.md).\n"
},
{
  "name": "microannotate",
  "files": {
    "/": [
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "VERSION",
      "bin",
      "infra",
      "microannotate",
      "requirements.txt",
      "setup.py",
      "test-requirements.txt",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# microannotate\n\nA set of tools to generate word-level annotate data and view it.\n"
},
{
  "name": "glam",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".env-dist",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.js",
      ".storybook",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "babel.config.js",
      "bin",
      "docker-compose.yml",
      "dockerfiles",
      "docs",
      "glam",
      "jest.config.js",
      "manage.py",
      "package-lock.json",
      "package.json",
      "public",
      "pytest.ini",
      "requirements.in",
      "requirements.txt",
      "rollup.config.js",
      "scripts",
      "setup.cfg",
      "src",
      "stories",
      "tests"
    ],
    "/docs": [
      "adr",
      "data-imports.md",
      "deployment.md",
      "development.md",
      "getting-production-data.md",
      "profiling.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build up shell test\n\nbuild:\n\tdocker-compose build\n\nup:\n\tdocker-compose up\n\nshell:\n\tdocker-compose run --rm server /bin/bash\n\nformat:\n\tpython3 -m black glam ./*.py\n\tpython3 -m flake8 glam ./*.py\n\nlint:\n\tpython3 -m flake8 --max-line-length 100 .\n\tpython3 -m black --check glam ./*.py\n\ntest: lint\n\tdocker-compose run --rm server pytest -s --dc=Test glam/\n",
  "readme": "# Glean Aggregation Metrics (GLAM)\n\nGLAM is Mozilla's interactive dashboard for examining the distribution of\ntelemetry values over time and across different user populations. You can check\nout GLAM at [glam.telemetry.mozilla.org](https://glam.telemetry.mozilla.org).\n\nThis repository contains:\n\n- The GLAM server which provides the API\n- The GLAM front-end code\n- The design system for building new front-end components\n\nFor more information about GLAM, visit\n[Introduction to GLAM](https://docs.telemetry.mozilla.org/cookbooks/glam.html).\nIf you're looking to contribute, see\n[the development docs](https://github.com/mozilla/glam/blob/main/docs/development.md)\non how to get started.\n"
},
{
  "name": "bigquery-etl",
  "files": {
    "/": [
      ".bigqueryrc",
      ".circleci",
      ".dockerignore",
      ".eslintrc.yml",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".vscode",
      ".yamllint.yaml",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "GRAVEYARD.md",
      "LICENSE",
      "README.md",
      "bigquery_etl",
      "bqetl",
      "conftest.py",
      "dags.yaml",
      "dags",
      "docs",
      "netlify.toml",
      "pom.xml",
      "pytest.ini",
      "requirements.in",
      "requirements.txt",
      "script",
      "setup.py",
      "sql",
      "sql_generators",
      "src",
      "tests"
    ],
    "/docs": [
      "bqetl.md",
      "cookbooks",
      "favicon.png",
      "illustration.png",
      "index.md",
      "mkdocs.yml",
      "mozdata",
      "reference"
    ],
    "/.github": [
      "dependabot.yml",
      "pull_request_template.md",
      "workflows"
    ],
    "/.circleci": [
      "config.yml",
      "post-diff.js"
    ]
  },
  "makefile": null,
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/bigquery-etl.svg?style=shield&circle-token=742fb1108f7e6e5a28c11d43b21f62605037f5a4)](https://circleci.com/gh/mozilla/bigquery-etl)\n\n# BigQuery ETL\n\nThis repository contains Mozilla Data Team's:\n\n- Derived ETL jobs that do not require a custom container\n- User-defined functions (UDFs)\n- Airflow DAGs for scheduled bigquery-etl queries\n- Tools for query & UDF deployment, management and scheduling\n\nFor more information, see [https://mozilla.github.io/bigquery-etl/](https://mozilla.github.io/bigquery-etl/)\n\n## Quick Start\n\n> Apple Silicon (M1) user requirement\n>\n> Enable [Rosetta mode](https://support.apple.com/en-ca/HT211861) for your terminal _**BEFORE**_ installing below tools using your terminal. It'll save you a lot of headaches. For tips on maintaining parallel stacks of python and homebrew running with and without Rosetta, see blog posts from [Thinknum](https://medium.com/thinknum/how-to-install-python-under-rosetta-2-f98c0865e012) and [Sixty North](http://sixty-north.com/blog/pyenv-apple-silicon.html).\n\n### Pre-requisites\n- **Pyenv** (optional) Recommended if you want to install different versions of python, see instructions [here](https://github.com/pyenv/pyenv#basic-github-checkout). After the installation of pyenv, make sure that your terminal app is [configured to run the shell as a login shell](https://github.com/pyenv/pyenv/wiki/MacOS-login-shell).\n- **Homebrew** (not required, but useful for Mac) - Follow the instructions [here](https://brew.sh/) to install homebrew on your Mac.\n- **Python 3.8+** - (see [this guide](https://docs.python-guide.org/starting/install3/osx/) for instructions if you're on a mac and haven't installed anything other than the default system Python).\n- **Java JDK 8+** - (required for some functionality, e.g. [AdoptOpenJDK](https://adoptium.net/)) with `$JAVA_HOME` set.\n- **Maven** - (needed for downloading jar dependencies). Available via your package manager in most Linux distributions and from [homebrew](https://brew.sh/) on mac, or you can install yourself by [downloading a binary](https://maven.apache.org/download.cgi) and following maven's [install instructions](https://maven.apache.org/install.html).\n\n### GCP CLI tools\n\n- **For Mozilla Employees or Contributors (not in Data Engineering)** - Set up GCP command line tools, [as described on docs.telemetry.mozilla.org](https://docs.telemetry.mozilla.org/cookbooks/bigquery/access.html#using-the-bq-command-line-tool). Note that some functionality (e.g. writing UDFs or backfilling queries) may not be allowed.\n- **For Data Engineering** - In addition to setting up the command line tools, you will want to log in to `shared-prod` if making changes to production systems. Run `gcloud auth login --update-adc --project=moz-fx-data-shared-prod` (if you have not run it previously).\n\n### Installing bqetl\n\n1. Clone the repository\n```bash\ngit clone git@github.com:mozilla/bigquery-etl.git\ncd bigquery-etl\n```\n\n2. Install the `bqetl` command line tool\n```bash\n./bqetl bootstrap\n```\n\n3. Install standard pre-commit hooks\n```bash\nvenv/bin/pre-commit install\n```\n\n4. Build and install java dependencies\n```bash\nmvn package\n# specify `<(echo mozilla-bigquery-etl)` to retain bqetl from `./bqetl bootstrap`\nvenv/bin/pip-sync --pip-args=--no-deps requirements.txt <(echo mozilla-bigquery-etl)\n```\n\nFinally, if you are using Visual Studio Code, you may also wish to use our recommended defaults:\n```bash\ncp .vscode/settings.json.default .vscode/settings.json\n```\n\nAnd you should now be set up to start working in the repo! The easiest way to do this is for many tasks is to use [`bqetl`](https://mozilla.github.io/bigquery-etl/bqetl/). You may also want to read up on [common workflows](https://mozilla.github.io/bigquery-etl/cookbooks/common_workflows/).\n"
},
{
  "name": "application-services",
  "files": {
    "/": [
      ".buildconfig-android.yml",
      ".cargo",
      ".circleci",
      ".cron.yml",
      ".detekt.yml",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".mergify.yml",
      ".swiftlint.yml",
      ".taskcluster.yml",
      ".vscode",
      "CHANGELOG.md",
      "CHANGES_UNRELEASED.md",
      "CODE_OF_CONDUCT.md",
      "COPYRIGHT",
      "Cargo.lock",
      "Cargo.toml",
      "DEPENDENCIES.md",
      "LICENSE",
      "README.md",
      "automation",
      "build-scripts",
      "build.gradle",
      "clippy.toml",
      "codecov.yml",
      "components",
      "docs",
      "examples",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "libs",
      "megazords",
      "proguard-rules-consumer-jna.pro",
      "publish.gradle",
      "rust-toolchain.toml",
      "settings.gradle",
      "taskcluster",
      "testing",
      "tools",
      "xcconfig"
    ],
    "/docs": [
      "README.md",
      "SUMMARY.md",
      "adding-docs.md",
      "adr",
      "android-faqs.md",
      "book.toml",
      "build-and-publish-pipeline.md",
      "building.md",
      "contributing.md",
      "dependency-management.md",
      "design",
      "diagrams",
      "howtos",
      "logging.md",
      "naming-conventions.md",
      "shared"
    ],
    "/.github": [
      "dependabot.yml",
      "pull_request_template.md",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "<a href=\"https://codecov.io/gh/mozilla/application-services\"> <img src=\"https://codecov.io/gh/mozilla/application-services/branch/main/graph/badge.svg?token=HxeUysUWqx\"/> </a>\n\n# Firefox Application Services\n\nApplication Services (a-s) is a collection of Rust Components that are used to enable Firefox applications to integrate with Firefox accounts, sync, experimentation, etc. Each component is built using a core of shared code written in Rust, wrapped with native language bindings for different platforms.\n\n### Contributing\nTo contribute, please review the Mozilla [Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/) and then visit our [how to contribute](docs/contributing.md) guide.\n\n### Contact\nGet in touch with other community members on Matrix, or through issues here on GitHub.\n- Matrix: [#rust-components:mozilla.org](https://chat.mozilla.org/#/room/#rust-components:mozilla.org) ([How to connect](https://wiki.mozilla.org/Matrix#Connect_to_Matrix))\n\n# Documentation\n\n### High-level docs\n\nThe [Application Services Book](https://mozilla.github.io/application-services/book/index.html) contains high-level documentation about the code in this repository.  It's built from the [./docs/](docs) directory.\n\n### Package docs\n\nWe use rustdoc to document both the public API of the components and the various internal implementation details.  View them on [https://mozilla.github.io/application-services/book/rust-docs/fxa_client/index.html](https://mozilla.github.io/application-services/book/rust-docs/fxa_client/index.html).  Once you have completed the build steps, you can view the docs by running:\n\n```shell\ncargo doc --no-deps --document-private-items --open\n```\n\n# Building\n\n### Building the Rust Components\n1. Clone or Download the repository:\n```shell\n  $ git clone https://github.com/mozilla/application-services # (or use the ssh link)\n  $ cd application-services\n  $ git submodule init\n  $ git submodule update --recursive\n  ```\n2. Follow these instructions to install your [system-level dependencies](docs/building.md#building-application-services)\n3. Run the a-s Rust unit tests\n```shell\ncargo test\n```\n\n### Consumer build, integration and testing\nThe application-services library primary consumers are Fenix (Firefox on Android) and Firefox iOS. Assure you are able to run integration tests (for Android and iOS if using MacOS) by following the instructions to build for Android and iOS integrations.  \n\n#### Android integration builds and helpful tools\n* Build instructions to test [Fenix / android-components integration](docs/building.md#building-for-fenix)\n* [Fenix Auto-publication workflow for android-components and application-services](https://github.com/mozilla-mobile/fenix/#auto-publication-workflow-for-android-components-and-application-services)\n\n\n#### Firefox for iOS integration\n* Build instructions to test [Firefox iOS integration](docs/building.md#building-for-firefox-ios)\n\n#### Firefox Desktop\n* Build instructions to test [Firefox Desktop integration](docs/building.md#building-for-firefox-desktop)\n\n# Rust Components\n\n[./components/](components) contains the source for each component, and its\n  FFI bindings.\n\n> Please note that we are in the process of moving away from hand-written ffi code and instead favouring the use of the [uniffi](https://github.com/mozilla/uniffi-rs/) library.\n* See [./components/push/](components/places) for an example, where you can\n    find:\n  * The shared [rust code](components/places/src).\n  * The mapping into a [C FFI](components/places/ffi).\n  * The [Kotlin bindings](components/places/android) for use by Android\n      applications.\n  * The [Swift bindings](components/places/ios) for use by iOS applications.\n* See [./components/fxa-client](components/fxa-client) for an example that uses\n    [uniffi](https://github.com/mozilla/uniffi-rs/) to generate API wrappers for\n    multiple languages, such as Kotlin and Swift.\n\n### List of components\n* [autofill](components/autofill) - for storage and syncing of credit card and\n  address information\n* [crashtest](components/crashtest) - testing-purposes (crashing the Rust code)\n* [fxa-client](components/fxa-client) - for applications that need to sign in\n  with FxA, access encryption keys for sync, and more.\n* [logins](components/logins) - for storage and syncing of a user's saved login\n  credentials\n* [nimbus](components/nimbus) - for integrating with Mozilla's [experimentation](https://mozilla.github.io/experimenter-docs/) platform for Firefox\n* [places](components/places) - for storage and syncing of a user's saved\n  browsing history\n* [push](components/push) - for applications to receive real-time updates via\n  WebPush\n* [rc_log](components/rc_log) - for connecting component log output to the\n  application's log stream\n* [support](components/support) - low-level utility libraries\n  * [support/rc_crypto](components/rc_crypto) - handles cryptographic needs backed by Mozilla's\n    [NSS](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS) library\n  * [support/sql](components/support/sql) - utilities for storing data locally\n    with SQL\n* [sync15](components/sync15) - shared library for accessing data in Firefox\n  Sync\n* [sync_manager](components/sync_manager) - integrates multiple sync engines/\n  stores into a single framework\n* [tabs](components/tabs) - an in-memory syncing engine for remote browser tabs\n* [viaduct](components/viaduct) - an HTTP request library\n* [webext-storage](components/webext-storage) - powers an implementation of the\nchrome.storage.sync WebExtension API\n"
},
{
  "name": "cubeb",
  "files": {
    "/": [
      ".clang-format",
      ".github",
      ".gitignore",
      ".gitmodules",
      "AUTHORS",
      "CMakeLists.txt",
      "Config.cmake.in",
      "INSTALL.md",
      "LICENSE",
      "README.md",
      "cmake",
      "cubeb.supp",
      "docs",
      "googletest",
      "include",
      "scan-build-install.sh",
      "src",
      "subprojects",
      "test",
      "tools"
    ],
    "/docs": [
      "Doxyfile.in"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://github.com/mozilla/cubeb/actions/workflows/build.yml/badge.svg)](https://github.com/mozilla/cubeb/actions/workflows/build.yml)\n\nSee INSTALL.md for build instructions.\n\nSee [Backend Support](https://github.com/mozilla/cubeb/wiki/Backend-Support) in the wiki for the support level of each backend.\n\nLicensed under an ISC-style license.  See LICENSE for details.\n"
},
{
  "name": "lookml-generator",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".yamllint.yaml",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "architecture",
      "bin",
      "custom-namespaces.yaml",
      "docker-compose.yml",
      "generator",
      "namespaces-disallowlist.yaml",
      "netlify.toml",
      "pytest.ini",
      "requirements.in",
      "requirements.txt",
      "setup.py",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: help build run shell\n\nhelp:\n\t@echo \" build Builds the docker images for the docker-compose setup.\"\n\t@echo \" run   Runs a command.\"\n\t@echo \" shell Opens a bash shell\n\nbuild:\n\tdocker-compose build\n\nrun:\n\tdocker-compose run app $(COMMAND)\n\nshell:\n\tdocker-compose run --entrypoint /bin/bash app\n",
  "readme": "# lookml-generator\n[![mozilla](https://circleci.com/gh/mozilla/lookml-generator.svg?style=svg)](https://circleci.com/gh/mozilla/lookml-generator/?branch=main)\n\n*Under Active Development*\n\nLookML Generator for Glean and Mozilla Data.\n\nThe lookml-generator has two important roles:\n1. Generate a listing of all Glean/Mozilla namespaces and their associated BigQuery tables\n2. From that listing, generate LookML for views, explores, and dashboards and push those to the [Look Hub project](https://github.com/mozilla/looker-hub)\n\n## Generating Namespace Listings\n\nAt Mozilla, a namespace is a single functional area that is represented in Looker with (usually) one model*.\nEach Glean application is self-contained within a single namespace, containing the data from [across that application's channels](https://probeinfo.telemetry.mozilla.org/v2/glean/app-listings).\nWe also support custom namespaces, which can use wildcards to denote their BigQuery datasets and tables. These are described in `custom-namespaces.yaml`.\n\n![alt text](https://github.com/mozilla/lookml-generator/blob/main/architecture/namespaces.jpg?raw=true)\n\n> \\*  Though namespaces are not limited to a single model, we advise it for clarity's sake.\n\n## Adding Custom Namespaces\nCustom namespaces need to be defined explicitly in `custom-namespaces.yaml`. For each namespace views and explores to be generated need to be specified.\n\nMake sure the custom namespaces is _not_ listed in `namespaces-disallowlist.yaml`.\n\nOnce changes have been approved and merged, the [lookml-generator changes can get deployed](#deploying-new-lookml-generator-changes).\n\n## Generating LookML\nOnce we know which tables are associated with which namespaces, we can generate LookML files and update our Looker instance.\n\nLookml-generator generates LookML based on both the BigQuery schema and manual changes. For example, we would want to add `city` drill-downs for all `country` fields.\n![alt text](https://github.com/mozilla/lookml-generator/blob/main/architecture/lookml.jpg?raw=true)\n\n\n### Pushing Changes to Dev Branches\nIn addition to pushing new lookml to the [main branch](https://github.com/mozilla/looker-hub), we reset the dev branches to also\npoint to the commit at `main`. This only happens during production deployment runs.\n\nTo automate this process for your dev branch, add it to [this file](https://github.com/mozilla/lookml-generator/tree/main/bin/dev_branches).\nYou can edit that file in your browser. Open a PR and tag [data-looker](https://github.com/orgs/mozilla/teams/data-looker) for review.\nYou can find your dev branch by going to [Looker](https://mozilla.cloud.looker.com), entering development mode, opening the [`looker-hub`](https://mozilla.cloud.looker.com/projects/looker-hub)\nproject, clicking the \"Git Actions\" icon, and finding your personal branch in the \"Current Branch\" dropdown.\n\n## Setup\n\nEnsure Python 3.8+ is available on your machine (see [this guide](https://docs.python-guide.org/starting/install3/osx/) for instructions if you're on a mac and haven't installed anything other than the default system Python.)\n\nYou will also need the Google Cloud SDK with valid credentials.\nAfter setting up the Google Cloud SDK, run:\n\n```bash\ngcloud config set project moz-fx-data-shared-prod\ngcloud auth login --update-adc\n```\n\nInstall requirements in a Python venv\n```bash\npython3.8 -m venv venv/\nvenv/bin/pip install --no-deps -r requirements.txt\n```\n\nUpdate requirements when they change with `pip-sync`\n```bash\nvenv/bin/pip-sync\n```\n\nSetup pre-commit hooks\n```bash\nvenv/bin/pre-commit install\n```\n\nRun unit tests and linters\n```bash\nvenv/bin/pytest\n```\n\nRun integration tests\n```bash\nvenv/bin/pytest -m integration\n```\n\nNote that the integration tests require a valid login to BigQuery to succeed.\n\n## Testing generation locally\n\nYou can test namespace generation by running:\n\n```bash\n./bin/generator namespaces\n```\n\nTo generate the actual lookml (in `looker-hub`), run:\n\n```bash\n./bin/generator lookml\n```\n\n## Container Development\n\nMost code changes will not require changes to the generation script or container.\nHowever, you can test it locally. The following script will test generation, pushing\na new branch to the `looker-hub` repository:\n\n```\nexport HUB_BRANCH_PUBLISH=\"yourname-generation-test-1\"\nexport GIT_SSH_KEY_BASE64=$(cat ~/.ssh/id_rsa | base64)\nmake build && make run\n```\n\n## Deploying new `lookml-generator` changes\n\n`lookml-generator` runs daily to update the `looker-hub` and `looker-spoke-default` code. Changes\nto the underlying tables should automatically propogate to their respective views and explores.\n\nHowever, changes to `lookml-generator` need to be tested on stage and deployed. The general process\nis the following:\n1. Create a PR, test on dev. It is not necessary to add Looker credentials, but the container changes\n   should run using `make build && make run`, with changes reflected in LookML repos.\n2. Once merged, the changes should run on stage. They will run automatically after schema deploys,\n   but they can be run manually by clearing the `lookml_generator_staging` task in [Airflow](https://workflow.telemetry.mozilla.org/tree?dag_id=probe_scraper).\n3. Once the changes are confirmed in stage, we first tag a new release here. Add a description with\n   what the new release includes. Finally, change the Airflow variable `lookml_generator_release_str`\n   to the version string you created when cutting the release. Re-run the DAG and the changes\n   should take effect.\n"
},
{
  "name": "foundation.mozilla.org",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.a11y.json",
      ".eslintrc.json",
      ".github",
      ".gitignore",
      ".mergify.yml",
      ".prettierignore",
      ".prettierrc",
      ".profile",
      ".stylelintrc",
      ".stylelintrc-colors.js",
      ".vscode",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "ISSUE.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "app.json",
      "bin",
      "contribute.json",
      "copy-db.js",
      "cypress.json",
      "dev-requirements.in",
      "dev-requirements.txt",
      "docker-compose.yml",
      "dockerfiles",
      "docs",
      "env.default",
      "esbuild.config.js",
      "esbuild.react.shim.js",
      "invoke.yaml",
      "maintenance",
      "network-api",
      "package-lock.json",
      "package.json",
      "playwright.config.js",
      "postcss.config.js",
      "pyrightconfig.json",
      "release-steps.sh",
      "requirements.in",
      "requirements.txt",
      "runtime.txt",
      "source",
      "tailwind-plugins",
      "tailwind.config.js",
      "tasks.py",
      "test",
      "tests",
      "tox.ini",
      "translation-management.sh"
    ],
    "/docs": [
      "accessibility.md",
      "local_development.md",
      "ops_heroku_settings.md",
      "scheduled.md",
      "stack.md",
      "workflow.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "dependabot.yml",
      "pull_request_template.md",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# foundation.mozilla.org\n\n[![Dependency Status](https://david-dm.org/mozilla/network.svg)](https://david-dm.org/mozilla/network)\n[![Dev Dependency Status](https://david-dm.org/mozilla/network/dev-status.svg)](https://david-dm.org/mozilla/network/?type=dev)\n[![Uses Mofo Standards](https://MozillaFoundation.github.io/mofo-standards/badge.svg)](https://github.com/MozillaFoundation/mofo-standards)\n[![Code Coverage](https://coveralls.io/repos/github/mozilla/foundation.mozilla.org/badge.svg?branch=main)](https://coveralls.io/github/mozilla/foundation.mozilla.org)\n\n## Table of contents\n\n[Setup](#setup)\n\n[Setup with Docker](#how-to-setup-your-dev-environment-with-docker)\n\n[Local development](docs/local_development.md)\n\n[Engineer Workflow](docs/workflow.md)\n\n[OPS and Heroku Settings](docs/ops_heroku_settings.md)\n\n[Scheduled Task](docs/scheduled.md)\n\n[Stack](docs/stack.md)\n\n## How to Setup your Dev Environment with Docker\n\n**Requirements**: Docker ([Docker Desktop](https://www.docker.com/products/docker-desktop) for macOS and Windows or [Docker Compose](https://docs.docker.com/compose/install/) for Linux), [Python 3](https://www.python.org/downloads/) with the [invoke](https://www.pyinvoke.org/installing.html) package installed globally, and [git](https://git-scm.com/).\n\n### Installing Invoke\n\nWe recommend that you install Invoke using [pipx](https://pypi.org/project/pipx/), but any Python package manager should work (pip, poetry, etc).\n\n### Check your environment\n\n- `docker run hello-world`.\n- `invoke --version` should return 0.22.1 or higher.\n\n### Setup steps\n\nRun the following terminal commands to get started:\n\n- `git clone https://github.com/mozilla/foundation.mozilla.org.git`\n- `cd foundation.mozilla.org`\n- `inv new-env`\n\nYou're done :tada:\n\nThis task creates a `.env` that is in charge of managing your environment variables while running Docker. The installation will take a few minutes: you need to download images from the Docker Hub, install JS and Python dependencies, create fake data, migrate your database, etc.\n\nWhen it's done, run `docker-compose up`, wait until the static files to be built, and go to `0.0.0.0:8000`. You should have a local working version of the foundation site with fake data. When you want to stop, do `^C` to shut down your containers.\n\nTo log into the admin site, a superuser will have been created with username `admin` with password `admin`.\n\nTo catch up on new dependencies, migrations, etc. after initial setup, you can use the `inv catch-up` command. To get a full new environment with a new database, run `inv new-env` again.\n\nUse `inv -l` to get a list of all the available invoke commands.\n\nMore information on how to work with Docker and how to manage Python dependencies are available in the [local development](docs/local_development.md) part of the documentation.\n\n## Testing\n\n### Code tests\n\nWhen relevant, we encourage you to write tests. You can run the tests using `inv test`, or you can run the Node and Python testing suites separately:\n\n- Run Node tests: `inv test-node`\n- Run Python tests: `inv test-python`\n\n#### Fixing linting errors\n\nIf `inv test-node` shows linting errors for either JS/JSX or CSS/SCSS, you can run the `inv npm \"run fix\"` command to make the linting utilities automatically fix (or at least try to fix) any errors they knows how to fix. This will almost always be the only step required to ensure the linting phase of testing passes.\n\n### Integration tests\n\nIntegration testing is done using [Playwright](https://playwright.dev/), with the integration tests found in `./tests/integration`.\n\nYou can run these tests locally by running a one-time `npm install` and `npm run playwright:install` after which you should be able to run `npm run playwright` to run the visual tests, with `docker-compose up` running in a secondary terminal.\n\nIn order to run the same tests as will run during CI testing, make sure that `RANDOM_SEED=530910203` is set in your `.env` file, and that your local database is a new db based on that seed (`inv new-db`).\n\nNote that this is still a work in progress.\n\n### Visual regression tests\n\nWe also use Playwright in combination with Browserstack's [Percy](https://percy.io/) to perform visual regression testing for PRs, using `./tests/visual.spec.js` as screenshot baseline.\n\n### Accessibility tests\n\nAccessibility tests are currently unavailable but will use [axe-playwright](https://www.npmjs.com/package/axe-playwright) when the switchover from Cypress to Playwright is complete.\n\n## Mozilla Festival\n\nThe fake data generator can generate a site structure for the Mozilla Festival that can be served under it's own domain, or in the case of review apps on Heroku, where we're limited to a single domain, as a sub-directory of the main foundation site, at `{review_app_host}/mozilla-festival`.\n\nIn order to access the Mozilla Festival site locally on a different domain than the main Foundation site, you'll need to edit your hosts file (`/etc/hosts` on *nix systems, `C:\\Windows\\System32\\Drivers\\etc\\hosts` on Windows) to allow you to access the site at `mozfest.localhost:8000`. To enable this, add the following line to your hosts file: `127.0.0.1 mozfest.localhost`\n\nTicket purchases are implemented using a third-party integration with [Tito](https://ti.to/).\nThere is a `TitoWidget` Streamfield block that's used to place a button on a page to open the Tito widget.\nA webhook (Django view) receives requests from Tito when a ticket is completed in order to sign users up for the Mozilla newsletter. The event-specific environment variables `TITO_SECURITY_TOKEN` and `TITO_NEWSLETTER_QUESTION_ID` are required for this to work, and can be found in the Customize > Webhooks section of the Tito admin dashboard for the event. As these are currently global in the Mozilla site only one Tito event can be supported at a time.\n\n\n## Gotchas\n\nAs this is REST API and CMS built on top of Django, there are some \"gotcha!\"s to keep in mind due to the high level of magic in the Django code base (where things will happen automatically without the code explicitly telling you).\n\n#### **DEBUG=True**\n\nThe `DEBUG` flag does all sorts of magical things, to the point where testing with debugging turned on effectively runs a completely different setup compared to testing with debugging turned off. When debugging is on, the following things happen:\n\n- Django bypasses the `ALLOWED_HOST` restrictions, which again can lead to `400 Bad Request` errors in `DEBUG=False` setting.\n- Rather than HTTP error pages, Django will generate stack traces pages that expose pretty much all environment variables except any that match certain substrings such as `KEY`, `PASS`, etc. for obvious security reasons.\n- ...there are probably more gotchas just for `DEBUG` so if you find any please add them to this list.\n\n## Translations\n\nTranslations of UI strings (from the Django and React apps) are stored in [the fomo-l10n repository](https://github.com/mozilla-l10n/fomo-l10n). Translations are happening in Pontoon, in multiple projects: [Foundation website](https://pontoon.mozilla.org/projects/mozilla-foundation/), [\\*Privacy Not Included](https://pontoon.mozilla.org/projects/privacy-not-included/) and [Mozilla Festival](https://pontoon.mozilla.org/projects/mozilla-festival/).\n\nThe latest source strings are regularly exposed to Pontoon by a Localization PM using the following process:\n\n### Initial setup:\n- Clone the [`fomo-l10n`](https://github.com/mozilla-l10n/fomo-l10n) repository locally.\n- Set the `LOCAL_PATH_TO_L10N_REPO` variable in your `.env` file. Use the absolute path to your copy of the `fomo-l10n` repository and include the trailing slash. E.g. `LOCAL_PATH_TO_L10N_REPO=/Users/username/Documents/GitHub/fomo-l10n/`\n\n### Exposing latest source strings:\n- Make sure your local repositories of `fomo-l10n` and `foundation.mozilla.org` are matching the latest revision from main.\n- Run `inv docker-makemessages` from your `foundation.mozilla.org` repository.\n- Files should have been updated in your `fomo-l10n` repository. You can now create a pull-request.\n\n### Getting the latest translations for local dev\n\nLatest translations are uploaded to S3. To get them, run:\n- `curl -o translations.tar https://foundation-site-translations.s3.amazonaws.com/translations.tar`\n- `tar -C network-api -xvf translations.tar`\n\nYou don't need to run `compilemessages` and it works for both pipenv or docker workflows.\n\nThe `translations_github_commit_[...]` file from the archive is only used for debug purposes on Heroku. It can be safely deleted if needed.\n\n## Contributing\n\nWe love contributors, but the team maintaining this project is small and not structured to significantly support new and inexperienced contributors. If there's an unassigned issue that catches your eye, feel free to open a PR for it, but keep in mind our support will be limited. We usually don't have the capacity to walk you through the process of spinning up the project, opening a PR or describing what the solution to the issue could be.\n"
},
{
  "name": "firefox-translations",
  "files": {
    "/": [
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "docs",
      "extension",
      "package-lock.json",
      "package.json",
      "scripts",
      "update.json"
    ],
    "/docs": [
      "modelDownload.md",
      "sequence.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Build](https://github.com/mozilla/firefox-translations/actions/workflows/build_main.yml/badge.svg)](https://github.com/mozilla/firefox-translations/actions/workflows/build_main.yml) [![CodeQL](https://github.com/mozilla/firefox-translations/actions/workflows/codeql-analysis.yml/badge.svg)](https://github.com/mozilla/firefox-translations/actions/workflows/codeql-analysis.yml) [![End-to-End Tests](https://github.com/mozilla/firefox-translations/actions/workflows/e2etest.yml/badge.svg?branch=main&event=push)](https://github.com/mozilla/firefox-translations/actions/workflows/e2etest.yml)  [![Firefox Translations - Install Nightly](https://img.shields.io/badge/Firefox_Translations-Install_Nightly-2ea44f)](https://github.com/mozilla/firefox-translations/releases/download/nightly/firefox_translations.xpi)  [![CODE OF CONDUCT](https://img.shields.io/badge/Contributing-Code%20of%20Conduct-blue)](https://github.com/mozilla/firefox-translations/blob/master/CODE_OF_CONDUCT.md)  [![LICENSE](https://img.shields.io/badge/LICENSE-MPL-blue)](https://github.com/mozilla/firefox-translations/blob/master/LICENSE) [![Mozilla Add-on](https://img.shields.io/amo/v/firefox-translations.svg)](https://addons.mozilla.org/en-US/firefox/addon/firefox-translations/)\n\n# Firefox Translations\nFirefox Translations is a WebExtension that enables client side in-page translations for web browsers.\n\nFirefox Translations was developed with The Bergamot Project Consortium, coordinated by the University of Edinburgh with partners Charles University in Prague, the University of Sheffield, University of Tartu, and Mozilla. This project has received funding from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement No 825303.\n\n## Release version\n\nThe current release version is available for installation on Mozilla Add-ons\n\n[![AMO](https://ffp4g1ylyit3jdyti1hqcvtb-wpengine.netdna-ssl.com/addons/files/2015/11/get-the-addon-small.png)](https://addons.mozilla.org/firefox/addon/firefox-translations/)\n\n## Supported languages\n\n#### Production\n- Spanish\n- Estonian\n- English\n- German\n- Czech\n- Bulgarian\n- Portuguese\n- Italian\n- French\n- Polish\n\n#### Development\n- Russian\n- Persian (Farsi)\n- Icelandic\n- Norwegian Nynorsk\n- Norwegian Bokm\u00e5l\n- Ukrainian\n\n## Testing\n\n### Nightly builds\n\nYou can test nightly builds of the extension in Firefox Nightly or Developer Edition in one of the [supported languages](#supported-languages) by following the steps below:\n- Type `about:config` in the navigation bar and set the following preferences:\n\n```\n    xpinstall.signatures.required to false\n    extensions.experiments.enabled to true\n```\n\n- Then install the extension by clicking here  [![Firefox Translations - Install Nightly](https://img.shields.io/badge/Firefox_Translations-Install_Nightly-2ea44f)](https://github.com/mozilla/firefox-translations/releases/download/nightly/firefox_translations.xpi)\n- You may need to restart your browser and Firefox Translations will be ready to use. Just browse to a website in one of the [supported languages](#supported-languages) and the option to translate should be displayed.\n\n## Development\n\n### 3rd party dependencies\n\nThe extension does not utilize any npm modules, and the only vendored dependencies within are:\n\n- Bergamot Translator\n\n    - A WebAssembly wrapper around the actual Neural Machine Translator, [Marian](https://github.com/marian-nmt/marian-dev/). The code to build the WASM module can be found on its [repository](https://github.com/mozilla/bergamot-translator#build-wasm)\n\n- Fasttext\n    - We bundle the WebAssembly port of fasttext along its [compressed model](https://fasttext.cc/docs/en/language-identification.html) in order to detect the page's language. Instructions to build the WebAssembly module can be [found here](https://fasttext.cc/docs/en/webassembly-module.html)\n\n- Sentry\n    - We bundle [Sentry Javascript's SDK](https://github.com/getsentry/sentry-javascript) for error reporting.\n\n- serialize-error\n  - code of [serialize-error npm package](https://github.com/sindresorhus/serialize-error) is bundled for serialization of exceptions to\n    report errors from content scripts to background script\n\n### How to run\n- Install Firefox Nightly\n- Clone this repo and run `npm install`\n- Run `npm run once` and wait until Nightly starts\n- Go to `about:config` and set `extensions.experiments.enabled` to true\n- Browse to a page in any of the [supported languages](#supported-languages) to have the translation option to appear\n\n\n### Updating telemetry schema\n\nAfter adding new metrics to `extension/model/telemetry/metrics.yaml` or pings to `extension/model/telemetry/pings.yaml`, run\n```\nbash scripts/update-telemetry-schema.sh\n```\nto regenerate JS telemetry schema.\n\n### Updating bergamot-translator WASM module\n\nReplace\n- `extension/controller/translation/bergamot-translation-worker.js`\n- `extension/model/static/translation/bergamot-translator-worker.wasm`\n\nwith the new artifacts and then execute:\n\n```\nbash scripts/update-bergamot-translator.sh\n```\n\nto regenerate JS version file. This version is reported in telemetry.\n"
},
{
  "name": "addons-code-manager",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".env",
      ".env.common-local",
      ".env.dev",
      ".env.olympia",
      ".env.stage",
      ".eslintrc",
      ".github",
      ".gitignore",
      ".node-dev.json",
      ".prettierignore",
      ".prettierrc",
      ".yarnrc",
      "Dockerfile",
      "LICENSE.txt",
      "README.md",
      "codecov.yml",
      "package.json",
      "public",
      "renovate.json",
      "scripts",
      "src",
      "stories",
      "tsconfig.json",
      "version.json",
      "yarn.lock"
    ],
    "/.github": [
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "PULL_REQUEST_TEMPLATE.md",
      "stale.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# addons-code-manager\n\n[![CircleCI](https://circleci.com/gh/mozilla/addons-code-manager.svg?style=svg)](https://circleci.com/gh/mozilla/addons-code-manager) [![codecov](https://codecov.io/gh/mozilla/addons-code-manager/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/addons-code-manager)\n\nThis is a web application to manage add-on source code, such as reviewing code for add-ons submitted to https://addons.mozilla.org. This project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Requirements\n\n- You need [Node](https://nodejs.org/) 14 which is the current [LTS](https://github.com/nodejs/LTS) (long term support) release.\n- You need [yarn](https://yarnpkg.com/en/) to manage dependencies and run commands.\n\n## Getting started\n\n- Clone this repository\n- Type `yarn` to install everything\n- Type `yarn dev` to launch the test suite, development servers, and all other development processes in [stmux][]. Open [http://localhost:3000](http://localhost:3000) to view the development site. **Press CTRL-a-? for help**.\n\nAll available commands are documented below.\n\nRead [our contributing guidelines](.github/CONTRIBUTING.md) to get started on your first patch.\n\n## Prettier\n\nWe use [Prettier][] to automatically format our JavaScript code and stop all the on-going debates over styles. As a developer, you have to run it (with `yarn prettier-dev`) before submitting a Pull Request.\n\n## TypeScript\n\nAll code is written in [TypeScript][]. Currently, errors in test files won't be reported until you try to submit a pull request. Because of this, consider [configuring your editor](https://github.com/Microsoft/TypeScript/wiki/TypeScript-Editor-Support) for error reporting.\n\n## CSS\n\nAll styles are written in the [SASS](https://sass-lang.com/) pre-processor language as [css modules](https://github.com/css-modules/css-modules).\n\nExample: for a component like `StarIcon/index.tsx`, you'd create its module as `StarIcon/styles.module.scss`. You would import it at the top and reference it as a module. As an example, here is a simple stylesheet:\n\n```css\n.container {\n  padding: 12px;\n}\n```\n\nYou would reference this CSS class in your component like this:\n\n```js\nimport * as React from 'react';\n\nimport styles from './styles.module.scss';\n\nconst StarIcon = () => {\n  return <div className={styles.container} />;\n};\n\nexport default StarIcon;\n```\n\n## Storybook\n\nWe use [storybook](https://storybook.js.org/) to visualize the look and feel of our React components. Our Storybook is deployed on GitHub Pages: https://mozilla.github.io/addons-code-manager/.\n\nLaunch the development server like this:\n\n```\nyarn storybook\n```\n\nWhen developing a new component, always add a story for it. If you were creating a component like `src/components/StarIcon/index.tsx` then you'd put its story in `stories/StarIcon.stories.tsx`. The storybook server will automatically load files having the `.stories.tsx` suffix in this directory.\n\n## Configuration\n\nYou can configure the app by defining environment variables in `.env` files, the standard way to [configure Create React App](https://facebook.github.io/create-react-app/docs/adding-custom-environment-variables#adding-development-environment-variables-in-env), but read on below because **there are some differences**.\n\n- How to override an environment variable for local development\n  - Define it in `.env.common-local`. **This differs** from how Create React App wants you to do it. If you put an environment variable in `.env.local`, it will get erased because we are generating this file when the app starts.\n- How to define a new environment variable\n  - Add it to `.env` with the `REACT_APP_` prefix.\n- How to override an environment variable for a hosted site\n  - Define the variable in the [corresponding puppet config file](https://github.com/mozilla-services/cloudops-deployment/tree/master/projects/addons-code-manager/puppet/yaml/type). To define a variable for `code.addons-dev.allizom.org`, for example, you'd update `amo.code_manager.dev.yaml`. Adding a variable to `.env.dev` **will do nothing** since that only affects the `yarn dev` command.\n\n## Profiling performance\n\nHere are some tips for solving performance problems in addition to what's already in the [official docs](https://reactjs.org/docs/optimizing-performance.html).\n\n- First, ask yourself if there is a real performance problem. If you need to simulate a slow CPU in the profiler just to see anything dramatic, it might be too early to start profiling!\n- Try to profile against a production build, if possible, with something like `yarn start-local-dev`. The overhead of a development build could be misleading.\n- If you want to see execution timing grouped by React component, you will need a development build.\n- Use Chrome so you can get React integration, if needed. This is a [helpful guide](https://calibreapp.com/blog/react-performance-profiling-optimization/) for looking at the execution of React components in the _User Timing_ section.\n- When a React component is taking a long time to render and you don't see any other components underneath it, it's time to switch away from the _User Timing_ tab of the profiler to the _Main_ tab so you can look at actual function executions. Clicking on a function will give you information about its source.\n- You can try using the [React devtool extension](https://reactjs.org/docs/optimizing-performance.html#profiling-components-with-the-devtools-profiler) for profiling but it doesn't provide a great timeline so it's hard to visualize overall slowness.\n\n## Setting up VSCode\n\nIf you want to use [VSCode](https://code.visualstudio.com/) to develop Code Manager, some manual configuration is required. This is due to a [security ~~feature~~ bug](https://github.com/microsoft/vscode/issues/30069#issuecomment-312732928) that prevents automatically applying a local config file.\n\n- Make sure you've installed all dependencies as documented.\n- Open the root folder in VSCode.\n- Open any TypeScript file and click the TypeScript version number from the bottom status bar. Choose the option _Use Workspace Version_ to make sure you are developing with the correct version of TypeScript.\n\n## All Available Commands\n\nIn the project directory, you can run the following commands. There are a few commands not mentioned here (see `package.json`) but those are only used by internal processes.\n\n### `yarn build`\n\nThis builds the app for production to the `build` folder.\n\nIt correctly bundles React in production mode and optimizes the build for the best performance. The build is minified and the filenames include the hashes. Your app is ready to be deployed!\n\nNote: for this project, we use a node server (see `scripts/server.js`).\n\n### `yarn dev`\n\nThis runs all development processes in a single column using [stmux][]. Open [http://localhost:3000](http://localhost:3000) to view the development site. **Press CTRL-a-? for help**.\n\n### `yarn dev-2col`\n\nThis runs all development processes in two columns using [stmux][]. Open [http://localhost:3000](http://localhost:3000) to view the development site. **Press CTRL-a-? for help**.\n\n### `yarn dev-servers`\n\nThis starts all development servers using [stmux][], connected to the [-dev AMO API](https://addons-server.readthedocs.io/).\n\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser. **Press CTRL-a-? for help**.\n\n### `yarn eject`\n\nThis runs the [eject](https://facebook.github.io/create-react-app/docs/available-scripts#npm-run-eject) command. Hopefully we won't ever need this \ud83d\ude2d\n\n### `yarn eslint`\n\nThis runs [ESLint][] to discover problems within our codebase without executing it. ESLint also enforces some patterns and practices.\n\n### `yarn lint`\n\nThis runs all the _lint_ commands at once.\n\n### `yarn olympia`\n\nA prerequisite to run this command is to [install addons-server locally](https://addons-server.readthedocs.io/en/latest/topics/install/index.html).\n\nThis runs all development processes in a single column using [stmux][]. Open [http://olympia.test:5000](http://olympia.test:5000) to view the development site. **Press CTRL-a-? for help**. The application is configured to use a [local AMO API](https://addons-server.readthedocs.io/).\n\nKnown issue: it is possible to login/logout but the login process will redirect you to the AMO frontend (http://olympia.test) instead of http://olympia.test:5000. You will have to manually go to code-manager, after that there should be no other authentication problem.\n\n### `yarn prettier`\n\nThis runs [Prettier][] to automatically format the entire codebase.\n\n### `yarn prettier-dev`\n\nThis runs [Prettier][] on only your changed files. This is intended for development.\n\n### `yarn stage`\n\nThis runs all development processes in a single column using [stmux][]. Open [http://localhost:3000](http://localhost:3000) to view the development site. **Press CTRL-a-? for help**. The application is configured to use the [stage AMO API](https://addons-server.readthedocs.io/).\n\n### `yarn start-local-dev`\n\nThis builds the app for production to the `build` folder (see `yarn build` command), configured with the [-dev AMO API](https://addons-server.readthedocs.io/). It also starts a production server that serves the application (configured for local usage).\n\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\n### `yarn start-local-prod`\n\nThis builds the app for production to the `build` folder (see `yarn build` command), configured with the [production AMO API](https://addons-server.readthedocs.io/). It also starts a production server that serves the application (configured for local usage).\n\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\n:warning: It is currently not possible to authenticate users.\n\n### `yarn start-local-stage`\n\nThis builds the app for production to the `build` folder (see `yarn build` command), configured with the [stage AMO API](https://addons-server.readthedocs.io/). It also starts a production server that serves the application (configured for local usage).\n\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\n### `yarn test`\n\nThis launches [Jest](https://jestjs.io/) in the interactive watch mode.\n\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `yarn type-coverage`\n\nThis will check how much of the codebase is protect by safe static types and fail if it's below the configured threshold.\n\n### `yarn typecheck`\n\nThis checks for [TypeScript][] errors in all files, including test files.\n\nYou'd think that `build` does this but it does not check test files. See [create-react-app issue 5626](https://github.com/facebook/create-react-app/issues/5626).\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n\n[prettier]: https://prettier.io/\n[typescript]: https://www.typescriptlang.org/\n[stmux]: https://github.com/rse/stmux\n[eslint]: https://eslint.org/\n"
},
{
  "name": "glean-dictionary",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".netlify",
      ".prettierignore",
      ".prettierrc.js",
      ".storybook",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "__mocks__",
      "babel.config.js",
      "docs",
      "etl",
      "etl_tests",
      "jest.config.js",
      "netlify.toml",
      "package-lock.json",
      "package.json",
      "playwright.config.js",
      "playwright",
      "postcss.config.js",
      "public",
      "pyproject.toml",
      "requirements.in",
      "requirements.txt",
      "rollup.config.js",
      "scripts",
      "setup.cfg",
      "setup.py",
      "src",
      "stories",
      "svelte.config.js",
      "tests"
    ],
    "/docs": [
      "development.md"
    ],
    "/.github": [
      "dependabot.yml",
      "pull_request_template.md",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Glean Dictionary\n\n[![CircleCI](https://circleci.com/gh/mozilla/glean-dictionary.svg?style=svg)](https://circleci.com/gh/mozilla/glean-dictionary)\n\nThe Glean dictionary aims to provide a comprehensive index of datasets generated\ninside Mozilla for applications built using the\n[Glean SDKs](https://mozilla.github.io/glean/book/index.html).\n\nThis project is under active development. For up to date information on project\nstructure and governance, see:\n\nhttps://wiki.mozilla.org/Data/WorkingGroups/GleanDictionary\n\nThe production version of the Glean Dictionary is deployed at:\n\nhttps://dictionary.telemetry.mozilla.org\n\n## Getting Started\n\nYou should be able to create your own local copy of the dictionary so long as\nyou have [Python](https://www.python.org/) (version 3.8+) and\n[node.js](https://nodejs.org/) (version 12+) installed. You will also need npm\nv7 or greater: run `npm install -g npm@latest` if you need to upgrade.\n\nAssuming those requirements are met, follow these instructions:\n\n```bash\n# Create and activate a python virtual environment.\npython3 -m venv venv/\nvenv/bin/pip install -r requirements.txt\n\n# Build data needed by dashboard\n./scripts/gd build-metadata\n\n# Install npm dependencies and start a local\n# instance of the GUI\nnpm install\nnpm run dev\n```\n\nIf that worked, you should be able to see a local version of Glean at\nhttp://localhost:5000\n\nYou can speed up the \"build data\" step by appending the name of a set of\napplication(s) you want to build metadata for. This can speed up the process\nconsiderably. For example, to build a metadata index for Fenix (Firefox for\nAndroid) only, try:\n\n```bash\n./scripts/build-metadata fenix\n```\n\n## Search Service\n\nThe Glean Dictionary also includes a search service which enables searching\nthrough active metrics. Under the hood, this service is implemented with\n[netlify functions]. For example:\n\nhttps://dictionary.telemetry.mozilla.org/api/v1/metrics_search_burnham?search=techno\n\nYou can start it up via the [netlify command line interface] (assuming you have\nit installed):\n\n```bash\nnetlify dev\n```\n\nIf you have generated metadata as described above, you should then be able to\ntest the search functions locally:\n\nhttp://localhost:8888/api/v1/metrics_search_burnham?search=techno\nhttp://localhost:8888/api/v1/metrics_search_firefox_legacy?search=ms\n\n[netlify command line interface]: https://docs.netlify.com/cli/get-started/\n[netlify functions]: https://docs.netlify.com/functions/overview/\n\n## Storybook\n\nWe use [Storybook](https://storybook.js.org/) for developing and validating\nSvelte components used throughout the app. To view the existing list of stories,\nrun:\n\n```bash\nnpm run storybook\n```\n\n### Storybook Snapshot Testing\n\nTo give us more confidence that changes don't unintentionally break the UI, we\nrun\n[storybook snapshot tests](https://storybook.js.org/docs/react/workflows/snapshot-testing).\n\nYou can run them manually as follows:\n\n```bash\nnpm run test:jest\n```\n\nIf you intentionally made a change to a component that results in a change to\nthe output of the storybook snapshots, you can re-generate them using the\nfollowing command:\n\n```bash\nnpm run test:jest -- -u\n```\n\n## End-to-End Testing\n\nWe use [Playwright](https://playwright.dev/) for our end-to-end tests.\n\nBefore testing, download the supported browsers needed for Playwright to execute\nsuccessfully by running:\n\n```bash\nnpx playwright install\n```\n\nTo run the end-to-end tests along with other tests:\n\n```bash\nnpm run test\n```\n\nTo run only the Playwright tests:\n\n```bash\nnpx playwright test\n```\n\n## ETL Testing\n\nThe transforms used by the Glean Dictionary have their own tests. Assuming\nyou've run the set up as described above, you can run these tests by executing:\n\n```bash\nvenv/bin/pytest\n```\n\n## Glean Debugging\n\nIn order to enable\n[ping logging](https://mozilla.github.io/glean/book/reference/debug/logPings.html)\nset the `GLEAN_LOG_PINGS` environment variable.\n\n```bash\nGLEAN_LOG_PINGS=true npm run dev\n```\n\nIn order to send Glean pings to the\n[debug viewer](https://mozilla.github.io/glean/book/reference/debug/debugViewTag.html)\nset the `GLEAN_DEBUG_VIEW_TAG` environment variable.\n\n```bash\nGLEAN_DEBUG_VIEW_TAG=my-tag npm run dev\n```\n\n## Deployment\n\nThe production version of the Glean Dictionary\n(https://dictionary.telemetry.mozilla.org) is deployed from the `production`\nbranch on this repository, which usually corresponds to the latest GitHub\nrelease. To update the Glean Dictionary to the latest version, follow this\nprocedure:\n\n- Do a quick test of https://glean-dictionary-dev.netlify.app to make sure it's\n  working as expected.\n- Create a new release, typically off of the `main` branch (use the\n  [auto-generated release notes](https://docs.github.com/en/repositories/releasing-projects-on-github/automatically-generated-release-notes),\n  omitting dependency updates).\n- From a local checkout, update the `production` branch to be in sync with the\n  tag you just created, then push to the production branch. After the\n  integration tests pass, dictionary.telemetry.mozilla.org should be\n  automatically updated to the latest version.\n\nA version of the Glean Dictionary running the development branch (`main`) is\naccessible at https://glean-dictionary-dev.netlify.app/\n\n## Contributing\n\nFor more information on contributing, see [CONTRIBUTING.md](./CONTRIBUTING.md)\nin the root of this repository.\n"
},
{
  "name": "glean_parser",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".flake8",
      ".github",
      ".gitignore",
      ".swiftlint.yml",
      "AUTHORS.md",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "docs",
      "glean_parser",
      "requirements_dev.txt",
      "setup.cfg",
      "setup.py",
      "tests",
      "tools"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "authors.md",
      "conf.py",
      "contributing.md",
      "history.md",
      "index.rst",
      "installation.md",
      "make.bat",
      "metrics-yaml.rst",
      "pings-yaml.rst",
      "readme.md",
      "tags-yaml.rst"
    ],
    "/.github": [
      "ISSUE_TEMPLATE.md",
      "dependabot.yml",
      "pull_request_template.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: clean clean-test clean-pyc clean-build docs help\n\ndefine PRINT_HELP_PYSCRIPT\nimport re, sys\n\nfor line in sys.stdin:\n\tmatch = re.match(r'^([a-zA-Z_-]+):.*?## (.*)$$', line)\n\tif match:\n\t\ttarget, help = match.groups()\n\t\tprint(\"%-20s %s\" % (target, help))\nendef\nexport PRINT_HELP_PYSCRIPT\n\nhelp:\n\t@python -c \"$$PRINT_HELP_PYSCRIPT\" < $(MAKEFILE_LIST)\n\nclean: clean-build clean-pyc clean-test ## remove all build, test, coverage and Python artifacts\n\nclean-build: ## remove build artifacts\n\trm -fr build/\n\trm -fr dist/\n\trm -fr .eggs/\n\tfind . -name '*.egg-info' -exec rm -fr {} +\n\tfind . -name '*.egg' -exec rm -fr {} +\n\nclean-pyc: ## remove Python file artifacts\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -fr {} +\n\nclean-test: ## remove test and coverage artifacts\n\trm -f .coverage\n\trm -fr htmlcov/\n\trm -fr .pytest_cache\n\nlint: ## check style with flake8\n\tpython3 -m flake8 glean_parser tests\n\tpython3 -m black --check --diff glean_parser tests setup.py\n\tpython3 -m yamllint glean_parser tests\n\tpython3 -m mypy glean_parser\n\nfmt: ## autoformat files\n\tpython3 -m black glean_parser tests setup.py\n\ntest: ## run tests quickly with the default Python\n\tpy.test\n\ncoverage: ## check code coverage quickly with the default Python\n\tcoverage run --source glean_parser -m pytest\n\tcoverage report -m\n\tcoverage html\n\ndocs: ## generate Sphinx HTML documentation, including API docs\n\trm -f docs/glean_parser.rst\n\trm -f docs/modules.rst\n\tsphinx-apidoc -o docs/ glean_parser\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C docs html\n\nrelease: dist ## package and upload a release\n\ttwine upload dist/*\n\ndist: clean ## builds source and wheel package\n\tpython setup.py sdist\n\tpython setup.py bdist_wheel\n\tls -l dist\n\ninstall: clean ## install the package to the active Python's site-packages\n\tpip install .\n\ninstall-kotlin-linters: ## install ktlint and detekt for linting Kotlin output\n\ttest -f ktlint || curl -sSLO https://github.com/shyiko/ktlint/releases/download/0.29.0/ktlint\n\techo \"03c9f9f78f80bcdb44c292d95e4d9abf221daf5e377673c1b6675a8003eab94d *ktlint\" | sha256sum -c -\n\tchmod a+x ktlint\n\ttest -f detekt-cli.jar || curl -sSL --output \"detekt-cli.jar\" https://github.com/detekt/detekt/releases/download/v1.16.0/detekt-cli-1.16.0-all.jar\n\techo \"dd63aae60cce4b0516ffc11d31b6280d4010ab481754c7e49b2c9e78db877ca4 *detekt-cli.jar\" | sha256sum -c -\n",
  "readme": "# Glean Parser\n\nParser tools for Mozilla's Glean telemetry.\n\n## Features\n\nContains various utilities for handling `metrics.yaml` and `pings.yaml` for [the\nGlean SDKs](https://mozilla.github.io/glean). This includes producing generated\ncode for various integrations, linting and coverage testing.\n\n## Documentation\n\n- [How to Contribute](https://github.com/mozilla/glean_parser/blob/main/CONTRIBUTING.md). Please file bugs in [bugzilla](https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=nobody%40mozilla.org&bug_ignored=0&bug_severity=normal&bug_status=NEW&cf_fission_milestone=---&cf_fx_iteration=---&cf_fx_points=---&cf_status_firefox65=---&cf_status_firefox66=---&cf_status_firefox67=---&cf_status_firefox_esr60=---&cf_status_thunderbird_esr60=---&cf_tracking_firefox65=---&cf_tracking_firefox66=---&cf_tracking_firefox67=---&cf_tracking_firefox_esr60=---&cf_tracking_firefox_relnote=---&cf_tracking_thunderbird_esr60=---&product=Data%20Platform%20and%20Tools&component=Glean%3A%20SDK&contenttypemethod=list&contenttypeselection=text%2Fplain&defined_groups=1&flag_type-203=X&flag_type-37=X&flag_type-41=X&flag_type-607=X&flag_type-721=X&flag_type-737=X&flag_type-787=X&flag_type-799=X&flag_type-800=X&flag_type-803=X&flag_type-835=X&flag_type-846=X&flag_type-855=X&flag_type-864=X&flag_type-916=X&flag_type-929=X&flag_type-930=X&flag_type-935=X&flag_type-936=X&flag_type-937=X&form_name=enter_bug&maketemplate=Remember%20values%20as%20bookmarkable%20template&op_sys=Unspecified&priority=P3&&rep_platform=Unspecified&status_whiteboard=%5Btelemetry%3Aglean-rs%3Am%3F%5D&target_milestone=---&version=unspecified).\n- [User documentation for Glean](https://mozilla.github.io/glean/).\n- [`glean_parser` developer documentation](https://mozilla.github.io/glean_parser/).\n\n## Requirements\n\n-   Python 3.6 (or later)\n\nThe following library requirements are installed automatically when\n`glean_parser` is installed by `pip`.\n\n-   appdirs\n-   Click\n-   diskcache\n-   Jinja2\n-   jsonschema\n-   PyYAML\n\nAdditionally on Python 3.6:\n\n-   iso8601\n\n## Usage\n\n```sh\n$ glean_parser --help\n```\n\nRead in `metrics.yaml`, translate to Kotlin format, and\noutput to `output_dir`:\n\n```sh\n$ glean_parser translate -o output_dir -f kotlin metrics.yaml\n```\n\nCheck a Glean ping against the ping schema:\n\n```sh\n$ glean_parser check < ping.json\n```\n"
},
{
  "name": "looker-hub",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "accessibility",
      "activity_stream",
      "awesome_bar",
      "bedrock",
      "casa",
      "combined_browser_metrics",
      "contextual_services",
      "duet",
      "experimentation",
      "fenix",
      "firefox_accounts",
      "firefox_desktop",
      "firefox_desktop_background_update",
      "firefox_ios",
      "firefox_translations",
      "focus_android",
      "focus_ios",
      "glean_dictionary",
      "hub.model.lkml",
      "klar_android",
      "klar_ios",
      "kpi",
      "marketing",
      "monitoring",
      "mozilla_vpn",
      "mozilla_vpn_android",
      "namespaces.yaml",
      "operational_monitoring",
      "pocket",
      "regrets_reporter",
      "revenue",
      "search",
      "search_private",
      "shared",
      "sync",
      "user_journey",
      "websites"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Looker Hub - Base Branch\n\nMozilla uses a hub-and-spoke model for our Looker deploy.\nThis centralized repository contains all of the automated imports (called _views_ in Looker) of BigQuery tables that are used in any Looker explore.\nDownstream projects import these views and extend them to suit the needs of the project.\n\nThis is the base branch. All generated branches are derived from this branch directly, and files in this branch are kept as-is in the generated branches.\n"
},
{
  "name": "reticulum",
  "files": {
    "/": [
      ".formatter.exs",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Jenkinsfile",
      "LICENSE",
      "README.md",
      "TurkeyDockerfile",
      "assets",
      "config",
      "docker-compose.yml",
      "guides",
      "habitat",
      "lib",
      "mix.exs",
      "mix.lock",
      "priv",
      "rel",
      "scripts",
      "test"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Reticulum\nNote: **Due to our small team size, we don't support setting up Reticulum locally due to restrictions on developer credentials. Although relatively difficult and new territory, you're welcome to set up this up yourself. In addition to running Reticulum locally, you'll need to also run [Hubs](https://github.com/mozilla/hubs) and [Dialog](https://github.com/mozilla/dialog) locally because the developer Dialog server is locked down and your local Reticulum will not connect properly)**\n\nReference [this discussion thread](https://github.com/mozilla/hubs/discussions/3323) for more information. \n\nA hybrid game networking and web API server, focused on Social Mixed Reality.\n\n## Development\n\n### 1. Install Prerequisite Packages:\n\n#### PostgreSQL (recommended version 11.x):\n\nLinux:\n\nOn Ubuntu, you can use\n\n```\napt install postgresql\n```\n\nOtherwise, consult your package manager of choice for other Linux distributions\n\nWindows: https://www.postgresql.org/download/windows/\n\nWindows WSL: https://github.com/michaeltreat/Windows-Subsystem-For-Linux-Setup-Guide/blob/master/readmes/installs/PostgreSQL.md\n\n#### Erlang (v22) + Elixir (v1.8) + Phoenix\n\nhttps://elixir-lang.org/install.html\n\nNote: On Linux, you may also have to install the erlang-src package for your distribution in order to compile dependencies successfully.\n\nhttps://hexdocs.pm/phoenix/installation.html\n\n#### Ansible\n\nhttps://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html\n\n### 2. Setup Reticulum:\n\nRun the following commands at the root of the reticulum directory:\n\n1. `mix deps.get`\n2. `mix ecto.create`\n   - If step 2 fails, you may need to change the password for the `postgres` role to match the password configured `dev.exs`.\n   - From within the `psql` shell, enter `ALTER USER postgres WITH PASSWORD 'postgres';`\n   - If you receive an error that the `ret_dev` database does not exist, (using psql again) enter `create database ret_dev;`\n3. From the project directory `mkdir -p storage/dev`\n\n### 3. Start Reticulum\n\nRun `scripts/run.sh` if you have the hubs secret repo cloned. Otherwise `iex -S mix phx.server`\n\n## Run Hubs Against a Local Reticulum Instance\n\n### 1. Setup the `hubs.local` hostname\n\nWhen running the full stack for Hubs (which includes Reticulum) locally it is necessary to add a `hosts` entry pointing `hubs.local` to your local server's IP.\nThis will allow the CSP checks to pass that are served up by Reticulum so you can test the whole app. Note that you must also load hubs.local over https.\n\nOn MacOS or Linux:\n\n```bash\nnano /etc/hosts\n```\n\nFrom there, add a host alias\n\nExample:\n\n```bash\n127.0.0.1   hubs.local\n127.0.0.1   hubs-proxy.local\n```\n\n### 2. Setting up the Hubs Repository\n\nClone the Hubs repository and install the npm dependencies.\n\n```bash\ngit clone https://github.com/mozilla/hubs.git\ncd hubs\nnpm ci\n```\n\n### 3. Start the Hubs Webpack Dev Server\n\nBecause we are running Hubs against the local Reticulum client you'll need to use the `npm run local` command in the root of the `hubs` folder. This will start the development server on port 8080, but configure it to be accessed through Reticulum on port 4000.\n\n### 4. Navigate To The Client Page\n\nOnce both the Hubs Webpack Dev Server and Reticulum server are both running you can navigate to the client by opening up:\n\nhttps://hubs.local:4000?skipadmin\n\n> The `skipadmin` is a temporary measure to bypass being redirected to the admin panel. Once you have logged in you will no longer need this.\n\n### 5. Logging In\n\nTo log into Hubs we use magic links that are sent to your email. When you are running Reticulum locally we do not send those emails. Instead, you'll find the contents of that email in the Reticulum console output.\n\nWith the Hubs landing page open click the Sign In button at the top of the page. Enter an email address and click send.\n\nGo to the reticulum terminal session and find a url that looks like https://hubs.local:4000/?auth_origin=hubs&auth_payload=XXXXX&auth_token=XXXX\n\nNavigate to that url in your browser to finish signing in.\n\n### 6. Creating an Admin User\n\nAfter you've started Reticulum for the first time you'll likely want to create an admin user. Assuming you want to make the first account the admin, this can be done in the iex console using the following code:\n\n```\nRet.Account |> Ret.Repo.all() |> Enum.at(0) |> Ecto.Changeset.change(is_admin: true) |> Ret.Repo.update!()\n```\n\n### 7. Enabling Room Features\n\nRooms are created with restricted permissions by default, which means you can't spawn media objects. You can change this setting in the admin panel, or run the following code in the iex console:\n\n```\nRet.AppConfig.set_config_value(\"features|permissive_rooms\", true)\n```\n\n### 8. Start the Admin Panel server in local development mode\n\nWhen running locally, you will need to also run the admin panel, which routes to hubs.local:8989\nUsing a separate terminal instance, navigate to the `hubs/admin` folder and use:\n\n```\nnpm run local\n```\n\nYou can now navigate to https://hubs.local:4000/admin to access the admin control panel\n\n## Run Spoke Against a Local Reticulum Instance\n\n1. Follow the steps above to setup Hubs\n2. Clone and start spoke by running `./scripts/run_local_reticulum.sh` in the root of the spoke project\n3. Navigate to https://hubs.local:4000/spoke\n\n## Run Reticulum against a local Dialog instance\n\n1. Update the Janus host in `dev.exs`:\n\n```\ndev_janus_host = \"hubs.local\"\n```\n\n2. Update the Janus port in `dev.exs`:\n\n```\nconfig :ret, Ret.JanusLoadStatus, default_janus_host: dev_janus_host, janus_port: 4443\n```\n\n3. Add the Dialog meta endpoint to the CSP rules in `add_csp.ex`:\n\n```\ndefault_janus_csp_rule =\n   if default_janus_host,\n      do: \"wss://#{default_janus_host}:#{janus_port} https://#{default_janus_host}:#{janus_port} https://#{default_janus_host}:#{janus_port}/meta\",\n      else: \"\"\n```\n\n4. Edit the Dialog configuration file _turnserver.conf_ and update the PostgreSQL database connection string to use the _coturn_ schema from the Reticulum database:\n\n```\n   psql-userdb=\"host=hubs.local dbname=ret_dev user=postgres password=postgres options='-c search_path=coturn' connect_timeout=30\"\n```\n"
},
{
  "name": "bugbug",
  "files": {
    "/": [
      ".codecov.yml",
      ".dockerignore",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".prettierrc",
      ".taskcluster.yml",
      "CITATION.cff",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "VERSION",
      "bugbug",
      "docker-compose.yml",
      "docs",
      "extra-nlp-requirements.txt",
      "extra-nn-requirements.txt",
      "http_service",
      "infra",
      "requirements.txt",
      "scripts",
      "setup.py",
      "test-requirements.txt",
      "tests",
      "ui"
    ],
    "/docs": [
      "README.md",
      "models"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# bugbug\n\n[![Task Status](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/bugbug/master/badge.svg)](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/bugbug/master/latest)\n[![codecov](https://codecov.io/gh/mozilla/bugbug/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/bugbug)\n<a href=\"https://chat.mozilla.org/#/room/#bugbug:mozilla.org\" target=\"_blank\">\n<img src=\"https://img.shields.io/badge/chat%20on%20[m]-%23bugbug%3Amozilla.org-blue\">\n</a>\n\nBugbug aims at leveraging machine learning techniques to help with bug and quality management, and other software engineering tasks (such as test selection and defect prediction).\n\nChat with us in the [bugbug](https://chat.mozilla.org/#/room/#bugbug:mozilla.org) Matrix room.\n\nMore information on the Mozilla Hacks blog:\n\n- https://hacks.mozilla.org/2020/07/testing-firefox-more-efficiently-with-machine-learning/\n- https://hacks.mozilla.org/2019/04/teaching-machines-to-triage-firefox-bugs/\n\n## Classifiers\n\n- **assignee** - The aim of this classifier is to suggest an appropriate assignee for a bug.\n\n- **backout** - The aim of this classifier is to detect patches that might be more likely to be backed-out (because of build or test failures). It could be used for test prioritization/scheduling purposes.\n\n- **bugtype** - The aim of this classifier is to classify bugs according to their type. The labels are gathered automatically from bugs: right now they are \"crash/memory/performance/security\". The plan is to add more types after manual labeling.\n\n- **component** - The aim of this classifier is to assign product/component to (untriaged) bugs.\n\n- **defect vs enhancement vs task** - Extension of the **defect** classifier to detect differences also between feature requests and development tasks.\n\n- **defect** - Bugs on Bugzilla aren't always bugs. Sometimes they are feature requests, refactorings, and so on. The aim of this classifier is to distinguish between bugs that are actually bugs and bugs that aren't. The dataset currently contains 2110 bugs, the accuracy of the current classifier is ~93% (precision ~95%, recall ~94%).\n\n- **devdocneeded** - The aim of this classifier is to detect bugs which should be documented for developers.\n\n- **duplicate** - The aim of this classifier is to detect duplicate bugs.\n\n- [**needsdiagnosis**](https://github.com/webcompat/webcompat.com/blob/main/docs/ml-process.md) - The aim of this classifier is to detect issues that are likely invalid and don't need to be diagnosed for webcompat use case.\n\n- **qaneeded** - The aim of this classifier is to detect bugs that would need QA verification.\n\n- **regression vs non-regression** - Bugzilla has a `regression` keyword to identify bugs that are regressions. Unfortunately it isn't used consistently. The aim of this classifier is to detect bugs that are regressions.\n\n- **regressionrange** - The aim of this classifier is to detect regression bugs that have a regression range vs those that don't.\n\n- [**regressor**](docs/models/regressor.md) - The aim of this classifier is to detect patches which are more likely to cause regressions. It could be used to make riskier patches undergo more scrutiny.\n\n- **spam** - The aim of this classifier is to detect bugs which are spam.\n\n- **stepstoreproduce** - The aim of this classifier is to detect bugs that have steps to reproduce vs those that don't.\n\n- **testfailure** - The aim of this classifier is to detect patches that might be more likely to cause test failures.\n\n- **testselect** - The aim of this classifier is to select relevant tests to run for a given patch.\n\n- **tracking** - The aim of this classifier is to detect bugs to track.\n\n- **uplift** - The aim of this classifier is to detect bugs for which uplift should be approved and bugs for which uplift should not be approved.\n\n## Setup and Prerequisites\n\nInstall the Python dependencies:\n\n```\npip3 install -r requirements.txt\n```\n\nYou may also need `pip install -r test-requirements.txt`. Depending on the parts of bugbug you want to run, you might need to install dependencies from other requirement files (find them with `find . -name \"*requirements*\"`).\n\nCurrently, Python 3.9+ is required. You can double check the version we use by looking at setup.py.\n\nAlso, libgit2 (needs [v1.0.0](https://github.com/libgit2/libgit2/releases/tag/v1.0.0), only in [experimental on Debian](https://wiki.debian.org/DebianExperimental)), **might** be required (if you can't install it, skip this step).\n\n```\nsudo apt-get -t experimental install libgit2-dev\n```\n\n### Auto-formatting\n\nThis project is using [pre-commit](https://pre-commit.com/). Please run `pre-commit install` to install the git pre-commit hooks on your clone.\n\nEvery time you will try to commit, pre-commit will run checks on your files to make sure they follow our style standards and they aren't affected by some simple issues. If the checks fail, pre-commit won't let you commit.\n\n## Usage\n\n### Training\n\nRun the `trainer.py` script with the command `python -m scripts.trainer` (with `--help` to see the required and optional arguments of the command) to perform training (warning this takes 30min+).\n\n### Testing\n\nTo use a model to classify a given bug, you can run `python -m scripts.bug_classifier MODEL_NAME --bug-id ID_OF_A_BUG_FROM_BUGZILLA`. N.B.: If you run the classifier script without training a model first, it will automatically download an already trained model.\n\n### Example for the \"defect\" model\n\n**training** To train the model for mode `defect`:\n\n    python3 -m scripts.trainer defect\n\n**testing** To use the model to classify a given bug, you can run `python -m scripts.bug_classifier defect --bug-id ID_OF_A_BUG_FROM_BUGZILLA`.\n\n### Running the repository mining script\n\nNote: This section is only necessary if you want to perform changes to the repository mining script. Otherwise, you can simply use the commits data we generate automatically.\n\n1. Clone https://hg.mozilla.org/mozilla-central/.\n2. Run `./mach vcs-setup` in the directory where you have cloned mozilla-central.\n3. Enable the extensions mentioned in [infra/hgrc](https://github.com/mozilla/bugbug/blob/master/infra/hgrc). For example, if you are on Linux, you can add `firefoxtree` to the extensions section of the `~/.hgrc` file as:\n   ```\n   firefoxtree = ~/.mozbuild/version-control-tools/hgext/firefoxtree\n   ```\n4. Run the `repository.py` script, with the only argument being the path to the mozilla-central repository.\n\nNote: If you run into problems, it's possible the version of Mercurial you are using is not supported. Check the Docker definition at infra/dockerfile.commit_retrieval to see what we are using in production.\n\nNote: the script will take a long time to run (on my laptop more than 7 hours). If you want to test a simple change and you don't intend to actually mine the data, you can modify the repository.py script to limit the number of analyzed commits. Simply add `limit=1024` to the call to the `log` command.\n\n## Structure of the project\n\n- `bugbug/labels` contains manually collected labels;\n- `bugbug/db.py` is an implementation of a really simple JSON database;\n- `bugbug/bugzilla.py` contains the functions to retrieve bugs from the Bugzilla tracking system;\n- `bugbug/repository.py` contains the functions to mine data from the mozilla-central (Firefox) repository;\n- `bugbug/bug_features.py` contains functions to extract features from bug/commit data;\n- `bugbug/model.py` contains the base class that all models derive from;\n- `bugbug/models` contains implementations of specific models;\n- `bugbug/nn.py` contains utility functions to include Keras models into a scikit-learn pipeline;\n- `bugbug/utils.py` contains misc utility functions;\n- `bugbug/nlp` contains utility functions for NLP;\n- `bugbug/labels.py` contains utility functions for handling labels;\n- `bugbug/bug_snapshot.py` contains a module to play back the history of a bug;\n- `bugbug/github.py` contains functions to retrieve issues from GitHub for a specified owner/repository.\n\n## Using bugbug for non-Mozilla projects\n\nBugbug is focussing on Mozilla use-cases for Firefox, Bugzilla and GitHub.\nHowever, we will be happy to accept pull requests adding support for other projects or bug trackers.\n"
},
{
  "name": "sign-addon",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc",
      ".github",
      ".gitignore",
      ".npmignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "LICENSE.txt",
      "README.md",
      "jest.config.js",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "src",
      "tests",
      "tsconfig.json"
    ],
    "/.github": [
      "CODE_OF_CONDUCT.md",
      "stale.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Sign Add-on\n\nSign a Firefox add-on with Mozilla's [web service](http://addons-server.readthedocs.org/en/latest/topics/api/signing.html).\n\n[![CircleCI](https://circleci.com/gh/mozilla/sign-addon.svg?style=svg)](https://circleci.com/gh/mozilla/sign-addon) [![npm version](https://badge.fury.io/js/sign-addon.svg)](https://badge.fury.io/js/sign-addon) [![codecov](https://codecov.io/gh/mozilla/sign-addon/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/sign-addon)\n\n## Installation\n\n    npm install sign-addon\n\n## Getting started\n\nTo sign add-ons, you first need to generate API credentials, a JWT issuer and secret, from the [AMO Developer Hub](https://addons.mozilla.org/en-US/developers/addon/api/key/).\n\nCurrently, this is intended for use in [NodeJS](https://nodejs.org/) only and should work in version 10 or higher.\n\n## Programmatic use\n\nHere is how to retrieve a signed version of an [XPI file](https://developer.mozilla.org/en-US/docs/Mozilla/XPI):\n\n```javascript\nimport { signAddon } from 'sign-addon';\n\nsignAddon({\n  // Required arguments:\n\n  xpiPath: '/path/to/your/addon.xpi',\n  version: '0.0.1',\n  apiKey: 'Your JWT issuer',\n  apiSecret: 'Your JWT secret',\n\n  // Optional arguments:\n\n  // The explicit extension ID.\n  // WebExtensions do not require an ID.\n  // See the notes below about dealing with IDs.\n  id: 'your-addon-id@somewhere',\n  // The release channel (listed or unlisted).\n  // Ignored for new add-ons, which are always unlisted.\n  // Default: most recently used channel.\n  channel: undefined,\n  // Save downloaded files to this directory.\n  // Default: current working directory.\n  downloadDir: undefined,\n  // Number of milliseconds to wait before aborting the request.\n  // Default: 15 minutes.\n  timeout: undefined,\n  // Optional proxy to use for all API requests,\n  // such as \"http://yourproxy:6000\"\n  // Read this for details on how proxy requests work:\n  // https://github.com/request/request#proxies\n  apiProxy: undefined,\n  // Optional object to pass to request() for additional configuration.\n  // Some properties such as 'url' cannot be defined here.\n  // Available options:\n  // https://github.com/request/request#requestoptions-callback\n  apiRequestConfig: undefined,\n  // Optional override to the number of seconds until the JWT token for\n  // the API request expires. This must match the expiration time that\n  // the API server accepts.\n  apiJwtExpiresIn: undefined,\n  // Optional override to the URL prefix of the signing API.\n  // The production instance of the API will be used by default.\n  apiUrlPrefix: 'https://addons.mozilla.org/api/v4',\n})\n  .then(function (result) {\n    if (result.success) {\n      console.log('The following signed files were downloaded:');\n      console.log(result.downloadedFiles);\n      console.log('Your extension ID is:');\n      console.log(result.id);\n    } else {\n      console.error('Your add-on could not be signed!');\n      console.error('Error code: ' + result.errorCode);\n      console.error('Details: ' + result.errorDetails);\n    }\n    console.log(result.success ? 'SUCCESS' : 'FAIL');\n  })\n  .catch(function (error) {\n    console.error('Signing error:', error);\n  });\n```\n\n## Dealing With Extension IDs\n\nHere are some notes about dealing with IDs when using `signAddon()`:\n\n- [WebExtensions](https://developer.mozilla.org/en-US/Add-ons/WebExtensions) do not require you to pass `id` to `signAddon()`. In this case, an ID will be auto-generated for you. It is accessible in `signingResult.id`.\n- If a WebExtension's `manifest.json` already declares an ID, any `id` you pass to `signAddon()` will have no effect!\n- To push an updated version to a WebExtension that had its ID auto-generated, you need to pass in the original ID explicitly.\n- You must pass `id` to `signAddon()` for all other non-WebExtension add-ons.\n\n## Development\n\nHere's how to set up a development environment for the `sign-addon` package. Install all requirements and run tests from the source:\n\n```\n$ npm install\n$ npm test\n```\n\n### Prettier\n\nWe use [Prettier][] to automatically format our JavaScript code and stop all the on-going debates over styles. As a developer, you have to run it (with `npm run prettier-dev`) before submitting a Pull Request.\n\n### Useful commands\n\nIn the project directory, you can run the following commands. There are a few commands not mentioned here (see `package.json`) but those are only used by internal processes.\n\n#### `npm run build`\n\nThis packages the library for production into the `dist/` folder.\n\n#### `npm run changelog`\n\nThis creates a changelog of all unreleased changes (in markdown). See the [Releasing](#releasing) section for more information.\n\n### `npm run changelog-lint`\n\nThis lints the commit messages. See the [Writing commit messages](#writing-commit-messages) section for more information.\n\n#### `npm run eslint`\n\nThis runs [ESLint][] to discover problems within our codebase without executing it. ESLint also enforces some patterns and practices.\n\n#### `npm run lint`\n\nThis runs all the _lint_ commands at once.\n\n#### `npm run prettier`\n\nThis runs [Prettier][] to automatically format the entire codebase.\n\n#### `npm run prettier-dev`\n\nThis runs [Prettier][] on only your changed files. This is intended for development.\n\n#### `npm test`\n\nThis runs the test suite.\n\nYou can run this command in \"watch mode\" while working on this project:\n\n```\n$ npm test -- --watch\n```\n\n#### `npm run typecheck`\n\nThis checks for [TypeScript][] errors in all files, including test files.\n\nYou can run this command in \"watch mode\" while working on this project:\n\n```\n$ npm run typecheck -- --watch\n```\n\n### Linking\n\nThe `sign-addon` module is meant to be used as a dependency. If you need to test your local code inside another module, you can link it.\n\nFirst, link it your npm system:\n\n    cd /path/to/sign-addon\n    npm link\n\nNext, change into the module you want to use it in, citing [web-ext](https://github.com/mozilla/web-ext) as an example, and link back to `sign-addon`:\n\n    cd /path/to/web-ext\n    npm link sign-addon\n\n`web-ext` will now use your local version of `sign-addon`.\n\n### Writing commit messages\n\nWe follow the Angular style of [semantic messages](https://github.com/angular/angular.js/blob/master/CONTRIBUTING.md#commit) when writing a commit message. This allows us to auto-generate a changelog without too much noise in it. Be sure to write the commit message in past tense so it will read naturally as a historic changelog.\n\nExamples:\n\n- `feat: Added a systematic dysfunctioner`\n- `fix: Fixed hang in systematic dysfunctioner`\n- `docs: Improved contributor docs`\n- `style: Added no-console linting, cleaned up code`\n- `refactor: Split out dysfunctioner for testability`\n- `perf: Systematic dysfunctioner is now 2x faster`\n- `test: Added more tests for systematic dysfunctioner`\n- `chore: Upgraded yargs to 3.x.x`\n\nIf you want to use scopes then it would look more like: `feat(dysfunctioner): Added --quiet option`.\n\nYou can check if the commit message on your branch is formatted correctly by running this:\n\n    npm run changelog-lint\n\n### Releasing\n\nTo create a new release, do the following:\n\n- Pull from master to make sure you're up to date.\n- Bump the version in `package.json`.\n- Commit and push the version change (or create and merge a pull request for it).\n- Create a changelog by running `npm run changelog`. This will output Markdown of all unreleased changes.\n- Create a [new release](https://github.com/mozilla/sign-addon/releases/new) and paste in the changelog Markdown. It may require some manual editing. For example, some commit messages might have been truncated. Title the github release after the new version you just added in the previous commit to `package.json` (example: `1.0.4`).\n- When you publish the release, github creates a tag. When TravisCI builds the tag, it will automatically publish the package to [npm](https://www.npmjs.com/package/sign-addon).\n\n[eslint]: https://eslint.org/\n[prettier]: https://prettier.io/\n[typescript]: https://www.typescriptlang.org/\n"
},
{
  "name": "fx-private-relay",
  "files": {
    "/": [
      ".buildpacks",
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".env-dist",
      ".git-blame-ignore-revs",
      ".github",
      ".gitignore",
      ".gitmodules",
      "Dockerfile",
      "LICENSE",
      "METRICS.md",
      "Procfile",
      "README.md",
      "api",
      "docs",
      "emails",
      "frontend",
      "gunicorn.conf",
      "manage.py",
      "mypy_stubs",
      "package-lock.json",
      "package.json",
      "phones",
      "privaterelay",
      "pyproject.toml",
      "pytest.ini",
      "requirements.txt",
      "runtime.txt",
      "static",
      "tmp"
    ],
    "/docs": [
      "api_auth.md",
      "coding-standards.md",
      "end-to-end-local-dev.md",
      "frontend-architecture.md",
      "translations.md"
    ],
    "/.github": [
      "dependabot.yml",
      "pull_request_template.md",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Private Relay\nPrivate Relay provides generated email addresses to use in place of personal\nemail addresses.\n\nRecipients will still receive emails, but Private Relay keeps their personal\nemail address from being [harvested](https://blog.hubspot.com/marketing/what-is-a-landing-page-ht), \nand then [bought, sold, traded, or combined](https://www.bookyourdata.com/) \nwith  other data to personally identify, track, and/or [target\nthem](https://www.facebook.com/business/help/606443329504150?helpref=faq_content).\n\n## Development\n\nPlease refer to our [coding standards](docs/coding-standards.md) for code styles, naming conventions and other methodologies.\n\n### Requirements\n* python 3.9 (we recommend [virtualenv](https://docs.python-guide.org/dev/virtualenvs/))\n* PostgreSQL - even if you are using sqlite for development, requirements.txt installs\n  psycopg2 which [requires libpq](https://www.psycopg.org/docs/install.html#build-prerequisites) and Python header files.\n  The following should work:\n    * [On Windows](https://www.postgresql.org/download/windows/)\n    * On Ubuntu: `sudo apt install postgresql libpq-dev python3-dev`\n    * On OSX: `brew install postgresql libpq`\n    * On Fedora: `sudo dnf install libpq-devel python3-devel`\n* [SES](https://aws.amazon.com/ses/) if you want to send real emails\n* [Volta](https://volta.sh/) \u2013 Sets up the right versions of Node and npm, needed to compile the front-end\n\n### Install and Run the Site Locally\n\n1. Clone and change to the directory:\n\n    ```sh\n    git clone --recurse-submodules https://github.com/mozilla/fx-private-relay.git\n    cd fx-private-relay\n    ```\n\n2. Create and activate a virtual environment:\n\n    ```sh\n    virtualenv env\n    source env/bin/activate\n    ```\n\n3. Install Python and Node requirements:\n\n    ```sh\n    pip install -r requirements.txt\n    ```\n\n    ```sh\n    cd frontend\n    npm install\n    ```\n\n\n4. Copy `.env` file for\n   [`decouple`](https://pypi.org/project/python-decouple/) config:\n\n    ```sh\n    cp .env-dist .env\n    ```\n\n5. Add a `SECRET_KEY` value to `.env`:\n\n    ```ini\n    SECRET_KEY=secret-key-should-be-different-for-every-install\n    ```\n\n6. Migrate DB:\n\n    ```sh\n    python manage.py migrate\n    ```\n\n7. Create superuser:\n\n    ```sh\n    python manage.py createsuperuser\n    ```\n\n8. Run the backend:\n\n    ```sh\n    python manage.py runserver\n    ```\n\n    and in a different terminal, build the frontend:\n\n    ```sh\n    cd frontend\n    npm run watch\n    ```\n\n### Working with translations\nThe following docs will get you started with development, include creating new\nstrings to translate.  See [Translation and Localization](docs/translations.md)\nfor general information on Relay localization.\n\n#### Getting the latest translations\nWe use a [git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules)\nfor translated message files. The `--recurse-submodules` step of installation\nshould bring the message files into your working directory already, but you may\nwant also want to update the translations after install. The easiest way to do\nthat is:\n\n* `git submodule update --remote`\n\nTo update the submodule automatically when running `git pull` or other commands:\n\n* `git config --global submodule.recurse true`\n\n#### Add/update messages for translation\nThe `privaterelay/locales` directory is a git repository like any other, so to\nmake changes to the messages:\n\n1. Make whatever changes you need in `privaterelay/locales/en` as you work.\n\n2. `cd privaterelay/locales/en`\n\n3. `git branch message-updates-yyyymmdd`\n\n4. `git push -u origin message-updates-yyyymmdd`\n\nYou can then open a pull request from the `message-updates-yyyymmdd` branch to\n[the l10n repo](https://github.com/mozilla-l10n/fx-private-relay-l10n) `main` branch.\n\nIf you're not yet ready to submit some strings for translation, you can\ntentatively add them to `frontend/pendingTranslations.ftl`. Strings in that file\nwill show up until strings with the same ID are added to the l10n repository.\n\n#### Commit translations for release\nTo commit updates to the app's translations (e.g., before a release), we need\nto commit this submodule update. So, if the updated translations are ready to\nbe committed into the app, you can `git add` the submodule just like any other\nfile:\n\n* `git add privaterelay/locales`\n\nYou can then commit and push to set the app repository to the updated version\nof the translations submodule:\n\n* `git push`\n\nAn automated process updates the submodule daily, bringing in any new changes\nand translations from the Localization Team.\n\n### Recommended: Enable Firefox Accounts authentication\nTo enable Firefox Accounts authentication on your local server, you can use the\n\"Firefox Private Relay local dev\" OAuth app on accounts.stage.mozaws.net.\n\nTo do so:\n\n1. Set `ADMIN_ENABLED=True` in your `.env` file\n\n2. Shutdown the server if running, and add the admin tables with:\n\n    ```sh\n    python manage.py migrate\n    ```\n\n3. Run  the server, now with `/admin` endpoints:\n\n    ```sh\n    python manage.py runserver\n    ```\n\n4. Go to [the django admin page to change the default\n   site](http://127.0.0.1:8000/admin/sites/site/1/change/).\n\n5. Change `example.com` to `127.0.0.1:8000` and click Save.\n\n6. [Go to the django-allauth social app admin\n   page](http://127.0.0.1:8000/admin/socialaccount/socialapp/), sign in with the\n   superuser account you created above, and add a social app for Firefox Accounts:\n\n| Field | Value |\n|-------|-------|\n| Provider | Firefox Accounts |\n| Name | `accounts.stage.mozaws.net` |\n| Client id | `9ebfe2c2f9ea3c58` |\n| Secret key | Request this from `#fx-private-relay-eng` Slack channel |\n| Sites | `127.0.0.1:8000` -> Chosen sites |\n\nNow you can sign into [http://127.0.0.1:8000/](http://127.0.0.1:8000/) with an\nFxA.\n\n:warning: Remember that you'll need to use an account on https://accounts.stage.mozaws.net/, not\nthe production site, accounts.firefox.com.\n\n<!-- #### Optional: Enable SES\nTODO -->\n\n\n### Optional: Install and run the add-on locally\n\n*Note: The add-on is located in a [separate repo](https://github.com/mozilla/fx-private-relay-add-on/). See it for additional information on getting started.* \n\nThe add-on adds Firefox UI to generate and auto-fill email addresses across the web. Running the add-on locally allows it to communicate with your local server (`127.0.0.1:8000`) instead of the production server (`relay.firefox.com`).\n\n### Optional: Run a development server to compile the frontend\n\n`npm run watch` watches the `frontend/src` directory and builds the frontend\nwhen it detects changes. However, creating a production build is just time-consuming\nenough to interrupt your development flow. It is therefore also possible to run the\nfront-end on a separate server that only recompiles changed modules, and does not\napply production optimizations. To do so, instead of `npm run watch`, run\n`npm run dev`.\n\nThe frontend is now available at http://localhost:3000. Keep in mind that this\ndoes make your local development environment less similar to production; in\nparticular, authentication is normally bound to the backend server, and thus\nneeds to be simulated when running the frontend on a separate server. If\nyou make any changes related to authentication, make sure to test them using\n`npm run watch` as well.\n\n### Optional: Enable Premium Features\n\n**Note:** Premium features are automatically enabled for any user with an email address ending in\n`mozilla.com`, `getpocket.com`, or `mozillafoundation.org` (see `PREMIUM_DOMAINS` in\n`emails/models.py`). To mimic the customer's experience, it is recommended to follow the below\nprocedure.\n\nTo enable the premium Relay features, we integrate with the [FXA Subscription\nPlatform](https://mozilla.github.io/ecosystem-platform/docs/features/sub-plat/sub-plat-overview).\nAt a high level, to set up Relay premium subscription, we:\n\n1. [Enable Firefox Accounts Authentication](#recommended-enable-firefox-accounts-authentication) as described above.\n\n2. Create a product & price in our [Stripe dashboard](https://dashboard.stripe.com/).\n(Ask in #subscription-platform Slack channel to get access to our Stripe dashboard.)\n\n3. Link free users of Relay to the appropriate SubPlat purchase flow.\n\n4. Check users' FXA profile json for a `subscriptions` field to see if they can\n   access a premium, subscription-only feature.\n\nIn detail:\n\n1. [Enable Firefox Accounts Authentication](#recommended-enable-firefox-accounts-authentication) as described above.\n\n2. Go to our [Stripe dashboard](https://dashboard.stripe.com/).\n(Ask in #subscription-platform Slack channel to get access to our Stripe dashboard.)\n\n3. Create a new product in Stripe.\n\n4. Add all [required `product:` metadata](https://github.com/mozilla/fxa/blob/a0c7ac2b4bad0412a0f3a25fc82b5670922f8957/packages/fxa-auth-server/lib/routes/validators.js#L396-L437).\n   * Note: each piece of this metadata must have a `product:` prefix. So, for\n     example, `webIconURL` must be entered as `product:webIconURL`.\n\n5. Add `capabilities:` metadata.\n   * Note: Each piece of this metadata must have a format like\n     `capabilities:<fxa oauth client ID>`, and the value is a free-form string\n     to describe the \"capability\" that purchasing the subscription gives to the\n     user. E.g., `capabilities:9ebfe2c2f9ea3c58` with value of `premium-relay`.\n\n6. Set some env vars with values from the above steps:\n\n| Var | Value |\n|-------|-------|\n| `FXA_SUBSCRIPTIONS_URL` | `https://accounts.stage.mozaws.net/subscriptions` |\n| `PREMIUM_PROD_ID` | `prod_IyCWnXUbkYjDgL` (from Stripe)|\n| `PREMIUM_PRICE_ID` | `price_1IMG7KKb9q6OnNsL15Hsn1HE` (from Stripe)|\n| `SUBSCRIPTIONS_WITH_UNLIMITED` | `\"premium-relay\"` (match the `capabilities` value you used in Stripe)|\n\n### Optional: Debugging JavaScript bundle sizes\n\nIn `frontend/`, set `ANALYZE=true` when running `npm run build` to generate a\nreport detailing which modules are taking up most of the bundle size. A report\nwill be generated for both the client and server part of the frontend, but since\nwe only use the client, we're really only interested in that. The reports will\nautomatically open in your browser, and can also be found in\n`/frontend/.next/analyze/`.\n\n```sh\nANALYZE=true npm run build\n```\n\n#### Test Premium\n\nThere is a [comprehensive doc of test\ncases](https://docs.google.com/spreadsheets/d/1fMl4LHr1kIuGHfS9jyhLrv5vAyJMBUCr2AP0sODFmJw/edit#gid=0) for purchasing premium Relay.\n\nYou can use [Stripe's test credit card details](https://stripe.com/docs/testing#cards) for payment.\n\n## Production Environments\n\n### Requirements\nIn addition to the requirements for dev, production environments should use:\n\n* [PostgreSQL](https://www.postgresql.org/)-compatible DB\n\n### Environment Variables\nProduction environments should also set some additional environment variables:\n\n```\nDATABASE_URL=postgresql://<username>:<password>@<host>:<port>/<database>\nDJANGO_SECURE_HSTS_SECONDS=15768000\nDJANGO_SECURE_SSL_REDIRECT=True\n```\n"
},
{
  "name": "addons-scanner-utils",
  "files": {
    "/": [
      ".circleci",
      ".eslintrc",
      ".github",
      ".gitignore",
      ".npmignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "LICENSE.txt",
      "README.md",
      "jest.config.js",
      "package.json",
      "renovate.json",
      "src",
      "tsconfig.json",
      "yarn.lock"
    ],
    "/.github": [
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# addons-scanner-utils\n\n[![CircleCI](https://circleci.com/gh/mozilla/addons-scanner-utils.svg?style=svg)](https://circleci.com/gh/mozilla/addons-scanner-utils) [![codecov](https://codecov.io/gh/mozilla/addons-scanner-utils/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/addons-scanner-utils) [![npm version](https://badge.fury.io/js/addons-scanner-utils.svg)](https://badge.fury.io/js/addons-scanner-utils)\n\nVarious addons related helpers to build scanners.\n\n## Usage\n\n```\nyarn add addons-scanner-utils\n```\n\n## Requirements\n\n- You need [Node](https://nodejs.org/) 14, which is the current [LTS](https://github.com/nodejs/LTS) (long term support) release.\n- You need [yarn](https://yarnpkg.com/en/) to manage dependencies and run commands.\n\n## Development\n\n- Read [our contributing guidelines](./CONTRIBUTING.md) to get started on your first patch\n- Clone this repository\n- Type `yarn` to install everything\n- Run the test suite to make sure everything is set up: `yarn test`\n\n### Available development commands\n\nIn the project directory, you can run the following commands. There are a few commands not mentioned here (see `package.json`) but those are only used by internal processes.\n\n#### `yarn eslint`\n\nThis runs [ESLint][] to discover problems within our codebase without executing it. ESLint also enforces some patterns and practices.\n\n#### `yarn lint`\n\nThis runs all the _lint_ commands at once.\n\n#### `yarn prettier`\n\nThis runs [Prettier][] to automatically format the entire codebase.\n\n#### `yarn prettier-dev`\n\nThis runs [Prettier][] on only your changed files. This is intended for development.\n\n#### `yarn test`\n\nThis launches [Jest][] in the interactive watch mode.\n\n### Prettier\n\nWe use [Prettier][] to automatically format our JavaScript code and stop all the on-going debates over styles. As a developer, you have to run it (with `yarn prettier-dev`) before submitting a Pull Request.\n\n### Versioning\n\nThis project follows the [semantic versioning](https://semver.org/) specification.\n\nIn order to release a new version, one has to run the [`npm version`](https://docs.npmjs.com/cli/version) command with one of the following arguments: `minor`, `patch` or `major` (less frequent). This command (1) updates the `version` in `package.json`, (2) create a new commit for the release and (3) make a `git` tag.\n\n[eslint]: https://eslint.org/\n[jest]: https://jestjs.io/\n[prettier]: https://prettier.io/\n"
},
{
  "name": "eslint-plugin-amo",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE.txt",
      "README.md",
      "bin",
      "lib",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# eslint-plugin-amo\n\n[![CircleCI](https://circleci.com/gh/mozilla/eslint-plugin-amo.svg?style=svg)](https://circleci.com/gh/mozilla/eslint-plugin-amo) [![npm version](https://badge.fury.io/js/eslint-plugin-amo.svg)](https://badge.fury.io/js/eslint-plugin-amo)\n\nESLint plugin for [AMO](https://wiki.mozilla.org/AMO).\n\n## Installation\n\nYou'll first need to install [ESLint](http://eslint.org):\n\n```\n$ npm i eslint --save-dev\n```\n\nNext, install `eslint-plugin-amo`:\n\n```\n$ npm install eslint-plugin-amo --save-dev\n```\n\n**Note:** If you installed ESLint globally (using the `-g` flag) then you must also install `eslint-plugin-amo` globally.\n\n## Usage\n\nAdd `amo` to the plugins section of your `.eslintrc` configuration file. You can omit the `eslint-plugin-` prefix:\n\n```json\n{\n  \"plugins\": [\"amo\"]\n}\n```\n\nThen configure the rules you want to use under the rules section.\n\n```json\n{\n  \"rules\": {\n    \"amo/rule-name\": 2\n  }\n}\n```\n\nAlternatively, you can use the `recommended` preset to get reasonable defaults:\n\n```json\n{\n  \"extends\": [\"plugin:amo/recommended\"]\n}\n```\n\n### TypeScript\n\nYou can use the `typescript` preset to get reasonable defaults (it includes the `recommended` rules) as well as TypeScript specific rules:\n\n```json\n{\n  \"extends\": [\"plugin:amo/typescript\"]\n}\n```\n\n## Rules\n\n<!-- THIS SECTION IS AUTOMATICALLY GENERATED, PLEASE RUN: `npm run build-doc` -->\n\n<!--DOC_START-->\n\n- [`dangerously-set-inner-html`](#dangerously-set-inner-html)\n- [`describe-with-filename`](#describe-with-filename)\n- [`i18n-no-tagged-templates`](#i18n-no-tagged-templates)\n- [`no-sinon-assert-called-if-called-with`](#no-sinon-assert-called-if-called-with)\n- [`one-top-level-describe-per-test`](#one-top-level-describe-per-test)\n- [`only-log-strings`](#only-log-strings)\n- [`only-tsx-files`](#only-tsx-files)\n- [`redux-app-state`](#redux-app-state)\n- [`sort-destructured-props`](#sort-destructured-props)\n- [`with-router-hoc-first`](#with-router-hoc-first)\n\n### `dangerously-set-inner-html`\n\nEnsure `dangerouslySetInnerHTML` is used on elements that permit flow content:\n\n```js\n// BAD\n<p dangerouslySetInnerHTML={sanitizeUserHTML(content)} />\n\n// GOOD\n<div dangerouslySetInnerHTML={sanitizeUserHTML(content)} />\n```\n\n### `describe-with-filename`\n\nEnsure the top-level `describe` block has `__filename` as description:\n\n```js\n// BAD\ndescribe('foo', () => {});\n\n// GOOD\ndescribe(__filename, () => {});\n```\n\n:wrench: Use the ESLint `--fix` option on the command line to automatically fixes problems reported by this rule.\n\n:bulb: We enforce this rule because of the following issue: https://github.com/mozilla/addons-frontend/issues/2928.\n\n### `i18n-no-tagged-templates`\n\nEnsure no template literal tags are passed to i18n methods:\n\n```js\n// BAD\ni18n.gettext(tag`translated string`);\n\n// GOOD\ni18n.gettext('hello');\n```\n\n:wrench: Use the ESLint `--fix` option on the command line to automatically fixes problems reported by this rule.\n\n:bulb: We enforce this rule because of the following issue: https://github.com/mozilla/addons-frontend/issues/2108.\n\n### `no-sinon-assert-called-if-called-with`\n\nEnsure `sinon.assert.called()` is absent when `sinon.assert.calledWith()` is used:\n\n```js\n// BAD\nit('description', () => {\n  sinon.assert.called(stub);\n  sinon.assert.calledWith(stub, params);\n});\n\n// GOOD\nit('description', () => {\n  sinon.assert.calledWith(stub, params);\n});\n```\n\n:bulb: We enforce this rule because of the following issue: https://github.com/mozilla/addons-frontend/issues/2437.\n\n### `one-top-level-describe-per-test`\n\nEnsure there is a single top-level `describe` block per test file:\n\n```js\n// BAD\ndescribe('foo', () => {});\ndescribe('bar', () => {});\n\n// GOOD\ndescribe(__filename, () => {\n  describe('foo', () => {});\n  describe('bar', () => {});\n});\n```\n\n### `only-log-strings`\n\nEnsure we do not log full objects:\n\n```js\n// BAD\nlog.info('response:', response);\n\n// GOOD\nlog.info('this is a log message');\nlog.debug(oneLine`A very long string message`);\n_log.warn(`request ID: ${requestId}`);\n```\n\n:triangular_ruler: This rule can be configured with the following **options**:\n\n| Name      | Type  | Description                                         |\n| --------- | ----- | --------------------------------------------------- |\n| `methods` | array | A list of logger methods, e.g., `info` or `debug`.  |\n| `objects` | array | A list of logger objects, e.g., `log` or `console`. |\n\n:bulb: We enforce this rule because of the following issue: https://github.com/mozilla/addons-frontend/issues/6512.\n\n### `only-tsx-files`\n\nEnforce `.tsx` file extensions (definition files are ignored by this rule):\n\n- \u26d4\ufe0f `src/api/index.ts`\n- \u2705 `src/api/index.tsx`\n\n:bulb: We enforce this rule because of the following issue: https://github.com/mozilla/addons-code-manager/issues/75.\n\n### `redux-app-state`\n\nEnsure the `AppState` Flow type is used on `state` arguments:\n\n```js\n// BAD\nconst mapStateToProps = (state: Object) => {};\n\n// GOOD\nconst mapStateToProps = (state: AppState) => {};\n```\n\n:bulb: We enforce this rule because of the following issue: https://github.com/mozilla/addons-frontend/issues/4058.\n\n### `sort-destructured-props`\n\nEnsure destructured props are sorted:\n\n```js\n// BAD\nconst { a, _c, b, Component, ...otherProps } = this.props;\n\n// GOOD\nconst { Component, _c, a, b, ...otherProps } = this.props;\n```\n\n:wrench: Use the ESLint `--fix` option on the command line to automatically fixes problems reported by this rule.\n\n:warning: This rule is not part of the `recommended` preset.\n\n### `with-router-hoc-first`\n\nEnsures the `withRouter` HOC is the first in `compose()`:\n\n```js\n// BAD\ncompose(\n  connect(mapStateToProps),\n  withRouter\n)(MyComponent);\n\n// GOOD\ncompose(\n  withRouter,\n  connect(mapStateToProps)\n)(MyComponent);\n```\n\n<!--DOC_END-->\n\n## Contributing\n\nInstall the project dependencies:\n\n```\nnpm install\n```\n\nRun the test suite:\n\n```\nnpm test\n```\n\nNew rules can be added with the `npm run new-rule` command:\n\n```\nnpm run new-rule\n```\n\nThis command will ask a few questions and generate the source and test files.\n\nThe \"Rules\" documentation section is automatically generated with:\n\n```\nnpm run build-doc\n```\n\nFor further information, please see the [CONTRIBUTING.md](./CONTRIBUTING.md) file.\n\n## License\n\neslint-plugin-amo is released under the Mozilla Public License Version 2.0. See the bundled [LICENSE](./LICENSE.txt) file for details.\n"
},
{
  "name": "neqo",
  "files": {
    "/": [
      ".gitattributes",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "clippy.toml",
      "docker",
      "docs",
      "hooks",
      "neqo-client",
      "neqo-common",
      "neqo-crypto",
      "neqo-http3",
      "neqo-interop",
      "neqo-qpack",
      "neqo-server",
      "neqo-transport",
      "neqo.png",
      "neqo.svg",
      "qns",
      "test-fixture"
    ],
    "/docs": [
      "linux_build.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Neqo, an Implementation of QUIC written in Rust\n\n![neqo logo](https://github.com/mozilla/neqo/raw/main/neqo.png \"neqo logo\")\n\nTo run test HTTP/3 programs (neqo-client and neqo-server):\n\n* `cargo build`\n* `./target/debug/neqo-server [::]:12345 --db ./test-fixture/db`\n* `./target/debug/neqo-client http://127.0.0.1:12345/`\n\n## Faster Builds with Separate NSS/NSPR\n\nYou can clone NSS (https://hg.mozilla.org/projects/nss) and NSPR\n(https://hg.mozilla.org/projects/nspr) into the same directory and export an\nenvironment variable called `NSS_DIR` pointing to NSS.  This causes the build to\nuse the existing NSS checkout.  However, in order to run anything that depends\non NSS, you need to set `$\\[DY]LD\\_LIBRARY\\_PATH` to point to\n`$NSS_DIR/../dist/Debug/lib`.\n\nNote: If you did not compile NSS separately, you need to have mercurial (hg), installed.\nNSS builds require gyp, and ninja (or ninja-build) to be present also.\n\n## Debugging Neqo\n\n### Using SSLKEYLOGFILE to decrypt Wireshark logs\n\n[Info here](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/Key_Log_Format)\n\nTODO: What is the minimum Wireshark version needed?\nTODO: Above link may be incorrect, protocol now called TLS instead of SSL?\n\n### Using RUST_LOG effectively\n\nAs documented in the [env_logger documentation](https://docs.rs/env_logger/),\nthe `RUST_LOG` environment variable can be used to selectively enable log messages\nfrom Rust code. This works for Neqo's cmdline tools, as well as for when Neqo is\nincorporated into Gecko, although [Gecko needs to be built in debug mode](https://developer.mozilla.org/en-US/docs/Mozilla/Developer_guide/Build_Instructions/Configuring_Build_Options).\n\nSome examples:\n1. `RUST_LOG=neqo_transport::dump ./mach run` lists sent and received QUIC\n   packets and their frames' contents only.\n1. `RUST_LOG=neqo_transport=debug,neqo_http3=trace,info ./mach run` sets a\n   'debug' log level for transport, 'trace' level for http3, and 'info' log\n   level for all other Rust crates, both Neqo and others used by Gecko.\n1. `RUST_LOG=neqo=trace,error ./mach run` sets `trace` level for all modules\n   starting with \"neqo\", and sets `error` as minimum log level for other\n   unrelated Rust log messages.\n\n\n### Trying In-development Neqo code in Gecko\n\nIn a checked-out copy of Gecko source, set paths for the four Neqo crates to\nlocal versions in `netwerk/socket/neqo_glue/Cargo.toml`. For example, if Neqo\nwas checked out to /home/alice/git/neqo, change:\n\n```\nneqo-http3 = { tag = \"v0.1.7\", git = \"https://github.com/mozilla/neqo\" }\nneqo-transport = { tag = \"v0.1.7\", git = \"https://github.com/mozilla/neqo\" }\nneqo-common = { tag = \"v0.1.7\", git = \"https://github.com/mozilla/neqo\" }\n```\n\nto\n\n```\nneqo-http3 = { path = \"/home/alice/git/neqo/neqo-http3\" }\nneqo-transport = { path = \"/home/alice/git/neqo/neqo-transport\" }\nneqo-common = { path = \"/home/alice/git/neqo/neqo-common\" }\n```\n\nand\n\n```\n[dependencies.neqo-crypto]\ntag = \"v0.1.7\"\ngit = \"https://github.com/mozilla/neqo\"\ndefault-features = false\nfeatures = [\"gecko\"]\n```\n\nto\n\n```\n[dependencies.neqo-crypto]\npath = \"/home/alice/git/neqo/neqo-crypto\"\ndefault-features = false\nfeatures = [\"gecko\"]\n```\n\nNote: Using newer Neqo code with Gecko may also require changes (likely to `neqo_glue`) if\nsomething has changed.\n\nCompile Gecko as usual with `./mach build`.\n"
},
{
  "name": "wpt-sync",
  "files": {
    "/": [
      ".bandit",
      ".coveragerc",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".pyup.yml",
      "00-startup.py",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "ansible",
      "bin",
      "config",
      "docker",
      "docs",
      "requirements",
      "setup.cfg",
      "setup.py",
      "sync",
      "sync_prod.ini",
      "test"
    ],
    "/docs": [
      "behaviour.md",
      "deployment.md",
      "user-guide.md"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# wpt-sync\n\n[![Build Status](https://travis-ci.org/mozilla/wpt-sync.svg?branch=master)](https://travis-ci.org/mozilla/wpt-sync)\n\n## Description\n\nSynchronize changes between gecko and web-platform-tests\n\n## Additional documentation\n\nSee `./docs`\n\n## Development environment\n\nSetting up a development environment\nrequires [Docker](https://www.docker.com/).\n\nWe're _somewhat_ following [mozilla-services' Dockerflow](https://github.com/mozilla-services/Dockerflow).\n\nIf you are on MacOS and have brew installed you can do\n\n```\nbrew cask install docker\n```\n\n### Prerequisites\n\nDepending on what command you want to run, (e.g. wptsync listen), you\nmay need to provide custom configration files at docker run-time by\nplacing them in locations that are bind-mounted to the container and\nsetting container environment variables. See `Dockerfile.dev` and\n`start_wptsync.sh`\n\nYou can include these customizations in a shell script or a docker-compose.yml:\n\n*   There's a dev-env convenience script for building the docker image and\n    running the wptsync command in a corresponding container: `./bin/\n    run_docker_dev.sh`. It is similar to the script we use in production.\n\n### Quick Setup\n\nThe following setup assumes you have already installed Docker and have it running.\nSee Development Environment above.\n\n```\n./bin/run_docker_dev.sh build\n```\nThis will setup the container and make sure the relevant configuration files are in the right place.\nYou will be asked to enter in a passphrase when it creates development ssh keys, press enter\nto leave these blank.\n\nTo run tests do the following\n\n```\n./bin/run_docker_dev.sh test\n```\n\n### Configuration\n\nDevelopment configuration files are checked in to `config/` and\n`config/dev`. `config/dev/ssh` contains (usually non-functional) ssh keys\nand is not checked in to the repository.\n\nFor production, configuration goes in `config/prod`, following the\nlayout of `config/dev`. This is never committed to the repository. See\n`docs/deployment.md` for more information.\n\n### Raw docker commands\n\nFrom repo root:\n\nTo build an image called `wptsync_dev` with the repo root as the build context:\n\n```\ndocker build -t wptsync_dev --add-host=rabbitmq:127.0.0.1 --file wpt-sync/docker/Dockerfile.dev .\n```\n\n\nTo start all the services in the container:\n\n```\nexec docker run -it --add-host=rabbitmq:127.0.0.1 \\\n  --env WPTSYNC_CONFIG=${WPTSYNC_CONFIG:-/app/config/dev/sync.ini} \\\n  --env WPTSYNC_CREDS=${WPTSYNC_CREDS:-/app/config/dev/credentials.ini} \\\n  --env WPTSYNC_GECKO_CONFIG=${WPTSYNC_GECKO_CONFIG:/app/config/gecko_config} \\\n  --env WPTSYNC_WPT_CONFIG=${WPTSYNC_WPT_CONFIG:/app/config/wpt_config} \\\n  --env WPTSYNC_GH_SSH_KEY=${WPTSYNC_GH_SSH_KEY:/app/config/dev/ssh/id_github} \\\n  --env WPTSYNC_HGMO_SSH_KEY=${WPTSYNC_HGMO_SSH_KEY:/app/config/dev/ssh/id_hgmo} \\\n  --mount type=bind,source=$(pwd)/config,target=/app/config \\\n  --mount type=bind,source=$(pwd)/sync,target=/app/wpt-sync/sync \\\n  --mount type=bind,source=$(pwd)/test,target=/app/wpt-sync/test \\\n  --mount type=bind,source=$(pwd)/repos,target=/app/repos \\\n  --mount type=bind,source=$(pwd)/workspace,target=/app/workspace \\\n  wptsync_dev\n```\n\nThis runs the script designated by ENTRYPOINT in the Dockerfile with an `init`process. You could use `--env-file` instead of `--env` to set environment variables in the container.\n\nStop it with:\n\n```\ndocker stop [container name]\n```\n\nYou can see names of running containers with `docker container ls`.\n\nIf you want to run a different command in the container\ninteractively, use the `-it` and `--entrypoint` options like:\n\n\n```\ndocker run -it --env WPTSYNC_REPO_ROOT=/app/wpt-sync/test/testdata \\\n    --mount type=bind,source=$(pwd),target=/app/wpt-sync \\\n    --entrypoint \"some_command\" wptsync_dev\n```\n\nYou can pass additional flags to the entrypoint after the `wptsync_dev` part, like `... --entrypoint \"some_command\" wptsync_dev -x`\n\n### Volumes to --mount\n\nSee the VOLUMES directive in the Dockerfile for information about what\nvolumes it's expecting.\n\n### Permissions\n\nInside the Docker container we run as the app user with uid 10001. This user\nrequires write permissions to directories `repos`, `workspace`, and\n`config/dev/ssh`.\n\nIf you're on Linux, for each path run\n\n```\nsudo chown -R 10001 <path>\n```\n\nYou may not need to do this at all on mac.\n\n__Note__ that replacing the default entry point means that you're no longer running the `start_wptsync.sh` script at container start-up and therefore some\nconfiguration may be missing or incomplete. For example, the Dockerfile (build-time) doesn't set up any credentials; instead, credentials are only set up in the container at run-time with the above-mentioned script.\n"
},
{
  "name": "eslint-config-amo",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".npmignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "base.js",
      "index.js",
      "package.json",
      "renovate.json",
      "tests",
      "yarn.lock"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# eslint-config-amo\n\n[![CircleCI](https://circleci.com/gh/mozilla/eslint-config-amo.svg?style=svg)](https://circleci.com/gh/mozilla/eslint-config-amo) [![npm version](https://badge.fury.io/js/eslint-config-amo.svg)](https://badge.fury.io/js/eslint-config-amo)\n\nShared eslint config for all amo related JS projects.\n\n## Versioning\n\nThis project follows the [semantic versioning](https://semver.org/) specification.\n\nIn order to release a new version, please follow these steps:\n\n1. Make sure your local `master` branch is up-to-date.\n2. Run the following command from the `master` branch to (1) bump the version accordingly and (2) create a commit and a tag for the release:\n\n   ```\n   npm version minor\n   ```\n\n   Note: change `minor` to `patch` or `major` depending on the release (but `minor` is the most frequent).\n\n3. Push to the main repository (denoted `upstream` below):\n\n   ```\n   git push upstream master --tag\n   ```\n\n4. Create a GitHub Release for the new tag: https://github.com/mozilla/eslint-config-amo/releases (you can either select the tag itself or create a new \"draft\"). Please add a short description of the changes (see the other releases for inspiration).\n"
},
{
  "name": "stab-crashes",
  "files": {
    "/": [
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".prettierrc",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "addon_related_signatures.html",
      "addon_related_signatures.js",
      "all_missing_uplifts.html",
      "all_missing_uplifts.js",
      "beta-stability-pushlog.html",
      "beta-stability-pushlog.js",
      "buildid_changeset.html",
      "buildid_changeset.js",
      "buildid_changeset_page.js",
      "channels_diff.html",
      "channels_diff.js",
      "common_landings.html",
      "common_landings.js",
      "compare-betas.html",
      "compare-betas.js",
      "correlations.html",
      "correlations.js",
      "correlations_page.js",
      "exclamation_mark.svg",
      "generate-data.py",
      "gfx_critical_errors.py",
      "graphics_critical_errors.html",
      "graphics_critical_errors.js",
      "images",
      "index.html",
      "missing_uplifts.html",
      "missing_uplifts.js",
      "question_mark.svg",
      "requirements.txt",
      "rerank.html",
      "rerank.js",
      "rocket_fly.png",
      "run.sh",
      "scomp.css",
      "scomp.html",
      "scomp.js",
      "spin.svg",
      "style.css",
      "supergraph.html",
      "supergraph.js",
      "taskcluster-hook.json",
      "taskcluster_get_secret.py",
      "test-requirements.txt",
      "tox.ini",
      "versions_util.py"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# stab-crashes\n\nStability dashboard.\n"
},
{
  "name": "pino-mozlog",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".gitignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "README.md",
      "__tests__",
      "index.js",
      "mozlog-schema.json",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "src"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# pino-mozlog\n\n[![CircleCI](https://circleci.com/gh/mozilla/pino-mozlog.svg?style=svg)](https://circleci.com/gh/mozilla/pino-mozlog) [![npm version](https://badge.fury.io/js/pino-mozlog.svg)](https://badge.fury.io/js/pino-mozlog)\n\nA transport for transforming [pino logs](https://github.com/pinojs) into [mozlog](https://wiki.mozilla.org/Firefox/Services/Logging#MozLog_application_logging_standard).\n\n## Installation\n\n```\n$ npm i pino-mozlog\n```\n\n## Usage\n\nThis transport only reformats the logs into mozlog compatible strings:\n\n```\n$ node your-app.js | pino-mozlog\n```\n\nYou can specify the mozlog type:\n\n```\n$ node your-proxy.js | pino-mozlog --type proxy\n```\n\nBy default, errors are sent to `stderr` (not mozlog-compliant though). You can disable this behavior and ignore all errors with `--silent`:\n\n```\n$ node your-app.js | pino-mozlog --silent\n```\n\n## License\n\npino-mozlog is released under the Mozilla Public License Version 2.0. See the bundled [LICENSE](./LICENSE.txt) file for details.\n"
},
{
  "name": "perf-triage",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE",
      "README.md",
      "docs",
      "requirements.txt",
      "rotation.py",
      "rotations.pickle",
      "send-reminder.py"
    ],
    "/docs": [
      "index.html"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# perf-triage\nTools used for triaging performance bugs.\n\n### send-reminder.py\nNote: this script is a WIP and we're working on integrating it with the other\nscripts.\n\nThe first time you run, you'll need to create a virtualenv and install the\ndependencies:\n```sh\n# Create virtualenv\npython3 -m venv venv\n\n# Activate virtualenv\nsource venv/bin/activate\n\n# Install dependencies\npip install -r requirements.txt\n```\n\nSee `send-reminder.py` for details on running.\n"
},
{
  "name": "ecosystem-platform",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "PR_TEMPLATE.md",
      "README.md",
      "babel.config.js",
      "docs",
      "docusaurus.config.js",
      "package.json",
      "sidebars.js",
      "src",
      "static",
      "tsconfig.json",
      "yarn.lock"
    ],
    "/docs": [
      "additional-docs.md",
      "assets",
      "explanation",
      "how-tos",
      "intro.md",
      "reference",
      "relying-parties",
      "tutorials"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "## Firefox Ecosystem Platform Docs\n\nThis repo hosts the source code and tooling for the [Firefox Ecosystem\nPlatform](https://mozilla.github.io/ecosystem-platform/) documentation portal.\n\n### Scope and Target Audience\n\nThe docs in this repository are intended for engineers, designers or product managers working on\napplications in the Firefox Ecosystem.\n\nOur aim is to help them understand *how* and *why* to incorporate Account-related features into\ntheir application, in a way that makes the Firefox family of products feel like a cohesive\ncross-app and cross-device experience for users.\n\n### Working on the Docs\n\nThe site is build using [Docusaurus](https://docusaurus.io/en/) and is automatically\ndeployed from master to GitHub Pages using CircleCI. To build and run it locally you can:\n\n```\n$> yarn install\n$> yarn start\n```\n\nThat should open a new browser window automatically, or you can manually browse\nto http://localhost:3000/ecosystem-platform/ to view the docs.\n"
},
{
  "name": "remote-settings",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".flake8",
      ".github",
      ".gitignore",
      ".readthedocs.yaml",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.rst",
      "VERSION",
      "app.wsgi",
      "bin",
      "config",
      "docker-compose.yml",
      "docs",
      "kinto-remote-settings",
      "pyproject.toml",
      "requirements-dev.txt",
      "requirements.in",
      "requirements.txt",
      "tests"
    ],
    "/docs": [
      "Makefile",
      "README.md",
      "case-studies.rst",
      "conf.py",
      "getting-started.rst",
      "index.rst",
      "introduction.rst",
      "requirements.txt",
      "screencasts.rst",
      "screencasts",
      "support.rst",
      "target-filters.rst",
      "tutorial-attachments.rst",
      "tutorial-dev-kinto-admin.rst",
      "tutorial-dev-server.rst",
      "tutorial-local-server.rst",
      "tutorial-multi-signoff.rst",
      "tutorial-normandy-integration.rst"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "VENV := $(shell echo $${VIRTUAL_ENV-.venv})\nINSTALL_STAMP := $(VENV)/.install.stamp\nDOC_STAMP := $(VENV)/.doc.install.stamp\nSPHINX_BUILDDIR = docs/_build\nPSQL_INSTALLED := $(shell psql --version 2>/dev/null)\nVOLUMES_FOLDERS := autograph-certs mail\n\nclean:\n\tfind . -name '*.pyc' -delete\n\tfind . -name '__pycache__' -type d | xargs rm -rf\n\ndistclean: clean\n\trm -rf *.egg *.egg-info/ dist/ build/\n\nmaintainer-clean: distclean\n\tdeactivate ; rm -rf .venv/\n\trm -rf .pytest_cache\n\trm -rf tests/.pytest_cache\n\tfind . -name '*.orig' -delete\n\tdocker-compose stop\n\tdocker-compose rm -f\n\tRS_DB_DATA_VOL=$$(docker volume ls -q -f name=\"rs-db-data\") ;\\\n\t[ -z \"$$RS_DB_DATA_VOL\" ] && docker volume rm -f $$RS_DB_DATA_VOL ;\\\n\trm -rf $(VOLUMES_FOLDERS)\n\n$(VENV)/bin/python:\n\tpython3 -m venv $(VENV)\n\n$(INSTALL_STAMP): $(VENV)/bin/python requirements.txt requirements-dev.txt\n\t$(VENV)/bin/python -m pip install --upgrade pip wheel setuptools\n\t$(VENV)/bin/pip install -r requirements.txt\n\t$(VENV)/bin/pip install -e kinto-remote-settings\n\t$(VENV)/bin/pip install -r requirements-dev.txt\n\ttouch $(INSTALL_STAMP)\n\nformat: $(INSTALL_STAMP)\n\t$(VENV)/bin/isort . --virtual-env=$(VENV)\n\t$(VENV)/bin/black kinto-remote-settings tests\n\nlint: $(INSTALL_STAMP)\n\t$(VENV)/bin/isort . --check-only --virtual-env=$(VENV)\n\t$(VENV)/bin/black --check kinto-remote-settings tests --diff\n\t$(VENV)/bin/flake8 kinto-remote-settings tests\n\ntest: $(INSTALL_STAMP)\n\tPYTHONPATH=. $(VENV)/bin/pytest kinto-remote-settings\n\nintegration-test:\n\tmkdir -p -m 777 $(VOLUMES_FOLDERS)\n\tdocker-compose run --rm web migrate\n\tdocker-compose run --rm tests integration-test\n\nbrowser-test:\n\tmkdir -p -m 777 $(VOLUMES_FOLDERS)\n\tdocker-compose run --rm web migrate\n\tdocker-compose run --rm tests browser-test\n\nbuild:\n\tdocker-compose build\n\nbuild-db:\nifdef PSQL_INSTALLED\n\t@pg_isready 2>/dev/null 1>&2 || (echo Run PostgreSQL before starting tests. && exit 1)\n\t@echo Creating db...\n\t@psql -tc \"SELECT 1 FROM pg_database WHERE datname = 'testdb'\" -U postgres -h localhost | grep -q 1 || psql -c \"CREATE DATABASE testdb ENCODING 'UTF8' TEMPLATE template0;\" -U postgres -h localhost\n\t@psql -c \"ALTER DATABASE testdb SET TIMEZONE TO UTC;\"\n\t@echo Done!\nelse\n\t@echo PostgreSQL not installed. Please install PostgreSQL to use this command.\nendif\n\nstart:\n\tmake build\n\tdocker-compose up\n\nstop:\n\tdocker-compose stop\n\ndown:\n\tdocker-compose down\n\ninstall-docs: $(DOC_STAMP)\n$(DOC_STAMP): $(VENV)/bin/python docs/requirements.txt\n\t$(VENV)/bin/pip install -Ur docs/requirements.txt\n\ttouch $(DOC_STAMP)\n\ndocs: install-docs\n\t$(VENV)/bin/sphinx-build -a -W -n -b html -d $(SPHINX_BUILDDIR)/doctrees docs $(SPHINX_BUILDDIR)/html\n\t@echo\n\t@echo \"Build finished. The HTML pages are in $(SPHINX_BUILDDIR)/html/index.html\"\n\n",
  "readme": "Remote Settings\n===============\n\nRemote Settings is a Mozilla service that makes it easy to manage evergreen settings data in Firefox. A simple API is available in Firefox for accessing the synchronized data.\n\nhttps://remote-settings.readthedocs.io\n--------------------------------------\n\n.. image:: https://circleci.com/gh/mozilla/remote-settings/tree/main.svg?style=svg\n   :target: https://circleci.com/gh/mozilla/remote-settings\n\n\nContent\n-------\n\nThis *Remote Settings* repository contains the following:\n\n* ``bin/``: container entry point and script(s)\n* ``config/``: example configuration file(s)\n* ``docs/``: documentation source files\n* ``kinto-remote-settings/``: Kinto plugin specific to Remote Settings\n* ``tests/``: browser and integration tests\n* ``requirements.in``: Python packages for the service (source of truth for ``requirements.txt``)\n* ``requirements-dev.txt``: Python packages for local development and tests\n* ``VERSION``: SemVer version number that serves as both the version of the service and the ``kinto-remote-settings`` plugin\n\n\nRun\n---\n\nYou need Docker and ``docker-compose``. Ensure `buildkit <https://docs.docker.com/develop/develop-images/build_enhancements/>`_ is enabled on your Docker engine.\n\n.. code-block:: shell\n\n    make start\n\nYour *Remote Settings* instance is now ready at http://localhost:8888. See the `*Setup a Local Server* <https://remote-settings.readthedocs.io/en/latest/tutorial-local-server.html>`_ tutorial for more details.\n\n\nTest Locally\n------------\n\n**Kinto Remote Settings Unit Tests**\n\nTo run unit tests, you need Postgres installed and a database ``testdb`` available. This can be created with:\n\n.. code-block:: shell\n\n    make build-db\n\nAfter this setup is complete, tests can be run with ``pytest`` using ``make``:\n\n.. code-block:: shell\n\n    make test\n\n\n**Integration & Browser Tests**\n\nWith Docker and docker-compose, test that all components are working as expected with:\n\n.. code-block:: shell\n\n    make build\n    make integration-test\n    make browser-test\n\n.. note::\n\n    The ``docker-compose run web migrate`` command is only needed once, to prime the\n    PostgreSQL server (this is done automatically for you in the make command).\n    You can flush all the Kinto data in your local persistent PostgreSQL with\n    ``curl -XPOST http://localhost:8888/v1/__flush__``\n\nThat will start ``memcached``, ``postgresql``, ``autograph`` and Kinto (at ``web:8888``)\nand lastly the ``tests`` container that primarily\nuses ``pytest`` to test various things against ``http://web:8888/v1``.\n\nWhen you're done running the above command, the individual servers will still\nbe running and occupying those ports on your local network. When you're\nfinished, run:\n\n.. code-block:: shell\n\n    make stop\n\n\nTest Remote Server\n------------------\n\nIntegration tests can be executed on a remote server.\n\n.. code-block:: shell\n\n    docker-compose build tests\n\n.. code-block:: shell\n\n    docker-compose run \\\n        --env SERVER=https://settings.dev.mozaws.net/v1 \\\n        --env MAIL_DIR=\"\" `# disable tests about emails.` \\\n        --env SKIP_SERVER_SETUP=true \\\n        --env EDITOR_AUTH=editor:azerty123 \\\n        --env REVIEWER_AUTH=reviwer:s3cr3t \\\n        tests integration-test\n\n\nDebugging Locally (simple)\n--------------------------\n\nThe simplest form of debugging is to run a suite of tests against the kinto server:\n\n.. code-block:: shell\n\n    make integration-test\n    make browser-test\n\nDebugging Locally (advanced)\n----------------------------\n\nSuppose you want to play with running the Kinto server, then go into\na ``bash`` session like this:\n\n.. code-block:: shell\n\n    docker-compose run --service-ports --user 0 web bash\n\nNow you're ``root`` so you can do things like ``apt-get update && apt-get install jed``\nto install tools and editors. Also, because of the ``--service-ports`` if you do\nstart a Kinto server on ``:8888`` it will be exposed from the host.\n\nFor example, instead of starting Kinto with ``uwsgi`` you can start it\nmanually with ``kinto start``:\n\n.. code-block:: shell\n\n    kinto start --ini config/local.ini\n\nAnother thing you might want to debug is the ``tests`` container that tests\nagainst the Kinto server.\n\n.. code-block:: shell\n\n    docker-compose run tests bash\n\nNow, from that ``bash`` session you can reach the other services like:\n\n.. code-block:: shell\n\n    curl http://autograph:8000/__heartbeat__\n    curl http://web:8888/v1/__heartbeat__\n\n\nUpgrade Things\n--------------\n\nMost common use-case is that you want to upgrade one of the dependencies.\n\nTop level dependencies are listed in ``requirements.in``.\n\nWe use `pip-tools's pip-compile <https://pypi.org/project/pip-tools/>`_ command to generate the exhaustive list of pinned dependencies with their hash.\n\nTo upgrade a single package, run:\n\n.. code-block:: shell\n\n    pip-compile --upgrade-package kinto-attachment\n\nTo test that this installs run:\n\n.. code-block:: shell\n\n    docker-compose build web\n\n\nAbout versioning\n----------------\n\nWe respect `SemVer <http://semver.org>`_ here. However, the \"public API\" of this package is not the user-facing API of the service itself, but is considered to be the set of configuration and services that this package and its dependencies use. Accordingly, follow these rules:\n\n* **MAJOR** must be incremented if a change on configuration, system, or third-party service is required, or if any of the dependencies has a major increment\n* **MINOR** must be incremented if any of the dependencies has a minor increment\n* **PATCH** must be incremented if no major nor minor increment is necessary.\n\nIn other words, minor and patch versions are uncomplicated and can be deployed automatically, and major releases are very likely to require specific actions somewhere in the architecture.\n\n\nReleasing\n---------\n\nFirst:\n\n- Make sure the CHANGELOG is up-to-date and includes details about all the components included in the release\n\n.. code-block:: bash\n\n    git checkout -b prepare-X.Y.Z\n    prerelease\n\n- At this point, the ``CHANGELOG.rst`` header and version number in ``VERSION`` are set.\n\n.. code-block:: bash\n\n    git commit -a --amend\n    git push\n\n- Open a PR, and when the PR is approved:\n\n.. code-block:: bash\n\n    git checkout main\n    git pull\n    git tag -a X.Y.Z\n    git push origin X.Y.Z\n\n- Now prepare the next version:\n\n.. code-block:: bash\n\n    git checkout -b start-X.Y.Z\n    git push\n\n- Draft a release on Github: https://github.com/mozilla/remote-settings/releases\n  For release notes, just use the CHANGELOG entry for the release, but change all\n  the ReST-style section headings to Markdown-style ``##`` headings.\n\n\n.. note::\n\n    The Mozilla Jenkins job will catch the latest Docker container on Dockerhub\n    and immediately deploy it to Remote Settings DEV. It will deploy the latest tag\n    on Remote Settings STAGE.\n    Integration tests will be executed.\n    Results are reported in the Mozilla ``#kinto-standup`` Slack channel.\n"
},
{
  "name": "DeepSpeech",
  "files": {
    "/": [
      ".cardboardlint.yml",
      ".compute",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".isort.cfg",
      ".pylintrc",
      ".readthedocs.yml",
      "BIBLIOGRAPHY.md",
      "CODE_OF_CONDUCT.md",
      "CODE_OWNERS.rst",
      "CONTRIBUTING.rst",
      "DeepSpeech.py",
      "Dockerfile.build.tmpl",
      "Dockerfile.train.tmpl",
      "GRAPH_VERSION",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "Makefile",
      "README.rst",
      "RELEASE.rst",
      "SUPPORT.rst",
      "VERSION",
      "bazel.patch",
      "bin",
      "build-python-wheel.yml-DISABLED_ENABLE_ME_TO_REBUILD_DURING_PR",
      "ci_scripts",
      "data",
      "doc",
      "ds_generic.supp",
      "ds_lib.supp",
      "ds_openfst.supp",
      "ds_sox.supp",
      "evaluate.py",
      "evaluate_tflite.py",
      "examples",
      "images",
      "kenlm",
      "lm_optimizer.py",
      "native_client",
      "parse_valgrind_suppressions.sh",
      "requirements_eval_tflite.txt",
      "requirements_tests.txt",
      "requirements_transcribe.txt",
      "setup.py",
      "stats.py",
      "taskcluster.disabled.yml",
      "taskcluster",
      "tensorflow",
      "tensorflow_full_runtime.supp",
      "tensorflow_tflite_runtime.supp",
      "tests",
      "training",
      "transcribe.py"
    ],
    "/.github": [
      "actions",
      "lock.yml",
      "workflows"
    ]
  },
  "makefile": "DEEPSPEECH_REPO ?= https://github.com/mozilla/DeepSpeech.git\nDEEPSPEECH_SHA  ?= master\n\nDockerfile%: Dockerfile%.tmpl\n\tsed \\\n\t\t-e \"s|#DEEPSPEECH_REPO#|$(DEEPSPEECH_REPO)|g\" \\\n\t\t-e \"s|#DEEPSPEECH_SHA#|$(DEEPSPEECH_SHA)|g\" \\\n\t\t< $< > $@\n",
  "readme": "Project DeepSpeech\n==================\n\n\n.. image:: https://readthedocs.org/projects/deepspeech/badge/?version=latest\n   :target: https://deepspeech.readthedocs.io/?badge=latest\n   :alt: Documentation\n\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/macOS-amd64.yml\n   :alt: macOS builds\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/lint.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/lint.yml\n   :alt: Linters\n\n.. image:: https://github.com/mozilla/DeepSpeech/actions/workflows/docker.yml/badge.svg\n   :target: https://github.com/mozilla/DeepSpeech/actions/workflows/docker.yml\n   :alt: Docker Images\n\n\nDeepSpeech is an open-source Speech-To-Text engine, using a model trained by machine learning techniques based on `Baidu's Deep Speech research paper <https://arxiv.org/abs/1412.5567>`_. Project DeepSpeech uses Google's `TensorFlow <https://www.tensorflow.org/>`_ to make the implementation easier.\n\nDocumentation for installation, usage, and training models are available on `deepspeech.readthedocs.io <https://deepspeech.readthedocs.io/?badge=latest>`_.\n\nFor the latest release, including pre-trained models and checkpoints, `see the latest release on GitHub <https://github.com/mozilla/DeepSpeech/releases/latest>`_.\n\nFor contribution guidelines, see `CONTRIBUTING.rst <CONTRIBUTING.rst>`_.\n\nFor contact and support information, see `SUPPORT.rst <SUPPORT.rst>`_.\n"
},
{
  "name": "jetstream",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "contibuting.md",
      "docs",
      "jetstream",
      "mypy.ini",
      "platform_config.toml",
      "pyproject.toml",
      "requirements.in",
      "requirements.txt",
      "script",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/docs": [
      "adr-0001-publishing_metric_docs.md",
      "adr-0002-publishing_statistics_with_analysis_basis.md",
      "proposal-0001-flexible_handling_of_exposure_events.md"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/jetstream/tree/main.svg?style=shield)](https://circleci.com/gh/mozilla/jetstream/tree/main)\n\n# jetstream\n\nAutomated experiment analysis.\n\nJetstream automatically calculates metrics and applies statistical treatments to collected experiment data for different analysis windows.\n\nFor more information, see [the documentation](https://experimenter.info/jetstream/jetstream/).\n\n## Running tests\n\nMake sure `tox` is installed globally (run `brew install tox` or `pip install tox`).\n\nThen, run `tox` from wherever you cloned this repository. (You don't need to install jetstream first.)\n\nTo run integration tests, run `tox -e py38-integration`.\n\n\n## Local installation\n\n```bash\n# Create and activate a python virtual environment.\npython3 -m venv venv/\nsource venv/bin/activate\npip install -r requirements.txt\n```\n"
},
{
  "name": "jetstream-config",
  "files": {
    "/": [
      ".circleci",
      ".docs",
      ".python-version",
      ".scripts",
      "1-click-pin-experiment.toml",
      "CODEOWNERS",
      "LICENSE",
      "README.md",
      "android-beta-sponsored-shortcuts-validation.toml",
      "android-keystore-reliability-experiment-v2.toml",
      "android-nightly-sponsored-shortcuts-validation.toml",
      "bug-1647305-pref-comcast-steering-experiment-release-78-80.toml",
      "bug-1665467-pref-shirley-omnibus-experiment-release-81-82.toml",
      "bug-1668861-pref-measure-set-to-default-adoption-impact-of-chang-release-81-83.toml",
      "bug-1669364-pref-verify-changing-location-service-providers-still-release-82-83.toml",
      "bug-1671484-pref-validation-of-relpreload-performance-impact-release-82-83.toml",
      "bug-1671522-pref-assess-change-to-windows-mouse-wheel-scroll-anima-release-82-83.toml",
      "bug-1672227-pref-measure-set-to-default-adoption-impact-of-chang-release-81-83.toml",
      "bug-1674187-pref-holdback-study-for-quantumbar-v2-release-82-85.toml",
      "bug-1676470-pref-warpbuilder-experiment-release-83-83.toml",
      "bug-1689450-pref-pre-xul-skeleton-ui-startup-improvement-experimen-release-86-90.toml",
      "bug-1690385-pref-activity-stream-new-tab-default-content-release-85-87.toml",
      "bug-1692000-pref-activity-stream-new-tab-default-content-v2-fr-release-85-88.toml",
      "bug-1695015-pref-new-tab-modernized-ux-region-1-release-86-88.toml",
      "bug-1695026-pref-new-tab-modernized-ux-region-2-release-86-88.toml",
      "bug-1695035-pref-pre-xul-skeleton-ui-startup-improvement-experimen-release-86-90.toml",
      "bug-1695794-pref-new-tab-modernized-ux-region-1-existing-users-e-release-86-88.toml",
      "bug-1695815-pref-new-tab-modernized-ux-region-2-existing-users-d-release-86-88.toml",
      "bug-1696829-pref-new-tab-default-content-existing-users-ca-release-86-88.toml",
      "bug-1696831-pref-activity-stream-new-tab-default-content-existin-release-85-88.toml",
      "bug-1698297-pref-fission-m7-beta-experiment-beta-88-89.toml",
      "bug-1706428-pref-fission-m7-beta-experiment-with-memory-filter-beta-89-91.toml",
      "bug-1712979-pref-fission-beta-90-experiment-beta-90-91.toml",
      "bug-1719576-pref-fission-beta-91-experiment-beta-91-91.toml",
      "bug-1722112-pref-fission-process-count-nightly-92-94.toml",
      "bug-1722551-pref-full-js-parsing-experiment-nightly-94-94.toml",
      "bug-1724054-pref-fission-beta-92-experiment-beta-92-92.toml",
      "bug-1725055-pref-fission-release-92-experiment-release-92-92.toml",
      "bug-1726656-pref-tab-unloading-nightly-93-94.toml",
      "bug-1727420-pref-fission-beta-93-process-count-experiment-beta-93-93.toml",
      "bug-1731908-pref-fission-beta-94-95-experiment-beta-94-95.toml",
      "custom-messaging-in-aboutwelcome-for-chrome-users-to-import.toml",
      "default-browser-adoption-experiment-release.toml",
      "default-browser-adoption-experiment.toml",
      "default-browser-winning-branch-extended-holdback-study-release.toml",
      "defaults",
      "example_config.toml.example",
      "firefox-100-user-agent-beta-95-96.toml",
      "firefox-100-user-agent-nightly-95-96.toml",
      "firefox-100-user-agent-nightly-96-97.toml",
      "firefox-100-user-agent.toml",
      "firefox-android-sponsored-shortcuts-experiment.toml",
      "firefox-ios-homepage-experiment-sponsored-shortcuts.TOML",
      "firefox-ios-homepage-experiment-sponsored-shortcuts.toml",
      "firefox-ios-inactive-tabs.toml",
      "firefox-suggest-best-match.toml",
      "firefox-suggest-by-merino-beta.toml",
      "firefox-suggest-history-vs-offline.toml",
      "firefox-suggest-offline-vs-online.toml",
      "firefox-suggest-opt-in-modal-for-fx-100.toml",
      "firefox-suggest-opt-in-modal.toml",
      "keep-in-dock.toml",
      "mr2-upgrade-spotlight-holdback.toml",
      "multi-stage-aboutwelcome-set-default-as-first-screen-v2.toml",
      "multi-stage-onboarding-ask-and-show-to-pin.toml",
      "multistage-aboutwelcome-mr1-design.toml",
      "onboarding-modal-experiment.toml",
      "one-click-set-to-default.toml",
      "outcomes",
      "reminder-set-firefox-as-default.toml",
      "set-default-as-first-screen-100-roll-out.toml",
      "set-firefox-as-default-pdf-handler-on-windows-for-new-users-experiment.toml",
      "smarter-suggestions-in-the-awesomebar.toml",
      "user-messaging-set-default-trackers-blocked-notification.toml",
      "user-messaging-trackers-blocked-notification.toml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# jetstream-config\n\nCustom configs for experiments and outcome snippets\nanalyzed in [jetstream](https://github.com/mozilla/jetstream).\n\n## Adding Custom Configurations\n\nCustom configuration files are written in [TOML](https://toml.io/en/).\n\nTo add or update a custom configuration, open a pull request.\nCI checks will validate the columns, data sources, and SQL syntax.\nOnce CI completes, you may merge the pull request, which will trigger Jetstream to re-run your analysis.\nNo additional review is necessary to land configurations.\nAre your analyses not rendering as expected? See https://experimenter.info/jetstream/troubleshooting.\n\nLearn how to write a Jetstream configuration at <https://experimenter.info/jetstream/configuration>.\n\nLearn more about Outcomes at <https://experimenter.info/jetstream/outcomes>.\n\nSee what outcomes and pre-defined metrics are available at https://mozilla.github.io/jetstream-config\n"
},
{
  "name": "blurts-server",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".env-dist",
      ".eslintignore",
      ".eslintrc.json",
      ".git-blame-ignore-revs",
      ".github",
      ".gitignore",
      ".htmllintrc",
      ".npmignore",
      ".npmrc",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "__mocks__",
      "app-constants.js",
      "controllers",
      "db",
      "docs",
      "email-utils.js",
      "esbuild.js",
      "hibp.js",
      "ip-location-service.js",
      "l10n.toml",
      "lib",
      "locale-utils.js",
      "locales",
      "log.js",
      "middleware.js",
      "nodemon.json",
      "package-lock.json",
      "package.json",
      "public",
      "routes",
      "scan-results.js",
      "scripts",
      "server.js",
      "sha1-utils.js",
      "template-helpers",
      "tests",
      "views"
    ],
    "/docs": [
      "CODEOWNERS",
      "CONTRIBUTING.md",
      "analytics.md",
      "fx-integration.mmd",
      "fx-integration.png",
      "monitor-architecture.png"
    ],
    "/.github": [
      "linter_config.yml",
      "requirements.txt",
      "workflows"
    ],
    "/.circleci": [
      "config.yml",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Monitor Server\n\n## Summary\n\nFirefox Monitor notifies users when their credentials have been compromised in a data breach.\n\nThis code is for the monitor.firefox.com service & website.\n\nBreach data is powered by [haveibeenpwned.com](https://haveibeenpwned.com/).\n\nSee the [Have I Been Pwned about page](https://haveibeenpwned.com/About) for\nthe \"what\" and \"why\" of data breach alerts.\n\n## Architecture\n![Image of Monitor architecture](docs/monitor-architecture.png \"Firefox Monitor\")\n\n## Development\n\n### Requirements\n\n* [Node](https://nodejs.org/) (with npm)\n* [Postgres](https://www.postgresql.org/)\n\n### Code style\n\nLinting and formatting is enforced via [ESLint](https://eslint.org/) and [Stylelint](https://stylelint.io/) for JS and CSS.  Both are installed as dev-dependencies and can be run with `npm run lint`.  A push to origin will also trigger linting.\n\nESLint rules are based on [eslint-config-standard](https://github.com/standard/eslint-config-standard). To fix all auto-fixable problems, run `npx eslint . --fix`\n\nStylelint rules are based on [stylelint-config-standard](https://github.com/stylelint/stylelint-config-standard). To fix all auto-fixable problems, run `npx stylelint public/css/ --fix`\n\nTo run linting/formatting as you type or upon save, add the ESLint and Stylelint extensions and configure both to be the default formatter.  For VS Code, you may want to add properties to your personal settings.json file, similar to:\n```\n\"[javascript]\": {\n   \"editor.defaultFormatter\": \"dbaeumer.vscode-eslint\",\n   \"editor.codeActionsOnSave\": {\n      \"source.fixAll.eslint\": true\n   }\n},\n\"[css]\": {\n   \"editor.defaultFormatter\": \"stylelint.vscode-stylelint\",\n   \"editor.codeActionsOnSave\": {\n      \"source.fixAll.stylelint\": true\n   }\n}\n```\nSee here for more on Stylelint config with VSCode: https://github.com/stylelint/vscode-stylelint#editorcodeactionsonsave\n\n### Install\n\n1. Clone and change to the directory:\n\n    ```sh\n    git clone https://github.com/mozilla/blurts-server.git\n    cd blurts-server\n    ```\n\n2. Install dependencies:\n\n    ```sh\n    npm install\n    ```\n\n3. Copy the `.env-dist` file to `.env`:\n\n    ```sh\n    cp .env-dist .env\n    ```\n\n### Run\n\n1. Run the server:\n\n    ```sh\n    npm start\n    ```\n\nNote: `npm start` uses `onchange` and `nodemon` to automatically detect file\nchanges, re-compile static assets, and restart the express process. If you want\nmore control, see the `scripts` section of `package.json` for more commands.\n\n2. Navigate to [localhost:6060/](http://localhost:6060/)\n\n#### Database\n\nTo create the database tables ...\n\n1. Create the `blurts` database:\n\n   ```sh\n   createdb blurts\n   createdb test-blurts # for tests\n   ```\n\n2. Update the `DATABASE_URL` value in your `.env` file with your local db\n   credentials:\n\n   ```\n   DATABASE_URL=\"postgres://<username>@localhost:<port>/blurts\"\n   ```\n\n3. Run the migrations:\n\n   ```\n   npm run db:migrate\n   ```\n\n#### Trigger Breach Alert Email\nBreach alert emails are triggered via HIBP. For dev purposes, we can trigger them ourselves to send to a [Mailinator](https://www.mailinator.com) email address.\n\nTo set up your environment for email testing with Mailinator:\n1. In your .env file, confirm or add values for `SMTP_URL`, `EMAIL_FROM`, `HIBP_KANON_API_TOKEN`, and `HIBP_API_TOKEN` (Ask for values in #fx-monitor-engineering)\n\n2. If you don't have a local FxA account, sign up on localhost.  You'll need to ensure `FXA_ENABLED=true` and confirm/add the value for `OAUTH_CLIENT_SECRET` in your .env file. (Ask in #fx-monitor-engineering)\n\n3. Start/restart your server\n\n4. Login to your Monitor account at http://localhost:6060/, and scroll to the bottom of [your dashboard](http://localhost:6060/user/dashboard) to add localmonitor20200827@mailinator.com to your list of monitored email addresses\n\n5. In your [Monitor settings](http://localhost:6060/user/dashboard), make sure notification preferences specify \"Send breach alerts to the affected email address\" (should be default).  This will send the alert to the Mailinator account.\n\n   You *could* set it to forward to your main email address/account (e.g. Gmail), but localhost images will be broken.  The Mailinator account displays existing images automagically.\n\n6. To trigger a breach alert email, you need to make a `POST /hibp/notify` request:\n   * `Authorization: Bearer` header token value that matches `HIBP_NOTIFY_TOKEN`\n   * `Content-Type: application/json` header\n   * JSON body with `breachName`, `hashPrefix`, and `hashSuffix` values\n      * `breachName` - string of a breach name in Monitor\n      * `hashPrefix` - string of first 6 chars of a subscriber's `primary_sha1`\n      * `hashSuffix` - array of strings of the remaining chars of the sha1 hash\n\n   e.g., a localhost `curl` command that triggers a breach alert email for the Adobe breach to the `localmonitor20200827@mailinator.com` subscriber:\n\n   ```\n   curl -v -H \"Authorization: Bearer unsafe-default-token-for-dev\" -H \"Content-Type: application/json\" -d '{\"breachName\": \"Adobe\", \"hashPrefix\": \"365050\", \"hashSuffixes\": [\"53cbb89874fc738c0512daf12bc4d91765\"]}' http://localhost:6060/hibp/notify\n   ```\n\n7. Visit https://www.mailinator.com/v4/public/inboxes.jsp?to=localmonitor20200827# to view the email\n\n\n\n#### Firefox Accounts\n\nSubscribe with a Firefox Account is controlled via the `FXA_ENABLED`\nenvironment variable. (See `.env-dist`)\n\nThe repo comes with a development FxA oauth app pre-configured in `.env`, which\nshould work fine running the app on http://localhost:6060. You'll need to get\nthe `OAUTH_CLIENT_SECRET` value from someone in #fxmonitor-engineering.\n\n## Testing\n\nThe full test suite can be run via `npm test`.  \n\nAt the end of a test suite run, coverage info will be sent to [Coveralls](https://coveralls.io/) to assess coverage changes and provide a neat badge.  For this step to complete locally, you need a root `.coveralls.yml` which contains a token \u2013 get this from another member of the Monitor team.  Alternatively, without the token you can simply ignore the `coveralls` error.  \n\n*TODO:* Disable Coveralls step for local testing?\n\n### Individual tests\n\nTo run individual tests, use `NODE_ENV=tests` and `jest`:\n\n```\nNODE_ENV=tests jest --runInBand tests/home.test.js\n```\n\nTo run tests with interactive `debugger` lines enabled:\n\n```\nNODE_ENV=tests node inspect --harmony ./node_modules/.bin/jest tests/home.test.js\n```\n\n### Test Firefox Integration\n\nFirefox's internal about:protections page (\"Protections Dashboard\") fetches and\ndisplays breach stats for Firefox users who are signed into their FXA.\n\nTo test this part of Monitor:\n\n1. [Set a Firefox profile to use the staging Firefox Accounts\n   server.](https://mozilla.github.io/ecosystem-platform/docs/process/using-the-staging-environment#working-with-staging-firefox-accounts)\n2. In the same profile, go to about:config and replace [all\n   `https://monitor.firefox.com`\n   values](https://searchfox.org/mozilla-central/search?q=monitor.firefox.com&path=browser/app/profile/firefox.js) with `http://localhost:6060`\n3. Restart Firefox with that profile.\n4. Go to `about:protections`\n5. Everything should be using your localhost instance of Monitor.\n\n### Lint\n\nAfter installing the dependencies, you can lint the code by calling:\n\n```sh\nnpm run lint\n```\n\n## Localization\n\nThis repo includes a dedicated branch for localization called... `localization`.  To add localized text, add or update the relevant `.ftl` file under `locales/en`.  Be sure to reference the [localization documentation](https://mozilla-l10n.github.io/documentation/localization/dev_best_practices.html) for best practices.  \n\nTo trigger translations, open a pull request against `localization`. Please be mindful that Mozilla localizers are volunteers, and translations come from different locales at different times \u2013 usually after a week or more. It's best to initiate a PR when your strings are more-or-less final. Your PR should be automatically tagged with a reviewer from the [Mozilla L10n team](https://wiki.mozilla.org/L10n:Mozilla_Team) to approve your request.\n\nAfter your updates are merged into `localization`, you will start to see commits from Pontoon, Mozilla's localization platform. You can also check translation status via the [Pontoon site](https://pontoon.mozilla.org/projects/firefox-monitor-website/). \n\nWhen enough translations have been commited, you should merge `localization` into `main`, or back into your feature branch if it's not yet merged to `main`. Note it's unlikely to have 100% of locales translated. You might discuss with stakeholders which locales are priority.\n\n**Important:** Do not use \"Squash\" or \"Rebase\" to merge `localization` into `main` or vice versa.  Doing so creates new commit hashes and the branches will appear out of sync.\n\nTODO: auto-sync `localization` with `main`\n\n## Deployment\n\nFirefox Monitor Breach Alerts is designed with [12-factor](https://12factor.net/) methodology.\n\n### Deploy on Heroku\n\nWe use Heroku apps for dev review only \u2013 official stage and production apps are built by the Dockerfile and CircleCI config, with deploy overseen by the Site Reliability Engineering team.\n\nDeploys from the `main` branch to Heroku are automatic.  We also employ Heroku's \"Review Apps\" to check Pull Requests.  These are currently set to auto-deploy: you can find the app link in your GitHub Pull Request. Review apps auto-destroy after 2 days of inactivity.\n\nIf you encounter issues with Heroku deploys, be sure to check your environment variables, including those required in `app-constants.js`.  Review apps also share a database and you should not assume good data integrity if testing db-related features.\n\n## VPN Banner\n\nA banner has been added to inform users whether their IP address is being masked by Mozilla VPN.  It also uses their IP address to demonstrate geolocation. This can inform users why they might use Mozilla VPN for privacy.  \n\nThe IP location data includes GeoLite2 data created by MaxMind, available from https://www.maxmind.com.  For localhost, a test MaxMind database with limited data is included with this repo. For the Heroku Dev site, the following buildpack is used to enable geolocation: https://github.com/HiMamaInc/heroku-buildpack-geoip-geolite2. For stage and prod environments, a shared database is set via env vars."
},
{
  "name": "Spoke",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".dockerignore",
      ".env.defaults",
      ".env.prod",
      ".env.test",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.json",
      ".storybook",
      ".stylelintrc",
      ".vscode",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Jenkinsfile",
      "LICENSE",
      "PROMOTION.md",
      "README.md",
      "REMIXING.md",
      "RetPageOriginDockerfile",
      "docs",
      "habitat",
      "package.json",
      "scripts",
      "src",
      "test",
      "webpack.config.js",
      "yarn.lock"
    ],
    "/docs": [
      "README.md",
      "assets",
      "creating-custom-elements.md",
      "creating-kits.md"
    ],
    "/.github": [
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "<h1 align=\"center\">Spoke</h1>\n\n<p align=\"center\"><a href=\"https://hubs.mozilla.com/spoke\" target=\"_blank\"><img width=\"480\" alt=\"Spoke\" src=\"https://user-images.githubusercontent.com/21111451/66261819-ffd9ff00-e799-11e9-88bf-981d238b4f20.gif\"></a></p>\n\n[![CircleCI](https://circleci.com/gh/mozilla/Spoke.svg?style=svg)](https://circleci.com/gh/mozilla/Spoke)\n\n\n  **Easily create custom 3D environments for [Mozilla Hubs](https://hubs.mozilla.com).**\n\n**[Launch Spoke](https://hubs.mozilla.com/spoke)**\n\n**[Spoke Documentation](https://hubs.mozilla.com/docs/spoke-creating-projects.html)**\n\n## Features\n\n:telescope: **Discover**: Explore images, videos, and 3D models from around the web, all without opening up a new tab. With media integrations from Sketchfab and Google Poly, you'll be on your way to creating a scene in no time.\n\n:pencil2: **Create**: No external software or 3D modeling experience required - build 3D scenes using the Spoke web editor so you can have a space that's entirely custom to your needs. From a board room to outer space and beyond, your space is in your control.\n\n:tada: **Share**: Invite people to meet in your new space by publishing your content to Hubs immediately. With just a few clicks, you'll have a world of your own to experience and share - all from your browser.\n\n## Contributing\n\nInfo on contributing to Spoke and general Spoke development can be found in the [CONTRIBUTING.md](./CONTRIBUTING.md) doc.\n\nAdditional developer documentation can be found in the [docs](./docs/README.md) folder.\n\n## Credits\n\nParts of this project are derived from the [three.js editor](https://threejs.org/editor/)\nwith thanks to [Mr.doob](https://github.com/mrdoob) and three.js' many contributors.\n\nNavigation mesh generation via recast.wasm, thanks to [Recast](https://github.com/recastnavigation/recastnavigation) and but0n's [RecastCLI wrapper](https://github.com/but0n/recastCLI.js).\n\nSee the [LICENSE](LICENSE) for details.\n"
},
{
  "name": "extension-workshop",
  "files": {
    "/": [
      ".circleci",
      ".eleventyignore",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierignore",
      ".prettierrc",
      ".stylelintrc",
      ".utils",
      "README.md",
      "bin",
      "eleventy.config.js",
      "libs",
      "package.json",
      "renovate.json",
      "screenshots",
      "src",
      "tests",
      "yarn.lock"
    ],
    "/.github": [
      "CODE_OF_CONDUCT.md",
      "ISSUE_TEMPLATE.md",
      "contributing.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/extension-workshop/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla/extension-workshop/tree/master)\n\n# Firefox Extension Workshop\n\nWelcome to Firefox Extension Workshop, a launchpad for building Firefox extensions! \ud83d\ude80\n\n## Updating Content\n\nIf you would like to update content or other resources on Firefox Extension Workshop, please refer to [`contributing.md`](.github/contributing.md)\n\n## Development Guide: Getting Started\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes. \n\nFor notes on how to deploy the project on a live system, see [Deployment](#deployment).\n\n### Prerequisites\n\n- [Node JS](https://nodejs.org/en/). Runnning the LTS release is recommended.\n- [Yarn](https://yarnpkg.com/en/) for package management.\n\n```\nyarn install\n```\n\nTo start local development, run:\n\n```\nyarn start\n```\n\n**\u2139\ufe0f\u00a0NOTE:** Running locally will show unpublished content that uses the `published: false` convention in frontmatter. Content with `published: false` will not be available on staging or production.\n\n\n### Available yarn commands\n\n| Command                | Description                                                                             |\n| ---------------------- | --------------------------------------------------------------------------------------- |\n| `yarn start`             | Starts eleventy and includes unpublished content.                                       |\n| `yarn build:production`  | Builds the site for production.                                                         |\n| `yarn build:unpublished` | Builds the site for production with unpublished content.                                |\n| `yarn clean`             | Clears the output directory. (You probably won't need to use this manually.)             |\n\n## How the site is built\n\nThe site is built with [Eleventy](https://www.11ty.dev/), a NodeJS-based static site generator.\n\nThe site works in slightly different ways depending on whether you're running the site for local development or building the site for production.\n\n### Development builds\n\nWhen you run `yarn start` the CSS and JS is built in parallel with the eleventy build. Once up and running both eleventy and the JS and CSS build scripts watch for changes. When something changes the site is re-built.\n\nIn development Eleventy knows nothing about the CSS and JavaScript builds. For automatic reloading of the JS and CSS, each script uses a fetch to the public API to tell browserSync there is new code and it reloads it for you.\n\n### Production builds\n\nBuilding for production is slightly different. The Eleventy process and the JS and CSS builds happen in series. Then a 3rd `asset-pipeline` process initiates and takes the the built content from `./build` directory and runs it through various optimizations.\n\nDuring these optimizations, the following takes place:\n\n* Binary files are versioned with hashes in the file names.\n* References to these file in CSS and JS are updated.\n* CSS and JS are minified.\n* The HTML is processed to update the references to the assets new hash-based filenames.\n\nAll of this means that we can serve the site with far-future `Expires` headers. If the resource is in the browser's cache, the browser won't even make a request for it. To break the cache, the resource's URL needs to change. When something is updated and the script is re-run, the hash in the filename will change, so the new filename won't be cached and the browser will know to fetch it. This helps the site be fast.\n\nWhilst the `asset-pipline` script is custom, it leverages a lot of existing libs where possible, these include Terser, postHTML, postCSS, and various plugins.\n\nIt's likely that some day, 11ty will have its own mechanism for wrangling assets. At that point, this will no longer be required.\n\n#### Asset paths\n\nFor the `asset-pipeline` script to do its thing, all you need to do is refer to all assets with a path beginning with `/assets/`. If you do that, everything else is handled for you \u2728\n\n## Development Guide: Content Updates\n\nThis site has three templates: \n\n1. A full-width page\n2. A sidebar \"page\" for documentation\n3. A Content Guidelines page\n\n### Repo layout\n\n```bash\nextensionworkshop.com\n\u251c\u2500\u2500 bin\n\u2502   \u251c\u2500\u2500 asset-pipeline           # The asset build script\n\u2502   \u251c\u2500\u2500 build-script             # The JS build script\n\u2502   \u2514\u2500\u2500 build-styles             # The CSS build script\n\u2502\n\u251c\u2500\u2500 build                        # Where eleventy builds the site to\n\u2502\n\u251c\u2500\u2500 dist                         # Where production builds are built\n\u2502\n\u251c\u2500\u2500 libs\n\u2502   \u251c\u2500\u2500 markdown.js              # The markdown renderer instance and plugins\n\u2502   \u251c\u2500\u2500 slugify.js               # The central slug function\n\u2502   \u2514\u2500\u2500 templates.js             # The liquidjs template instance\n\u2502\n\u251c\u2500\u2500 screenshots                  # Screenshots used in README.md\n\u2502\n\u251c\u2500\u2500 src\n\u2502   \u251c\u2500\u2500 assets                   # Assets (CSS, JavaScript, fonts and images)\n\u2502   \u251c\u2500\u2500 content                  # Content (Markdown and JS (generated))\n\u2502   \u251c\u2500\u2500 data                     # Data files (JSON)\n\u2502   \u251c\u2500\u2500 includes                 # Components (Liquid)\n\u2502   \u2514\u2500\u2500 layouts                  # Layout templates\n\u2502\n\u251c\u2500\u2500 tests                        # Test files run by jest `yarn test`.\n\u2502\n\u251c\u2500\u2500 eleventy.config.js           # Eleventy configuration\n\u251c\u2500\u2500 .eleventyignore              # Files ignored by Eleventy\n\u251c\u2500\u2500 .gitignore                   # Files not tracked by Git\n\u251c\u2500\u2500 .stylelintrc                 # Stylelint configuration\n\u251c\u2500\u2500 .prettierrc                  # Prettier config\n\u251c\u2500\u2500 .prettierignore              # Files ignored by prettier\n\u251c\u2500\u2500 .eslintrc                    # eslint config\n\u251c\u2500\u2500 .eslintignore                # Files ignored by eslint\n\u251c\u2500\u2500 package.json                 # Node.js package manifest\n\u251c\u2500\u2500 renovate.json                # Renovate configuration\n\u251c\u2500\u2500 yarn.lock                    # Package manager lock file\n\u2514\u2500\u2500 README.md                    # This file\n```\n\n### Uploading media\n\n1. Add the image files to `src/assets/img/`\n2. In your page, link to images using this page structure:\n\nYou can reference images with the full path from the `assets/` directory (e.g, `/assets/img/image.png`).\n\nHere's an example in `markdown`:\n\n```markdown\n![Remembear subtitle screenshot](/assets/img/remembear-subtitle.png \"Remembear subtitle text\")\n```\n\n### Adding notes and alerts\n\nFor a note, use the markdown syntax extensions as follows. (These markdown extensions are supplied by a plugin to the markdown renderer.)\n\n```markdown\n::: note\nThis is a note\n:::\n```\n\nLooks like this ![Note Screenshot](../master/screenshots/note.png)\n\nFor an alert, use the following:\n\n```markdown\n::: note alert\nThis is an alert\n:::\n```\n\nLooks like this ![Alert Screenshot](../master/screenshots/alert.png)\n\n### How to add a \"sidebar\" layout page\n\n1. Open `data/pages.json`.\n2. Add a node with appropriate attributes, in the appropriate location, for the new page. See below: [Understanding the `pages.json` structure](#understanding-the-pagejson-structure).\n3. Create a new page, nested inside a folder struture that matches the URL path. For example, for permalink `/documentation/develop/best-practices-for-collecting-user-data-consents/`, you would create a file called `best-practices-for-collecting-user-data-consents.md` and place it in `documentation \u25b6\ufe0e develop`.\n4. For reference on how to create a page, review the `sidebar-master-template.md` file, which lists all available modules. Some notes:\n   - `published: false` will withhold this content from staging and production. To publish content, remove this line.\n   - `skip_index: true` is used for pages that shouldn't be indexed for search results.\n   - When creating page sections that should be listed in the table of contents, add an `id` attribute to the section container that matches the `subpageitems` entry added to `pages.json`. If your layout requires several sections for one table of contents entry, nest your sections inside a containing element which has the `id` attribute.\n   - Rule for creating section `id`s: use the `h2` title of the section, converted to lowercase, spaces replaced with dashes, all non-alphanumeric characters removed. For example, the section `h2` title \"Know your privacy settings\" would be converted to `know-your-privacy-settings` for the section `id`.\n   - The first section following the \"Page Hero\" module should be the \"Table of Contents\" module: `modules/column-w-toc.html`.\n\n<h4 id=\"understanding-the-pagesjson-structure\">Understanding the <code>pages.json</code> structure</h4>\n\n- Each page has a `title` and `url` attribute.<br>\n    **\u2139\ufe0f\u00a0NOTE:** The `url` attribute must exactly match the `permalink` attribute of the page's front matter _(including leading and trailing slashes)_.\n- Pages may also have a `subpageitems` node for sections within the page to be referenced in the table of contents for that page:\n  - Each `subpageitem` node has a `title` and `id` attribute. The value of `id` matches the `id` attribute of the section container.<br>\n    (**\u2139\ufe0f\u00a0NOTE:** `id`s must be added to the containing element, rather than the heading element, of the section. This ensures that highlighting for the section remains active, even when the section title is out of view.)\n- Overview pages have `category` nodes for each of their contained (sub) `categories`.\n- Categories have a `category` attibute (which denotes the category title), and a `pages` attribute (which lists sub-pages of the overview page).\n- The Documentation Topics section pages are nested inside a `subfolderitems` node, which creates the dropdown panel.\n\n<details>\n   <summary>General overview of the <code>pages.json</code> layout:</summary>\n\n```json\n[\n  {\n    \"title\": \"Documentation Topics\",\n    \"subfolderitems\": [\n      {\n        \"title\": \"Develop\",\n        \"url\": \"/documentation/develop/\",\n        \"subpageitems\": [\n          {\n            \"title\": \"Firefox Tools\",\n            \"id\": \"firefox-tools\"\n          }\n        ],\n        \"categories\": [\n          {\n            \"category\": \"Getting Started\",\n            \"pages\": [\n              {\n                \"title\": \"Firefox Workflow Overview\",\n                \"url\": \"/documentation/develop/firefox-workflow-overview/\"\n              }\n            ]\n          }\n        ]\n      },\n      {\n        \"title\": \"Publish\",\n        \"url\": \"/documentation/publish/\",\n        \"subpageitems\": [\n          {\n            \"title\": \"Get your extension signed\",\n           \"id\": \"get-your-extension-signed\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Manage\",\n        \"url\": \"/documentation/manage/\",\n        \"subpageitems\": [\n          {\n            \"title\": \"Stay informed when Firefox changes\",\n            \"id\": \"stay-informed-when-firefox-changes\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Enterprise\",\n        \"url\": \"/documentation/enterprise/\",\n        \"subpageitems\": [\n          {\n            \"title\": \"Section Title\",\n            \"id\": \"introduction\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Themes\",\n        \"url\": \"/documentation/themes/\",\n        \"subpageitems\": [\n          {\n            \"title\": \"What themes are\",\n            \"id\": \"what-themes-are\"\n          }\n        ]\n      }\n    ]\n  },\n  {\n    \"title\": \"Extension Basics\",\n    \"url\": \"/extension-basics/\",\n    \"subpageitems\": [\n      {\n        \"title\": \"Getting started\",\n        \"id\": \"getting-started\"\n      }\n    ]\n  },\n  {\n    \"title\": \"Community\",\n    \"url\": \"/community/\",\n    \"subpageitems\": [\n      {\n        \"title\": \"Who is part of the community?\",\n        \"id\": \"who-is-part-of-the-community\"\n      }\n    ],\n    \"categories\": [\n      {\n        \"category\": \"About the Community\",\n        \"pages\": [\n          {\n            \"title\": \"\",\n            \"url\": \"\"\n          }\n        ]\n      }\n    ]\n  }\n]\n```\n\n</details>\n\n### How to add a \"Content Guidelines\" page\n\n#### Create a new page\n\n1. Create new file\n2. Add frontmatter (see example below)\n3. Copy 'modules' needed from `content-guidelines/master-template.md` and paste in new file\n4. Save as markdown: `content-guidelines/page-name.md`\n\n```yaml\n---\nlayout: guides\ntitle: Page Name\npermalink: /content-guidelines/page-name/\npublished: false\n---\n```\n\n**\u2139\ufe0f\u00a0NOTE:** `published: false` will withhold this content from staging and production. To publish content, remove this line.\n\n\n#### Add the page to the menu\n\nGo to `data/content-guidelines-pages.json` and add a new entry for your page:\n\n```json\n{\n  \"title\": \"Page Name\",\n  \"url\": \"/content-guidelines/page-name/\",\n  \"draft-label\": true\n}\n```\n\n#### Controlling draft labelling\n\nIf you don't want the page to be labelled as a draft (such as and when it's ready), remove `\"draft-label\": true` from the relevant entry in `data/content-guidelines-pages.json`.\n\n### Tagging\n\nTags should aim to follow the AMO calendar format: `YYYY.MM.DD` with an optional\n`-x` suffix for cherry-picking, and an optional additional `-stage` suffix for stage\ndeploys as mentioned below.\n\n## Deployment\n\nAll deployments for staging and production are handled via the [Releases](https://github.com/mozilla/extension-workshop/releases) page.\n\n### Dev Deploys\n\nThe site is auto-deployed on commits to `master` to https://extensionworkshop-dev.allizom.org/. You can check the version on -dev with [the dev version link](https://extensionworkshop-dev.allizom.org/__version__).\n\n### Stage Deploys\n\nTags matching `^20\\d{2}\\.\\d{2}\\.\\d{2}(?:-\\d+)?-stage$` will be deployed to https://extensionworkshop.allizom.org/. You can check the version on stage with [the stage version link](https://extensionworkshop.allizom.org/__version__).\n\nA good example tag for a stage deploy would be `2022.03.03-stage`.\n\n### Production Deploys\n\nTags matching `^20\\d{2}\\.\\d{2}\\.\\d{2}(?:-\\d+)?$` regular expression will be deployed to https://extensionworkshop.com/. You can check the version on production with [the production version link](https://extensionworkshop.com/__version__).\n\nA good example tag for a production deploy would be `2022.03.03`.\n"
},
{
  "name": "perfcompare",
  "files": {
    "/": [
      ".circleci",
      ".eslintrc.js",
      ".gitignore",
      ".prettierrc.js",
      "README.md",
      "package-lock.json",
      "package.json",
      "public",
      "screenshot.png",
      "src",
      "tsconfig.json",
      "webpack.config.js"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# PerfCompare\n\n[![CircleCI](https://circleci.com/gh/mozilla/perfcompare/tree/master.svg?style=shield)](https://circleci.com/gh/mozilla/perfcompare/tree/master)\n[![codecov](https://codecov.io/gh/mozilla/perfcompare/branch/master/graph/badge.svg?token=XHP440JFDQ)](https://codecov.io/gh/mozilla/perfcompare)\n![GitHub issues](https://img.shields.io/github/issues/mozilla/perfcompare)\n![GitHub pull requests](https://img.shields.io/github/issues-pr/mozilla/perfcompare)\n\nPerformance Comparison Tool\n\n![screenshot](screenshot.png)\n\n## Setup\n\n### Requirements\n\n- [nodejs](https://nodejs.org/en/download/)\n\n### Installation\n\n```\n# Clone the repo\ngit clone https://github.com/mozilla/perfcompare.git\ncd perfcompare\n\n# Install node modules\nnpm install\n\n# Runs on localhost:3000 by default\nnpm run start-dev\n```\n\n### Contributing\n\nWe welcome contributions to our project.\n\nIf you find an issue that you'd like to work on that is not assigned to anyone, leave a comment on the issue and request that it be assigned to you.\n\nIf you do not receive a response within 2-3 days, you can follow up in the #PerfCompare matrix channel.\n\nAfter addressing the issue, ensure both tests and linting pass before submitting a pull request.\n\nWhen submitting a pull request, please mention the issue number to link the pull request and issue to one another. You can do this by typing # following immediately by the issue number, i.e., `#123`\n\nSubmit your pull request to the `staging` branch. `staging` is merged to master weekly on Monday.\n\nWe recommend the following workflow to contribute to PerfCompare:\n\n1. [Set an upstream remote](https://docs.github.com/en/get-started/getting-started-with-git/managing-remote-repositories/) that points to the project [repository](https://github.com/mozilla/perfcompare.git), and an 'origin' remote that points to your fork.\n2. To keep your fork up-to-date, use `git rebase upstream` rather than merging. This causes fewer merge conflicts and keeps the git history cleaner.\n\n```\n# Git commands for keeping your branch up to date with the lastest master\ngit fetch upstream\ngit rebase upstream/master\ngit push --force origin <local branch>\n```\n\n### Validating JavaScript\n\nWe run our JavaScript code in the frontend through [ESLint](https://eslint.org/) to ensure that new code has a consistent style and doesn't suffer from common errors.\n\n```\n# To run ESLint by itself, you may run the lint task:\nnpm run lint\n\n# Automatically fix linting issues found (where possible):\nnpm run lint:fix\n\n# Checking formatting issues with Prettier:\nnpm run format:check\n\n# Automatically fix format issues found (where possible):\nnpm run format\n```\n\n### Running Tests\n\nTests can be run with the following commands:\n\n```\nnpm run test\n\n# Run tests and watch for changes\nnpm run test:watch\n\n# Run tests with coverage\nnpm run test:coverage\n```\n\n#### Snapshot Tests\n\nWhen making changes to the UI, snapshots should also be updated to match. Snapshot tests\nensure no UI changes occur unexpectedly.\n\nAfter manually verifying the UI renders as intended, run the following command to update\nsnapshots:\n`jest --updateSnapshot`\n\nSnapshot files should be included in your pull request(s).\n"
},
{
  "name": "PRESC",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitignore",
      ".pre-commit-config.yaml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "datasets",
      "docs",
      "environment.yml",
      "examples",
      "literature",
      "presc",
      "scripts",
      "setup.py",
      "sphinx_docs",
      "tests"
    ],
    "/docs": [
      "ROADMAP.md",
      "_images",
      "configuration.md",
      "evaluations.md",
      "getting_started.md",
      "installation.md",
      "ml_copies.md",
      "overview.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: upload pytest setup_conda\n\nall: pytest\n\nsetup_conda:\n\t# Install all dependencies and setup repo in dev mode\n\tconda env create -f environment.yml\n\npytest:\n\tpytest\n\tflake8 presc tests\n\npytest_ci:\n\tpytest -sv\n\tflake8 presc tests\n\t$(MAKE) -C sphinx_docs clean html\n\n# build:\n# \tbin/create_version\n# \tdocker build -t ${IMAGE_NAME} .\n\nupload:\n\ttwine upload --repository-url https://upload.pypi.org/legacy/ dist/*\n",
  "readme": "# PRESC: Performance and Robustness Evaluation for Statistical Classifiers\n\n[![CircleCI](https://circleci.com/gh/mozilla/PRESC.svg?style=svg)](https://circleci.com/gh/mozilla/PRESC)\n[![Join the chat at https://gitter.im/PRESC-outreachy/community](https://badges.gitter.im/PRESC-outreachy/community.svg)](https://gitter.im/PRESC-outreachy/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nPRESC is a toolkit for the evaluation of machine learning classification\nmodels.\nIts goal is to provide insights into model performance which extend beyond\nstandard scalar accuracy-based measures and into areas which tend to be\nunderexplored in application, including:\n\n- Generalizability of the model to unseen data for which the training set may\n  not be representative\n- Sensitivity to statistical error and methodological choices\n- Performance evaluation localized to meaningful subsets of the feature space\n- In-depth analysis of misclassifications and their distribution in the feature\n  space\n\nMore details about the specific features we are considering are presented in the\n[project roadmap](./docs/ROADMAP.md).\nWe believe that these evaluations are essential for developing confidence in\nthe selection and tuning of machine learning models intended to address user\nneeds, and are important prerequisites towards building\n[trustworthy AI](https://foundation.mozilla.org/en/internet-health/trustworthy-artificial-intelligence/).\n\nIt also includes a package to carry out copies of machine learning classifiers.\n\nAs a tool, PRESC is intended for use by ML engineers to assist in the\ndevelopment and updating of models.\nIt is usable in the following ways:\n\n- As a standalone tool which produces a graphical report evaluating a given\n  model and dataset\n- As a Python package/API which can be integrated into an existing pipeline\n\nA further goal is to use PRESC:\n\n- As a step in a Continuous Integration workflow: evaluations run as a part of\n  CI, for example, on regular model updates, and fail if metrics produce\n  unacceptable values.\n\nFor the time being, the following are considered __out of scope__:\n\n- User-facing evaluations, eg. explanations\n- Evaluations which depend explicitly on domain context or value judgements of\n  features, eg. protected demographic attributes. A domain expert could use\n  PRESC to study misclassifications across such protected groups, say, but the\n  PRESC evaluations themselves should be agnostic to such determinations.\n- Analyses which do not involve the model, eg. class imbalance in the training\n  data\n\nThere is a considerable body of recent academic research addressing these\ntopics, as well as a number of open-source projects solving related problems.\nWhere possible, we plan to offer integration with existing tools which align\nwith our vision and goals.\n\n## Documentation\n\nProject documentation is available\n[here](https://mozilla.github.io/PRESC/index.html)\nand provides much more detail, including:\n\n- Getting set up\n- Running a report\n- Computing evaluations\n- Configuration\n- Package API\n\n### Examples\n\nAn example script demonstrating how to run a report is available\n[here](./examples/report/sample_report.py).\n\nThere are a number of notebooks and explorations in the\n[`examples/`](./examples/)\ndir, but they are not guaranteed to run or be up-to-date as the package has\nundergone major changes recently and we have not yet finished updating these.\n\nSome well-known datasets are provided in CSV format in the\n[`datasets/`](./datasets/)\ndir for exploration purposes.\n\n\n## Notes for contributors\n\nContributions are welcome.\nWe are using the repo [issues](https://github.com/mozilla/PRESC/issues) to\nmanage project tasks in alignment with the [roadmap](./docs/ROADMAP.md), as well\nas hosting discussions.\nYou can also reach out on [Gitter](https://gitter.im/PRESC-outreachy/community).\n\nWe recommend that submissions for new feature implementations include a Juypter\nnotebook demonstrating their application to a real-world dataset and model.\n\nThis repo adheres to [Python black](https://pypi.org/project/black/)\nformatting, which is enforced by a pre-commit hook (see below).\n\nAlong with code contributions, we welcome general feedback:\n\n- Testing out the package functionality. Try running the report on a\n  classification model and dataset. You can also try running individual\n  evaluations in a Jupyter notebook.\n    * If you don't have a dataset or classification model to work with, you can\n      use one of the datasets in the repo, and create a classifier using\n      `scikit-learn`. Some examples are given in the [`examples/`](./examples)\n      dir.\n    * If you can apply PRESC to a classification problem you have already been\n      working on, we'd be very excited to hear your feedback. If your data &\n      model can be considered public, you are welcome to submit any artifacts to\n      our `examples/` dir.\n- Please open issues for any bugs you encounter (including things that don't\n  work as you expect or aren't well explained).\n    * If you want to offer a PR for a fix, that is welcome too.\n- We would welcome any feedback on the general approach, the evaluations\n  described in the roadmap, the results you get from running PRESC, etc,\n  including similar projects you're familiar with. You can start a discussion by\n  opening an issue.\n  \nThe development of the ML Classifier Copies package is being carried out in the branch [`model-copying`](\nhttps://github.com/mozilla/PRESC/tree/model-copying\n).\n\n## Setting up a dev environment\n\nMake sure you have conda (eg. [Miniconda](https://docs.conda.io/en/latest/miniconda.html))\ninstalled. `conda init` should be run during installation to set the PATH\nproperly.\n\nSet up and activate the environment. This will also enable a pre-commit hook to\nverify that code conforms to flake8 and black formatting rules.\nOn Windows, these commands should be run from the Anaconda command prompt.\n\n```shell\n$ conda env create -f environment.yml\n$ conda activate presc\n$ python setup.py develop\n$ pre-commit install\n```\n\nTo run tests:\n\n```shell\n$ pytest\n```\n\n## Acknowledgements\n\nThis project is maintained by [Mozilla](https://www.mozilla.org/)'s Data Science\nteam.\nWe have also received code contributions by participants in the following programs, and\nwe are grateful for their support:\n\n- [Outreachy](https://www.outreachy.org/)\n- [CANOSP](https://canosp.ca/)\n- [NLnet](https://nlnet.nl/)\n- [Universitat de Barcelona Master in Fundamental Principles of Data Science](https://mat.ub.edu/sciencedata/)\n\nThe ML Classifier Copying package is being funded through the [NGI0 Discovery Fund](https://nlnet.nl/project/PRESC/), a fund established by NLnet with financial support from the European Commission's Next Generation Internet programme, under the aegis of DG Communications Networks, Content and Technology under grant agreement No 825322.\n"
},
{
  "name": "kitsune",
  "files": {
    "/": [
      ".adr-dir",
      ".circleci",
      ".devcontainer",
      ".dockerignore",
      ".editorconfig",
      ".env-build",
      ".env-dist",
      ".env-test",
      ".eslintrc",
      ".gitignore",
      ".nvmrc",
      ".pre-commit-config.yaml",
      ".readthedocs.yml",
      ".stylelintrc",
      ".vscode",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTORS.rst",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "configs",
      "contribute.json",
      "docker-compose.yml",
      "docker",
      "docs",
      "jsi18n",
      "k8s",
      "kitsune",
      "manage.py",
      "media",
      "newrelic.ini",
      "package-lock.json",
      "package.json",
      "poetry.lock",
      "postcss.config.js",
      "pyproject.toml",
      "scripts",
      "setup.cfg",
      "styleguide",
      "webpack.common.js",
      "webpack.dev.js",
      "webpack.prod.js",
      "webpack.test.js",
      "webpack",
      "wsgi"
    ],
    "/docs": [
      "Makefile",
      "SUMO architecture 2019.pdf",
      "SUMO architecture 2019.png",
      "advanced-search.md",
      "api.rst",
      "architectural-decisions.rst",
      "architecture",
      "badges.rst",
      "browser_permissions.md",
      "celery.rst",
      "conf.py",
      "contactus.rst",
      "contributors.rst",
      "conventions.rst",
      "deployments.rst",
      "development.rst",
      "email.rst",
      "frontend.md",
      "hacking_howto.md",
      "index.rst",
      "k8s.md",
      "localization.rst",
      "notes.rst",
      "patching.rst",
      "questions.rst",
      "search.md",
      "sla.rst",
      "sumo-elbs-and-dns.graffle",
      "sumo-elbs-and-dns.png",
      "tests.rst",
      "users.rst",
      "wsgi.rst",
      "zendesk.md"
    ],
    "/.circleci": [
      "build.html",
      "config.yml"
    ]
  },
  "makefile": "DC := $(shell command -v docker-compose 2> /dev/null)\nifeq (DC,)\nDC = $(shell which docker-compose)\nelse\nDC = $(shell which docker) compose\nendif\n\ndefault: help\n\t@echo \"\"\n\t@echo \"You need to specify a subcommand.\"\n\t@exit 1\n\nhelp:\n\t@echo \"build         - build docker images for dev\"\n\t@echo \"run           - docker-compose up the entire system for dev\"\n\t@echo \"\"\n\t@echo \"init          - initialize the database and install Node packages\"\n\t@echo \"djshell       - start a Django Python shell (ipython)\"\n\t@echo \"dbshell       - start a MySQL shell\"\n\t@echo \"shell         - start a bash shell\"\n\t@echo \"runshell      - start a bash shell with ports bound so you can run the server\"\n\t@echo \"clean         - remove all build, test, coverage and Python artifacts\"\n\t@echo \"rebuild       - force a rebuild of the dev docker image\"\n\t@echo \"lint          - run pre-commit hooks\"\n\t@echo \"test          - run python tests\"\n\t@echo \"test-js       - run js tests\"\n\t@echo \"docs          - generate Sphinx HTML documentation\"\n\n.env:\n\t@if [ ! -f .env ]; then \\\n\t\techo \"Copying .env-dist to .env...\"; \\\n\t\tcp .env-dist .env; \\\n\tfi\n\n.docker-build:\n\t${MAKE} build\n\nbuild:\n\t${DC} build web\n\ttouch .docker-build\n\nrebuild: clean build\n\nrun: .docker-build\n\t${DC} up web\n\ninit: .docker-build\n\t${DC} run web bin/run-bootstrap.sh\n\nshell: .docker-build\n\t${DC} run web bash\n\nrunshell: .docker-build\n\t${DC} run --service-ports web bash\n\ndjshell: .docker-build\n\t${DC} run web python manage.py shell\n\ndbshell: .docker-build\n\t${DC} run web python manage.py dbshell\n\nclean:\n#\tpython related things\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -rf {} +\n#\ttest related things\n\t-rm -f .coverage\n#\tdocs files\n\t-rm -rf docs/_build/\n#\tstate files\n\t-rm -f .docker-build*\n#\tnode stuff\n\t-rm -rf node_modules\n\nlint: .docker-build\n\t${DC} run web pre-commit run --all-files\n\ntest: .docker-build\n\t${DC} run web ./bin/run-unit-tests.sh\n\ntest-js: .docker-build\n\t${DC} run web npm run webpack:test\n\ndocs: .docker-build\n\t${DC} run web $(MAKE) -C docs/ clean\n\t${DC} run web $(MAKE) -C docs/ html\n\n.PHONY: build rebuild run init shell runshell djshell clean lint test test-js docs\n",
  "readme": "# Kitsune\n\nKitsune is the platform that powers [SuMo (support.mozilla.org)](https://support.mozilla.org)\n\nIt is a [Django](http://www.djangoproject.com/) application. There is [documentation](https://kitsune.readthedocs.io/en/latest/) online.\n\n## Deployment\nYou can access the staging site at <https://support.allizom.org/>.\n\n## Code of Conduct\nBy participating in this project, you're agreeing to uphold the [Mozilla Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/). If you need to report a problem, please see our [Code of Conduct](./CODE_OF_CONDUCT.md) guide.\n\n## Contribute\n\nSee our [contribution guide](https://kitsune.readthedocs.io/en/latest/contributors.html), or dive into [setting up your development environment](https://kitsune.readthedocs.io/en/latest/hacking_howto.html).\n"
},
{
  "name": "sumo-kb",
  "files": {
    "/": [
      "README.md",
      "install-firefox-linux",
      "installing-thunderbird-linux"
    ]
  },
  "makefile": null,
  "readme": "# sumo-kb\n\nRepo to host snippet codes for Mozilla Support Knowledge Base article\n\n## Instructions\n\nPlease add your files in a folder named after the KB articles slug. For example, if the article URL is https://support.mozilla.org/kb/install-firefox-linux, then use `install-firefox-linux` as the folder name.\n\nIf you need to add specific instructions for the files of an article, create a README.md file inside the article folder and add your instructions inside it.\n"
},
{
  "name": "fxa",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".eslintignore",
      ".git-blame-ignore-revs",
      ".github",
      ".gitignore",
      ".nvmrc",
      ".prettierignore",
      ".vscode",
      ".yarn",
      ".yarnrc.yml",
      "AUTHORS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "SECURITY.md",
      "_dev",
      "_scripts",
      "assets",
      "docs",
      "package-scripts.js",
      "package.json",
      "packages",
      "release-feature-branch.sh",
      "release.sh",
      "types",
      "yarn.lock"
    ],
    "/docs": [
      "adr"
    ],
    "/.github": [
      ".codecov.yaml",
      "CODEOWNERS",
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE.md",
      "dependabot.yml",
      "workflows"
    ],
    "/.circleci": [
      "README.md",
      "assert-branch.sh",
      "base-install.sh",
      "build-all.sh",
      "build.sh",
      "config.yml",
      "deploy-all.sh",
      "deploy.sh",
      "modules-to-test.js",
      "test-package.sh"
    ]
  },
  "makefile": null,
  "readme": "[![pullreminders](https://pullreminders.com/badge.svg)](https://pullreminders.com?ref=badge)\n\n## Firefox Accounts\n\nThe Firefox Accounts (fxa) monorepo\n\n### Table of Contents\n\n[Getting Started](#getting-started)\\\n[Contributing](#contributing)\\\n[Documentation](#documentation)\n[Documentation for Scripts](#documentation-for-scripts)\n\n---\n\n### Getting Started\n\nPlease read [our documentation](https://mozilla.github.io/ecosystem-platform/tutorials/development-setup)\n\n---\n\n### Contributing\n\nSee the separate [CONTRIBUTING.md](https://github.com/mozilla/fxa/blob/main/CONTRIBUTING.md) to learn how to contribute.\n\n---\n\n### Documentation\n\nThe [Firefox Ecosystem Platform](https://mozilla.github.io/ecosystem-platform/) serves as a documentation hub for Firefox Accounts and Subscription Platform.\n\n---\n\n### Documentation for Scripts\n\n#### \\_scripts/legal-md-to-pdf.sh\n\n##### Purpose\n\nThis bash script converts markdown documents into pdfs. The purpose is to provide an efficient way for Mozilla VPN legal documents to be converted and made available to end-users.\n\n##### Usage\n\n_Pre-requisites_: The script requires the following:\n\n- Local copy of Mozilla legal-docs repo: https://github.com/mozilla/legal-docs\n- pandoc: https://github.com/jgm/pandoc/blob/master/INSTALL.md\n- LaTeX: https://www.latex-project.org/get/\n\n##### To Run:\n\n```bash\n# from root fxa directory\n_scripts/legal-md-to-pdf.sh '/absolute/path/to/legal-docs/'\n```\n\nThe script traverses the legal-docs directory looking for localized copies of the Mozilla VPN legal documents. When found, the script converts the document from .md to .pdf and writes it to `assets/legal/<document_name>.<locale>.pdf`.\n\nExample:\ndirectory provided: `/Users/test/github/mozilla/legal-docs/`\nresulting file: `assets/legal/<document_name>.<locale>.pdf`\n"
},
{
  "name": "experimenter",
  "files": {
    "/": [
      ".circleci",
      ".env.integration-tests",
      ".env.sample",
      ".git-blame-ignore-revs",
      ".github",
      ".gitignore",
      ".vscode",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "app",
      "contributing.md",
      "docker-compose-integration-test.yml",
      "docker-compose-legacy.yml",
      "docker-compose-prod.yml",
      "docker-compose-test.yml",
      "docker-compose.yml",
      "kinto",
      "nginx",
      "scripts"
    ],
    "/.github": [
      "CODEOWNERS",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml",
      "get_firefox_versions.sh"
    ]
  },
  "makefile": "SHELL = /bin/bash\n\nWAIT_FOR_DB = /app/bin/wait-for-it.sh -t 30 db:5432 &&\nWAIT_FOR_RUNSERVER = /app/bin/wait-for-it.sh -t 30 localhost:7001 &&\n\nCOMPOSE = docker-compose -f docker-compose.yml\nCOMPOSE_LEGACY = ${COMPOSE} -f docker-compose-legacy.yml\nCOMPOSE_TEST = docker-compose -f docker-compose-test.yml\nCOMPOSE_PROD = docker-compose -f docker-compose-prod.yml\nCOMPOSE_INTEGRATION = ${COMPOSE_PROD} -f docker-compose-integration-test.yml\n\nJOBS = 4\nPARALLEL = parallel --halt now,fail=1 --jobs ${JOBS} {} :::\nNOCOLOR= \\033[0m\nRED = \\033[0;31m\nGREEN = \\033[0;32m\nPAD = -------------------------------------------------\\n\nCOLOR_CHECK = && echo \"${GREEN}${PAD}All Checks Passed\\n${PAD}${NOCOLOR}\" || (echo \"${RED}${PAD}Some Checks Failed\\n${PAD}${NOCOLOR}\";exit 1)\nPY_IMPORT_SORT =  python -m isort . --profile black\nPY_IMPORT_CHECK =  python -m isort . --profile black --check\nPYTHON_TEST = pytest --cov --cov-report term-missing\nPYTHON_CHECK_MIGRATIONS = python manage.py makemigrations --check --dry-run --noinput\nPYTHON_MIGRATE = python manage.py migrate\nESLINT_LEGACY = yarn workspace @experimenter/core lint\nESLINT_FIX_CORE = yarn workspace @experimenter/core lint-fix\nESLINT_NIMBUS_UI = yarn workspace @experimenter/nimbus-ui lint\nESLINT_FIX_NIMBUS_UI = yarn workspace @experimenter/nimbus-ui lint-fix\nTYPECHECK_NIMBUS_UI = yarn workspace @experimenter/nimbus-ui lint:tsc\n\nJS_TEST_LEGACY = yarn workspace @experimenter/core test\nJS_TEST_NIMBUS_UI = CI=yes yarn workspace @experimenter/nimbus-ui test:cov\nNIMBUS_SCHEMA_CHECK = python manage.py graphql_schema --out experimenter/nimbus-ui/test_schema.graphql&&diff experimenter/nimbus-ui/test_schema.graphql experimenter/nimbus-ui/schema.graphql || (echo GraphQL Schema is out of sync please run make generate_types;exit 1)\nNIMBUS_TYPES_GENERATE = python manage.py graphql_schema --out experimenter/nimbus-ui/schema.graphql&&yarn workspace @experimenter/nimbus-ui generate-types\nFLAKE8 = flake8 .\nBLACK_CHECK = black -l 90 --check --diff . --exclude node_modules\nBLACK_FIX = black -l 90 . --exclude node_modules\nCHECK_DOCS = python manage.py generate_docs --check=true\nGENERATE_DOCS = python manage.py generate_docs\nLOAD_COUNTRIES = python manage.py loaddata ./experimenter/base/fixtures/countries.json\nLOAD_LOCALES = python manage.py loaddata ./experimenter/base/fixtures/locales.json\nLOAD_LANGUAGES = python manage.py loaddata ./experimenter/base/fixtures/languages.json\nLOAD_FEATURES = python manage.py load_feature_configs\nLOAD_DUMMY_EXPERIMENTS = [[ -z $$SKIP_DUMMY ]] && python manage.py load_dummy_experiments || echo \"skipping dummy experiments\"\n\n\nJETSTREAM_CONFIG_URL = https://github.com/mozilla/jetstream-config/archive/main.zip\nFEATURE_MANIFEST_DESKTOP_URL = https://hg.mozilla.org/mozilla-central/raw-file/tip/toolkit/components/nimbus/FeatureManifest.yaml\nFEATURE_MANIFEST_FENIX_URL = https://raw.githubusercontent.com/mozilla-mobile/fenix/main/.experimenter.yaml\nFEATURE_MANIFEST_FXIOS_URL = https://raw.githubusercontent.com/mozilla-mobile/firefox-ios/main/.experimenter.yaml\nFEATURE_MANIFEST_FOCUS_ANDROID = https://raw.githubusercontent.com/mozilla-mobile/focus-android/main/.experimenter.yaml\n\nssl: nginx/key.pem nginx/cert.pem\n\nnginx/key.pem:\n\topenssl genrsa -out nginx/key.pem 4096\n\nnginx/cert.pem: nginx/key.pem\n\topenssl req -new -x509 -nodes -sha256 -key nginx/key.pem \\\n\t\t-subj \"/C=US/ST=California/L=Mountain View/O=Mozilla/CN=experiment_local\" \\\n\t\t> nginx/cert.pem\n\nsecretkey:\n\topenssl rand -hex 24\n\nauth_gcloud:\n\tgcloud auth login --update-adc\n\njetstream_config:\n\tcurl -LJ -o app/experimenter/outcomes/jetstream-config.zip $(JETSTREAM_CONFIG_URL)\n\tunzip -o -d app/experimenter/outcomes app/experimenter/outcomes/jetstream-config.zip\n\trm -Rf app/experimenter/outcomes/jetstream-config-main/.scripts/\n\nfeature_manifests:\n\tcurl -LJ --create-dirs -o app/experimenter/features/manifests/firefox-desktop.yaml $(FEATURE_MANIFEST_DESKTOP_URL)\n\tcurl -LJ --create-dirs -o app/experimenter/features/manifests/fenix.yaml $(FEATURE_MANIFEST_FENIX_URL)\n\tcurl -LJ --create-dirs -o app/experimenter/features/manifests/ios.yaml $(FEATURE_MANIFEST_FXIOS_URL)\n\tcurl -LJ --create-dirs -o app/experimenter/features/manifests/focus-android.yaml $(FEATURE_MANIFEST_FOCUS_ANDROID)\n\nfetch_external_resources: jetstream_config feature_manifests\n\techo \"External Resources Fetched\"\n\nupdate_kinto:\n\tdocker pull mozilla/kinto-dist:latest\n\nbuild_dev: fetch_external_resources ssl\n\tDOCKER_BUILDKIT=1 docker build --target dev -f app/Dockerfile -t app:dev --build-arg BUILDKIT_INLINE_CACHE=1 --cache-from mozilla/experimenter:build_dev $$([[ -z \"$${CIRCLECI}\" ]] || echo \"--progress=plain\") app/\n\nbuild_test: fetch_external_resources ssl\n\tDOCKER_BUILDKIT=1 docker build --target test -f app/Dockerfile -t app:test --build-arg BUILDKIT_INLINE_CACHE=1 --cache-from mozilla/experimenter:build_test $$([[ -z \"$${CIRCLECI}\" ]] || echo \"--progress=plain\") app/\n\nbuild_ui: fetch_external_resources ssl\n\tDOCKER_BUILDKIT=1 docker build --target ui -f app/Dockerfile -t app:ui --build-arg BUILDKIT_INLINE_CACHE=1 --cache-from mozilla/experimenter:build_ui $$([[ -z \"$${CIRCLECI}\" ]] || echo \"--progress=plain\") app/\n\nbuild_prod: build_ui fetch_external_resources ssl\n\tDOCKER_BUILDKIT=1 docker build --target deploy -f app/Dockerfile -t app:deploy --build-arg BUILDKIT_INLINE_CACHE=1 --cache-from mozilla/experimenter:latest $$([[ -z \"$${CIRCLECI}\" ]] || echo \"--progress=plain\") app/\n\ncompose_stop:\n\t$(COMPOSE) kill || true\n\t$(COMPOSE_INTEGRATION) kill || true\n\t$(COMPOSE_PROD) kill || true\n\ncompose_rm:\n\t$(COMPOSE) rm -f -v || true\n\t$(COMPOSE_INTEGRATION) rm -f -v || true\n\t$(COMPOSE_PROD) rm -f -v || true\n\nvolumes_rm:\n\tdocker volume prune -f\n\nstatic_rm:\n\trm -Rf app/node_modules\n\trm -Rf app/experimenter/legacy/legacy-ui/core/node_modules/\n\trm -Rf app/experimenter/nimbus-ui/node_modules/\n\trm -Rf app/experimenter/legacy/legacy-ui/assets/\n\trm -Rf app/experimenter/nimbus-ui/build/\n\nkill: compose_stop compose_rm volumes_rm\n\techo \"All containers removed!\"\n\ncheck: build_test\n\t$(COMPOSE_TEST) run app sh -c '$(WAIT_FOR_DB) (${PARALLEL} \"$(NIMBUS_SCHEMA_CHECK)\" \"$(PYTHON_CHECK_MIGRATIONS)\" \"$(CHECK_DOCS)\" \"${PY_IMPORT_CHECK}\" \"$(BLACK_CHECK)\" \"$(FLAKE8)\" \"$(ESLINT_NIMBUS_UI)\" \"$(TYPECHECK_NIMBUS_UI)\" \"$(JS_TEST_NIMBUS_UI)\" \"$(PYTHON_TEST)\") ${COLOR_CHECK}'\n\ncheck_legacy: build_test\n\t$(COMPOSE_TEST) run app sh -c '$(WAIT_FOR_DB) (${PARALLEL} \"$(NIMBUS_SCHEMA_CHECK)\" \"$(PYTHON_CHECK_MIGRATIONS)\" \"$(CHECK_DOCS)\" \"${PY_IMPORT_CHECK}\" \"$(BLACK_CHECK)\" \"$(FLAKE8)\" \"$(ESLINT_LEGACY)\" \"$(ESLINT_NIMBUS_UI)\" \"$(TYPECHECK_NIMBUS_UI)\" \"$(JS_TEST_LEGACY)\" \"$(JS_TEST_NIMBUS_UI)\" \"$(JS_TEST_REPORTING)\" \"$(PYTHON_TEST)\") ${COLOR_CHECK}'\n\npytest: build_test\n\t$(COMPOSE_TEST) run app sh -c '$(WAIT_FOR_DB) $(PYTHON_TEST)'\n\nup: build_dev\n\t$(COMPOSE) up\n\nup_legacy: build_dev\n\t$(COMPOSE_LEGACY) up\n\nup_prod: build_prod\n\t$(COMPOSE_PROD) up\n\nup_prod_detached: build_prod\n\t$(COMPOSE_PROD) up -d\n\nup_db: build_dev\n\t$(COMPOSE) up db redis kinto autograph\n\nup_django: build_dev\n\t$(COMPOSE) up nginx app worker beat db redis kinto autograph\n\nup_detached: build_dev\n\t$(COMPOSE) up -d\n\ngenerate_docs: build_dev\n\t$(COMPOSE) run app sh -c \"$(GENERATE_DOCS)\"\n\ngenerate_types: build_dev\n\t$(COMPOSE) run app sh -c \"$(NIMBUS_TYPES_GENERATE)\"\n\ncode_format: build_dev\n\t$(COMPOSE) run app sh -c '${PARALLEL} \"${PY_IMPORT_SORT};$(BLACK_FIX)\" \"$(ESLINT_FIX_CORE)\" \"$(ESLINT_FIX_NIMBUS_UI)\"'\n\nmakemigrations: build_dev\n\t$(COMPOSE) run app python manage.py makemigrations\n\nmigrate: build_dev\n\t$(COMPOSE) run app sh -c \"$(WAIT_FOR_DB) $(PYTHON_MIGRATE)\"\n\nbash: build_dev\n\t$(COMPOSE) run app bash\n\nrefresh: kill build_dev\n\t$(COMPOSE) run -e SKIP_DUMMY=$$SKIP_DUMMY app bash -c '$(WAIT_FOR_DB) $(PYTHON_MIGRATE)&&$(LOAD_LOCALES)&&$(LOAD_COUNTRIES)&&$(LOAD_LANGUAGES)&&$(LOAD_FEATURES)&&$(LOAD_DUMMY_EXPERIMENTS)'\n\ndependabot_approve:\n\techo \"Install and configure the Github CLI https://github.com/cli/cli\"\n\tgh pr list --author app/dependabot | awk '{print $$1}' | xargs -n1 gh pr review -a -b \"@dependabot squash and merge\"\n\n# integration tests\nintegration_shell:\n\t$(COMPOSE_INTEGRATION) run firefox bash\n\nintegration_sdk_shell:\n\t$(COMPOSE_INTEGRATION) run rust-sdk bash\n\nintegration_vnc_up:\n\t$(COMPOSE_INTEGRATION) up\n\nintegration_vnc_up_detached:\n\t$(COMPOSE_INTEGRATION) up -d firefox\n\nintegration_test_legacy:\n\tMOZ_HEADLESS=1 $(COMPOSE_INTEGRATION) run firefox sh -c \"sudo chmod a+rwx /code/app/tests/integration/.tox;tox -c app/tests/integration -e integration-test-legacy $(TOX_ARGS) -- -n 2 $(PYTEST_ARGS)\"\n\nintegration_test_nimbus:\n\tMOZ_HEADLESS=1 $(COMPOSE_INTEGRATION) run firefox sh -c \"if [ \"$$UPDATE_FIREFOX_VERSION\" = \"true\" ]; then sudo ./app/tests/integration/nimbus/utils/nightly-install.sh; fi; firefox -V; sudo apt-get -qqy update && sudo apt-get -qqy install tox;sudo chmod a+rwx /code/app/tests/integration/.tox;tox -c app/tests/integration -e integration-test-nimbus $(TOX_ARGS) -- $(PYTEST_ARGS)\"\n\nintegration_test_nimbus_rust:\n\tMOZ_HEADLESS=1 $(COMPOSE_INTEGRATION) run rust-sdk sh -c \"chmod a+rwx /code/app/tests/integration/.tox;tox -c app/tests/integration -e integration-test-nimbus-rust $(TOX_ARGS) -- -n 2 $(PYTEST_ARGS)\"\n",
  "readme": "# Mozilla Experimenter\n\n[![CircleCI](https://circleci.com/gh/mozilla/experimenter.svg?style=svg)](https://circleci.com/gh/mozilla/experimenter)\n\nExperimenter is a platform for managing experiments in [Mozilla Firefox](https://www.mozilla.org/en-US/firefox/?utm_medium=referral&utm_source=firefox-com).\n\n## Important Links\n\nCheck out the [\ud83c\udf29 **Nimbus Documentation Hub**](https://experimenter.info) or go to [the repository](https://github.com/mozilla/experimenter-docs/) that house those docs.\n\n| Link            | Prod                                                  | Staging                                                            | Local Dev (Default)                           |\n| --------------- | ----------------------------------------------------- | ------------------------------------------------------------------ | --------------------------------------------- |\n| Legacy Home     | [experimenter.services.mozilla.com][legacy_home_prod] | [stage.experimenter.nonprod.dataops.mozgcp.net][legacy_home_stage] | https://localhost                             |\n| Nimbus Home     | [/nimbus][nimbus_home_prod]                           | [/nimbus][nimbus_home_stage]                                       | [/nimbus][nimbus_home_local]                  |\n| Nimbus REST API | [/api/v6/experiments/][nimbus_rest_api_prod]          | [/api/v6/experiments/][nimbus_rest_api_stage]                      | [/api/v6/experiments/][nimbus_rest_api_local] |\n| GQL Playground  | [/api/v5/nimbus-api-graphql][gql_prod]                | [/api/v5/nimbus-api-graphql][gql_stage]                            | [/api/v5/nimbus-api-graphql][gql_local]       |\n| Remote Settings | [settings-writer.prod.mozaws.net/v1/admin][rs_prod]   | [settings-writer.stage.mozaws.net/v1/admin][rs_stage]              | http://localhost:8888/v1/admin                |\n\n[legacy_home_prod]: https://experimenter.services.mozilla.com/\n[legacy_home_stage]: https://stage.experimenter.nonprod.dataops.mozgcp.net/\n[nimbus_home_prod]: https://experimenter.services.mozilla.com/nimbus\n[nimbus_home_stage]: https://stage.experimenter.nonprod.dataops.mozgcp.net/nimbus\n[nimbus_home_local]: https://localhost/nimbus\n[nimbus_rest_api_prod]: https://experimenter.services.mozilla.com/api/v6/experiments/\n[nimbus_rest_api_stage]: https://stage.experimenter.nonprod.dataops.mozgcp.net/api/v6/experiments/\n[nimbus_rest_api_local]: https://localhost/api/v6/experiments/\n[gql_prod]: https://experimenter.services.mozilla.com/api/v5/nimbus-api-graphql/\n[gql_stage]: https://stage.experimenter.nonprod.dataops.mozgcp.net/api/v5/nimbus-api-graphql/\n[gql_local]: https://localhost/api/v5/nimbus-api-graphql/\n[rs_prod]: https://settings-writer.prod.mozaws.net/v1/admin/\n[rs_stage]: https://settings-writer.stage.mozaws.net/v1/admin/\n\n## Installation\n\n### General Setup\n\n1. Prerequisites\n\n    On all platforms:\n    - Install [Node](https://nodejs.org/en/download/releases/) to match [current version](https://github.com/mozilla/experimenter/blob/main/app/Dockerfile#L29)\n\n    On Linux:\n    - Install [Docker](https://www.docker.com/)\n    - Install [yarn](https://classic.yarnpkg.com/lang/en/docs/install)\n    - [Setup docker to run as non-root](https://docs.docker.com/engine/security/rootless/)\n\n    On MacOS:\n    - Install [Docker](https://docs.docker.com/desktop/mac/install/)\n      - Adjust resource settings\n        - CPU: Max number of cores\n        - Memory: 50% of system memory\n        - Swap: Max 4gb\n        - Disk: 100gb+\n    - Install [yarn](https://github.com/yarnpkg)\n\n1. Clone the repo\n\n        git clone <your fork>\n\n1. Copy the sample env file\n\n        cp .env.sample .env\n\n1. Set DEBUG=True for local development\n\n        vi .env\n\n1. Create a new secret key and put it in .env\n\n        make secretkey\n\n\tvi .env\n\n\t```\n\t...\n\tSECRETKEY=mynewsecretkey\n\t...\n\t```\n\n1. Run tests\n\n        make check\n\n1. Setup the database\n\n        make refresh\n\n#### Fully Dockerized Setup (continuation from General Setup 1-7)\n\n1. Run a dev instance\n\n        make up\n\n1. Navigate to it and add an SSL exception to your browser\n\n        https://localhost/\n\n#### Semi Dockerized Setup (continuation from General Setup 1-7)\n\nOne might choose the semi dockerized approach for:\n\n1. faster startup/teardown time (not having to rebuild/start/stop containers)\n1. better IDE integration\n\nNotes:\n\n- [osx catalina, reinstall command line tools](https://medium.com/flawless-app-stories/gyp-no-xcode-or-clt-version-detected-macos-catalina-anansewaa-38b536389e8d)\n\n- Install [poetry](https://python-poetry.org/docs/#installation)\n\n##### Semi Dockerized Setup Steps\n\n1.  Pre reqs\n    macOS instructions:\n\n        brew install postgresql llvm openssl yarn\n\n        echo 'export PATH=\"/usr/local/opt/llvm/bin:$PATH\"' >> ~/.bash_profile\n        export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/opt/openssl/lib/\n\n    Ubuntu 20.04 instructions:\n\n        # general deps (also see `poetry` link above)\n        sudo apt install postgresql llvm openssl yarn\n\n        # add'l deps* for poetry / python setup\n        sudo apt install libpq5=12.9-0ubuntu0.20.04.1\n        sudo apt install libpq-dev\n\n    _*Notes_\n    - _the specific libpq5 version shown here is required for libpq-dev at time of writing_\n    - _`poetry install` (next step) requires python 3.9, but there are multiple options for resolving this, see [here](https://python-poetry.org/docs/managing-environments/#switching-between-environments)_\n\n2.  Install dependencies\n\n        source .env\n\n        cd app\n        poetry install # see note above\n\n        yarn install\n\n3.  env values\n\n        .env (set at root):\n        DEBUG=True\n        DB_HOST=localhost\n        HOSTNAME=localhost\n\n4.  Start postgresql, redis, autograph, kinto\n\n        make up_db (from project root)\n\n5.  Django app\n\n        # in app\n\n        poetry shell\n\n        yarn workspace @experimenter/nimbus-ui build\n        yarn workspace @experimenter/core build\n\n        # run in separate shells (`poetry shell` in each)\n        yarn workspace @experimenter/nimbus-ui start\n        ./manage.py runserver 0.0.0.0:7001\n\n*Pro-tip*: we have had at least one large code refactor. You can ignore specific large commits when blaming by setting the Git config's `ignoreRevsFile` to `.git-blame-ignore-revs`:\n\n```\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\n```\n\n### VSCode setup\n\n1. If using VSCode, configure workspace folders\n\n   - Add `/experimenter/` and `/experimenter/app` folders to your workspace (File -> Add Folder to Workspace -> `path/to/experimenter/app`)\n   - From the `/experimenter/app` folder, run `yarn install`\n       - Make sure you are using the correct version of node\n\n           node -v\n\n       - Troubleshooting:\n           - [Changing node version](https://stackoverflow.com/a/50817276/12178648)\n           - Clear npm cache: `npm cache clean --force`\n\n### Google Credentials for Jetstream\n\nOn certain pages an API endpoint is called to receive experiment analysis data from Jetstream to display visualization tables. To see experiment visualization data, you must provide GCP credentials.\n\n0. Prequisites\n    - Install GCP CLI\n      - Follow the instructions [here](https://cloud.google.com/sdk/docs/install)\n      - Project: `moz-fx-data-experiments`\n   - Verify/request project permissions\n      - Check if you already have access to the storage bucket [here](https://console.cloud.google.com/storage/browser/mozanalysis)\n      - If needed, ask in `#nimbus-dev` for a project admin to grant `storage.objects.list` permissions on the `moz-fx-data-experiments` project\n\n1. Authorize CLI with your account\n    - `make auth_gcloud`\n      - this will save your credentials locally to a well-known location for use by any library that requests ADC\n      - **Note**: if this returns `Error saving Application Default Credentials: Unable to write file [...]: [Errno 21] Is a directory: ...`, delete the directory and try again (`rm -rf ~/.config/gcloud`)\n\n2. The next time you rebuild the docker-compose environment, your credentials will be loaded as a volume\n    - Note that this will require the existing volume to be removed (hint: run `make refresh`)\n\n3. (optional) Verify access\n    - `make refresh`\n    - `make bash`\n    - `./manage.py shell`\n        - ```\n          from django.core.files.storage import default_storage\n          default_storage.listdir('/')\n          ```\n        - Confirm this second command prints a list instead of an error\n\n\n### Google Cloud Bucket for Media Storage\n\nWe support user uploads of media (e.g. screenshots) for some features.\n\nIn local development, the default is to store these files in `/app/media` using Django's `FileSystemStorage` class and the `MEDIA_ROOT` and `MEDIA_URL` settings.\n\nIn production, a GCP bucket and credentials are required.\n\nThe bucket name is configured with the `UPLOADS_GS_BUCKET_NAME` setting. For example:\n\n```\nUPLOADS_GS_BUCKET_NAME=nimbus-experimenter-media-dev-uploads\n```\n\nFor local testing of a production-like environment, The credentials should be configured as described in the previous section on Google Credentials for Jetstream.\n\nIn the real production deployment, credentials are configured via [workload identity in Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity).\n\n## Usage\n\nExperimenter uses [docker](https://www.docker.com/) for all development, testing, and deployment.\n\n### Building\n#### make build\n\nBuild the application container by executing the [build script](https://github.com/mozilla/experimenter/blob/main/scripts/build.sh)\n\n#### make compose_build\n\nBuild the supporting services (nginx, postgresql) defined in the [compose file](https://github.com/mozilla/experimenter/blob/main/docker-compose.yml)\n\n#### make ssl\n\nCreate dummy SSL certs to use the dev server over a locally secure\nconnection. This helps test client behaviour with a secure\nconnection. This task is run automatically when needed.\n\n#### make kill\n\nStop and delete all docker containers.\nWARNING: this will remove your database and all data. Use this to reset your dev environment.\n\n#### make migrate\n\nApply all django migrations to the database.  This must be run after removing database volumes before starting a dev instance.\n\n#### make load_dummy_experiments\n\nPopulates the database with dummy experiments of all types/statuses using the test factories\n\n#### make refresh\n\nRun kill, migrate, load_locales_countries load_dummy_experiments.  Useful for resetting your dev environment when switching branches or after package updates.\n\n### Running a dev instance\n#### make up\n\nStart a dev server listening on port 80 using the [Django runserver](https://docs.djangoproject.com/en/1.10/ref/django-admin/#runserver).  It is useful to run `make refresh` first to ensure your database is up to date with the latest migrations and test data.\n\n#### make up_db\n\nStart postgresql, redis, autograph, kinto on their respective ports to allow running the Django runserver and yarn watchers locally (non containerized)\n\n#### make up_django\n\nStart Django runserver, Celery worker, postgresql, redis, autograph, kinto on their respective ports to allow running the yarn watchers locally (non containerized)\n\n#### make up_detached\n\nStart all containers in the background (not attached to shell).  They can be stopped using `make kill`.\n\n#### make update_kinto\n\nPull in the latest Kinto Docker image. Kinto is not automatically updated when new versions are available, so this command can be used occasionally to stay in sync.\n\n### Running tests and checks\n\n#### make check\n\nRun all test and lint suites, this is run in CI on all PRs and deploys.\n\n#### make py_test\n\nRun only the python test suite.\n\n#### make bash\n\nStart a bash shell inside the container.  This lets you interact with the containerized filesystem and run Django management commands.\n\n##### Helpful Python Tips\nYou can run the entire python test suite without coverage using the Django test runner:\n\n```sh\n./manage.py test\n```\n\nFor faster performance you can run all tests in parallel:\n\n```sh\n./manage.py test --parallel\n```\n\nYou can run only the tests in a certain module by specifying its Python import path:\n\n```sh\n./manage.py test experimenter.experiments.tests.api.v5.test_serializers\n```\n\nFor more details on running Django tests refer to the [Django test documentation](https://docs.djangoproject.com/en/3.1/topics/testing/overview/#running-tests)\n\nTo debug a test, you can use ipdb by placing this snippet anywhere in your code, such as within a test method or inside some application logic:\n\n```py\nimport ipdb\nipdb.set_trace()\n```\n\nThen invoke the test using its full path:\n\n```sh\n./manage.py test experimenter.some_module.tests.some_test_file.SomeTestClass.test_some_thing\n```\n\nAnd you will enter an interactive iPython shell at the point where you placed the ipdb snippet, allowing you to introspect variables and call methods\n\nFor coverage you can use pytest, which will run all the python tests and track their coverage, but it is slower than using the Django test runner:\n\n```sh\npytest --cov --cov-report term-missing\n```\n\nYou can also enter a Python shell to import and interact with code directly, for example:\n\n```sh\n./manage.py shell\n```\n\nAnd then you can import and execute arbitrary code:\n\n```py\nfrom experimenter.experiments.models import NimbusExperiment\nfrom experimenter.experiments.tests.factories import NimbusExperimentFactory\nfrom experimenter.kinto.tasks import nimbus_push_experiment_to_kinto\n\nexperiment = NimbusExperimentFactory.create_with_status(NimbusExperiment.Status.DRAFT, name=\"Look at me, I'm Mr Experiment\")\nnimbus_push_experiment_to_kinto(experiment.id)\n```\n\n##### Helpful Yarn Tips\nYou can also interact with the yarn commands, such as checking TypeScript for Nimbus UI:\n\n```sh\nyarn workspace @experimenter/nimbus-ui lint:tsc\n```\n\nOr the test suite for Nimbus UI:\n\n```sh\nyarn workspace @experimenter/nimbus-ui test:cov\n```\n\nFor a full reference of all the common commands that can be run inside the container, refer to [this section of the Makefile](https://github.com/mozilla/experimenter/blob/main/Makefile#L16-L38)\n\n\n\n\n#### make integration_test_legacy\n\nRun the integration test suite for experimenter inside a containerized instance of Firefox. You must also be already running a `make up` dev instance in another shell to run the integration tests.\n\n#### make integration_test_nimbus\n\nRun the integration test suite for nimbus inside a containerized instance of Firefox. You must also be already running a `make up` dev instance in another shell to run the integration tests.\n\n#### make integration_vnc_up\n\nFirst start a prod instance of Experimenter with:\n\n```bash\nmake refresh&&make up_prod_detached\n```\n\nThen start the VNC service:\n\n```bash\nmake integration_vnc_up\n```\n\nThen open your VNC client (Safari does this on OSX or just use [VNC Viewer](https://www.realvnc.com/en/connect/download/viewer/)) and open `vnc://localhost:5900` with password `secret`. Right click on the desktop and select `Applications > Shell > Bash` and enter:\n\n```bash\ncd app\nsudo mkdir -m 0777 tests/integration/.tox/logs\ntox -c tests/integration/\n```\n\nThis should run the integration tests and watch them run in a Firefox instance you can watch and interact with.\n\n#### Integration Test options\n\n- `TOX_ARGS`: [Tox](https://tox.readthedocs.io/en/latest/config.html#tox) commandline variables.\n- `PYTEST_ARGS`: [Pytest](https://docs.pytest.org/en/6.2.x/usage.html#) commandline variables.\n\nAn example using PYTEST_ARGS to run one test.\n```bash\nmake integration_test_legacy PYTEST_ARGS=\"-k test_addon_rollout_experiment_e2e\"\n```\n\n## Accessing Remote Settings locally\n\nIn development you may wish to approve or reject changes to experiments as if they were on Remote Settings. You can do so here: `http://localhost:8888/v1/admin/`\n\nThere are three accounts you can log into Kinto with depending on what you want to do:\n\n- `admin` / `admin` - This account has permission to view and edit all of the collections.\n- `experimenter` / `experimenter` - This account is used by Experimenter to push its changes to Remote Settings and mark them for review.\n- `review` / `review` - This account should generally be used by developers testing the workflow, it can be used to approve/reject changes pushed from Experimenter.\n\nThe `admin` and `review` credentials are hard-coded [here](https://github.com/mozilla/experimenter/blob/main/app/bin/setup_kinto.py#L7-L8), and the `experimenter` credentials can be found or updated in your `.env` file under `KINTO_USER` and `KINTO_PASS`.\n\nAny change in remote settings requires two accounts:\n\n- One to make changes and request a review\n- One to review and approve/reject those changes\n\nAny of the accounts above can be used for any of those two roles, but your local Experimenter will be configured to make its changes through the `experimenter` account, so that account can't also be used to approve/reject those changes, hence the existence of the `review` account.\n\nFor more detailed information on the Remote Settings integration please see the [Kinto module documentation](app/experimenter/kinto/README.md).\n\n\n## Frontend\n\nExperimenter has two front-end UIs:\n\n- [`core`](./app/experimenter/legacy/legacy-ui/core) is the legacy UI used for Experimenter intake which will remain until `nimbus-ui` supersedes it\n- [`nimbus-ui`](./app/experimenter/nimbus-ui) is the Nimbus Console UI for Experimenter that is actively being developed\n\nLearn more about the organization of these UIs [here](./app/experimenter/legacy/legacy-ui/README.md).\n\n**Also see the [nimbus-ui README](https://github.com/mozilla/experimenter/tree/main/app/experimenter/nimbus-ui) for relevent Nimbus documentation.**\n\n## API\n\nAPI documentation can be found [here](https://htmlpreview.github.io/?https://github.com/mozilla/experimenter/blob/main/app/experimenter/docs/swagger-ui.html)\n\n## Contributing\n\nPlease see our [Contributing Guidelines](https://github.com/mozilla/experimenter/blob/main/contributing.md)\n\n## License\n\nExperimenter uses the [Mozilla Public License](https://www.mozilla.org/en-US/MPL/)\n"
},
{
  "name": "multi-account-containers",
  "files": {
    "/": [
      ".env",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".htmllintrc",
      ".jpmignore",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "bin",
      "docs",
      "icon.png",
      "package.json",
      "src",
      "test"
    ],
    "/docs": [
      "release.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Multi-Account Containers\n\n[![Test](https://github.com/mozilla/multi-account-containers/actions/workflows/test.yaml/badge.svg)](https://github.com/mozilla/multi-account-containers/actions/workflows/test.yaml)\n\nThe Firefox Multi-Account Containers extension lets you carve out a separate box for each of your online lives \u2013 no more opening a different browser just to check your work email!\n\nLearn more about Multi-Account Containers in\n[our end-user documentation][enduser].\n\n## Contributing\n\nEveryone is welcome to contribute to Multi-Account Containers. To learn how\nto contribute a patch to Multi-Account Container, please\n[read our contributing guide][contributing].\n\nYou can also chat with us on [our Matrix room][matrix] or [our forum][forum].\n\nThis repository is governed by Mozilla's code of conduct and etiquette\nguidelines. For more details, [please read the Mozilla Community Participation Guidelines][cpg].\n\n### License\n\nThis Source Code Form is subject to the terms of the Mozilla Public\nLicense, v. 2.0. If a copy of the MPL was not distributed with this\nfile, You can obtain one at https://mozilla.org/MPL/2.0/.\n\n<!-- Please keep the list in alphabetical order -->\n[contributing]: CONTRIBUTING.md\n[cpg]: https://www.mozilla.org/about/governance/policies/participation/\n[enduser]: https://support.mozilla.org/en-US/kb/containers\n[forum]: https://discourse.mozilla.org/c/containers/223\n[matrix]: https://matrix.to/#/#containers:mozilla.org\n"
},
{
  "name": "OneCRL-Tools",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bugs",
      "bugzilla",
      "ccadb",
      "ccadb2OneCRL",
      "certdata",
      "certdataDiffCCADB",
      "cmd",
      "config",
      "containers",
      "entryMaker",
      "go.mod",
      "go.sum",
      "kinto",
      "obsDiffCCADB",
      "observatory",
      "oneCRL",
      "oneCRL2CSV",
      "oneCRL2RevocationsTxt",
      "oneCRLDiff",
      "one_crl_to_cert_storage",
      "salesforce",
      "salesforce2OneCRL",
      "summarizeOneCRL",
      "tests",
      "tools",
      "transaction",
      "util"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# OneCRL-Tools\n\n[![Build Status](https://travis-ci.org/mozilla/OneCRL-Tools.svg?branch=master)](https://travis-ci.org/mozilla/OneCRL-Tools)\n\nSome tools for supporting OneCRL\n"
},
{
  "name": "pontoon",
  "files": {
    "/": [
      ".codecov.yml",
      ".dockerignore",
      ".eslintignore",
      ".eslintrc.js",
      ".flake8",
      ".git-blame-ignore-revs",
      ".github",
      ".gitignore",
      ".npmrc",
      ".prettierignore",
      ".pyup.yml",
      "Aptfile",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "Procfile",
      "README.md",
      "SECURITY.md",
      "app.json",
      "babel.config.json",
      "bin",
      "contribute.json",
      "docker-compose.yml",
      "docker",
      "docs",
      "error_pages",
      "l10n.toml",
      "manage.py",
      "media",
      "package-lock.json",
      "package.json",
      "pontoon",
      "pytest.ini",
      "requirements.txt",
      "requirements",
      "runtime.txt",
      "setup.cfg",
      "setup.py",
      "specs",
      "tag-admin",
      "translate"
    ],
    "/docs": [
      "Makefile",
      "admin",
      "conf.py",
      "dev",
      "img",
      "index.rst",
      "requirements.txt",
      "user"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "DC := $(shell which docker-compose)\nDOCKER := $(shell which docker)\n\n# *IMPORTANT*\n# Don't use this instance in a production setting. More info at:\n# https://docs.djangoproject.com/en/dev/ref/django-admin/#runserver\nSITE_URL ?= http://localhost:8000\n\nUSER_ID?=1000\nGROUP_ID?=1000\n\n.PHONY: build build-translate build-tagadmin build-server server-env setup run clean shell ci test test-translate test-tagadmin test-server jest pytest format lint types eslint prettier check-prettier flake8 pyupgrade check-pyupgrade black check-black dropdb dumpdb loaddb sync-projects requirements\n\nhelp:\n\t@echo \"Welcome to Pontoon!\\n\"\n\t@echo \"The list of commands for local development:\\n\"\n\t@echo \"  build            Builds the docker images for the docker-compose setup\"\n\t@echo \"  build-translate  Builds just the translate frontend component\"\n\t@echo \"  build-tagadmin   Builds just the tag-admin frontend component\"\n\t@echo \"  build-server     Builds just the Django server image\"\n\t@echo \"  server-env       Regenerates the env variable file used by server\"\n\t@echo \"  setup            Configures a local instance after a fresh build\"\n\t@echo \"  run              Runs the whole stack, served on http://localhost:8000/\"\n\t@echo \"  clean            Forces a rebuild of docker containers\"\n\t@echo \"  shell            Opens a Bash shell in a server docker container\"\n\t@echo \"  ci               Test and lint all code\"\n\t@echo \"  test             Runs all test suites\"\n\t@echo \"  test-translate   Runs the translate frontend test suite (Jest)\"\n\t@echo \"  test-tagadmin    Runs the tag-admin test suite (Jest)\"\n\t@echo \"  test-server      Runs the server test suite (Pytest)\"\n\t@echo \"  format           Runs all formatters\"\n\t@echo \"  lint             Runs all linters\"\n\t@echo \"  types            Runs the tsc compiler to check TypeScript on all frontend code\"\n\t@echo \"  eslint           Runs a code linter on the JavaScript code\"\n\t@echo \"  prettier         Runs the Prettier formatter\"\n\t@echo \"  check-prettier   Runs a check for format issues with the Prettier formatter\"\n\t@echo \"  flake8           Runs the flake8 style guides on all Python code\"\n\t@echo \"  pyupgrade        Upgrades all Python code to newer syntax of Python\"\n\t@echo \"  check-pyupgrade  Runs a check for outdated syntax of Python with the pyupgrade formatter\"\n\t@echo \"  black            Runs the black formatter on all Python code\"\n\t@echo \"  check-black      Runs a check for format issues with the black formatter\"\n\t@echo \"  dropdb           Completely remove the postgres container and its data\"\n\t@echo \"  dumpdb           Create a postgres database dump with timestamp used as file name\"\n\t@echo \"  loaddb           Load a database dump into postgres, file name in DB_DUMP_FILE\"\n\t@echo \"  sync-projects    Runs the synchronization task on all projects\"\n\t@echo \"  requirements     Compiles all requirements files with pip-compile\\n\"\n\ntranslate/dist:\n\tmake build-translate\ntag-admin/dist:\n\tmake build-tagadmin\n.server-build:\n\tmake build-server\nnode_modules:\n\tnpm install\n\nbuild: build-translate build-tagadmin build-server\n\nbuild-translate: node_modules\n\tnpm run build -w translate\n\nbuild-tagadmin: node_modules\n\tnpm run build -w tag-admin\n\nbuild-server: server-env translate/dist tag-admin/dist\n\t\"${DC}\" build --build-arg USER_ID=$(USER_ID) --build-arg GROUP_ID=$(GROUP_ID) server\n\ttouch .server-build\n\nserver-env:\n\tsed -e 's/#SITE_URL#/$(subst /,\\/,${SITE_URL})/g' \\\n\t./docker/config/server.env.template > ./docker/config/server.env\n\nsetup: .server-build\n\t\"${DC}\" run server //app/docker/server_setup.sh\n\nrun: translate/dist tag-admin/dist .server-build\n\t\"${DC}\" up --detach\n\tbash -c 'set -m; bash ./bin/watch.sh'\n\t\"${DC}\" stop\n\nclean:\n\trm -rf translate/dist tag-admin/dist .server-build\n\nshell:\n\t\"${DC}\" run --rm server //bin/bash\n\nci: test lint\n\ntest: test-server test-translate test-tagadmin\n\ntest-translate: jest\njest:\n\tnpm test -w translate\n\ntest-tagadmin:\n\tnpm test -w tag-admin\n\ntest-server: pytest\npytest:\n\t\"${DC}\" run ${run_opts} --rm server pytest --cov-report=xml:pontoon/coverage.xml --cov=. $(opts)\n\nformat: prettier pyupgrade black\n\nlint: types eslint check-prettier flake8 check-pyupgrade check-black\n\ntypes:\n\tnpm run types -w translate\n\neslint:\n\tnpm run eslint\n\nprettier:\n\tnpm run prettier\n\ncheck-prettier:\n\tnpm run check-prettier\n\nflake8:\n\t\"${DC}\" run --rm server flake8 pontoon/\n\npyupgrade:\n\t\"${DC}\" run --rm server pyupgrade --exit-zero-even-if-changed --py38-plus *.py `find pontoon -name \\*.py`\n\ncheck-pyupgrade:\n\t\"${DC}\" run --rm server pyupgrade --py38-plus *.py `find pontoon -name \\*.py`\n\nblack:\n\t\"${DC}\" run --rm server black pontoon/\n\ncheck-black:\n\t\"${DC}\" run --rm server black --check pontoon\n\ndropdb:\n\t\"${DC}\" down --volumes postgresql\n\ndumpdb:\n\t\"${DOCKER}\" exec -t `\"${DC}\" ps -q postgresql` pg_dumpall -c -U pontoon > dump_`date +%d-%m-%Y\"_\"%H_%M_%S`.sql\n\nloaddb:\n\t# Stop connections to the database so we can drop it.\n\t-\"${DC}\" stop server\n\t# Make sure the postgresql container is running.\n\t-\"${DC}\" start postgresql\n\t-\"${DC}\" exec postgresql dropdb -U pontoon pontoon\n\t\"${DC}\" exec postgresql createdb -U pontoon pontoon\n\t# Note: docker-compose doesn't support the `-i` (--interactive) argument\n\t# that we need to send the dump file through STDIN. We thus are forced to\n\t# use docker here instead.\n\t\"${DOCKER}\" exec -i `\"${DC}\" ps -q postgresql` pg_restore -U pontoon -d pontoon -O < \"${DB_DUMP_FILE}\"\n\nsync-projects:\n\t\"${DC}\" run --rm server .//manage.py sync_projects $(opts)\n\nrequirements:\n\t# Pass --upgrade to upgrade all dependencies\n\t# The arguments are passed through to pip-compile\n\t\"${DC}\" run --rm server //app/docker/compile_requirements.sh ${opts}\n",
  "readme": "# Pontoon &mdash; Mozilla's Localization Platform\n\nPontoon is a translation management system used and developed by the\n[Mozilla localization community](https://pontoon.mozilla.org/). It\nspecializes in open source localization that is driven by the community and\nuses version-control systems for storing translations.\n\n[\ud83d\udcda **Documentation**](https://mozilla-pontoon.readthedocs.io/)\n\n## Installing Pontoon\n\nIf you are looking to host your own instance of Pontoon, there are several ways to do so.\nTo deploy Pontoon to Heroku without leaving your web browser, click the **Deploy to\nHeroku** button below.\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy)\n\nAlternatively, you can deploy to Heroku manually by following our\n[Deployment Documentation](https://mozilla-pontoon.readthedocs.io/en/latest/admin/deployment.html).\n\nWe don't have documentation for deploying to other platforms yet, so we recommend that\nyou read the previously linked documentation and adapt it for your needs.\n\nIf you only want to deploy a **local instance of Pontoon**, for development or\ntesting for example, see our\n[Developer Setup using Docker](https://mozilla-pontoon.readthedocs.io/en/latest/dev/setup.html).\nPlease note that you should **not** deploy a production instance with Docker.\n\n## Contributing to Pontoon\n\nDo you want to help us make Pontoon better? We are very glad!\n\nTo help you get started with contributing, we wrote\n[**The Guide to your First Contribution to Pontoon**](https://mozilla-pontoon.readthedocs.io/en/latest/dev/first-contribution.html).\nIt contains all the information you need to know to install Pontoon, populate its\ndatabase, run tests, and send your contribution.\n\nIf you want to go further, you can:\n\n- Check out development roadmap on the [wiki](https://wiki.mozilla.org/Pontoon)\n- Report an [issue](https://github.com/mozilla/pontoon/issues/new)\n- Check [existing issues](https://github.com/mozilla/pontoon/issues)\n- See Mozilla's Pontoon servers:\n  - [Staging](https://mozilla-pontoon-staging.herokuapp.com/)\n  - [Production](https://pontoon.mozilla.org/)\n- For discussing Pontoon's development, get in touch with us on [chat.mozilla.org](https://chat.mozilla.org/#/room/#pontoon:mozilla.org)\n- For feedback, support, and 3rd party deployments, check out [Discourse](https://discourse.mozilla.org/c/pontoon/)\n\n## License\n\nThis software is licensed under the\n[New BSD License](https://creativecommons.org/licenses/BSD/). For more\ninformation, read [LICENSE](https://github.com/mozilla/pontoon/blob/master/LICENSE).\n\n## Screenshots\n\n![](docs/img/screenshots/teams-dashboard.png)\n_Teams dashboard_\n\n![](docs/img/screenshots/translation-app.png)\n_Translation app_\n"
},
{
  "name": "bergamot-translator",
  "files": {
    "/": [
      ".circleci",
      ".clang-format",
      ".clang-format-ignore",
      ".clang-tidy",
      ".github",
      ".gitignore",
      ".gitmodules",
      "3rd_party",
      "BERGAMOT_VERSION",
      "CMakeLists.txt",
      "Doxyfile.in",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "app",
      "bergamot-translator-tests",
      "bindings",
      "build-wasm.sh",
      "cmake",
      "doc",
      "examples",
      "patches",
      "run-clang-format.py",
      "setup.py",
      "src",
      "wasm"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Bergamot Translator\n\n[![CircleCI badge](https://img.shields.io/circleci/project/github/browsermt/bergamot-translator/main.svg?label=CircleCI)](https://circleci.com/gh/browsermt/bergamot-translator/)\n\nBergamot translator provides a unified API for ([Marian NMT](https://marian-nmt.github.io/) framework based) neural machine translation functionality in accordance with the [Bergamot](https://browser.mt/) project that focuses on improving client-side machine translation in a web browser.\n\n## Build Instructions\n\n### Build Natively\nCreate a folder where you want to build all the artifacts (`build-native` in this case) and compile\n\n```bash\nmkdir build-native\ncd build-native\ncmake ../\nmake -j2\n```\n\n### Build WASM\n#### Prerequisite\n\nBuilding on wasm requires Emscripten toolchain. It can be downloaded and installed using following instructions:\n\n* Get the latest sdk: `git clone https://github.com/emscripten-core/emsdk.git`\n* Enter the cloned directory: `cd emsdk`\n* Install the sdk: `./emsdk install 3.1.8`\n* Activate the sdk: `./emsdk activate 3.1.8`\n* Activate path variables: `source ./emsdk_env.sh`\n\n#### <a name=\"Compile\"></a> Compile\n\nTo build a version that translates with higher speeds on Firefox Nightly browser, follow these instructions:\n\n   1. Create a folder where you want to build all the artifacts (`build-wasm` in this case) and compile\n       ```bash\n       mkdir build-wasm\n       cd build-wasm\n       emcmake cmake -DCOMPILE_WASM=on ../\n       emmake make -j2\n       ```\n\n       The wasm artifacts (.js and .wasm files) will be available in the build directory (\"build-wasm\" in this case).\n\n   2. Enable SIMD Wormhole via Wasm instantiation API in generated artifacts\n       ```bash\n       bash ../wasm/patch-artifacts-enable-wormhole.sh\n       ```\n\n   3. Patch generated artifacts to import GEMM library from a separate wasm module\n       ```bash\n       bash ../wasm/patch-artifacts-import-gemm-module.sh\n       ```\n\nTo build a version that runs on all browsers (including Firefox Nightly) but translates slowly, follow these instructions:\n\n  1. Create a folder where you want to build all the artifacts (`build-wasm` in this case) and compile\n      ```bash\n      mkdir build-wasm\n      cd build-wasm\n      emcmake cmake -DCOMPILE_WASM=on -DWORMHOLE=off ../\n      emmake make -j2\n      ```\n\n  2. Patch generated artifacts to import GEMM library from a separate wasm module\n       ```bash\n       bash ../wasm/patch-artifacts-import-gemm-module.sh\n       ```\n\n#### Recompiling\nAs long as you don't update any submodule, just follow [Compile](#Compile) steps.\\\nIf you update a submodule, execute following command in repository root folder before executing\n[Compile](#Compile) steps.\n```bash\ngit submodule update --init --recursive\n```\n\n\n## How to use\n\n### Using Native version\n\nThe builds generate library that can be integrated to any project. All the public header files are specified in `src` folder.\\\nA short example of how to use the APIs is provided in `app/main.cpp` file.\n\n### Using WASM version\n\nPlease follow the `README` inside the `wasm` folder of this repository that demonstrates how to use the translator in JavaScript.\n"
},
{
  "name": "gcp-ingestion",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      ".mermaid",
      ".prettierignore",
      ".spelling",
      "CODE_OF_CONDUCT.md",
      "GRAVEYARD.md",
      "LICENSE",
      "README.md",
      "bin",
      "checkstyle",
      "codecov.yml",
      "custom_docs_theme",
      "docs",
      "ingestion-beam",
      "ingestion-core",
      "ingestion-edge",
      "ingestion-sink",
      "mkdocs.yml",
      "pom.xml"
    ],
    "/docs": [
      "architecture",
      "diagrams",
      "index.md",
      "ingestion-beam",
      "ingestion-edge",
      "ingestion-sink"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Telemetry Ingestion on Google Cloud Platform\n\n[![CircleCI](https://circleci.com/gh/mozilla/gcp-ingestion.svg?style=svg&circle-token=d98a470269580907d5c6d74d0e67612834a21be7)](https://circleci.com/gh/mozilla/gcp-ingestion)\n\nA monorepo for documentation and implementation of the Mozilla telemetry\ningestion system deployed to Google Cloud Platform (GCP).\n\nFor more information, see [the documentation](https://mozilla.github.io/gcp-ingestion).\n"
},
{
  "name": "mozanalysis",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "docs",
      "setup.cfg",
      "setup.py",
      "src",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "_ext",
      "about.rst",
      "api.rst",
      "api",
      "conf.py",
      "guide.rst",
      "index.rst"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Experiments Analysis [![CircleCI](https://circleci.com/gh/mozilla/mozanalysis.svg?style=svg)](https://circleci.com/gh/mozilla/mozanalysis) [![codecov](https://codecov.io/gh/mozilla/mozanalysis/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/mozanalysis) [![CalVer - Timely Software Versioning](https://img.shields.io/badge/calver-YYYY.M.MINOR-22bfda.svg)](https://calver.org/)\n\nThe `mozanalysis` Python library is a library to standardize experiment analysis\nat Mozilla for the purpose of producing decision reports templates that are\nedited by data scientists.\n\n## Documentation\n\nOnline documentation is available at https://mozilla.github.io/mozanalysis/\n\n## Installing from pypi\n- To install this package from pypi run:\n```\npip install mozanalysis\n```\n\n## Testing locally\n\n### with Tox\n\nInstall tox into your global Python environment and run `tox`.\n\nYou can pass flags to tox to limit the different environments you test in\nor the tests you run. Options after `--` or positional arguments are forwarded to pytest.\n\nFor example, you can run:\n\n* `tox -e lint` to lint\n* `tox -e py37 -- -k utils` to only run tests with \"utils\" somewhere in the name, on Python 3.7\n* `tox tests/test_utils.py` to run tests in a specific file\n\n### with the CircleCI utilities\n\nTo test/debug this package locally, you can run exactly the job that\nCircleCI runs for continuous integration by\n[installing the CircleCI local CLI](https://circleci.com/docs/2.0/local-cli/#installing-the-circleci-local-cli-on-macos-and-linux-distros)\nand invoking:\n\n```bash\ncircleci build --job py37\n```\n\nSee [.circleci/config.yml](https://github.com/mozilla/mozanalysis/blob/main/.circleci/config.yml)\nfor the other configured job names (for running tests on different python versions).\n\n## Deploying a new release\n\nReleasing mozanalysis happens by tagging a CalVer based Git tag with the\nfollowing pattern:\n\n    YYYY.M.MINOR\n\nwhere YYYY is the four-digit year number, M is a single-digit month number and\nMINOR is a single-digit zero-based counter which does NOT relate to the day of\nthe release. Valid versions numbers are:\n\n    2017.10.0\n    2018.1.0\n    2018.12.12\n\nOnce the (signed) Git tag has been pushed to the main GitHub repository using\ngit push origin --tags, Circle CI will automatically build and push a release to\nPyPI after the tests have passed.\n"
},
{
  "name": "hubs",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".defaults.env",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".htmlhintrc",
      ".prettierignore",
      ".prettierrc.json",
      ".storybook",
      ".stylelintrc",
      ".vscode",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Jenkinsfile",
      "LICENSE",
      "PRIVACY.md",
      "PROMOTION.md",
      "README.md",
      "REMIXING.md",
      "RetPageOriginDockerfile",
      "SECURITY.md",
      "TERMS.md",
      "admin",
      "doc",
      "habitat",
      "package-lock.json",
      "package.json",
      "scripts",
      "src",
      "test",
      "themes.json",
      "webpack.config.js"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "config.yml",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# [Mozilla Hubs](https://hubs.mozilla.com/)\n\n[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](https://opensource.org/licenses/MPL-2.0) [![Build Status](https://travis-ci.org/mozilla/hubs.svg?branch=master)](https://travis-ci.org/mozilla/hubs) [![Discord](https://img.shields.io/discord/498741086295031808)](https://discord.gg/CzAbuGu)\n\nThe client-side code for [Mozilla Hubs](https://hubs.mozilla.com/), an online 3D collaboration platform that works for desktop, mobile, and VR platforms.\n\n[Learn more about Hubs](https://hubs.mozilla.com/docs/welcome.html)\n\n## Getting Started\n\nIf you would like to run Hubs on your own servers, check out [Hubs Cloud](https://hubs.mozilla.com/docs/hubs-cloud-intro.html).\n\nIf you would like to deploy a custom client to your existing Hubs Cloud instance please refer to [this guide](https://hubs.mozilla.com/docs/hubs-cloud-custom-clients.html).\n\nIf you would like to contribute to the main fork of the Hubs client please see the [contributor guide](./CONTRIBUTING.md).\n\nIf you just want to check out how Hubs works and make your own modifications continue on to our Quick Start Guide.\n\n### Quick Start\n\n[Install NodeJS](https://nodejs.org) if you haven't already. We recommend version 12 or above.\n\nRun the following commands:\n\n```bash\ngit clone https://github.com/mozilla/hubs.git\ncd hubs\nnpm ci\nnpm run dev\n```\n\nThe backend dev server is configured with CORS to only accept connections from \"hubs.local:8080\", so you will need to access it from that host. To do this, you likely want to add \"hubs.local\" and \"hubs-proxy.local\" to the [local \"hosts\" file](https://phoenixnap.com/kb/how-to-edit-hosts-file-in-windows-mac-or-linux) on your computer:\n\n```\n127.0.0.1\thubs.local\n127.0.0.1\thubs-proxy.local\n```\n\nThen visit https://hubs.local:8080 (note: HTTPS is required, you'll need to accept the warning for the self-signed SSL certificate)\n\n> Note: When running the Hubs client locally, you will still connect to the development versions of our [Janus WebRTC](https://github.com/mozilla/janus-plugin-sfu) and [reticulum](https://github.com/mozilla/reticulum) servers. These servers do not allow being accessed outside of localhost. If you want to host your own Hubs servers, please check out [Hubs Cloud](https://hubs.mozilla.com/docs/hubs-cloud-intro.html).\n\n## Documentation\n\nThe Hubs documentation can be found [here](https://hubs.mozilla.com/docs).\n\n## Community\n\nJoin us on our [Discord Server](https://discord.gg/CzAbuGu) or [follow us on Twitter](https://twitter.com/MozillaHubs).\n\n## Contributing\n\nRead our [contributor guide](./CONTRIBUTING.md) to learn how you can submit bug reports, feature requests, and pull requests.\n\nWe're also looking for help with localization. The Hubs redesign has a lot of new text and we need help from people like you to translate it. Follow the [localization docs](./src/assets/locales/README.md) to get started.\n\nContributors are expected to abide by the project's [Code of Conduct](./CODE_OF_CONDUCT.md) and to be respectful of the project and people working on it.\n\n## Additional Resources\n\n* [Reticulum](https://github.com/mozilla/reticulum) - Phoenix-based backend for managing state and presence.\n* [NAF Janus Adapter](https://github.com/mozilla/naf-janus-adapter) - A [Networked A-Frame](https://github.com/networked-aframe) adapter for the Janus SFU service.\n* [Janus Gateway](https://github.com/meetecho/janus-gateway) - A WebRTC proxy used for centralizing network traffic in this client.\n* [Janus SFU Plugin](https://github.com/mozilla/janus-plugin-sfu) - Plugins for Janus which enables it to act as a SFU.\n* [Hubs-Ops](https://github.com/mozilla/hubs-ops) - Infrastructure as code + management tools for running necessary backend services on AWS.\n\n## Privacy\n\nMozilla and Hubs believe that privacy is fundamental to a healthy internet. Read our [privacy policy](https://www.mozilla.org/en-US/privacy/hubs/) for more info.\n\n## License\n\nHubs is licensed with the [Mozilla Public License 2.0](./LICENSE)\n"
},
{
  "name": "fx-private-relay-add-on",
  "files": {
    "/": [
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".gitmodules",
      "LICENSE",
      "README.md",
      "config-domain.sh",
      "package-lock.json",
      "package.json",
      "src"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Private Relay\nPrivate Relay generates email aliases to use in place of personal email addresses.\n\nRecipients will still receive emails, but Private Relay keeps their personal\nemail address from being [harvested](https://blog.hubspot.com/marketing/what-is-a-landing-page-ht), \nand then [bought, sold, traded, or combined](https://www.bookyourdata.com/) \nwith  other data to personally identify, track, and/or [target\nthem](https://www.facebook.com/business/help/606443329504150?helpref=faq_content).\n\n## Usage (for now)\n\n1. Install the extension.\n\n2. Go to [relay.firefox.com](https://relay.firefox.com) and sign in.\n\n3. In any `<input>` element, right-click and select \"Make a relay address\"\n   * The extension will populate the options with your relay addresses.\n\n\n## Local Extension Development\n\n1. Clone, change to the directory, install dependencies:\n\n    ```\n    git clone --recurse-submodules git@github.com:mozilla/fx-private-relay-add-on.git\n    npm install\n    ```\n\n2. Run with `npm`:\n\n    ```\n    npm run web-ext-run\n    ```\n\n   By default, this will open and run the extension in Firefox. If you'd like to run this in Chrome, run the following: \n\n      ```\n      npm run web-ext-run:chrome\n      ```\n\n   If you'd like to run the extension in both Chrome and Firefox at the same time, use this command: \n\n      ```\n      npm run web-ext-run:all\n      ```\n\n   If you want to run the extension on Android, you'll need to install the [Android SDK platform tools](https://developer.android.com/studio/releases/platform-tools.html):\n\n      ```\n      npm run web-ext-run:android --device=DEVICE_ID\n      ```\n\n   Please refer to [extensionworkshop.com](https://extensionworkshop.com/documentation/develop/developing-extensions-for-firefox-for-android/) to learn more about how to run an extension on Android.\n\n3. Visit http://127.0.0.1:8000\n\n\n### Working with translations\n#### Getting the latest translations\nWe use a [git submodule](https://git-scm.com/book/en/v2/Git-Tools-Submodules)\nfor translated message files. The `--recurse-submodules` step of installation\nshould bring the message files into your working directory already, but you may\nwant also want to udpate the translations after install. The easiest way to do\nthat is:\n\n* `git submodule update --init --remote`\n\n#### Running with the latest translations\n\nTo run the latest translated version of the add-on, you will need to \n\n1. Create a Firefox profile set to use the target language\n2. Either:\n   * Run `npm run web-ext-run` with that profile\n   * Install the latest\n     [pre-release](https://github.com/mozilla/fx-private-relay-add-on/releases)\n     in that profile\n\n##### Create a Firefox profile set to use the target language\n1. Make a new Firefox profile - e.g., \"swedish\"\n2. In the profile, install the [language\n   pack](https://addons.mozilla.org/en-US/firefox/language-tools/) for one of \n   the add-on's [supported\n   languages](https://pontoon.mozilla.org/projects/firefox-relay-add-on/)\n   * Note: language packs only work on Release & Beta channels of Firefox - not\n     Nightly\n3. In `about:preferences`, go to the \"Language\" section, and click the \"Set\n   Alternatives\" button next to \"Choose the languages used to display menus,\n   messages, and notifications from Firefox.\"\n4. Pick the language pack you installed.\n5. Quit Firefox so the profile is saved.\n\n##### Run `web-ext` with that profile\nUse `npm run web-ext-run` to run the add-on, and pass the profile argument. \nE.g., `npm run web-ext-run -- -p \"swedish\"`\n\n#### Add/update messages for translation\nThe `privaterelay/locales` directory is a git repository like any other, so to\nmake changes to the messages:\n\n1. Make whatever changes you need in `src/_locales/en` as you work.\n\n2. `cd src/_locales/en`\n\n3. `git branch message-updates-yyyymmdd`\n\n4. `git push -u origin message-updates-yyyymmdd`\n\nYou can then open a pull request from the `message-updates-yyyymmdd` branch to\n[the l10n repo](https://github.com/mozilla-l10n/fx-private-relay-add-on-l10n/) `main` branch.\n\n## Build for other environments\n\nThese scripts will build the add-on to work with dev, stage, or prod servers.\n\n * `npm run build:dev`: https://dev.fxprivaterelay.nonprod.cloudops.mozgcp.net/\n * `npm run build:stage`: https://stage.fxprivaterelay.nonprod.cloudops.mozgcp.net/\n * `npm run build:prod`: https://relay.firefox.com/\n\n### Distributing\n#### Continuous Pre-releases\nThe `sign-and-release-to-github` action creates a signed add-on after every\nmerge to `main`. These pre-releases are available on the [GitHub Releases\npage](https://github.com/mozilla/fx-private-relay-add-on/releases).\n\nTo comply with [WebExtension\nversion](https://developer.chrome.com/docs/extensions/mv3/manifest/version/)\nrequirements for AMO signing, the pre-release versions are [Calendar\nVersioned](https://calver.org/) as `YYYY.MM.DD.minutes-since-midnight`\n\nThe signed `.xpi` file is named\n`private_relay-${{ YYYY.MM.DD.minutes }}.xpi` and automatically attached\nto each release, under the release \"Assets\" section.\n\n#### Make the new version\n\n1. Bump the version number in `package.json` and `manifest.json`\n2. Commit the version number bump\n3. Create a git tag for the version: `git tag <version>`\n4. Push the tag up to GitHub: `git push --tags`\n\n#### Publish to AMO\n\n1. `npm run config:prod`\n2. `npm run-script build`\n3. [Upload the `.zip` to AMO](https://addons.mozilla.org/en-US/developers/addon/private-relay/versions/submit/)\n\n**Note: Be sure the *\"Where to Host Version\"* is set to _\"On this site\"._**\n\n#### Publish to Chrome \n\n1. `npm run config:prod`\n2. `npm run package:chrome`\n3. [Upload the `.zip` to Chrome](https://developer.chrome.com/docs/webstore/publish//)\n\n**Note: Be sure the publisher is set to *Mozilla Corportation*.**\n\n#### Publish to GitHub\nFinally, we also publish the release to GitHub for those followers.\n\n1. Download the signed `.xpi` from [the addon versions page](https://addons.mozilla.org/en-US/developers/addon/private-relay/versions)\n2. [Make the new release on\n   GitHub](https://github.com/mozilla/fx-private-relay-add-on/releases/new)\n   * Use the version number for \"Tag version\" and \"Release title\"\n   * Release notes: copy the output of `git log --no-merges --pretty=format:\"%h %s\" <previous-version>..<new-version>`\n   * Attach binaries: select the signed `.xpi` file\n"
},
{
  "name": "mozjexl",
  "files": {
    "/": [
      ".circleci",
      ".eslintrc.js",
      ".gitignore",
      ".therapist.yml",
      "CHANGELOG.md",
      "LICENSE.txt",
      "README.md",
      "bors.toml",
      "lib",
      "package.json",
      "test",
      "yarn.lock"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozjexl [![CircleCI](https://img.shields.io/circleci/project/github/mozilla/mozjexl.svg)](https://github.com/mozilla/mozjexl)\n\nJavascript Expression Language: Powerful context-based expression\nparser and evaluator Mozjexl is a fork of Jexl for use at Mozilla,\nspecifically as a part of SHIELD and Normandy.\n\n## Quick start\nUse it with promises or callbacks:\n\n```javascript\nvar context = {\n    name: {first: 'Sterling', last: 'Archer'},\n    assoc: [\n        {first: 'Lana', last: 'Kane'},\n        {first: 'Cyril', last: 'Figgis'},\n        {first: 'Pam', last: 'Poovey'}\n    ],\n    age: 36\n};\n\n// Filter an array\nmozjexl.eval('assoc[.first == \"Lana\"].last', context).then(function(res) {\n    console.log(res); // Output: Kane\n});\n\n// Do math\nmozjexl.eval('age * (3 - 1)', context, function(err, res) {\n    console.log(res); // Output: 72\n});\n\n// Concatenate\nmozjexl.eval('name.first + \" \" + name[\"la\" + \"st\"]', context).then(function(res) {\n    console.log(res); // Output: Sterling Archer\n});\n\n// Compound\nmozjexl.eval('assoc[.last == \"Figgis\"].first == \"Cyril\" && assoc[.last == \"Poovey\"].first == \"Pam\"', context)\n    .then(function(res) {\n        console.log(res); // Output: true\n    });\n\n// Use array indexes\nmozjexl.eval('assoc[1]', context, function(err, res) {\n    console.log(res.first + ' ' + res.last); // Output: Cyril Figgis\n});\n\n// Use conditional logic\nmozjexl.eval('age > 62 ? \"retired\" : \"working\"', context).then(function(res) {\n    console.log(res); // Output: working\n});\n\n// Transform\nmozjexl.addTransform('upper', function(val) {\n    return val.toUpperCase();\n});\nmozjexl.eval('\"duchess\"|upper + \" \" + name.last|upper', context).then(function(res) {\n    console.log(res); // Output: DUCHESS ARCHER\n});\n\n// Transform asynchronously, with arguments\nmozjexl.addTransform('getStat', function(val, stat) {\n    return dbSelectByLastName(val, stat); // Returns a promise\n});\nmozjexl.eval('name.last|getStat(\"weight\")', context, function(err, res) {\n    if (err) console.log('Database Error', err.stack);\n    else console.log(res); // Output: 184\n});\n\n// Add your own (a)synchronous operators\n// Here's a case-insensitive string equality\nmozjexl.addBinaryOp('_=', 20, function(left, right) {\n    return left.toLowerCase() === right.toLowerCase();\n});\nmozjexl.eval('\"Guest\" _= \"gUeSt\"').then(function(val) {\n    console.log(res); // Output: true\n});\n```\n\n## Installation\n\nFor Node.js or Web projects, type this in your project folder:\n\n    yarn add mozjexl\n\nAccess Mozjexl the same way, backend or front:\n\n    import mozjexl from 'mozjexl';\n\n## All the details\n### Unary Operators\n\n| Operation | Symbol |\n|-----------|:------:|\n| Negate    |    !   |\n\n### Binary Operators\n\n| Operation        |      Symbol      |\n|------------------|:----------------:|\n| Add, Concat      |         +        |\n| Subtract         |         -        |\n| Multiply         |         *        |\n| Divide           |         /        |\n| Divide and floor |        //        |\n| Modulus          |         %        |\n| Power of         |         ^        |\n| Logical AND      |        &&        |\n| Logical OR       |   &#124;&#124;   |\n\n### Comparisons\n\n| Comparison                 | Symbol |\n|----------------------------|:------:|\n| Equal                      |   ==   |\n| Not equal                  |   !=   |\n| Greater than               |    >   |\n| Greater than or equal      |   >=   |\n| Less than                  |    <   |\n| Less than or equal         |   <=   |\n| Element in array or string |   in   |\n\n#### A note about `in`:\nThe `in` operator can be used to check for a substring:\n`\"Cad\" in \"Ron Cadillac\"`, and it can be used to check for an array element:\n`\"coarse\" in ['fine', 'medium', 'coarse']`.  However, the `==` operator is used\nbehind-the-scenes to search arrays, so it should not be used with arrays of\nobjects.  The following expression returns false: `{a: 'b'} in [{a: 'b'}]`.\n\n### Ternary operator\n\nConditional expressions check to see if the first segment evaluates to a truthy\nvalue. If so, the consequent segment is evaluated.  Otherwise, the alternate\nis. If the consequent section is missing, the test result itself will be used\ninstead.\n\n| Expression                        | Result |\n|-----------------------------------|--------|\n| \"\" ? \"Full\" : \"Empty\"             | Empty  |\n| \"foo\" in \"foobar\" ? \"Yes\" : \"No\"  | Yes    |\n| {agent: \"Archer\"}.agent ?: \"Kane\" | Archer |\n\n### Native Types\n\n| Type     |            Examples            |\n|----------|:------------------------------:|\n| Booleans |         `true`, `false`        |\n| Strings  | \"Hello \\\"user\\\"\", 'Hey there!' |\n| Numerics |      6, -7.2, 5, -3.14159      |\n| Objects  |        {hello: \"world!\"}       |\n| Arrays   |       ['hello', 'world!']      |\n\n### Groups\n\nParentheses work just how you'd expect them to:\n\n| Expression                          | Result |\n|-------------------------------------|:-------|\n| (83 + 1) / 2                        | 42     |\n| 1 < 3 && (4 > 2 &#124;&#124; 2 > 4) | true   |\n\n### Identifiers\n\nAccess variables in the context object by just typing their name. Objects can\nbe traversed with dot notation, or by using brackets to traverse to a dynamic\nproperty name.\n\nExample context:\n\n```javascript\n{\n    name: {\n        first: \"Malory\",\n        last: \"Archer\"\n    },\n    exes: [\n        \"Nikolai Jakov\",\n        \"Len Trexler\",\n        \"Burt Reynolds\"\n    ],\n    lastEx: 2\n}\n```\n\n| Expression        | Result        |\n|-------------------|---------------|\n| name.first        | Malory        |\n| name['la' + 'st'] | Archer        |\n| exes[2]           | Burt Reynolds |\n| exes[lastEx - 1]  | Len Trexler   |\n\n### Collections\n\nCollections, or arrays of objects, can be filtered by including a filter\nexpression in brackets. Properties of each collection can be referenced by\nprefixing them with a leading dot. The result will be an array of the objects\nfor which the filter expression resulted in a truthy value.\n\nExample context:\n```javascript\n{\n    employees: [\n        {first: 'Sterling', last: 'Archer', age: 36},\n        {first: 'Malory', last: 'Archer', age: 75},\n        {first: 'Lana', last: 'Kane', age: 33},\n        {first: 'Cyril', last: 'Figgis', age: 45},\n        {first: 'Cheryl', last: 'Tunt', age: 28}\n    ],\n    retireAge: 62\n}\n```\n\n| Expression                                    | Result                                                                                |\n|-----------------------------------------------|---------------------------------------------------------------------------------------|\n| employees[.first == 'Sterling']               | [{first: 'Sterling', last: 'Archer', age: 36}]                                        |\n| employees[.last == 'Tu' + 'nt'].first         | Cheryl                                                                                |\n| employees[.age >= 30 && .age < 40]            | [{first: 'Sterling', last: 'Archer', age: 36},{first: 'Lana', last: 'Kane', age: 33}] |\n| employees[.age >= 30 && .age < 40][.age < 35] | [{first: 'Lana', last: 'Kane', age: 33}]                                              |\n| employees[.age >= retireAge].first            | Malory                                                                                |\n\n### Transforms\n\nThe power of Mozjexl is in transforming data, synchronously or asynchronously.\nTransform functions take one or more arguments: The value to be transformed,\nfollowed by anything else passed to it in the expression. They must return\neither the transformed value, or a Promise that resolves with the transformed\nvalue. Add them with `mozjexl.addTransform(name, function)`.\n\n```javascript\nmozjexl.addTransform('split', function(val, char) {\n    return val.split(char);\n});\nmozjexl.addTransform('lower', function(val) {\n    return val.toLowerCase();\n});\n```\n\n| Expression                                 | Result                |\n|--------------------------------------------|-----------------------|\n| \"Pam Poovey\"&#124;lower&#124;split(' ')[1] | poovey                |\n| \"password==guest\"&#124;split('=' + '=')    | ['password', 'guest'] |\n\n#### Advanced Transforms\nUsing Transforms, Mozjexl can support additional string formats like embedded\nJSON, YAML, XML, and more.  The following, with the help of the\n[xml2json](https://github.com/buglabs/node-xml2json) module, allows XML to be\ntraversed just as easily as plain javascript objects:\n\n```javascript\nvar xml2json = require('xml2json');\n\nmozjexl.addTransform('xml', function(val) {\n    return xml2json.toJson(val, {object: true});\n});\n\nvar context = {\n    xmlDoc:\n        \"<Employees>\" +\n            \"<Employee>\" +\n                \"<FirstName>Cheryl</FirstName>\" +\n                \"<LastName>Tunt</LastName>\" +\n            \"</Employee>\" +\n            \"<Employee>\" +\n                \"<FirstName>Cyril</FirstName>\" +\n                \"<LastName>Figgis</LastName>\" +\n            \"</Employee>\" +\n        \"</Employees>\"\n};\n\nvar expr = 'xmlDoc|xml.Employees.Employee[.LastName == \"Figgis\"].FirstName';\n\nmozjexl.eval(expr, context).then(function(res) {\n    console.log(res); // Output: Cyril\n});\n```\n\n### Context\n\nVariable contexts are straightforward Javascript objects that can be accessed\nin the expression, but they have a hidden feature: they can include a Promise\nobject, and when that property is used, Mozjexl will wait for the Promise to\nresolve and use that value!\n\n### API\n\n#### mozjexl.Jexl\nA reference to the Jexl constructor. To maintain separate instances of Jexl\nwith each maintaining its own set of transforms, simply re-instantiate with\n`new mozjexl.Jexl()`.\n\n#### mozjexl.addBinaryOp(_{string} operator_, _{number} precedence_, _{function} fn_)\nAdds a binary operator to the Jexl instance. A binary operator is one that\nconsiders the values on both its left and right, such as \"+\" or \"==\", in order\nto calculate a result. The precedence determines the operator's position in the\norder of operations (please refer to `lib/grammar.js` to see the precedence of\nexisting operators). The provided function will be called with two arguments:\na left value and a right value. It should return either the resulting value,\nor a Promise that resolves to the resulting value.\n\n#### mozjexl.addUnaryOp(_{string} operator_, _{function} fn_)\nAdds a unary operator to the Jexl instance. A unary operator is one that\nconsiders only the value on its right, such as \"!\", in order to calculate a\nresult. The provided function will be called with one argument: the value to\nthe operator's right. It should return either the resulting value, or a Promise\nthat resolves to the resulting value.\n\n#### mozjexl.addTransform(_{string} name_, _{function} transform_)\nAdds a transform function to this Jexl instance.  See the **Transforms**\nsection above for information on the structure of a transform function.\n\n#### mozjexl.addTransforms(_{{}} map_)\nAdds multiple transforms from a supplied map of transform name to transform\nfunction.\n\n#### mozjexl.getTransform(_{string} name_)\n**Returns `{function|undefined}`.** Gets a previously set transform function,\nor `undefined` if no function of that name exists.\n\n#### mozjexl.eval(_{string} expression_, _{{}} [context]_, _{function} [callback]_)\n**Returns `{Promise<*>}`.** Evaluates an expression.  The context map and\ncallback function are optional. If a callback is specified, it will be called\nwith the standard signature of `{Error}` first argument, and the expression's\nresult in the second argument.  Note that if a callback function is supplied,\nthe returned Promise will already have a `.catch()` attached to it.\n\n#### mozjexl.removeOp(_{string} operator_)\nRemoves a binary or unary operator from the Jexl instance. For example, \"^\" can\nbe passed to eliminate the \"power of\" operator.\n\n## Hacking\n\n```shell\n$ yarn install\n$ yarn test\n```\n\n### Precommit hook\n\nMozjexl provides a config for\n[Therapist](http://therapist.readthedocs.io/en/latest/overview.html). Install\nit with Pip, and then run `therapist install` in this repo to set it\nup. It will automatically format your Javascript with Prettier, and\nrun ESLint checks before committing your code.\n\n## License\nMozjexl is licensed under the MIT license. Please see `LICENSE.txt` for full\ndetails.\n\n## Credits\nJexl was designed and created at [TechnologyAdvice](http://technologyadvice.com).\n"
},
{
  "name": "glean-swift",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Package.swift",
      "README.md",
      "bin"
    ]
  },
  "makefile": null,
  "readme": "# Glean SDK - Swift Package\n\nThe `Glean SDK` is a modern approach for a Telemetry library and is part of the [Glean project][project-overview].\n\nThis repository provides a Swift Package pointing to the correct pre-compiled XCFramework bundle.\n\n## Documentation\n\nThe full Glean SDK documentation is available online:\n\n## [The Glean SDK Book][book]\n\n## Development\n\nThe Glean SDK is developed in the [mozilla/glean](https://github.com/mozilla/glean) repository.\n\n## Contact\n\nTo contact us you can:\n\n* Find us in the [#glean channel on chat.mozilla.org](https://chat.mozilla.org/#/room/#glean:mozilla.org).\n* To report issues or request changes, file a bug in [Bugzilla in Data Platform & Tools :: Glean: SDK][newbugzilla].\n* Send an email to *glean-team@mozilla.com*.\n\n## License\n\n    This Source Code Form is subject to the terms of the Mozilla Public\n    License, v. 2.0. If a copy of the MPL was not distributed with this\n    file, You can obtain one at http://mozilla.org/MPL/2.0/\n\n[project-overview]: https://docs.telemetry.mozilla.org/concepts/glean/glean.html\n[book]: https://mozilla.github.io/glean/\n[newbugzilla]: https://bugzilla.mozilla.org/enter_bug.cgi?product=Data+Platform+and+Tools&component=Glean%3A+SDK&priority=P3&status_whiteboard=%5Btelemetry%3Aglean-rs%3Am%3F%5D\n"
},
{
  "name": "libmozdata",
  "files": {
    "/": [
      ".coveragerc",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "VERSION",
      "libmozdata",
      "mozdata.ini-TEMPLATE",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "test-requirements.txt",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# libmozdata\n\n> Library to access and aggregate several Mozilla data sources\n\nThe goal is to provide a library giving access to a wide range of Mozilla data sources. This library also provides some post processing on data.\n\n[![Build Status](https://api.travis-ci.org/mozilla/libmozdata.svg?branch=master)](https://travis-ci.org/mozilla/libmozdata)\n[![codecov.io](https://img.shields.io/codecov/c/github/mozilla/libmozdata/master.svg)](https://codecov.io/github/mozilla/libmozdata?branch=master)\n\n## Setup\n\nInstall the prerequisites via `pip`:\n\n```sh\nsudo pip install -r requirements.txt\n```\n\n## Running tests\n\nInstall test prerequisites via `pip`:\n\n```sh\nsudo pip install -r test-requirements.txt\n```\n\nRun tests:\n\n```sh\ncoverage run --source=libmozdata -m unittest discover tests/\n```\n\n## Credentials\n\nCopy the file mozdata.ini-TEMPLATE into mozdata.ini and fill the token entries.\n\n## Bugs\n\nhttps://github.com/mozilla/libmozdata/issues/new\n\n## Contact\n\nEmail: calixte@mozilla.com or marco@mozilla.com\n"
},
{
  "name": "looker-spoke-default",
  "files": {
    "/": [
      ".circleci",
      "LICENSE",
      "README.md",
      "accessibility",
      "activity_stream",
      "awesome_bar",
      "bedrock",
      "burnham",
      "combined_browser_metrics",
      "duet",
      "experimentation",
      "fenix",
      "firefox_accounts",
      "firefox_desktop",
      "firefox_desktop_background_update",
      "firefox_ios",
      "firefox_translations",
      "focus_android",
      "focus_ios",
      "glean_dictionary",
      "klar_android",
      "klar_ios",
      "kpi",
      "lookml_dashboards.yaml",
      "manifest.lkml",
      "marketing",
      "monitoring",
      "mozilla_vpn",
      "mozilla_vpn_android",
      "operational_monitoring",
      "pine",
      "pocket",
      "pocket_snowflake",
      "rally_citp_search_engine_usage",
      "regrets_reporter",
      "search",
      "shared",
      "src",
      "sync",
      "user_journey",
      "views",
      "websites"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Looker Spoke Default\nMozilla Looker Models. Creates explores from views defined in the hub [0].\n\nMost of the code here is edited inside Looker and pushed, via the UI, to the associated branch. The dev workflow is:\n1. Edit LookML in Looker. Ensure explores, definitions, and views are correct for how end users should see them. Pull in definitions from looker-hub when possible.\n2. Commit the code in Looker (Add a relevant commit message!) and open a PR.\n3. Request review from another developer. They should not only look over your LookML code, but should also inspect the explores and generated queries for correctness and readability.\n4. Once changes are approved, merge to `main`. CI will push the changes to production.\n\n[0] https://github.com/mozilla/looker-hub\n"
},
{
  "name": "sccache",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "docs",
      "scripts",
      "snap",
      "src",
      "tests"
    ],
    "/docs": [
      "Configuration.md",
      "Distributed.md",
      "DistributedQuickstart.md",
      "Jenkins.md",
      "Releasing.md",
      "Rust.md"
    ],
    "/.github": [
      "actions",
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://github.com/mozilla/sccache/workflows/ci/badge.svg)](https://github.com/mozilla/sccache/actions?query=workflow%3Aci)\n\nsccache - Shared Compilation Cache\n==================================\n\nsccache is a [ccache](https://ccache.dev/)-like compiler caching tool. It is used as a compiler wrapper and avoids compilation when possible, storing cached results either on [local disk](#local) or in one of [several cloud storage backends](#storage-options).\n\nsccache includes support for caching the compilation of C/C++ code, [Rust](docs/Rust.md), as well as NVIDIA's CUDA using [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html).\n\nsccache also provides [icecream](https://github.com/icecc/icecream)-style distributed compilation (automatic packaging of local toolchains) for all supported compilers (including Rust). The distributed compilation system includes several security features that icecream lacks such as authentication, transport layer encryption, and sandboxed compiler execution on build servers. See [the distributed quickstart](docs/DistributedQuickstart.md) guide for more information.\n\n---\n\nTable of Contents (ToC)\n======================\n\n* [Installation](#installation)\n* [Build Requirements](#build-requirements)\n* [Build](#build)\n* [Usage](#usage)\n* [Storage Options](#storage-options)\n  * [Local](#local)\n  * [S3](#s3)\n  * [Redis](#redis)\n  * [Memcached](#memcached)\n  * [Google Cloud Storage](#google-cloud-storage)\n  * [Azure](#azure)\n* [Debugging](#debugging)\n* [Interaction with GNU `make` jobserver](#interaction-with-gnu-make-jobserver)\n* [Known Caveats](#known-caveats)\n\n---\n\n## Installation\n\nThere are prebuilt x86-64 binaries available for Windows, Linux (a portable binary compiled against musl), and macOS [on the releases page](https://github.com/mozilla/sccache/releases/latest). Several package managers also include sccache packages, you can install the latest release from source using cargo, or build directly from a source checkout.\n\n### macOS\n\nOn macOS sccache can be installed via [Homebrew](https://brew.sh/):\n\n```bash\nbrew install sccache\n```\n\n### Windows\n\nOn Windows, sccache can be installed via [scoop](https://scoop.sh/):\n\n```\nscoop install sccache\n```\n\n### Via cargo\n\nIf you have a Rust toolchain installed you can install sccache using cargo. **Note that this will compile sccache from source which is fairly resource-intensive. For CI purposes you should use prebuilt binary packages.**\n\n\n```bash\ncargo install sccache\n```\n\n---\n\nUsage\n-----\n\nRunning sccache is like running ccache: prefix your compilation commands with it, like so:\n\n```bash\nsccache gcc -o foo.o -c foo.c\n```\n\nIf you want to use sccache for caching Rust builds you can define `build.rustc-wrapper` in the\n[cargo configuration file](https://doc.rust-lang.org/cargo/reference/config.html).  For example, you can set it globally\nin `$HOME/.cargo/config.toml` by adding:\n\n```toml\n[build]\nrustc-wrapper = \"/path/to/sccache\"\n```\n\nNote that you need to use cargo 1.40 or newer for this to work.\n\nAlternatively you can use the environment variable `RUSTC_WRAPPER`:\n\n```bash\nexport RUSTC_WRAPPER=/path/to/sccache\ncargo build\n```\n\nsccache supports gcc, clang, MSVC, rustc, NVCC, and [Wind River's diab compiler](https://www.windriver.com/products/development-tools/#diab_compiler).\n\nIf you don't [specify otherwise](#storage-options), sccache will use a local disk cache.\n\nsccache works using a client-server model, where the server runs locally on the same machine as the client. The client-server model allows the server to be more efficient by keeping some state in memory. The sccache command will spawn a server process if one is not already running, or you can run `sccache --start-server` to start the background server process without performing any compilation.\n\nYou can run `sccache --stop-server` to terminate the server. It will also terminate after (by default) 10 minutes of inactivity.\n\nRunning `sccache --show-stats` will print a summary of cache statistics.\n\nSome notes about using `sccache` with [Jenkins](https://jenkins.io) are [here](docs/Jenkins.md).\n\nTo use sccache with cmake, provide the following command line arguments to cmake 3.4 or newer:\n\n```\n-DCMAKE_C_COMPILER_LAUNCHER=sccache\n-DCMAKE_CXX_COMPILER_LAUNCHER=sccache\n```\n\nTo generate PDB files for debugging with MSVC, you can use the [`/Z7` option](https://docs.microsoft.com/en-us/cpp/build/reference/z7-zi-zi-debug-information-format?view=msvc-160). Alternatively, the `/Zi` option together with `/Fd` can work if `/Fd` names a different PDB file name for each object file created. Note that CMake sets `/Zi` by default, so if you use CMake, you can use `/Z7` by adding code like this in your CMakeLists.txt:\n\n```\nif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n  string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\")\n  string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_DEBUG \"${CMAKE_C_FLAGS_DEBUG}\")\nelseif(CMAKE_BUILD_TYPE STREQUAL \"Release\")\n  string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE}\")\n  string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE}\")\nelseif(CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\")\n  string(REPLACE \"/Zi\" \"/Z7\" CMAKE_CXX_FLAGS_RELWITHDEBINFO \"${CMAKE_CXX_FLAGS_RELWITHDEBINFO}\")\n  string(REPLACE \"/Zi\" \"/Z7\" CMAKE_C_FLAGS_RELWITHDEBINFO \"${CMAKE_C_FLAGS_RELWITHDEBINFO}\")\nendif()\n```\n\nBy default, sccache will fail your build if it fails to successfully communicate with its associated server. To have sccache instead gracefully failover to the local compiler without stopping, set the environment variable `SCCACHE_IGNORE_SERVER_IO_ERROR=1`.\n\n---\n\nBuild Requirements\n------------------\n\nsccache is a [Rust](https://www.rust-lang.org/) program. Building it requires `cargo` (and thus `rustc`). sccache currently requires **Rust 1.58.0**. We recommend you install Rust via [Rustup](https://rustup.rs/).\n\nBuild\n-----\n\nIf you are building sccache for non-development purposes make sure you use `cargo build --release` to get optimized binaries:\n\n```bash\ncargo build --release [--no-default-features --features=s3|redis|gcs|memcached|azure]\n```\n\nBy default, `sccache` builds with support for all storage backends, but individual backends may be disabled by resetting the list of features and enabling all the other backends. Refer the [Cargo Documentation](http://doc.crates.io/manifest.html#the-features-section) for details on how to select features with Cargo.\n\n### Building portable binaries\n\nWhen building with the `dist-server` feature, `sccache` will depend on OpenSSL, which can be an annoyance if you want to distribute portable binaries. It is possible to statically link against OpenSSL using the `openssl/vendored` feature.\n\n#### Linux\n\nBuild with `cargo` and use `ldd` to check that the resulting binary does not depend on OpenSSL anymore.\n\n#### macOS\n\nBuild with `cargo` and use `otool -L` to check that the resulting binary does not depend on OpenSSL anymore.\n\n#### Windows\n\nOn Windows, the binary might also depend on a few MSVC CRT DLLs that are not available on older Windows versions.\n\nIt is possible to statically link against the CRT using a `.cargo/config.toml` file with the following contents.\n\n```toml\n[target.x86_64-pc-windows-msvc]\nrustflags = [\"-Ctarget-feature=+crt-static\"]\n```\n\nBuild with `cargo` and use `dumpbin /dependents` to check that the resulting binary does not depend on MSVC CRT DLLs anymore.\n\nWhen statically linking with OpenSSL, you will need Perl available in your `$PATH`.\n\n---\n\nStorage Options\n---------------\n\n### Local\nsccache defaults to using local disk storage. You can set the `SCCACHE_DIR` environment variable to change the disk cache location. By default it will use a sensible location for the current platform: `~/.cache/sccache` on Linux, `%LOCALAPPDATA%\\Mozilla\\sccache` on Windows, and `~/Library/Caches/Mozilla.sccache` on MacOS.\n\nThe default cache size is 10 gigabytes. To change this, set `SCCACHE_CACHE_SIZE`, for example `SCCACHE_CACHE_SIZE=\"1G\"`.\n\nThe local storage only supports a single sccache server at a time. Multiple concurrent servers will race and cause spurious build failures.\n\n### S3\nIf you want to use S3 storage for the sccache cache, you need to set the `SCCACHE_BUCKET` environment variable to the name of the S3 bucket to use.\n\nYou can use `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to set the S3 credentials.  Alternately, you can set `AWS_IAM_CREDENTIALS_URL` to a URL that returns credentials in the format supported by the [EC2 metadata service](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html#instance-metadata-security-credentials), and credentials will be fetched from that location as needed. In the absence of either of these options, credentials for the instance's IAM role will be fetched from the EC2 metadata service directly.\n\nIf you need to override the default endpoint you can set `SCCACHE_ENDPOINT`. To connect to a minio storage for example you can set `SCCACHE_ENDPOINT=<ip>:<port>`. If your endpoint requires TLS, set `SCCACHE_S3_USE_SSL=true`.\n\nYou can also define a prefix that will be prepended to the keys of all cache objects created and read within the S3 bucket, effectively creating a scope. To do that use the `SCCACHE_S3_KEY_PREFIX` environment variable. This can be useful when sharing a bucket with another application.\n\n\n### Redis\nSet `SCCACHE_REDIS` to a [Redis](https://redis.io/) url in format `redis://[:<passwd>@]<hostname>[:port][/<db>]` to store the cache in a Redis instance. Redis can be configured as a LRU (least recently used) cache with a fixed maximum cache size. Set `maxmemory` and `maxmemory-policy` according to the [Redis documentation](https://redis.io/topics/lru-cache). The `allkeys-lru` policy which discards the *least recently accessed or modified* key fits well for the sccache use case.\n\nRedis over TLS is supported. Use the [`rediss://`](https://www.iana.org/assignments/uri-schemes/prov/rediss) url scheme (note `rediss` vs `redis`). Append `#insecure` the the url to disable hostname verification and accept self-signed certificates (dangerous!). Note that this also disables [SNI](https://en.wikipedia.org/wiki/Server_Name_Indication).\n\n### Memcached\nSet `SCCACHE_MEMCACHED` to a [Memcached](https://memcached.org/) url in format `tcp://<hostname>:<port> ...` to store the cache in a Memcached instance.\n\n### Google Cloud Storage\nTo use [Google Cloud Storage](https://cloud.google.com/storage/), you need to set the `SCCACHE_GCS_BUCKET` environment variable to the name of the GCS bucket.\n\nIf you're using authentication, either:\n- Set `SCCACHE_GCS_KEY_PATH` to the location of your JSON service account credentials\n- (Deprecated) Set `SCCACHE_GCS_CREDENTIALS_URL` to a URL returning an OAuth token in non-standard `{\"accessToken\": \"...\", \"expireTime\": \"...\"}` format.\n- Set `SCCACHE_GCS_OAUTH_URL` to a URL returning an OAuth token. If you are running on a Google Cloud instance, this is of the form `http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/${YOUR_SERVICE_ACCOUNT}/token`\n\nBy default, SCCACHE on GCS will be read-only. To change this, set `SCCACHE_GCS_RW_MODE` to either `READ_ONLY` or `READ_WRITE`.\n\nYou can also define a prefix that will be prepended to the keys of all cache objects created and read within the GCS bucket, effectively creating a scope. To do that use the `SCCACHE_GCS_KEY_PREFIX` environment variable. This can be useful when sharing a bucket with another application.\n\n### Azure\nTo use Azure Blob Storage, you'll need your Azure connection string and an _existing_ Blob Storage container name.  Set the `SCCACHE_AZURE_CONNECTION_STRING`\nenvironment variable to your connection string, and `SCCACHE_AZURE_BLOB_CONTAINER` to the name of the container to use.  Note that sccache will not create\nthe container for you - you'll need to do that yourself.\n\nYou can also define a prefix that will be prepended to the keys of all cache objects created and read within the container, effectively creating a scope. To do that use the `SCCACHE_AZURE_KEY_PREFIX` environment variable. This can be useful when sharing a bucket with another application.\n\n**Important:** The environment variables are only taken into account when the server starts, i.e. only on the first run.\n\n---\n\nSeparating caches between invocations\n-------------------------------------\n\nIn situations where several different compilation invocations\nshould not reuse the cached results from each other,\none can set `SCCACHE_C_CUSTOM_CACHE_BUSTER` to a unique value\nthat'll be mixed into the hash.\n`MACOSX_DEPLOYMENT_TARGET` and `IPHONEOS_DEPLOYMENT_TARGET` variables\nalready exhibit such reuse-suppression behaviour.\nThere are currently no such variables for compiling Rust.\n\n---\n\nOverwriting the cache\n---------------------\n\nIn situations where the cache contains broken build artifacts, it can be necessary to overwrite the contents in the cache. That can be achieved by setting the `SCCACHE_RECACHE` environment variable.\n\n---\n\nDebugging\n---------\n\nYou can set the `SCCACHE_ERROR_LOG` environment variable to a path and set `SCCACHE_LOG` to get the server process to redirect its logging there (including the output of unhandled panics, since the server sets `RUST_BACKTRACE=1` internally).\n\n    SCCACHE_ERROR_LOG=/tmp/sccache_log.txt SCCACHE_LOG=debug sccache\n\nYou can also set these environment variables for your build system, for example\n\n    SCCACHE_ERROR_LOG=/tmp/sccache_log.txt SCCACHE_LOG=debug cmake --build /path/to/cmake/build/directory\n\nAlternatively, if you are compiling locally, you can run the server manually in foreground mode by running `SCCACHE_START_SERVER=1 SCCACHE_NO_DAEMON=1 sccache`, and send logging to stderr by setting the [`SCCACHE_LOG` environment variable](https://docs.rs/env_logger/0.7.1/env_logger/#enabling-logging) for example. This method is not suitable for CI services because you need to compile in another shell at the same time.\n\n    SCCACHE_LOG=debug SCCACHE_START_SERVER=1 SCCACHE_NO_DAEMON=1 sccache\n\n---\n\nInteraction with GNU `make` jobserver\n-------------------------------------\n\nsccache provides support for a [GNU make jobserver](https://www.gnu.org/software/make/manual/html_node/Job-Slots.html). When the server is started from a process that provides a jobserver, sccache will use that jobserver and provide it to any processes it spawns. (If you are running sccache from a GNU make recipe, you will need to prefix the command with `+` to get this behavior.) If the sccache server is started without a jobserver present it will create its own with the number of slots equal to the number of available CPU cores.\n\nThis is most useful when using sccache for Rust compilation, as rustc supports using a jobserver for parallel codegen, so this ensures that rustc will not overwhelm the system with codegen tasks. Cargo implements its own jobserver ([see the information on `NUM_JOBS` in the cargo documentation](https://doc.rust-lang.org/stable/cargo/reference/environment-variables.html#environment-variables-cargo-sets-for-build-scripts)) for rustc to use, so using sccache for Rust compilation in cargo via `RUSTC_WRAPPER` should do the right thing automatically.\n\n---\n\nKnown Caveats\n-------------\n\n### General\n\n* Absolute paths to files must match to get a cache hit. This means that even if you are using a shared cache, everyone will have to build at the same absolute path (i.e. not in `$HOME`) in order to benefit each other. In Rust this includes the source for third party crates which are stored in `$HOME/.cargo/registry/cache` by default.\n\n### Rust\n\n* Crates that invoke the system linker cannot be cached. This includes `bin`, `dylib`, `cdylib`, and `proc-macro` crates. You may be able to improve compilation time of large `bin` crates by converting them to a `lib` crate with a thin `bin` wrapper.\n* Incrementally compiled crates cannot be cached. By default, in the debug profile Cargo will use incremental compilation for workspace members and path dependencies. [You can disable incremental compilation.](https://doc.rust-lang.org/cargo/reference/profiles.html#incremental)\n\n[More details on Rust caveats](/docs/Rust.md)\n\n### Symbolic links\n\n* Symbolic links to sccache won't work. Use hardlinks: `ln sccache /usr/local/bin/cc`\n"
},
{
  "name": "mozperftest-tools",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "artifact_downloader.py",
      "fenix-retrieval",
      "gen_backfill_report.py",
      "gen_backfill_report_v2.py",
      "generate_side_by_side.py",
      "generate_test_report.py",
      "high-value-tests",
      "mozperftest_tools",
      "pageload-summary",
      "pyproject.toml",
      "requirements.txt",
      "task_processor.py",
      "variance-analysis"
    ]
  },
  "makefile": null,
  "readme": "# moz-current-tests\n\nThis repository is a collection of various tools that are useful for the things we do in Performance and Performance Testing. You can find the most interesting ones documented below.\n\n## Setup\n\nFor convenience, you can use poetry to manage dependencies and virtual environments.\n\nWhen running for the first time, you will need to [install poetry](https://python-poetry.org/docs/#installation) and then run `poetry install` to create the virtual environment and install dependencies.\n\nThen, you can simply run `poetry run python` followed by the path to the script you'd like to run. For example, `poetry run python generate_test_report.py --tests browsertime`.\n\nYou can update the dependencies by running `poetry update` and can add dependencies using `poetry add`. See the [poetry documentation](https://python-poetry.org/docs/) for more details.\n\n## Generating a Test Report\n\nThe code in `generate_test_report.py` can be used to determine where all tests are running, what tests are running on which platform or what platforms are running which tests. It is produced from a given `full-task-graph.json` artifact.\n\nSample command: `python3 generate_test_report.py --tests raptor gecko --platform-breakdown --match-all-tests --ignore-no-projects`\nThis will print out all raptor gecko tests and where they are running broken down by platform:\n```\nReport Breakdown\n\ntest-android-hw-g5-7-0-arm7-api-16/opt\n    raptor-tp6m-1-geckoview-e10s: mozilla-central\n    raptor-tp6m-10-geckoview-e10s: mozilla-central\n    raptor-tp6m-16-geckoview-cold-e10s: mozilla-central\n    raptor-tp6m-2-geckoview-e10s: mozilla-central\n    raptor-tp6m-3-geckoview-e10s: mozilla-central\n    raptor-tp6m-4-geckoview-e10s: mozilla-central\n    raptor-tp6m-5-geckoview-e10s: mozilla-central\n    raptor-tp6m-6-geckoview-e10s: mozilla-central\n    raptor-tp6m-7-geckoview-e10s: mozilla-central\n    raptor-tp6m-8-geckoview-e10s: mozilla-central\n    raptor-tp6m-9-geckoview-e10s: mozilla-central\n\ntest-android-hw-g5-7-0-arm7-api-16/pgo\n    raptor-speedometer-geckoview-e10s: mozilla-beta, trunk\n    raptor-tp6m-1-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-1-geckoview-e10s: mozilla-central\n    raptor-tp6m-10-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-10-geckoview-e10s: mozilla-central\n    raptor-tp6m-11-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-12-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-13-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-14-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-15-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-16-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-17-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-18-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-19-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-2-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-2-geckoview-e10s: mozilla-central\n    raptor-tp6m-20-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-21-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-22-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-23-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-24-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-25-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-26-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-27-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-28-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-3-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-3-geckoview-e10s: mozilla-central\n    raptor-tp6m-4-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-4-geckoview-e10s: mozilla-central\n    raptor-tp6m-5-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-5-geckoview-e10s: mozilla-central\n    raptor-tp6m-6-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-6-geckoview-e10s: mozilla-central\n    raptor-tp6m-7-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-7-geckoview-e10s: mozilla-central\n    raptor-tp6m-8-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-8-geckoview-e10s: mozilla-central\n    raptor-tp6m-9-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-9-geckoview-e10s: mozilla-central\n    raptor-unity-webgl-geckoview-e10s: mozilla-beta, mozilla-central\n    raptor-youtube-playback-geckoview-e10s: mozilla-central\n\ntest-android-hw-p2-8-0-android-aarch64/opt\n    raptor-speedometer-geckoview-e10s: mozilla-central\n    raptor-tp6m-1-geckoview-e10s: mozilla-central\n    raptor-tp6m-10-geckoview-e10s: mozilla-central\n    raptor-tp6m-16-geckoview-cold-e10s: mozilla-central\n    raptor-tp6m-2-geckoview-e10s: mozilla-central\n    raptor-tp6m-3-geckoview-e10s: mozilla-central\n    raptor-tp6m-4-geckoview-e10s: mozilla-central\n    raptor-tp6m-5-geckoview-e10s: mozilla-central\n    raptor-tp6m-6-geckoview-e10s: mozilla-central\n    raptor-tp6m-7-geckoview-e10s: mozilla-central\n    raptor-tp6m-8-geckoview-e10s: mozilla-central\n    raptor-tp6m-9-geckoview-e10s: mozilla-central\n\ntest-android-hw-p2-8-0-android-aarch64/pgo\n    raptor-speedometer-geckoview-e10s: mozilla-beta, trunk\n    raptor-tp6m-1-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-1-geckoview-e10s: mozilla-central\n    raptor-tp6m-10-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-10-geckoview-e10s: mozilla-central\n    raptor-tp6m-11-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-12-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-13-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-14-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-15-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-16-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-17-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-18-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-19-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-2-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-2-geckoview-e10s: mozilla-central\n    raptor-tp6m-20-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-21-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-22-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-23-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-24-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-25-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-26-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-27-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-28-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-3-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-3-geckoview-e10s: mozilla-central\n    raptor-tp6m-4-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-4-geckoview-e10s: mozilla-central\n    raptor-tp6m-5-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-5-geckoview-e10s: mozilla-central\n    raptor-tp6m-6-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-6-geckoview-e10s: mozilla-central\n    raptor-tp6m-7-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-7-geckoview-e10s: mozilla-central\n    raptor-tp6m-8-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-8-geckoview-e10s: mozilla-central\n    raptor-tp6m-9-geckoview-cold-e10s: mozilla-beta, trunk\n    raptor-tp6m-9-geckoview-e10s: mozilla-central\n    raptor-youtube-playback-geckoview-e10s: mozilla-central\n\ntest-android-hw-p2-8-0-arm7-api-16/opt\n    No tests satisfying criteria\n\n```\n\nRun `python3 generate_test_report.py --help` for more options.\n\n\n## Browsertime Side-by-Side Video Comparisons\n\nThe `generate_side_by_side.py` script can be used to generate a side-by-side comparion of two browsertime videos. This can be useful for determining if a regression/improvement is legitimate or not. It uses the similarity metric which is calculated using video histograms. See below for more information.\n\n```\n$ python3 generate_side_by_side.py --help\nusage: This tool can be used to generate a side-by-side visualization of two videos.\n\n When using this tool, make sure that the `--test-name` is an exact match, i.e. if you are comparing  the task `test-linux64-shippable-qr/opt-browsertime-tp6-firefox-linkedin-e10s` between two revisions, then use `browsertime-tp6-firefox-linkedin-e10s` as the suite name and `test-linux64-shippable-qr/opt` as the platform.\n\n The side-by-side video produced will be found in the output folder. The video on the left-hand side of the screen is the old/base video, and the video on the right-hand side of the screen is the new video.\n       [-h] --base-revision BASE_REVISION [--base-branch BASE_BRANCH]\n       --new-revision NEW_REVISION [--new-branch NEW_BRANCH] --test-name\n       TEST_NAME --platform PLATFORM [--overwrite] [--skip-download]\n       [--output OUTPUT]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --base-revision BASE_REVISION\n                        The base revision to compare a new revision to.\n  --base-branch BASE_BRANCH\n                        Branch to search for the base revision.\n  --new-revision NEW_REVISION\n                        The new revision to compare a base revision to.\n  --new-branch NEW_BRANCH\n                        Branch to search for the new revision.\n  --test-name TEST_NAME\n                        The name of the test task to get videos from.\n  --platform PLATFORM   Platforms to return results for. Defaults to all.\n  --overwrite           If set, the downloaded task group data will be deleted\n                        before it gets re-downloaded.\n  --skip-download       If set, we won't try to download artifacts again and\n                        we'll try using what already exists in the output\n                        folder.\n  --output OUTPUT       This is where the data will be saved. Defaults to CWD.\n                        You can include a name for the file here, otherwise it\n                        will default to side-by-side.mp4.\n```\n\n## Other tools\n\nThere are other useful tools in this repo as well. For instance, the `high-value-tests` folder contains logic for determining the what tests are the most valuable given a set of alerts and also produces a minimized list of tests that could catch all alerts.\n\n\n"
},
{
  "name": "libnfldap",
  "files": {
    "/": [
      "AUTHORS.rst",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "__init__.py",
      "examples",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "src"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n# Copyright (c) 2014 Mozilla Corporation\n# Author: gdestuynder@mozilla.com\n\nall:\n\t./setup.py build\n\ninstall:\n\t./setup.py install\n\nrpm:\n\tfpm -s python -t rpm --python-bin python3 -d python3-ldap --no-python-fix-name ./setup.py\n\ndeb:\n\tfpm -s python -t deb ./setup.py\n\nclean:\n\trm -rf *pyc\n\trm -rf build\n\trm -rf __pycache__\n\trm -rf src/libnfldap.egg-info\n",
  "readme": "=========\nlibnfldap\n=========\n\nA Python module to generate IPTables and IPSet rules from LDAP records.\nSee example.py for a demo.\n\nInstallation\n------------\n\nUse PyPi:\n\n.. code:: bash\n\n\t$ sudo pip install libnfldap\n\nOr build a RPM using:\n\n.. code:: bash\n\n\t$ python setup.py bdist_rpm\n\nThe latter will include an RPM dependency for `python-ldap`.\n\nExample\n-------\n\nThe script at `example_allusers.py` will build iptables and ipset rules for all\nusers in LDAP. You can provide the script an ldap filter as argv[1] to limit the\nscope.\n\n.. code:: bash\n\n\t$ time python example_allusers.py '(uid=jvehent)'\n\tIPTables rules written in /tmp/tmpT7JgOW\n\tIPSet rules written in /tmp/tmpJYtWM5\n\n\treal    0m0.605s\n\tuser    0m0.061s\n\tsys     0m0.014s\n\n`example.py` does something similar but for a single user identified by its\nuidNumber (unix user ID).\n\n.. code:: bash\n\n\t$ python example.py 2297\n\t#Generating rules for user ID 1664\n\t#====== ACL details ======\n\tjvehent has access to .....\n\nAuthors\n-------\nJulien Vehent & Guillaume Destuynder (@ mozilla)\n"
},
{
  "name": "FirefoxColor",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.yaml",
      ".gitignore",
      ".npmrc",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "bin",
      "docs",
      "gen-environment.sh",
      "package-lock.json",
      "package.json",
      "src",
      "webpack.common.js",
      "webpack.extension.js",
      "webpack.web.js"
    ],
    "/docs": [
      "acceptance.md",
      "app-flow.md",
      "theme-schema.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Color\n\n[![CircleCI](https://circleci.com/gh/mozilla/FirefoxColor.svg?style=svg)](https://circleci.com/gh/mozilla/FirefoxColor)\n\n## Get Started\n\n1. Install Node 10.18.1+ (e.g. using [node version manger][nvm])\n\n1. Clone the repo, install dependencies, start the dev environment:\n   ```\n   git clone https://github.com/mozilla/FirefoxColor.git\n   cd FirefoxColor\n   npm ci\n   npm start\n   ```\n\n   This will start a webpack-dev-server instance at port 8080 and start a\n   watcher that will rebuild the browser extension with every file change.\n\n1. To activate the extension:\n\n   1. Find the XPI for the environment:\n\n      - Locally: `npm run package` which adds an addon.xpi to the root of the project\n\n      - DEV / STAGE: Visit the dev or stage version of the website and click on \"Get Firefox Color\" (i.e. open testing.html) and use one of the referenced XPI files\n\n   2. Now load the XPI (from the previous step) to the browser by one of the following ways:\n\n      - Go to `about:debugging` and click on \"Load Temporary Add-on...\" and add the xpi\n\n      - (or) Go to `about:config` and add setting `xpinstall.signatures.required` and set to `false`. Next drag and drop the XPI to the browser. Note you must use Beta, Dev or Nightly browser with this approach.\n\nnotes:\n\nTo debug the background file, go to `about:debugging` and click the \"Inspect\" button\n\nTo toggle the add-on on and off or remove, you can go to `about:addons`\n\n\n4. Visit `http://localhost:8080` to see the web-based theme editor - changes\n   should be relayed through the temporarily installed add-on and alter the\n   browser theme\n\n[nvm]: https://github.com/creationix/nvm\n\n**Note:** If you have problems seeing the editor at `http://localhost:8080/` on\nyour computer, it's possible that you already have some other service using\nport 8080. You can change the port that Firefox Color uses for local development:\n\n* For Linux & OS X: `PORT=9090 npm start`\n* For Windows: `.\\node_modules\\.bin\\cross-env PORT=9090 npm start`\n\nThis example switches to port 9090, but you can supply a different port as\nneeded.\n\n## Environment variables\n\nThere are a few environment variables used in building the site and extension\nthat are handy to know about:\n\n- `PORT` - (default: `8080`) Port at which the webpack dev server will start up\n- `NODE_ENV` - (default: `production`) setting to `development` will enable some features for development work\n- `SITE_URL` - (default: `http://localhost:8080`) the URL where the web app is hosted\n- `SITE_ID` - (default: empty string) the ID of the site for the extension - e.g. \"\", \"local\", \"stage\", \"dev\"\n- `DOWNLOAD_FIREFOX_UTM_SOURCE` - host name used in metrics when the button to download Firefox is clicked\n- `LOADER_DELAY_PERIOD` - (default: `2000`) delay in ms used for web site loader, can be set to `0` during development to make the site appear faster but with visual glitches\n\n## Build & Release\n\nDeploying a development release consists of pushing to the `development` branch\non this repo. Production release consists of pushing to the `production` branch.\n\nUpon push, CircleCI will run the following steps, as defined in the `.circleci/config.yml` file:\n\n* Run gen-environment.sh to define the `SITE_URL` and `ADDON_URL` applicable to the current branch.\n\n* Run code linter\n\n* Build the site for the current branch\n\n* Build the add-ons for all build targets (development, stage, release).\n\n* Run tests on the current branch.\n\nWhen pushed to the development branch, `npm run deploy` is run to deploy the site to Github Pages.\nThe stage and and production branches are updated by a push to an AWS S3 bucket.\n\nThe build includes unsigned xpi files for all branches. To finalize the deployment, the unsigned\nxpi file for the production branch should be uploaded to AMO by an AMO admin\n(who is allowed to upload an add-on with \"Firefox\" in the name).\n\nDeployment for the development branch depends on\n[`GH_TOKEN` being set with an access token from GitHub][ghtoken].\nThe stage and production branches rely on AWS tokens, managed by ops. These\nare currently configured in CircleCI to support deployment after successful\ntest runs.\n\n[ghtoken]: https://github.com/settings/tokens\n\n## Build, test and publish add-on\nThe script `npm run xpi` in `package.json` generates unsigned xpi files, which\nare added to `build/web` (and published to the root of `SITE_URL` by CircleCI),\non all branches (development, stage, production). These XPIs can be loaded at\n`about:debugging` for manual testing.\n\n- `firefox-color-dev-unsigned.xpi` - test with Development (testing only).\n- `firefox-color-stage-unsigned.xpi` - test with Stage (testing only).\n- `firefox-color-unsigned.xpi` - test with Production (release candidate).\n\nAfter passing QA, the XPI can be published by manually uploading it to AMO.\nEvery release requires a version bump, because version numbers cannot be reused.\n\n### Environment list\n\n| Environment | Github Branch                                                           | URL                                     |\n|-------------|-------------------------------------------------------------------------|-----------------------------------------|\n| Development | [development](https://github.com/mozilla/FirefoxColor/tree/development) | https://mozilla.github.io/FirefoxColor/ |\n| Stage       | [stage](https://github.com/mozilla/FirefoxColor/tree/stage)             | https://color.stage.mozaws.net/         |\n| Production  | [production](https://github.com/mozilla/FirefoxColor/tree/production)   | https://color.firefox.com/              |\n\n## UI to install the addon:\n\n* Coming from AMO\n  - The user clicks on the \"Install\" button and after granting permissions, a new tab opens to the addon's home page.\n\n* Coming from the addon's home page:\n  The user can click on the \"Get Firefox Color\" button which will direct the user to a page from where the add-on can be installed, usually AMO.\n\n\n## Notes\n\n- Further reading for themes\n  - Other addons for managing & creating themes\n    - https://addons.mozilla.org/en-US/firefox/collections/ntim/theming-extensions/\n  - An example of a more complex dynamically changing theme\n    - https://github.com/mdn/webextensions-examples/tree/master/dynamic-theme\n  - Dynamic theme with colors based on favicon\n    - https://addons.mozilla.org/en-US/firefox/addon/vivaldifox/\n  - Theme API\n    - https://developer.mozilla.org/en-US/Add-ons/WebExtensions/API/theme\n  - Hacks post on Theme API\n    - https://hacks.mozilla.org/2017/12/using-the-new-theming-api-in-firefox/\n  - theme.getCurrent()\n    - Useful for other webextensions to match current theme colors with their own UIs\n    - Maybe pre-load web page with current theme?\n    - https://developer.mozilla.org/en-US/Add-ons/WebExtensions/API/theme/getCurrent\n"
},
{
  "name": "web-ext",
  "files": {
    "/": [
      ".ackrc",
      ".babelrc",
      ".circleci",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc",
      ".flowconfig",
      ".githooks",
      ".gitignore",
      ".mocharc.cjs",
      ".npmignore",
      ".npmrc",
      ".nsprc",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "README.md",
      "artifacts",
      "bin",
      "commitlint.config.cjs",
      "flow-typed",
      "index.js",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "scripts",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Web-ext\n\nThis is a command line tool to help build, run, and test\n[WebExtensions](https://wiki.mozilla.org/WebExtensions).\n\n[![CircleCI](https://circleci.com/gh/mozilla/web-ext.svg?style=svg)](https://circleci.com/gh/mozilla/web-ext)\n[![codecov](https://codecov.io/gh/mozilla/web-ext/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/web-ext)\n[![Dependency Status](https://david-dm.org/mozilla/web-ext.svg)](https://david-dm.org/mozilla/web-ext)\n[![devDependency Status](https://david-dm.org/mozilla/web-ext/dev-status.svg)](https://david-dm.org/mozilla/web-ext#info=devDependencies)\n[![npm version](https://badge.fury.io/js/web-ext.svg)](https://badge.fury.io/js/web-ext)\n\nUltimately, it aims to support browser extensions in a standard, portable,\ncross-platform way. Initially, it will provide a streamlined experience for developing\n[Firefox Extensions](https://developer.mozilla.org/en-US/Add-ons/WebExtensions).\n\n## Documentation\n\n* [Getting started with web-ext][web-ext-user-docs]\n* [Command reference](https://extensionworkshop.com/documentation/develop/web-ext-command-reference)\n\nHere are the commands you can run. Click on each one for detailed documentation or use `--help` on the command line, such as `web-ext build --help`.\n\n* [`run`](https://extensionworkshop.com/documentation/develop/web-ext-command-reference#web-ext-run)\n  * Run the extension\n* [`lint`](https://extensionworkshop.com/documentation/develop/web-ext-command-reference#web-ext-lint)\n  * Validate the extension source\n* [`sign`](https://extensionworkshop.com/documentation/develop/web-ext-command-reference#web-ext-sign)\n  * Sign the extension so it can be installed in Firefox\n* [`build`](https://extensionworkshop.com/documentation/develop/web-ext-command-reference#web-ext-build)\n  * Create an extension package from source\n* [`docs`](https://extensionworkshop.com/documentation/develop/web-ext-command-reference#web-ext-docs)\n  * Open the `web-ext` documentation in a browser\n\n## Installation from npm\n\nFirst, make sure you are running the current\n[LTS](https://github.com/nodejs/LTS)\n(long term support) version of\n[NodeJS](https://nodejs.org/en/).\n\n### Global command\n\nYou can install this command onto your machine globally with:\n\n    npm install --global web-ext\n\n### For your project\n\nAlternatively, you can install this command as one of the\n[`devDependencies`](https://docs.npmjs.com/files/package.json#devdependencies)\nof your project.  This method can help you control the version of `web-ext`\nas used by your team.\n\n    npm install --save-dev web-ext\n\nNext you can use the `web-ext` command in your project as an\n[npm script](https://docs.npmjs.com/misc/scripts).\nHere is an example where the `--source-dir` argument specifies where to find\nthe source code for your extension.\n\n`package.json`\n```json\n\"scripts\": {\n  \"start:firefox\": \"web-ext run --source-dir ./extension-dist/\",\n}\n```\n\nYou can always pass in additional commands to your npm scripts using\nthe `--` suffix. For example, the previous script could specify the Firefox\nversion on the command line with this:\n\n    npm run start:firefox -- --firefox=nightly\n\n## Installation from source\n\nYou'll need:\n* [Node.js](https://nodejs.org/en/), 14.0.0 or higher\n* [npm](https://www.npmjs.com/), 6.9.0 or higher is recommended\n\nOptionally, you may like:\n* [nvm](https://github.com/creationix/nvm), which helps manage node versions\n\nIf you had already installed `web-ext` from npm,\nyou may need to uninstall it first:\n\n    npm uninstall --global web-ext\n\nChange into the source and install all dependencies:\n\n    git clone https://github.com/mozilla/web-ext.git\n    cd web-ext\n    npm ci\n\nBuild the command:\n\n    npm run build\n\nLink it to your node installation:\n\n    npm link\n\nYou can now run it from any directory:\n\n    web-ext --help\n\nTo get updates, just pull changes and rebuild the executable. You don't\nneed to relink it.\n\n    cd /path/to/web-ext\n    git pull\n    npm run build\n\n## Using web-ext in NodeJS code\n\n**Note:** There is limited support for this API.\n\nAside from [using web-ext on the command line][web-ext-user-docs], you may wish to execute `web-ext` in NodeJS code.\n\nAs of version `7.0.0`, the `web-ext` npm package exports NodeJS native ES modules only. If you are using CommonJS, you will have to use [dynamic imports][dynamic-imports].\n\n### Examples\n\nYou are able to execute command functions without any argument validation. If you want to execute `web-ext run` you would do so like this:\n\n```js\nimport webExt from 'web-ext';\n\nwebExt.cmd.run({\n  // These are command options derived from their CLI conterpart.\n  // In this example, --source-dir is specified as sourceDir.\n  firefox: '/path/to/Firefox-executable',\n  sourceDir: '/path/to/your/extension/source/',\n}, {\n  // These are non CLI related options for each function.\n  // You need to specify this one so that your NodeJS application\n  // can continue running after web-ext is finished.\n  shouldExitProgram: false,\n})\n  .then((extensionRunner) => {\n    // The command has finished. Each command resolves its\n    // promise with a different value.\n    console.log(extensionRunner);\n    // You can do a few things like:\n    // extensionRunner.reloadAllExtensions();\n    // extensionRunner.exit();\n  });\n```\n\nIf you would like to run an extension on Firefox for Android:\n\n```js\nimport adbUtils from \"web-ext/util/adb\";\n\n// Path to adb binary (optional parameter, auto-detected if missing)\nconst adbBin = \"/path/to/adb\";\n// Get an array of device ids (Array<string>)\nconst deviceIds = await adbUtils.listADBDevices(adbBin);\nconst adbDevice = ...\n// Get an array of Firefox APKs (Array<string>)\nconst firefoxAPKs = await adbUtils.listADBFirefoxAPKs(\n  deviceId, adbBin\n);\nconst firefoxApk = ...\n\nwebExt.cmd.run({\n  target: 'firefox-android',\n  firefoxApk,\n  adbDevice,\n  sourceDir: ...\n}).then((extensionRunner) => {...});\n```\n\nIf you would like to control logging, you can access the logger object. Here is an example of turning on verbose logging:\n\n```js\nimport webExtLogger from \"web-ext/util/logger\";\n\nwebExtLogger.consoleStream.makeVerbose();\nwebExt.cmd.run({sourceDir: './src'}, {shouldExitProgram: false});\n```\n\nYou can also disable the use of standard input:\n\n```js\nwebExt.cmd.run({noInput: true}, {shouldExitProgram: false});\n```\n\n`web-ext` is designed for WebExtensions but you can try disabling manifest validation to work with legacy extensions. This is not officially supported.\n\n```js\nwebExt.cmd.run(\n  {sourceDir: './src'},\n  {\n    getValidatedManifest: () => ({\n      name: 'some-fake-name',\n      version: '1.0.0',\n    }),\n    shouldExitProgram: false,\n  },\n);\n```\n\n\n## Should I Use It?\n\nYes! The web-ext tool enables you to build and ship extensions for Firefox.\nThis platform stabilized in\n[Firefox 48](https://blog.mozilla.org/addons/2016/04/29/webextensions-in-firefox-48/)\nwhich was released in April of 2016.\n\n## Get Involved\n\nHi! This tool is under active development. To get involved you can watch the repo,\nfile issues, create pull requests, or ask a question on\n[dev-addons](https://mail.mozilla.org/listinfo/dev-addons).\nRead the [contributing section](CONTRIBUTING.md) for how to develop new features.\n\n## Some Questions and Answers\n\n### Why do we need a command line tool?\n\nThis is a great question and one that we will ask ourselves for each new web-ext\nfeature. Most WebExtension functionality is baked into the browsers\nthemselves but a complimentary command line tool will still be helpful.\nHere is a partial list of examples:\n\n* File watching.\n  * When you edit a file, you may need to trigger certain commands (tests,\n    installation, etc).\n* Integrating with services.\n  * Mozilla offers some useful services such as\n    [linting](https://github.com/mozilla/addons-linter) and\n    [signing](https://addons-server.readthedocs.io/en/latest/topics/api/signing.html)\n    extensions.\n\n[web-ext-user-docs]: https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext\n[dynamic-imports]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Statements/import#dynamic_imports\n"
},
{
  "name": "media-triage",
  "files": {
    "/": [
      "README.md",
      "css",
      "fonts",
      "images",
      "index.html",
      "js"
    ]
  },
  "makefile": null,
  "readme": "# media-triage\n\nThis is completely based on https://github.com/FirefoxGraphics/triage, but not really a fork,\nin the sense that it's using the same structure, but otherwise not the same project.\n\n"
},
{
  "name": "hubs-cloud",
  "files": {
    "/": [
      "CHANGELOG.md",
      "Hubs-Cloud-AWS-Agreement.pdf",
      "README.md",
      "asset-packs",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "![Hubs Cloud](https://hubs-cloud.s3-us-west-1.amazonaws.com/hubs-cloud-logo.png)\n\n## Hubs Cloud\n\nThis repo collects asset packs, release notes, and scripts for your HC instances.\n\nSee [Hubs Cloud Releases and Updates Changelog](CHANGELOG.md)\n\n## What is Hubs Cloud?\n\nHubs Cloud creates and manages all of the AWS resources needed to host your own immersive spaces from your company or organization\u2019s own account. Use your own domain or use Route53 to create a new site, running on a single EC2 instance or scaled up to multiple servers for greater system-wide concurrency. Easily customize your platform with your own branding, upload your own 3D content, or select from the vast array of avatars and scenes licensed under Creative Commons for the Hubs platform. Now available to deploy on offered on DigitalOcean (alpha).\n\n## Interested in deploying Hubs Cloud?\n\n[Get Started with Hubs Cloud](https://hubs.mozilla.com/cloud).\n\nFull documentation available at [Hubs Docs](https://hubs.mozilla.com/docs/hubs-cloud-intro.html).\n\nHubs Cloud is now deployable on [DigitalOcean (alpha)](https://hubs.mozilla.com/docs/hubs-cloud-do-quick-start.html).\n\n## Releases\n\nSee our [Hubs Cloud Releases and Updates Changelog](CHANGELOG.md) for latest changes on Hubs Cloud.\n\n## Dependency Repos\n\nClient + Admin: https://github.com/mozilla/hubs\n\nApp Server: https://github.com/mozilla/reticulum\n\nWebRTC Streaming Server: https://github.com/mozilla/dialog\n\nSpoke: https://github.com/mozilla/Spoke\n\nCloudformation + Habitat plans + other HC ops scripts: https://github.com/mozilla/hubs-ops\n"
},
{
  "name": "symbol-scrapers",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "arch",
      "common.sh",
      "debian",
      "fedora",
      "mint",
      "opensuse",
      "ubuntu"
    ]
  },
  "makefile": null,
  "readme": "# symbol-scrapers\nA bunch of scripts to scrape symbols from Linux distributions\n\nEach scripts needs to be run in its own directory as it uses the current\nworking directory to unpack and process debug-information.\n\nBefore running the scripts the following environment variables need to be set:\n\n* `DUMP_SYMS` - The path to the `dump_syms` tool, the version built as part\n  of mozilla-central is currently required as the upstream Breakpad version\n  lacks some important functionality\n* `SYMBOLS_API_TOKEN` - An API token for https://symbols.mozilla.org\n* `CRASHSTATS_API_TOKEN` - An API token for https://crash-stats.mozilla.org, it\n  needs the reprocess permission set in order to reprocess crashes\n"
},
{
  "name": "hubs-ops",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "ansible",
      "arbortect",
      "bin",
      "check_dns",
      "cloudformation",
      "db",
      "helpers.sh",
      "jenkins",
      "manage_amis",
      "packer",
      "plans",
      "terraform",
      "workers"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Hubs by Mozilla Ops\n\nThis repo contains all the necessary scripts and tools for standing up infrastructure for Hubs by Mozilla on AWS.\n\n## Contents:\n\n### `ansible` - Contains scripts for performing configuration deploys to the live Habitat ring, and other runbooks.\n\n### `bin` - Useful scripts for managing Hubs services\n\nExpects ssh-agent to have mozilla mr ssh key registered and present in `~/.ssh/mozilla_mr_id_rsa`.\n\nhost-types can be any ansible role such as: `bots`, `discord`, `janus`, `migrate`, `postgrest`, `ret`, and `ssl`. Or `ci`.\n\nhostnames can be any server host name such as: `quixotic-duck`\n\nenvironments include: `prod` and `dev`\n\nSee the top of each script for usage instructions.\n\n### `helpers.sh` - Functions for managing Hubs services.\n  \nLoad in your `.bashrc` or `.zshrc` file by adding `source ~/path/to/hubs-ops/helpers.sh`\n\nExpects an ssh config in `~/.ssh/config` like the following:\n\n```\nHost *.reticulum.io\nUser ubuntu\nPreferredAuthentications publickey,keyboard-interactive\nIdentityFile ~/.ssh/mozilla_mr_id_rsa\nForwardAgent yes\n```\n\nSee the `helpers.sh` source for more documenation on each command.\n\nUseful commands include:\n\n- `moz-ec2 [env] [asg]`\n  Lists active hosts from EC2, displaying environment, ASG, name, private IP, and public IP.\n- `moz-ssh target ...cmd-args`\n  SSHes into the given target through its bastion host, e.g. `moz-ssh dazzling-druid`.\n- `moz-admin`\n  Opens an SSH tunnel to the prod Postgrest admin console.\n- `moz-admin-dev`\n  Opens an SSH tunnel to the dev Postgrest admin console.\n- `moz-iex target ...cmd-args`\n  SSHes into a Reticulum host and opens an Elixir console.\n- `moz-ci`\n  Creates a tunnel to the CI host's web interface on port 8088.\n- `moz-scp env ...scp-args`\n  Proxies SCP over a bastion host, e.g. `moz-scp prod dazzling-druid-local.reticulum.io:~/core core`.\n\n### `packer` - Packer AMI definitions\n\n### `plans` - Habitat plans\n\n### `terraform` - Terraform + terragrunt scripts\n\n\n"
},
{
  "name": "opmon",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "docs",
      "mypy.ini",
      "opmon",
      "pyproject.toml",
      "requirements.in",
      "requirements.txt",
      "script",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/docs": [
      "README.md",
      "adr",
      "images",
      "troubleshooting.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "Operational Monitoring\n===\n\nOperational Monitoring (OpMon) is a self-service tool that aggregates and summarizes operational metrics that indicate the health of software. OpMon can be used to continuously monitor rollouts, experiments (including experiments with continuous enrollments) or the population of a specific product (for example, Firefox Desktop).\n\nFor more information on how to set up an Operational Monitoring project, see the [documentation on dtmo](https://docs.telemetry.mozilla.org/cookbooks/operational_monitoring.html).\n\n## Local installation\n\n```\n# Create and activate a python virtual environment.\npython3 -m venv venv/\nsource venv/bin/activate\npip install -r requirements.txt\npip install .\n```\n\nThe `opmon` CLI tool will be available to run locally:\n\n```\n$ opmon --help\nUsage: opmon [OPTIONS] COMMAND [ARGS]...\n\n  Initialize CLI.\n\nOptions:\n  --log_project_id, --log-project-id TEXT\n                                  GCP project to write logs to\n  --log_dataset_id, --log-dataset-id TEXT\n                                  Dataset to write logs to\n  --log_table_id, --log-table-id TEXT\n                                  Table to write logs to\n  --log_to_bigquery, --log-to-bigquery\n  --help                          Show this message and exit.\n\nCommands:\n  backfill         Backfill a specific project.\n  run              Execute the monitoring ETL for a specific date.\n  validate_config  Validate config files.\n```\n\n## Documentation\n\nUser documentation is available [on dtmo](https://docs.telemetry.mozilla.org/cookbooks/operational_monitoring.html).\nDeveloper documentation is available in the [`docs/`](https://github.com/mozilla/opmon/tree/main/docs) directory.\n"
},
{
  "name": "fathom",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "cli",
      "docs",
      "fathom",
      "fathom_fox",
      "smoo"
    ],
    "/docs": [
      "Makefile",
      "clustering.rst",
      "commands",
      "conf.py",
      "debugging.rst",
      "deploy-docs",
      "development.rst",
      "example.rst",
      "exceptions.rst",
      "fnodes.rst",
      "glossary.rst",
      "img",
      "index.rst",
      "installing.rst",
      "integrating.rst",
      "intro.rst",
      "maintaining.rst",
      "rules.rst",
      "ruleset.rst",
      "samples.rst",
      "theme",
      "training.rst",
      "utilities.rst",
      "versions.rst",
      "zoo.rst",
      "zoo"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# Convenience targets for executing common actions from the root of the repo\n\nall: docs\n\t$(MAKE) -C fathom\n\t$(MAKE) -C cli\n\ndocs:\n\t$(MAKE) -C docs clean html\n\nlint:\n\t$(MAKE) -C cli lint\n\t$(MAKE) -C fathom lint\n\ntest:\n\t$(MAKE) -C cli test\n\t$(MAKE) -C fathom test\n\nclean:\n\t$(MAKE) -C cli clean\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C fathom clean\n\n\n.PHONY: clean docs lint test\n",
  "readme": "# Fathom\n\nFathom is a supervised-learning system for recognizing parts of web pages\u2014pop-ups, address forms, slideshows\u2014or for classifying a page as a whole. A DOM flows in one side, and DOM nodes flow out the other, tagged with types and probabilities that those types are correct. A Prolog-like language makes it straightforward to specify the \u201csmells\u201d that suggest each type, and a neural-net-based trainer determines the optimal contribution of each smell. Finally, the FathomFox web extension lets you collect and label a corpus of web pages for training.\n\nContinue reading at <https://mozilla.github.io/fathom/intro.html#why>.\n\n__[Documentation](https://mozilla.github.io/fathom)__\n"
},
{
  "name": "perf-automation",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "benchmarks",
      "pagesets"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "nixpkgs-mozilla",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.rst",
      "compilers-overlay.nix",
      "default.nix",
      "deploy_rsa.enc",
      "firefox-overlay.nix",
      "flake.nix",
      "git-cinnabar-overlay.nix",
      "lib-overlay.nix",
      "lib",
      "overlays.nix",
      "package-set.nix",
      "phlay-overlay.nix",
      "pinned.nix",
      "pkgs",
      "release.nix",
      "rr-overlay.nix",
      "rust-overlay-install.sh",
      "rust-overlay.nix",
      "rust-src-overlay.nix",
      "update.nix"
    ]
  },
  "makefile": null,
  "readme": "nixpkgs-mozilla\n===============\n\nGathering nix efforts in one repository.\n\n\nCurrent packages\n----------------\n\n- gecko (https://github.com/mozilla/gecko-dev)\n- firefox-bin variants including Nightly\n\nfirefox-bin variants\n--------------------\n\nNixpkgs already has definitions for `firefox\n<https://github.com/NixOS/nixpkgs/blob/246d2848ff657d56fcf2d8596709e8869ce8616a/pkgs/applications/networking/browsers/firefox/packages.nix>`_,\nwhich is built from source, as well as `firefox-bin\n<https://github.com/NixOS/nixpkgs/blob/ba2fe3c9a626a8fb845c786383b8b23ad8355951/pkgs/applications/networking/browsers/firefox-bin/default.nix>`_,\nwhich is the binary Firefox version built by Mozilla.\n\nThe ``firefox-overlay.nix`` in this repository adds definitions for\nsome other firefox-bin variants that Mozilla ships:\n``firefox-nightly-bin``, ``firefox-beta-bin``, and\n``firefox-esr-bin``. All are exposed under a ``latest`` attribute,\ne.g. ``latest.firefox-nightly-bin``.\n\nUnfortunately, these variants do not auto-update, and you may see some\nannoying pop-ups complaining about this.\n\nNote that all the ``-bin`` packages are \"unfree\" (because of the\nFirefox trademark, held by Mozilla), so you will need to set\n``nixpkgs.config.allowUnfree`` in order to use them. More info `here\n<https://nixos.wiki/wiki/FAQ#How_can_I_install_a_proprietary_or_unfree_package.3F>`_.\n\nRust overlay\n------------\n\n**NOTE:** Nix overlays only works on up-to-date versions of NixOS/nixpkgs, starting from 17.03.\n\nA nixpkgs overlay is provided to contain all of the latest rust releases.\n\nTo use the rust overlay run the ``./rust-overlay-install.sh`` command. It will\nlink the current ``./rust-overlay.nix`` into your ``~/.config/nixpkgs/overlays`` folder.\n\nOnce this is done, use ``nix-env -iA nixpkgs.latest.rustChannels.nightly.rust`` for\nexample. Replace the ``nixpkgs.`` prefix with ``nixos.`` on NixOS.\n\nUsing in nix expressions\n------------------------\n\nExample of using in ```shell.nix```:\n\n.. code:: nix\n\n let\n   moz_overlay = import (builtins.fetchTarball https://github.com/mozilla/nixpkgs-mozilla/archive/master.tar.gz);\n   nixpkgs = import <nixpkgs> { overlays = [ moz_overlay ]; };\n in\n   with nixpkgs;\n   stdenv.mkDerivation {\n     name = \"moz_overlay_shell\";\n     buildInputs = [\n       # to use the latest nightly:\n       nixpkgs.latest.rustChannels.nightly.rust\n       # to use a specific nighly:\n       (nixpkgs.rustChannelOf { date = \"2018-04-11\"; channel = \"nightly\"; }).rust\n       # to use the project's rust-toolchain file:\n       (nixpkgs.rustChannelOf { rustToolchain = ./rust-toolchain; }).rust\n     ];\n   }\n\nFlake usage\n-----------\nThis repository contains a minimal flake interface for the various\noverlays in this repository. To use it in your own flake, add it as\nan input to your ``flake.nix``:\n\n.. code:: nix\n {\n   inputs.nixpkgs.url = github:NixOS/nixpkgs;\n   inputs.nixpkgs-mozilla.url = github:mozilla/nixpkgs-mozilla;\n\n   outputs = { self, nixpkgs, nixpkgs-mozilla }: {\n     devShell.\"x86_64-linux\" = let\n       pkgs = import nixpkgs { system = \"x86_64-linux\"; overlays = [ nixpkgs-mozilla.overlay ]; };\n     in pkgs.mkShell {\n       buildInputs = [ pkgs.latest.rustChannels.nightly.rust ];\n     };\n   };\n  }\nThe available overlays are ``nixpkgs-mozilla.overlay`` for the\ndefault overlay containing everything, and\n``nixpkgs-mozilla.{lib, rust, rr, firefox, git-cinnabar}-overlay``\nrespectively. Depending on your use case, you might need to set the\n``--impure`` flag when invoking the ``nix`` command. This is because\nthis repository fetches resources from non-pinned URLs\nnon-reproducibly.\n\nFirefox Development Environment\n-------------------------------\n\nThis repository provides several tools to facilitate development on\nFirefox. Firefox is built on an engine called Gecko, which lends its\nname to some of the files and derivations in this repo.\n\nChecking out Firefox\n~~~~~~~~~~~~~~~~~~~~\n\nTo build Firefox from source, it is best to have a local checkout of\n``mozilla-central``. ``mozilla-central`` is hosted in Mercurial, but\nsome people prefer to access it using ``git`` and\n``git-cinnabar``. The tools in this repo support either using\nmercurial or git.\n\nThis repository provides a ``git-cinnabar-overlay.nix`` which defines\na ``git-cinnabar`` derivation. This overlay can be used to install\n``git-cinnabar``, either using ``nix-env`` or as part of a system-wide\n``configuration.nix``.\n\nBuilding Firefox\n~~~~~~~~~~~~~~~~\n\nThe ``firefox-overlay.nix`` provides an environment to build Firefox\nfrom its sources, once you have finished the checkout of\n``mozilla-central``. You can use ``nix-shell`` to enter this\nenvironment to launch ``mach`` commands to build Firefox and test your\nbuild.\n\nSome debugging tools are available in this environment as well, but\nother development tools (such as those used to submit changes for\nreview) are outside the scope of this environment.\n\nThe ``nix-shell`` environment is available in the\n``gecko.<arch>.<cc>`` attribute of the ``release.nix`` file provided\nin this repository.\n\nThe ``<arch>`` attribute is either ``x86_64-linux`` or ``i686-linux``. The first\none would create a native toolchain for compiling on x64, while the second one\nwould give a native toolchain for compiling on x86. Note that due to the size of\nthe compilation units on x86, the compilation might not be able to complete, but\nsome sub part of Gecko, such as SpiderMonkey would compile fine.\n\nThe ``<cc>`` attribute is either ``gcc`` or ``clang``, or any specific version\nof the compiler available in the ``compiler-overlay.nix`` file which is repeated\nin ``release.nix``. This compiler would only be used for compiling Gecko, and\nthe rest of the toolchain is compiled against the default ``stdenv`` of the\narchitecture.\n\nWhen first entering the ``nix-shell``, the toolchain will pull and build all\nthe dependencies necessary to build Gecko, this includes might take some time.\nThis work will not be necessary the second time, unless you use a different\ntoolchain or architecture.\n\n.. code:: sh\n\n  ~/$ cd mozilla-central\n  ~/mozilla-central$ nix-shell ../nixpkgs-mozilla/release.nix -A gecko.x86_64-linux.gcc --pure\n    ... pull the rust compiler\n    ... compile the toolchain\n  # First time only - initialize virtualenv\n  [~/mozilla-central] python ./mach create-mach-environment\n     ... create .mozbuild/_virtualenvs/mach\n  [~/mozilla-central] python ./mach build\n    ... build firefox desktop\n  [~/mozilla-central] python ./mach run\n    ... run firefox\n\nWhen entering the ``nix-shell``, the ``MOZCONFIG`` environment variable is set\nto a local file, named ``.mozconfig.nix-shell``, created each time you enter the\n``nix-shell``. You can create your own ``.mozconfig`` file which extends the\ndefault one, with your own options.\n\n.. code:: sh\n\n  ~/mozilla-central$ nix-shell ../nixpkgs-mozilla/release.nix -A gecko.x86_64-linux.gcc --pure\n  [~/mozilla-central] cat .mozconfig\n  # Import current nix-shell config.\n  . .mozconfig.nix-shell\n\n  ac_add_options --enable-js-shell\n  ac_add_options --disable-tests\n  [~/mozilla-central] export MOZCONFIG=\"$(pwd)/.mozconfig\"\n  [~/mozilla-central] python ./mach build\n\nTo avoid repeating yourself, you can also rely on the ``NIX_SHELL_HOOK``\nenvironment variable, to reset the ``MOZCONFIG`` environment variable for you.\n\n.. code:: sh\n\n  ~/mozilla-central$ export NIX_SHELL_HOOK=\"export MOZCONFIG=$(pwd)/.mozconfig;\"\n  ~/mozilla-central$ nix-shell ../nixpkgs-mozilla/release.nix -A gecko.x86_64-linux.gcc --pure\n  [~/mozilla-central] python ./mach build\n\nSubmitting Firefox patches\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFirefox development happens in `Mozilla Phabricator\n<https://phabricator.services.mozilla.com/>`_. Mozilla Phabricator\ndocs are `here\n<https://moz-conduit.readthedocs.io/en/latest/phabricator-user.html>`_.\n\nTo get your commits into Phabricator, some options include:\n\n- Arcanist, the upstream tool for interacting with\n  Phabricator. Arcanist is packaged in nixpkgs already; you can find\n  it in `nixos.arcanist`. Unfortunately, as of this writing, upstream\n  Arcanist does not support ``git-cinnabar`` (according to `the\n  \"Setting up Arcanist\"\n  <https://moz-conduit.readthedocs.io/en/latest/phabricator-user.html#setting-up-arcanist>`_\n  documentation). `Mozilla maintains a fork of Arcanist\n  <https://github.com/mozilla-conduit/arcanist>`_ but it isn't yet\n  packaged. (PRs welcome.)\n\n- `moz-phab <https://github.com/mozilla-conduit/review>`_, an in-house\n  CLI for Phabricator. It's available in nix packages (unstable channel).\n\n- `phlay <https://github.com/mystor/phlay>`_, a small Python script\n  that speaks to the Phabricator API directly. This repository ships a\n  ``phlay-overlay.nix`` that you can use to make ``phlay`` available\n  in a nix-shell or nix-env.\n\nNote: although the ``nix-shell`` from the previous section may have\nall the tools you would normally use to do Firefox development, it\nisn't recommended that you use that shell for anything besides tasks\nthat involve running ``mach``. Other development tasks such as\ncommitting code and submitting patches to code review are best handled\nin a separate nix-shell.\n\nTODO\n----\n\n- setup hydra to have binary channels\n\n- make sure pinned revisions get updated automatically (if build passes we\n  should update revisions in default.nix)\n\n- pin to specific (working) nixpkgs revision (as we do for other sources)\n\n- can we make this work on darwin as well?\n\n- assign maintainers for our packages that will montior that it \"always\" builds\n\n- hook it with vulnix report to monitor CVEs (once vulnix is ready, it must be\n  ready soon :P)\n"
},
{
  "name": "geckoview",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "Gemfile.lock",
      "README.md",
      "_config.yml",
      "_includes",
      "_layouts",
      "_sass",
      "assets",
      "consumer",
      "contributor",
      "index.md",
      "javadoc"
    ]
  },
  "makefile": null,
  "readme": "This GitHub repository contains the documentation for [GeckoView][8]. If you are looking for the code for GeckoView you can find it at [Mozilla Central][9].\n\nIf there is documentation that you feel is missing, or an existing document doesn't cover the aspect that you are looking for, please request it by [raising an issue][10].\n\nIf you have a GeckoView bug that you want to raise, please do so on our [Bugzilla][11].\n\n## Get Started with GeckoView\n\n* [GeckoView Quick Start Guide][1]\n* [Interacting with Web content and WebExtension][7]\n\n\n## API Documentation\n\n* [Changelog][2]\n* [API][12]\n\n## Get Started as a Contributor\n\n* [GeckoView Contributor Quick Start Guide][3]\n* [Mozilla Central Quick Start Guide][4]\n* [Mozilla Central Contributor Guide][5]\n* [Guide to Native Debugging in Android Studio][6]\n\n\n## More information\nYou can read more about GeckoView on the [wiki](https://wiki.mozilla.org/Mobile/GeckoView).\n\n\n[1]:https://firefox-source-docs.mozilla.org/mobile/android/geckoview/consumer/geckoview-quick-start.html\n[2]:https://geckoview.dev/javadoc/mozilla-central/org/mozilla/geckoview/doc-files/CHANGELOG\n[3]:https://firefox-source-docs.mozilla.org/mobile/android/geckoview/contributor/geckoview-quick-start.html\n[4]:https://firefox-source-docs.mozilla.org/mobile/android/geckoview/contributor/mc-quick-start.html\n[5]:https://firefox-source-docs.mozilla.org/mobile/android/geckoview/contributor/contributing-to-mc.html\n[6]:https://firefox-source-docs.mozilla.org/mobile/android/geckoview/contributor/native-debugging.html\n[7]:https://firefox-source-docs.mozilla.org/mobile/android/geckoview/consumer/web-extensions.html\n[8]:https://geckoview.dev\n[9]:https://searchfox.org/mozilla-central/source/mobile/android/geckoview\n[10]:https://github.com/mozilla/geckoview/issues\n[11]:https://bugzilla.mozilla.org/enter_bug.cgi?product=GeckoView\n[12]:https://geckoview.dev/javadoc/mozilla-central/index.html\n"
},
{
  "name": "release-notes",
  "files": {
    "/": [
      ".gitignore",
      ".gitlab-ci.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "releases",
      "requirements.in",
      "requirements.txt",
      "update_docker.sh",
      "update_releases.py"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Release Notes\n\nRelease notes and system requirements for Mozilla products in JSON format.\n\n## Usage\n\nTo update the data in this repo you can run a simple script which will pull the data from an instance of [Nucleus][]\nand populate the JSON files in the `releases` folder. After following one of the update procedures below you can commit\nand push the changes to the JSON files if you need to manually update the release notes data on the website.\n\n### Docker\n\nThis is the recommended way to do it since it requires no steps other than having [Docker][] installed. Simply run the following commands:\n\n```bash\n$ ./update_docker.sh\n```\n\nThis will build the docker image and run it to update the local JSON files.\n\n### Local Python\n\nYou'll need Python 3.6 or above and [virtualenv][] to run the following steps.\n\n```bash\n$ virtualenv -p python3 venv\n$ source venv/bin/activate\n$ pip install -r requirements.txt\n$ ./update_releases.py\n```\n\n[Nucleus]: https://nucleus.mozilla.org\n[Docker]: https://www.docker.com/community-edition\n[virtualenv]: https://virtualenv.pypa.io/en/stable/\n\n\n## License\n\nMozilla Public License v2. See [LICENSE](LICENSE) file for details.\n"
},
{
  "name": "sops",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".sops.yaml",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "Dockerfile.alpine",
      "LICENSE",
      "Makefile",
      "README.rst",
      "aes",
      "age",
      "audit",
      "azkv",
      "bin",
      "cmd",
      "config",
      "decrypt",
      "example.ini",
      "example.json",
      "example.txt",
      "example.yaml",
      "examples",
      "functional-tests",
      "gcpkms",
      "go.mod",
      "go.sum",
      "hcvault",
      "keys",
      "keyservice",
      "kms",
      "logging",
      "make_download_page.sh",
      "pgp",
      "publish",
      "shamir",
      "sops.go",
      "sops_test.go",
      "stores",
      "test.sh",
      "usererrors.go",
      "validation",
      "version"
    ],
    "/.github": [
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nPROJECT\t\t:= go.mozilla.org/sops/v3\nGO \t\t:= GOPROXY=https://proxy.golang.org go\nGOLINT \t\t:= golint\n\nall: test vet generate install functional-tests\norigin-build: test vet generate install functional-tests-all\n\ninstall:\n\t$(GO) install go.mozilla.org/sops/v3/cmd/sops\n\ntag: all\n\tgit tag -s $(TAGVER) -a -m \"$(TAGMSG)\"\n\nlint:\n\t$(GOLINT) $(PROJECT)\n\nvendor:\n\t$(GO) mod tidy\n\t$(GO) mod vendor\n\nvet:\n\t$(GO) vet $(PROJECT)\n\ntest: vendor\n\tgpg --import pgp/sops_functional_tests_key.asc 2>&1 1>/dev/null || exit 0\n\t./test.sh\n\nshowcoverage: test\n\t$(GO) tool cover -html=coverage.out\n\ngenerate: keyservice/keyservice.pb.go\n\t$(GO) generate\n\n%.pb.go: %.proto\n\tprotoc --go_out=plugins=grpc:. $<\n\nfunctional-tests:\n\t$(GO) build -o functional-tests/sops go.mozilla.org/sops/v3/cmd/sops\n\tcd functional-tests && cargo test\n\n# Ignored tests are ones that require external services (e.g. AWS KMS)\n# \tTODO: Once `--include-ignored` lands in rust stable, switch to that.\nfunctional-tests-all:\n\t$(GO) build -o functional-tests/sops go.mozilla.org/sops/v3/cmd/sops\n\tcd functional-tests && cargo test && cargo test -- --ignored\n\n# Creates variables during target re-definition. Basically this block allows the particular variables to be used in the final target\nbuild-deb-%: OS = $(word 1,$(subst -, ,$*))\nbuild-deb-%: ARCH = $(word 2,$(subst -, ,$*))\nbuild-deb-%: FPM_ARCH = $(word 3,$(subst -, ,$*))\n# Poor-mans function with parameters being split out from the variable part of it's name\nbuild-deb-%:\n\trm -rf tmppkg\n\tmkdir -p tmppkg/usr/local/bin\n\tGOOS=$(OS) GOARCH=\"$(ARCH)\" CGO_ENABLED=0 go build -mod vendor -o tmppkg/usr/local/bin/sops go.mozilla.org/sops/v3/cmd/sops\n\tfpm -C tmppkg -n sops --license MPL2.0 --vendor mozilla \\\n\t\t--description \"Sops is an editor of encrypted files that supports YAML, JSON and BINARY formats and encrypts with AWS KMS and PGP.\" \\\n\t\t-m \"AJ Bahnken <ajvb+sops@mozilla.com>\" \\\n\t\t--url https://go.mozilla.org/sops \\\n\t\t--architecture $(FPM_ARCH) \\\n\t\t-v \"$$(grep '^const Version' version/version.go |cut -d \\\" -f 2)\" \\\n\t\t-s dir -t deb .\n\n# Create .deb packages for multiple architectures\ndeb-pkg: vendor build-deb-linux-amd64-x86_64 build-deb-linux-arm64-arm64\n\n# Creates variables during target re-definition. Basically this block allows the particular variables to be used in the final target\nbuild-rpm-%: OS = $(word 1,$(subst -, ,$*))\nbuild-rpm-%: ARCH = $(word 2,$(subst -, ,$*))\nbuild-rpm-%: FPM_ARCH = $(word 3,$(subst -, ,$*))\n# Poor-mans function with parameters being split out from the variable part of it's name\nbuild-rpm-%:\n\trm -rf tmppkg\n\tmkdir -p tmppkg/usr/local/bin\n\tGOOS=$(OS) GOARCH=\"$(ARCH)\" CGO_ENABLED=0 go build -mod vendor -o tmppkg/usr/local/bin/sops go.mozilla.org/sops/v3/cmd/sops\n\tfpm -C tmppkg -n sops --license MPL2.0 --vendor mozilla \\\n\t\t--description \"Sops is an editor of encrypted files that supports YAML, JSON and BINARY formats and encrypts with AWS KMS and PGP.\" \\\n\t\t-m \"AJ Bahnken <ajvb+sops@mozilla.com>\" \\\n\t\t--url https://go.mozilla.org/sops \\\n\t\t--architecture $(FPM_ARCH) \\\n\t\t--rpm-os $(OS) \\\n\t\t-v \"$$(grep '^const Version' version/version.go |cut -d \\\" -f 2)\" \\\n\t\t-s dir -t rpm .\n\n# Create .rpm packages for multiple architectures\nrpm-pkg: vendor build-rpm-linux-amd64-x86_64 build-rpm-linux-arm64-arm64\n\ndmg-pkg: install\nifneq ($(OS),darwin)\n\t\techo 'you must be on MacOS and set OS=darwin on the make command line to build an OSX package'\nelse\n\trm -rf tmppkg\n\tmkdir -p tmppkg/usr/local/bin\n\tcp $$GOPATH/bin/sops tmppkg/usr/local/bin/\n\tfpm -C tmppkg -n sops --license MPL2.0 --vendor mozilla \\\n\t\t--description \"Sops is an editor of encrypted files that supports YAML, JSON and BINARY formats and encrypts with AWS KMS and PGP.\" \\\n\t\t-m \"Mozilla Security <security@mozilla.org>\" \\\n\t\t--url https://go.mozilla.org/sops \\\n\t\t--architecture x86_64 \\\n\t\t-v \"$$(grep '^const Version' version/version.go |cut -d \\\" -f 2)\" \\\n\t\t-s dir -t osxpkg \\\n\t\t--osxpkg-identifier-prefix org.mozilla.sops \\\n\t\t-p tmppkg/sops-$$(git describe --abbrev=0 --tags).pkg .\n\thdiutil makehybrid -hfs -hfs-volume-name \"Mozilla Sops\" \\\n\t\t-o tmppkg/sops-$$(git describe --abbrev=0 --tags).dmg tmpdmg\nendif\n\ndownload-index:\n\tbash make_download_page.sh\n\nmock:\n\tgo install github.com/vektra/mockery/.../\n\tmockery -dir vendor/github.com/aws/aws-sdk-go/service/kms/kmsiface/ -name KMSAPI -output kms/mocks\n\n.PHONY: all test generate clean vendor functional-tests mock\n",
  "readme": "SOPS: Secrets OPerationS\n========================\n\n**sops** is an editor of encrypted files that supports YAML, JSON, ENV, INI and BINARY\nformats and encrypts with AWS KMS, GCP KMS, Azure Key Vault, age, and PGP.\n(`demo <https://www.youtube.com/watch?v=YTEVyLXFiq0>`_)\n\n.. image:: https://i.imgur.com/X0TM5NI.gif\n\n------------\n\n.. image:: https://pkg.go.dev/badge/go.mozilla.org/sops/v3.svg\n\t:target: https://pkg.go.dev/go.mozilla.org/sops/v3\n\nDownload\n--------\n\nStable release\n~~~~~~~~~~~~~~\nBinaries and packages of the latest stable release are available at `https://github.com/mozilla/sops/releases <https://github.com/mozilla/sops/releases>`_.\n\nDevelopment branch\n~~~~~~~~~~~~~~~~~~\nFor the adventurous, unstable features are available in the `develop` branch, which you can install from source:\n\n.. code:: bash\n\n\t$ mkdir -p $GOPATH/src/go.mozilla.org/sops/\n        $ git clone https://github.com/mozilla/sops.git $GOPATH/src/go.mozilla.org/sops/\n        $ cd $GOPATH/src/go.mozilla.org/sops/\n        $ git checkout develop\n        $ make install\n\n(requires Go >= 1.17)\n\nIf you don't have Go installed, set it up with:\n\n.. code:: bash\n\n\t$ {apt,yum,brew} install golang\n\t$ echo 'export GOPATH=~/go' >> ~/.bashrc\n\t$ source ~/.bashrc\n\t$ mkdir $GOPATH\n\nOr whatever variation of the above fits your system and shell.\n\nTo use **sops** as a library, take a look at the `decrypt package <https://pkg.go.dev/go.mozilla.org/sops/v3/decrypt>`_.\n\n.. sectnum::\n.. contents:: Table of Contents\n\nUsage\n-----\n\nFor a quick presentation of Sops, check out this Youtube tutorial:\n\n.. image:: https://img.youtube.com/vi/V2PRhxphH2w/0.jpg\n   :target: https://www.youtube.com/watch?v=V2PRhxphH2w\n\nIf you're using AWS KMS, create one or multiple master keys in the IAM console\nand export them, comma separated, in the **SOPS_KMS_ARN** env variable. It is\nrecommended to use at least two master keys in different regions.\n\n.. code:: bash\n\n\texport SOPS_KMS_ARN=\"arn:aws:kms:us-east-1:656532927350:key/920aff2e-c5f1-4040-943a-047fa387b27e,arn:aws:kms:ap-southeast-1:656532927350:key/9006a8aa-0fa6-4c14-930e-a2dfb916de1d\"\n\nYour AWS credentials must be present in ``~/.aws/credentials``. sops uses aws-sdk-go.\n\n.. code::\n\n\t$ cat ~/.aws/credentials\n\t[default]\n\taws_access_key_id = AKI.....\n\taws_secret_access_key = mw......\n\nIf you want to use PGP, export the fingerprints of the public keys, comma\nseparated, in the **SOPS_PGP_FP** env variable.\n\n.. code:: bash\n\n\texport SOPS_PGP_FP=\"85D77543B3D624B63CEA9E6DBC17301B491B3F21,E60892BB9BD89A69F759A1A0A3D652173B763E8F\"\n\nNote: you can use both PGP and KMS simultaneously.\n\nThen simply call ``sops`` with a file path as argument. It will handle the\nencryption/decryption transparently and open the cleartext file in an editor\n\n.. code:: shell\n\n\t$ sops mynewtestfile.yaml\n\tmynewtestfile.yaml doesn't exist, creating it.\n\tplease wait while an encryption key is being generated and stored in a secure fashion\n\tfile written to mynewtestfile.yaml\n\nEditing will happen in whatever ``$EDITOR`` is set to, or, if it's not set, in vim.\nKeep in mind that sops will wait for the editor to exit, and then try to reencrypt\nthe file. Some GUI editors (atom, sublime) spawn a child process and then exit\nimmediately. They usually have an option to wait for the main editor window to be\nclosed before exiting. See `#127 <https://github.com/mozilla/sops/issues/127>`_ for\nmore information.\n\nThe resulting encrypted file looks like this:\n\n.. code:: yaml\n\n    myapp1: ENC[AES256_GCM,data:Tr7o=,iv:1=,aad:No=,tag:k=]\n    app2:\n        db:\n            user: ENC[AES256_GCM,data:CwE4O1s=,iv:2k=,aad:o=,tag:w==]\n            password: ENC[AES256_GCM,data:p673w==,iv:YY=,aad:UQ=,tag:A=]\n        # private key for secret operations in app2\n        key: |-\n            ENC[AES256_GCM,data:Ea3kL5O5U8=,iv:DM=,aad:FKA=,tag:EA==]\n    an_array:\n    - ENC[AES256_GCM,data:v8jQ=,iv:HBE=,aad:21c=,tag:gA==]\n    - ENC[AES256_GCM,data:X10=,iv:o8=,aad:CQ=,tag:Hw==]\n    - ENC[AES256_GCM,data:KN=,iv:160=,aad:fI4=,tag:tNw==]\n    sops:\n        kms:\n        -   created_at: 1441570389.775376\n            enc: CiC....Pm1Hm\n            arn: arn:aws:kms:us-east-1:656532927350:key/920aff2e-c5f1-4040-943a-047fa387b27e\n        -   created_at: 1441570391.925734\n            enc: Ci...awNx\n            arn: arn:aws:kms:ap-southeast-1:656532927350:key/9006a8aa-0fa6-4c14-930e-a2dfb916de1d\n        pgp:\n        -   fp: 85D77543B3D624B63CEA9E6DBC17301B491B3F21\n            created_at: 1441570391.930042\n            enc: |\n                -----BEGIN PGP MESSAGE-----\n                hQIMA0t4uZHfl9qgAQ//UvGAwGePyHuf2/zayWcloGaDs0MzI+zw6CmXvMRNPUsA\n\t\t\t\t...=oJgS\n                -----END PGP MESSAGE-----\n\nA copy of the encryption/decryption key is stored securely in each KMS and PGP\nblock. As long as one of the KMS or PGP method is still usable, you will be able\nto access your data.\n\nTo decrypt a file in a ``cat`` fashion, use the ``-d`` flag:\n\n.. code:: bash\n\n\t$ sops -d mynewtestfile.yaml\n\n``sops`` encrypted files contain the necessary information to decrypt their content.\nAll a user of ``sops`` needs is valid AWS credentials and the necessary\npermissions on KMS keys.\n\nGiven that, the only command a ``sops`` user needs is:\n\n.. code:: bash\n\n\t$ sops <file>\n\n`<file>` will be opened, decrypted, passed to a text editor (vim by default),\nencrypted if modified, and saved back to its original location. All of these\nsteps, apart from the actual editing, are transparent to the user.\n\nTest with the dev PGP key\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want to test **sops** without having to do a bunch of setup, you can use\nthe example files and pgp key provided with the repository::\n\n\t$ git clone https://github.com/mozilla/sops.git\n\t$ cd sops\n\t$ gpg --import pgp/sops_functional_tests_key.asc\n\t$ sops example.yaml\n\nThis last step will decrypt ``example.yaml`` using the test private key.\n\n\nEncrypting using age\n~~~~~~~~~~~~~~~~~~~~\n\n`age <https://age-encryption.org/>`_ is a simple, modern, and secure tool for\nencrypting files. It's recommended to use age over PGP, if possible.\n\nYou can encrypt a file for one or more age recipients (comma separated) using\nthe ``--age`` option or the **SOPS_AGE_RECIPIENTS** environment variable:\n\n.. code:: bash\n\n   $ sops --encrypt --age age1yt3tfqlfrwdwx0z0ynwplcr6qxcxfaqycuprpmy89nr83ltx74tqdpszlw test.yaml > test.enc.yaml\n\nWhen decrypting a file with the corresponding identity, sops will look for a\ntext file name ``keys.txt`` located in a ``sops`` subdirectory of your user\nconfiguration directory. On Linux, this would be ``$XDG_CONFIG_HOME/sops/age/keys.txt``.\nOn macOS, this would be ``$HOME/Library/Application Support/sops/age/keys.txt``. On\nWindows, this would be ``%AppData%\\sops\\age\\keys.txt``. You can specify the location\nof this file manually by setting the environment variable **SOPS_AGE_KEY_FILE**.\nAlternatively you can provide the the key(s) directly by setting the **SOPS_AGE_KEY**\nenvironment variable.\n\nThe contents of this key file should be a list of age X25519 identities, one\nper line. Lines beginning with ``#`` are considered comments and ignored. Each\nidentity will be tried in sequence until one is able to decrypt the data.\n\nEncrypting with SSH keys via age is not yet supported by sops.\n\n\nEncrypting using GCP KMS\n~~~~~~~~~~~~~~~~~~~~~~~~\nGCP KMS uses `Application Default Credentials\n<https://developers.google.com/identity/protocols/application-default-credentials>`_.\nIf you already logged in using\n\n.. code:: bash\n\n\t$ gcloud auth login\n\nyou can enable application default credentials using the sdk::\n\n\t$ gcloud auth application-default login\n\nEncrypting/decrypting with GCP KMS requires a KMS ResourceID. You can use the\ncloud console the get the ResourceID or you can create one using the gcloud\nsdk:\n\n.. code:: bash\n\n\t$ gcloud kms keyrings create sops --location global\n\t$ gcloud kms keys create sops-key --location global --keyring sops --purpose encryption\n\t$ gcloud kms keys list --location global --keyring sops\n\n\t# you should see\n\tNAME                                                                   PURPOSE          PRIMARY_STATE\n\tprojects/my-project/locations/global/keyRings/sops/cryptoKeys/sops-key ENCRYPT_DECRYPT  ENABLED\n\nNow you can encrypt a file using::\n\n\t$ sops --encrypt --gcp-kms projects/my-project/locations/global/keyRings/sops/cryptoKeys/sops-key test.yaml > test.enc.yaml\n\nAnd decrypt it using::\n\n\t $ sops --decrypt test.enc.yaml\n\nEncrypting using Azure Key Vault\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe Azure Key Vault integration tries several authentication methods, in\nthis order:\n\n  1. Client credentials\n  2. Client Certificate\n  3. Username Password\n  4. MSI\n  5. Azure CLI auth\n\nYou can force a specific authentication method through the AZURE_AUTH_METHOD\nenvironment variable, which may be one of: clientcredentials, clientcertificate,\nusernamepassword, msi, or cli (default).\n\nFor example, you can use service principals with the following environment variables:\n\n.. code:: bash\n\n\tAZURE_TENANT_ID\n\tAZURE_CLIENT_ID\n\tAZURE_CLIENT_SECRET\n\nYou can create a service principal using the cli like this:\n\n.. code:: bash\n\n\t$ az ad sp create-for-rbac -n my-keyvault-sp\n\n\t{\n\t\t\"appId\": \"<some-uuid>\",\n\t\t\"displayName\": \"my-keyvault-sp\",\n\t\t\"name\": \"http://my-keyvault-sp\",\n\t\t\"password\": \"<some-uuid>\",\n\t\t\"tenant\": \"<tenant-id>\"\n\t}\n\nThe appId is the client id, and the password is the client secret.\n\nEncrypting/decrypting with Azure Key Vault requires the resource identifier for\na key. This has the following form::\n\n\thttps://${VAULT_URL}/keys/${KEY_NAME}/${KEY_VERSION}\n\nTo create a Key Vault and assign your service principal permissions on it\nfrom the commandline:\n\n.. code:: bash\n\n\t# Create a resource group if you do not have one:\n\t$ az group create --name sops-rg --location westeurope\n\t# Key Vault names are globally unique, so generate one:\n\t$ keyvault_name=sops-$(uuidgen | tr -d - | head -c 16)\n\t# Create a Vault, a key, and give the service principal access:\n\t$ az keyvault create --name $keyvault_name --resource-group sops-rg --location westeurope\n\t$ az keyvault key create --name sops-key --vault-name $keyvault_name --protection software --ops encrypt decrypt\n\t$ az keyvault set-policy --name $keyvault_name --resource-group sops-rg --spn $AZURE_CLIENT_ID \\\n\t\t--key-permissions encrypt decrypt\n\t# Read the key id:\n\t$ az keyvault key show --name sops-key --vault-name $keyvault_name --query key.kid\n\n\thttps://sops.vault.azure.net/keys/sops-key/some-string\n\nNow you can encrypt a file using::\n\n\t$ sops --encrypt --azure-kv https://sops.vault.azure.net/keys/sops-key/some-string test.yaml > test.enc.yaml\n\nAnd decrypt it using::\n\n\t $ sops --decrypt test.enc.yaml\n\n\nEncrypting using Hashicorp Vault\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWe assume you have an instance (or more) of Vault running and you have privileged access to it. For instructions on how to deploy a secure instance of Vault, refer to Hashicorp's official documentation.\n\nTo easily deploy Vault locally: (DO NOT DO THIS FOR PRODUCTION!!!) \n\n.. code:: bash\n\n\t$ docker run -d -p8200:8200 vault:1.2.0 server -dev -dev-root-token-id=toor\n\n\n.. code:: bash\n\n\t$ # Substitute this with the address Vault is running on\n\t$ export VAULT_ADDR=http://127.0.0.1:8200 \n\n\t$ # this may not be necessary in case you previously used `vault login` for production use\n\t$ export VAULT_TOKEN=toor \n\t\n\t$ # to check if Vault started and is configured correctly\n\t$ vault status\n\tKey             Value\n\t---             -----\n\tSeal Type       shamir\n\tInitialized     true\n\tSealed          false\n\tTotal Shares    1\n\tThreshold       1\n\tVersion         1.2.0\n\tCluster Name    vault-cluster-618cc902\n\tCluster ID      e532e461-e8f0-1352-8a41-fc7c11096908\n\tHA Enabled      false\n\n\t$ # It is required to enable a transit engine if not already done (It is suggested to create a transit engine specifically for sops, in which it is possible to have multiple keys with various permission levels)\n\t$ vault secrets enable -path=sops transit\n\tSuccess! Enabled the transit secrets engine at: sops/\n\n\t$ # Then create one or more keys\n\t$ vault write sops/keys/firstkey type=rsa-4096\n\tSuccess! Data written to: sops/keys/firstkey\n\n\t$ vault write sops/keys/secondkey type=rsa-2048\n\tSuccess! Data written to: sops/keys/secondkey\n\n\t$ vault write sops/keys/thirdkey type=chacha20-poly1305\n\tSuccess! Data written to: sops/keys/thirdkey\n\n\t$ sops --hc-vault-transit $VAULT_ADDR/v1/sops/keys/firstkey vault_example.yml\n\n\t$ cat <<EOF > .sops.yaml\n\tcreation_rules:\n\t\t- path_regex: \\.dev\\.yaml$\n\t\t  hc_vault_transit_uri: \"$VAULT_ADDR/v1/sops/keys/secondkey\"\n\t\t- path_regex: \\.prod\\.yaml$\n\t\t  hc_vault_transit_uri: \"$VAULT_ADDR/v1/sops/keys/thirdkey\"\n\tEOF\n\n\t$ sops --verbose -e prod/raw.yaml > prod/encrypted.yaml\n\nAdding and removing keys\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen creating new files, ``sops`` uses the PGP, KMS and GCP KMS defined in the\ncommand line arguments ``--kms``, ``--pgp``, ``--gcp-kms`` or ``--azure-kv``, or from\nthe environment variables ``SOPS_KMS_ARN``, ``SOPS_PGP_FP``, ``SOPS_GCP_KMS_IDS``,\n``SOPS_AZURE_KEYVAULT_URLS``. That information is stored in the file under the\n``sops`` section, such that decrypting files does not require providing those\nparameters again.\n\nMaster PGP and KMS keys can be added and removed from a ``sops`` file in one of\nthree ways::\n\n1. By using a .sops.yaml file and the ``updatekeys`` command.\n\n2. By using command line flags.\n\n3. By editing the file directly.\n\nThe sops team recommends the ``updatekeys`` approach.\n\n\n``updatekeys`` command\n**********************\n\nThe ``updatekeys`` command uses the `.sops.yaml <#using-sops-yaml-conf-to-select-kms-pgp-for-new-files>`_\nconfiguration file to update (add or remove) the corresponding secrets in the\nencrypted file. Note that the example below uses the\n`Block Scalar yaml construct <https://yaml-multiline.info/>`_ to build a space\nseparated list.\n\n.. code:: yaml\n\n    creation_rules:\n        - pgp: >-\n            85D77543B3D624B63CEA9E6DBC17301B491B3F21,\n            FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4\n\n.. code:: bash\n\n\t$ sops updatekeys test.enc.yaml\n\nSops will prompt you with the changes to be made. This interactivity can be\ndisabled by supplying the ``-y`` flag.\n\nCommand Line\n************\n\nCommand line flag ``--add-kms``, ``--add-pgp``, ``--add-gcp-kms``, ``--add-azure-kv``,\n``--rm-kms``, ``--rm-pgp``, ``--rm-gcp-kms`` and ``--rm-azure-kv`` can be used to add\nand remove keys from a file.\nThese flags use the comma separated syntax as the ``--kms``, ``--pgp``, ``--gcp-kms``\nand ``--azure-kv`` arguments when creating new files.\n\nNote that ``-r`` or ``--rotate`` is mandatory in this mode. Not specifying\nrotate will ignore the ``--add-*`` options. Use ``updatekeys`` if you want to\nadd a key without rotating the data key.\n\n.. code:: bash\n\n\t# add a new pgp key to the file and rotate the data key\n\t$ sops -r -i --add-pgp 85D77543B3D624B63CEA9E6DBC17301B491B3F21 example.yaml\n\n\t# remove a pgp key from the file and rotate the data key\n\t$ sops -r -i --rm-pgp 85D77543B3D624B63CEA9E6DBC17301B491B3F21 example.yaml\n\n\nDirect Editing\n**************\n\nAlternatively, invoking ``sops`` with the flag **-s** will display the master keys\nwhile editing. This method can be used to add or remove kms or pgp keys under the\nsops section. Invoking ``sops`` with the **-i** flag will perform an in-place edit\ninstead of redirecting output to ``stdout``.\n\nFor example, to add a KMS master key to a file, add the following entry while\nediting:\n\n.. code:: yaml\n\n\tsops:\n\t    kms:\n\t    - arn: arn:aws:kms:us-east-1:656532927350:key/920aff2e-c5f1-4040-943a-047fa387b27e\n\nAnd, similarly, to add a PGP master key, we add its fingerprint:\n\n.. code:: yaml\n\n\tsops:\n\t    pgp:\n\t    - fp: 85D77543B3D624B63CEA9E6DBC17301B491B3F21\n\nWhen the file is saved, ``sops`` will update its metadata and encrypt the data key\nwith the freshly added master keys. The removed entries are simply deleted from\nthe file.\n\nWhen removing keys, it is recommended to rotate the data key using ``-r``,\notherwise owners of the removed key may have add access to the data key in the\npast.\n\nKMS AWS Profiles\n~~~~~~~~~~~~~~~~\n\nIf you want to use a specific profile, you can do so with `aws_profile`:\n\n.. code:: yaml\n\n\tsops:\n\t    kms:\n\t    -\tarn: arn:aws:kms:us-east-1:656532927350:key/920aff2e-c5f1-4040-943a-047fa387b27e\n\t        aws_profile: foo\n\nIf no AWS profile is set, default credentials will be used.\n\nSimilarly the `--aws-profile` flag can be set with the command line with any of the KMS commands.\n\n\nAssuming roles and using KMS in various AWS accounts\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSOPS has the ability to use KMS in multiple AWS accounts by assuming roles in\neach account. Being able to assume roles is a nice feature of AWS that allows\nadministrators to establish trust relationships between accounts, typically from\nthe most secure account to the least secure one. In our use-case, we use roles\nto indicate that a user of the Master AWS account is allowed to make use of KMS\nmaster keys in development and staging AWS accounts. Using roles, a single file\ncan be encrypted with KMS keys in multiple accounts, thus increasing reliability\nand ease of use.\n\nYou can use keys in various accounts by tying each KMS master key to a role that\nthe user is allowed to assume in each account. The `IAM roles\n<http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use.html>`_\ndocumentation has full details on how this needs to be configured on AWS's side.\n\nFrom the point of view of ``sops``, you only need to specify the role a KMS key\nmust assume alongside its ARN, as follows:\n\n.. code:: yaml\n\n\tsops:\n\t    kms:\n\t    -\tarn: arn:aws:kms:us-east-1:656532927350:key/920aff2e-c5f1-4040-943a-047fa387b27e\n\t        role: arn:aws:iam::927034868273:role/sops-dev-xyz\n\nThe role must have permission to call Encrypt and Decrypt using KMS. An example\npolicy is shown below.\n\n.. code:: json\n\n\t{\n\t  \"Sid\": \"Allow use of the key\",\n\t  \"Effect\": \"Allow\",\n\t  \"Action\": [\n\t\t\"kms:Encrypt\",\n\t\t\"kms:Decrypt\",\n\t\t\"kms:ReEncrypt*\",\n\t\t\"kms:GenerateDataKey*\",\n\t\t\"kms:DescribeKey\"\n\t  ],\n\t  \"Resource\": \"*\",\n\t  \"Principal\": {\n\t\t\"AWS\": [\n\t\t  \"arn:aws:iam::927034868273:role/sops-dev-xyz\"\n\t\t]\n\t  }\n\t}\n\nYou can specify a role in the ``--kms`` flag and ``SOPS_KMS_ARN`` variable by\nappending it to the ARN of the master key, separated by a **+** sign::\n\n\t<KMS ARN>+<ROLE ARN>\n\tarn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500+arn:aws:iam::927034868273:role/sops-dev-xyz\n\nAWS KMS Encryption Context\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSOPS has the ability to use `AWS KMS key policy and encryption context\n<http://docs.aws.amazon.com/kms/latest/developerguide/encryption-context.html>`_\nto refine the access control of a given KMS master key.\n\nWhen creating a new file, you can specify encryption context in the\n``--encryption-context`` flag by comma separated list of key-value pairs:\n\n.. code:: bash\n\n\t$ sops --encryption-context Environment:production,Role:web-server test.dev.yaml\n\nThe format of the Encrypt Context string is ``<EncryptionContext Key>:<EncryptionContext Value>,<EncryptionContext Key>:<EncryptionContext Value>,...``\n\nThe encryption context will be stored in the file metadata and does\nnot need to be provided at decryption.\n\nEncryption contexts can be used in conjunction with KMS Key Policies to define\nroles that can only access a given context. An example policy is shown below:\n\n.. code:: json\n\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::111122223333:role/RoleForExampleApp\"\n      },\n      \"Action\": \"kms:Decrypt\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"kms:EncryptionContext:AppName\": \"ExampleApp\",\n          \"kms:EncryptionContext:FilePath\": \"/var/opt/secrets/\"\n        }\n      }\n    }\n\nKey Rotation\n~~~~~~~~~~~~\n\nIt is recommended to renew the data key on a regular basis. ``sops`` supports key\nrotation via the ``-r`` flag. Invoking it on an existing file causes sops to\nreencrypt the file with a new data key, which is then encrypted with the various\nKMS and PGP master keys defined in the file.\n\n.. code:: bash\n\n\tsops -r example.yaml\n\nUsing .sops.yaml conf to select KMS/PGP for new files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt is often tedious to specify the ``--kms`` ``--gcp-kms`` and ``--pgp`` parameters for creation\nof all new files. If your secrets are stored under a specific directory, like a\n``git`` repository, you can create a ``.sops.yaml`` configuration file at the root\ndirectory to define which keys are used for which filename.\n\nLet's take an example:\n\n* file named **something.dev.yaml** should use one set of KMS A\n* file named **something.prod.yaml** should use another set of KMS B\n* other files use a third set of KMS C\n* all live under **mysecretrepo/something.{dev,prod,gcp}.yaml**\n\nUnder those circumstances, a file placed at **mysecretrepo/.sops.yaml**\ncan manage the three sets of configurations for the three types of files:\n\n.. code:: yaml\n\n\t# creation rules are evaluated sequentially, the first match wins\n\tcreation_rules:\n\t\t# upon creation of a file that matches the pattern *.dev.yaml,\n\t\t# KMS set A is used\n\t\t- path_regex: \\.dev\\.yaml$\n\t\t  kms: 'arn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500,arn:aws:kms:us-west-2:361527076523:key/5052f06a-5d3f-489e-b86c-57201e06f31e+arn:aws:iam::361527076523:role/hiera-sops-prod'\n\t\t  pgp: 'FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4'\n\n\t\t# prod files use KMS set B in the PROD IAM\n\t\t- path_regex: \\.prod\\.yaml$\n\t\t  kms: 'arn:aws:kms:us-west-2:361527076523:key/5052f06a-5d3f-489e-b86c-57201e06f31e+arn:aws:iam::361527076523:role/hiera-sops-prod,arn:aws:kms:eu-central-1:361527076523:key/cb1fab90-8d17-42a1-a9d8-334968904f94+arn:aws:iam::361527076523:role/hiera-sops-prod'\n\t\t  pgp: 'FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4'\n\t\t  hc_vault_uris: \"http://localhost:8200/v1/sops/keys/thirdkey\"\n\n\t\t# gcp files using GCP KMS\n\t\t- path_regex: \\.gcp\\.yaml$\n\t\t  gcp_kms: projects/mygcproject/locations/global/keyRings/mykeyring/cryptoKeys/thekey\n\n\t\t# Finally, if the rules above have not matched, this one is a\n\t\t# catchall that will encrypt the file using KMS set C\n\t\t# The absence of a path_regex means it will match everything\n\t\t- kms: 'arn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500,arn:aws:kms:us-west-2:142069644989:key/846cfb17-373d-49b9-8baf-f36b04512e47,arn:aws:kms:us-west-2:361527076523:key/5052f06a-5d3f-489e-b86c-57201e06f31e'\n\t\t  pgp: 'FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4'\n\nWhen creating any file under **mysecretrepo**, whether at the root or under\na subdirectory, sops will recursively look for a ``.sops.yaml`` file. If one is\nfound, the filename of the file being created is compared with the filename\nregexes of the configuration file. The first regex that matches is selected,\nand its KMS and PGP keys are used to encrypt the file. It should be noted that\nthe looking up of ``.sops.yaml`` is from the working directory (CWD) instead of\nthe directory of the encrypting file (see `Issue 242 <https://github.com/mozilla/sops/issues/242>`_).\n\nThe path_regex checks the path of the encrypting file relative to the .sops.yaml config file. Here is another example:\n\n* files located under directory **development** should use one set of KMS A\n* files located under directory **production** should use another set of KMS B\n* other files use a third set of KMS C\n\n.. code:: yaml\n\n    creation_rules:\n        # upon creation of a file under development,\n        # KMS set A is used\n        - path_regex: .*/development/.*\n          kms: 'arn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500,arn:aws:kms:us-west-2:361527076523:key/5052f06a-5d3f-489e-b86c-57201e06f31e+arn:aws:iam::361527076523:role/hiera-sops-prod'\n          pgp: 'FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4'\n\n        # prod files use KMS set B in the PROD IAM\n        - path_regex: .*/production/.*\n          kms: 'arn:aws:kms:us-west-2:361527076523:key/5052f06a-5d3f-489e-b86c-57201e06f31e+arn:aws:iam::361527076523:role/hiera-sops-prod,arn:aws:kms:eu-central-1:361527076523:key/cb1fab90-8d17-42a1-a9d8-334968904f94+arn:aws:iam::361527076523:role/hiera-sops-prod'\n          pgp: 'FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4'\n\n        # other files use KMS set C\n        - kms: 'arn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500,arn:aws:kms:us-west-2:142069644989:key/846cfb17-373d-49b9-8baf-f36b04512e47,arn:aws:kms:us-west-2:361527076523:key/5052f06a-5d3f-489e-b86c-57201e06f31e'\n          pgp: 'FBC7B9E2A4F9289AC0C1D4843D16CEE4A27381B4'\n\nCreating a new file with the right keys is now as simple as\n\n.. code:: bash\n\n\t$ sops <newfile>.prod.yaml\n\nNote that the configuration file is ignored when KMS or PGP parameters are\npassed on the sops command line or in environment variables.\n\nSpecify a different GPG executable\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``sops`` checks for the ``SOPS_GPG_EXEC`` environment variable. If specified,\nit will attempt to use the executable set there instead of the default\nof ``gpg``.\n\nExample: place the following in your ``~/.bashrc``\n\n.. code:: bash\n\n\tSOPS_GPG_EXEC = 'your_gpg_client_wrapper'\n\n\nSpecify a different GPG key server\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy default, ``sops`` uses the key server ``keys.openpgp.org`` to retrieve the GPG\nkeys that are not present in the local keyring.\nThis is no longer configurable. You can learn more about why from this write-up: `SKS Keyserver Network Under Attack <https://gist.github.com/rjhansen/67ab921ffb4084c865b3618d6955275f>`_.\n\n\nKey groups\n~~~~~~~~~~\n\nBy default, ``sops`` encrypts the data key for a file with each of the master keys,\nsuch that if any of the master keys is available, the file can be decrypted.\nHowever, it is sometimes desirable to require access to multiple master keys\nin order to decrypt files. This can be achieved with key groups.\n\nWhen using key groups in sops, data keys are split into parts such that keys from\nmultiple groups are required to decrypt a file. ``sops`` uses Shamir's Secret Sharing\nto split the data key such that each key group has a fragment, each key in the\nkey group can decrypt that fragment, and a configurable number of fragments (threshold)\nare needed to decrypt and piece together the complete data key. When decrypting a\nfile using multiple key groups, ``sops`` goes through key groups in order, and in\neach group, tries to recover the fragment of the data key using a master key from\nthat group. Once the fragment is recovered, ``sops`` moves on to the next group,\nuntil enough fragments have been recovered to obtain the complete data key.\n\nBy default, the threshold is set to the number of key groups. For example, if\nyou have three key groups configured in your SOPS file and you don't override\nthe default threshold, then one master key from each of the three groups will\nbe required to decrypt the file.\n\nManagement of key groups is done with the ``sops groups`` command.\n\nFor example, you can add a new key group with 3 PGP keys and 3 KMS keys to the\nfile ``my_file.yaml``:\n\n.. code:: bash\n\n    $ sops groups add --file my_file.yaml --pgp fingerprint1 --pgp fingerprint2 --pgp fingerprint3 --kms arn1 --kms arn2 --kms arn3\n\nOr you can delete the 1st group (group number 0, as groups are zero-indexed)\nfrom ``my_file.yaml``:\n\n.. code:: bash\n\n    $ sops groups delete --file my_file.yaml 0\n\nKey groups can also be specified in the ``.sops.yaml`` config file,\nlike so:\n\n.. code:: yaml\n\n    creation_rules:\n        - path_regex: .*keygroups.*\n          key_groups:\n          # First key group\n          - pgp:\n            - fingerprint1\n            - fingerprint2\n            kms:\n            - arn: arn1\n              role: role1\n              context:\n                foo: bar\n            - arn: arn2\n          # Second key group\n          - pgp:\n            - fingerprint3\n            - fingerprint4\n            kms:\n            - arn: arn3\n            - arn: arn4\n          # Third key group\n          - pgp:\n            - fingerprint5\n\nGiven this configuration, we can create a new encrypted file like we normally\nwould, and optionally provide the ``--shamir-secret-sharing-threshold`` command line\nflag if we want to override the default threshold. ``sops`` will then split the data\nkey into three parts (from the number of key groups) and encrypt each fragment with\nthe master keys found in each group.\n\nFor example:\n\n.. code:: bash\n\n    $ sops --shamir-secret-sharing-threshold 2 example.json\n\nAlternatively, you can configure the Shamir threshold for each creation rule in the ``.sops.yaml`` config\nwith ``shamir_threshold``:\n\n.. code:: yaml\n\n    creation_rules:\n        - path_regex: .*keygroups.*\n          shamir_threshold: 2\n          key_groups:\n          # First key group\n          - pgp:\n            - fingerprint1\n            - fingerprint2\n            kms:\n            - arn: arn1\n              role: role1\n              context:\n                foo: bar\n            - arn: arn2\n          # Second key group\n          - pgp:\n            - fingerprint3\n            - fingerprint4\n            kms:\n            - arn: arn3\n            - arn: arn4\n          # Third key group\n          - pgp:\n            - fingerprint5\n\nAnd then run ``sops example.json``.\n\nThe threshold (``shamir_threshold``) is set to 2, so this configuration will require\nmaster keys from two of the three different key groups in order to decrypt the file.\nYou can then decrypt the file the same way as with any other SOPS file:\n\n.. code:: bash\n\n    $ sops -d example.json\n\nKey service\n~~~~~~~~~~~\n\nThere are situations where you might want to run ``sops`` on a machine that\ndoesn't have direct access to encryption keys such as PGP keys. The ``sops`` key\nservice allows you to forward a socket so that ``sops`` can access encryption\nkeys stored on a remote machine. This is similar to GPG Agent, but more\nportable.\n\nSOPS uses a client-server approach to encrypting and decrypting the data\nkey. By default, SOPS runs a local key service in-process. SOPS uses a key\nservice client to send an encrypt or decrypt request to a key service, which\nthen performs the operation. The requests are sent using gRPC and Protocol\nBuffers. The requests contain an identifier for the key they should perform\nthe operation with, and the plaintext or encrypted data key. The requests do\nnot contain any cryptographic keys, public or private.\n\n**WARNING: the key service connection currently does not use any sort of\nauthentication or encryption. Therefore, it is recommended that you make sure\nthe connection is authenticated and encrypted in some other way, for example\nthrough an SSH tunnel.**\n\nWhenever we try to encrypt or decrypt a data key, SOPS will try to do so first\nwith the local key service (unless it's disabled), and if that fails, it will\ntry all other remote key services until one succeeds.\n\nYou can start a key service server by running ``sops keyservice``.\n\nYou can specify the key services the ``sops`` binary uses with ``--keyservice``.\nThis flag can be specified more than once, so you can use multiple key\nservices. The local key service can be disabled with\n``enable-local-keyservice=false``.\n\nFor example, to decrypt a file using both the local key service and the key\nservice exposed on the unix socket located in ``/tmp/sops.sock``, you can run:\n\n.. code:: bash\n\n    $ sops --keyservice unix:///tmp/sops.sock -d file.yaml`\n\nAnd if you only want to use the key service exposed on the unix socket located\nin ``/tmp/sops.sock`` and not the local key service, you can run:\n\n.. code:: bash\n\n    $ sops --enable-local-keyservice=false --keyservice unix:///tmp/sops.sock -d file.yaml\n\nAuditing\n~~~~~~~~\n\nSometimes, users want to be able to tell what files were accessed by whom in an\nenvironment they control. For this reason, SOPS can generate audit logs to\nrecord activity on encrypted files. When enabled, SOPS will write a log entry\ninto a pre-configured PostgreSQL database when a file is decrypted. The log\nincludes a timestamp, the username SOPS is running as, and the file that was\ndecrypted.\n\nIn order to enable auditing, you must first create the database and credentials\nusing the schema found in ``audit/schema.sql``. This schema defines the\ntables that store the audit events and a role named ``sops`` that only has\npermission to add entries to the audit event tables. The default password for\nthe role ``sops`` is ``sops``. You should change this password.\n\nOnce you have created the database, you have to tell SOPS how to connect to it.\nBecause we don't want users of SOPS to be able to control auditing, the audit\nconfiguration file location is not configurable, and must be at\n``/etc/sops/audit.yaml``. This file should have strict permissions such\nthat only the root user can modify it.\n\nFor example, to enable auditing to a PostgreSQL database named ``sops`` running\non localhost, using the user ``sops`` and the password ``sops``,\n``/etc/sops/audit.yaml`` should have the following contents:\n\n.. code:: yaml\n\n    backends:\n        postgres:\n            - connection_string: \"postgres://sops:sops@localhost/sops?sslmode=verify-full\"\n\n\nYou can find more information on the ``connection_string`` format in the\n`PostgreSQL docs <https://www.postgresql.org/docs/current/static/libpq-connect.html#libpq-connstring>`_.\n\nUnder the ``postgres`` map entry in the above YAML is a list, so one can\nprovide more than one backend, and SOPS will log to all of them:\n\n.. code:: yaml\n\n    backends:\n        postgres:\n            - connection_string: \"postgres://sops:sops@localhost/sops?sslmode=verify-full\"\n            - connection_string: \"postgres://sops:sops@remotehost/sops?sslmode=verify-full\"\n\nSaving Output to a File\n~~~~~~~~~~~~~~~~~~~~~~~\nBy default ``sops`` just dumps all the output to the standard output. We can use the\n``--output`` flag followed by a filename to save the output to the file specified.\nBeware using both ``--in-place`` and ``--output`` flags will result in an error.\n\nPassing Secrets to Other Processes\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nIn addition to writing secrets to standard output and to files on disk, ``sops``\nhas two commands for passing decrypted secrets to a new process: ``exec-env``\nand ``exec-file``. These commands will place all output into the environment of\na child process and into a temporary file, respectively. For example, if a\nprogram looks for credentials in its environment, ``exec-env`` can be used to\nensure that the decrypted contents are available only to this process and never\nwritten to disk.\n\n.. code:: bash\n\n   # print secrets to stdout to confirm values\n   $ sops -d out.json\n   {\n           \"database_password\": \"jf48t9wfw094gf4nhdf023r\",\n           \"AWS_ACCESS_KEY_ID\": \"AKIAIOSFODNN7EXAMPLE\",\n           \"AWS_SECRET_KEY\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n   }\n\n   # decrypt out.json and run a command\n   # the command prints the environment variable and runs a script that uses it\n   $ sops exec-env out.json 'echo secret: $database_password; ./database-import'\n   secret: jf48t9wfw094gf4nhdf023r\n\n   # launch a shell with the secrets available in its environment\n   $ sops exec-env out.json 'sh'\n   sh-3.2# echo $database_password\n   jf48t9wfw094gf4nhdf023r\n\n   # the secret is not accessible anywhere else\n   sh-3.2$ exit\n   $ echo your password: $database_password\n   your password:\n\n\nIf the command you want to run only operates on files, you can use ``exec-file``\ninstead. By default ``sops`` will use a FIFO to pass the contents of the\ndecrypted file to the new program. Using a FIFO, secrets are only passed in\nmemory which has two benefits: the plaintext secrets never touch the disk, and\nthe child process can only read the secrets once. In contexts where this won't\nwork, eg platforms like Windows where FIFOs unavailable or secret files that need\nto be available to the child process longer term, the ``--no-fifo`` flag can be\nused to instruct ``sops`` to use a traditional temporary file that will get cleaned\nup once the process is finished executing. ``exec-file`` behaves similar to\n``find(1)`` in that ``{}`` is used as a placeholder in the command which will be\nsubstituted with the temporary file path (whether a FIFO or an actual file).\n\n.. code:: bash\n\n   # operating on the same file as before, but as a file this time\n   $ sops exec-file out.json 'echo your temporary file: {}; cat {}'\n   your temporary file: /tmp/.sops894650499/tmp-file\n   {\n           \"database_password\": \"jf48t9wfw094gf4nhdf023r\",\n           \"AWS_ACCESS_KEY_ID\": \"AKIAIOSFODNN7EXAMPLE\",\n           \"AWS_SECRET_KEY\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n   }\n\n   # launch a shell with a variable TMPFILE pointing to the temporary file\n   $ sops exec-file --no-fifo out.json 'TMPFILE={} sh'\n   sh-3.2$ echo $TMPFILE\n   /tmp/.sops506055069/tmp-file291138648\n   sh-3.2$ cat $TMPFILE\n   {\n           \"database_password\": \"jf48t9wfw094gf4nhdf023r\",\n           \"AWS_ACCESS_KEY_ID\": \"AKIAIOSFODNN7EXAMPLE\",\n           \"AWS_SECRET_KEY\": \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n   }\n   sh-3.2$ ./program --config $TMPFILE\n   sh-3.2$ exit\n\n   # try to open the temporary file from earlier\n   $ cat /tmp/.sops506055069/tmp-file291138648\n   cat: /tmp/.sops506055069/tmp-file291138648: No such file or directory\n\nAdditionally, on unix-like platforms, both ``exec-env`` and ``exec-file``\nsupport dropping privileges before executing the new program via the\n``--user <username>`` flag. This is particularly useful in cases where the\nencrypted file is only readable by root, but the target program does not\nneed root privileges to function. This flag should be used where possible\nfor added security.\n\nTo overwrite the default file name (``tmp-file``) in ``exec-file`` use the\n``--filename <filename>`` parameter.\n\n.. code:: bash\n\n   # the encrypted file can't be read by the current user\n   $ cat out.json\n   cat: out.json: Permission denied\n\n   # execute sops as root, decrypt secrets, then drop privileges\n   $ sudo sops exec-env --user nobody out.json 'sh'\n   sh-3.2$ echo $database_password\n   jf48t9wfw094gf4nhdf023r\n\n   # dropped privileges, still can't load the original file\n   sh-3.2$ id\n   uid=4294967294(nobody) gid=4294967294(nobody) groups=4294967294(nobody)\n   sh-3.2$ cat out.json\n   cat: out.json: Permission denied\n\nUsing the publish command\n~~~~~~~~~~~~~~~~~~~~~~~~~\n``sops publish $file`` publishes a file to a pre-configured destination (this lives in the sops\nconfig file). Additionally, support re-encryption rules that work just like the creation rules.\n\nThis command requires a ``.sops.yaml`` configuration file. Below is an example:\n\n.. code:: yaml\n\n   destination_rules:\n      - s3_bucket: \"sops-secrets\"\n        path_regex: s3/*\n        recreation_rule:\n           pgp: F69E4901EDBAD2D1753F8C67A64535C4163FB307\n      - gcs_bucket: \"sops-secrets\"\n        path_regex: gcs/*\n        recreation_rule:\n           pgp: F69E4901EDBAD2D1753F8C67A64535C4163FB307\n      - vault_path: \"sops/\"\n        vault_kv_mount_name: \"secret/\" # default\n        vault_kv_version: 2 # default\n        path_regex: vault/*\n        omit_extensions: true\n\nThe above configuration will place all files under ``s3/*`` into the S3 bucket ``sops-secrets``,\nall files under ``gcs/*`` into the GCS bucket ``sops-secrets``, and the contents of all files under\n``vault/*`` into Vault's KV store under the path ``secrets/sops/``. For the files that will be\npublished to S3 and GCS, it will decrypt them and re-encrypt them using the\n``F69E4901EDBAD2D1753F8C67A64535C4163FB307`` pgp key.\n\nYou would deploy a file to S3 with a command like: ``sops publish s3/app.yaml``\n\nTo publish all files in selected directory recursively, you need to specify ``--recursive`` flag.\n\nIf you don't want file extension to appear in destination secret path, use ``--omit-extensions``\nflag or ``omit_extensions: true`` in the destination rule in ``.sops.yaml``.\n\nPublishing to Vault\n*******************\n\nThere are a few settings for Vault that you can place in your destination rules. The first\nis ``vault_path``, which is required. The others are optional, and they are\n``vault_address``, ``vault_kv_mount_name``, ``vault_kv_version``.\n\n``sops`` uses the official Vault API provided by Hashicorp, which makes use of `environment\nvariables <https://www.vaultproject.io/docs/commands/#environment-variables>`_ for\nconfiguring the client.\n\n``vault_kv_mount_name`` is used if your Vault KV is mounted somewhere other than ``secret/``.\n``vault_kv_version`` supports ``1`` and ``2``, with ``2`` being the default.\n\nIf destination secret path already exists in Vault and contains same data as the source file, it\nwill be skipped.\n\nBelow is an example of publishing to Vault (using token auth with a local dev instance of Vault).\n\n.. code:: bash\n\n   $ export VAULT_TOKEN=...\n   $ export VAULT_ADDR='http://127.0.0.1:8200'\n   $ sops -d vault/test.yaml\n   example_string: bar\n   example_number: 42\n   example_map:\n       key: value\n   $ sops publish vault/test.yaml\n   uploading /home/user/sops_directory/vault/test.yaml to http://127.0.0.1:8200/v1/secret/data/sops/test.yaml ? (y/n): y\n   $ vault kv get secret/sops/test.yaml\n   ====== Metadata ======\n   Key              Value\n   ---              -----\n   created_time     2019-07-11T03:32:17.074792017Z\n   deletion_time    n/a\n   destroyed        false\n   version          3\n\n   ========= Data =========\n   Key               Value\n   ---               -----\n   example_map       map[key:value]\n   example_number    42\n   example_string    bar\n\n\nImportant information on types\n------------------------------\n\nYAML and JSON type extensions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``sops`` uses the file extension to decide which encryption method to use on the file\ncontent. ``YAML``, ``JSON``, ``ENV``, and ``INI`` files are treated as trees of data, and key/values are\nextracted from the files to only encrypt the leaf values. The tree structure is also\nused to check the integrity of the file.\n\nTherefore, if a file is encrypted using a specific format, it need to be decrypted\nin the same format. The easiest way to achieve this is to conserve the original file\nextension after encrypting a file. For example:\n\n.. code:: bash\n\n\t$ sops -e -i myfile.json\n\t$ sops -d myfile.json\n\nIf you want to change the extension of the file once encrypted, you need to provide\nsops with the ``--input-type`` flag upon decryption. For example:\n\n.. code:: bash\n\n\t$ sops -e myfile.json > myfile.json.enc\n\n\t$ sops -d --input-type json myfile.json.enc\n\nWhen operating on stdin, use the ``--input-type`` and ``--output-type`` flags as follows:\n\n.. code:: bash\n\n    $ cat myfile.json | sops --input-type json --output-type json -d /dev/stdin\n\nYAML anchors\n~~~~~~~~~~~~\n``sops`` only supports a subset of ``YAML``'s many types. Encrypting YAML files that\ncontain strings, numbers and booleans will work fine, but files that contain anchors\nwill not work, because the anchors redefine the structure of the file at load time.\n\nThis file will not work in ``sops``:\n\n.. code:: yaml\n\n\tbill-to:  &id001\n\t    street: |\n\t        123 Tornado Alley\n\t        Suite 16\n\t    city:   East Centerville\n\t    state:  KS\n\n\tship-to:  *id001\n\n``sops`` uses the path to a value as additional data in the AEAD encryption, and thus\ndynamic paths generated by anchors break the authentication step.\n\nJSON and TEXT file types do not support anchors and thus have no such limitation.\n\nYAML Streams\n~~~~~~~~~~~~\n\n``YAML`` supports having more than one \"document\" in a single file, while\nformats like ``JSON`` do not. ``sops`` is able to handle both. This means the\nfollowing multi-document will be encrypted as expected:\n\n.. code:: yaml\n\n\t---\n\tdata: foo\n\t---\n\tdata: bar\n\nNote that the ``sops`` metadata, i.e. the hash, etc, is computed for the physical\nfile rather than each internal \"document\".\n\nTop-level arrays\n~~~~~~~~~~~~~~~~\n``YAML`` and ``JSON`` top-level arrays are not supported, because ``sops``\nneeds a top-level ``sops`` key to store its metadata.\n\nThis file will not work in sops:\n\n.. code:: yaml\n\n\t---\n\t  - some\n\t  - array\n\t  - elements\n\nBut this one will work because the ``sops`` key can be added at the same level as the\n``data`` key.\n\n.. code:: yaml\n\n\tdata:\n\t  - some\n\t  - array\n\t  - elements\n\nSimilarly, with ``JSON`` arrays, this document will not work:\n\n.. code:: json\n\n\t[\n\t  \"some\",\n\t  \"array\",\n\t  \"elements\"\n\t]\n\n\nBut this one will work just fine:\n\n.. code:: json\n\n\t{\n\t  \"data\": [\n\t    \"some\",\n\t    \"array\",\n\t    \"elements\"\n\t  ]\n\t}\n\n\nExamples\n--------\n\nTake a look into the `examples <https://github.com/mozilla/sops/tree/master/examples>`_ folder for detailed use cases of sops in a CI environment. The section below describes specific tips for common use cases.\n\nCreating a new file\n~~~~~~~~~~~~~~~~~~~\n\nThe command below creates a new file with a data key encrypted by KMS and PGP.\n\n.. code:: bash\n\n\t$ sops --kms \"arn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500\" --pgp C9CAB0AF1165060DB58D6D6B2653B624D620786D /path/to/new/file.yaml\n\nEncrypting an existing file\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSimilar to the previous command, we tell sops to use one KMS and one PGP key.\nThe path points to an existing cleartext file, so we give sops flag ``-e`` to\nencrypt the file, and redirect the output to a destination file.\n\n.. code:: bash\n\n\t$ export SOPS_KMS_ARN=\"arn:aws:kms:us-west-2:927034868273:key/fe86dd69-4132-404c-ab86-4269956b4500\"\n\t$ export SOPS_PGP_FP=\"C9CAB0AF1165060DB58D6D6B2653B624D620786D\"\n\t$ sops -e /path/to/existing/file.yaml > /path/to/new/encrypted/file.yaml\n\nDecrypt the file with ``-d``.\n\n.. code:: bash\n\n\t$ sops -d /path/to/new/encrypted/file.yaml\n\nEncrypt or decrypt a file in place\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nRather than redirecting the output of ``-e`` or ``-d``, sops can replace the\noriginal file after encrypting or decrypting it.\n\n.. code:: bash\n\n\t# file.yaml is in cleartext\n\t$ sops -e -i /path/to/existing/file.yaml\n\t# file.yaml is now encrypted\n\t$ sops -d -i /path/to/existing/file.yaml\n\t# file.yaml is back in cleartext\n\nEncrypting binary files\n~~~~~~~~~~~~~~~~~~~~~~~\n\n``sops`` primary use case is encrypting YAML and JSON configuration files, but it\nalso has the ability to manage binary files. When encrypting a binary, sops will\nread the data as bytes, encrypt it, store the encrypted base64 under\n``tree['data']`` and write the result as JSON.\n\nNote that the base64 encoding of encrypted data can actually make the encrypted\nfile larger than the cleartext one.\n\nIn-place encryption/decryption also works on binary files.\n\n.. code::\n\n\t$ dd if=/dev/urandom of=/tmp/somerandom bs=1024\n\tcount=512\n\t512+0 records in\n\t512+0 records out\n\t524288 bytes (524 kB) copied, 0.0466158 s, 11.2 MB/s\n\n\t$ sha512sum /tmp/somerandom\n\t9589bb20280e9d381f7a192000498c994e921b3cdb11d2ef5a986578dc2239a340b25ef30691bac72bdb14028270828dad7e8bd31e274af9828c40d216e60cbe /tmp/somerandom\n\n\t$ sops -e -i /tmp/somerandom\n\tplease wait while a data encryption key is being generated and stored securely\n\n\t$ sops -d -i /tmp/somerandom\n\n\t$ sha512sum /tmp/somerandom\n\t9589bb20280e9d381f7a192000498c994e921b3cdb11d2ef5a986578dc2239a340b25ef30691bac72bdb14028270828dad7e8bd31e274af9828c40d216e60cbe /tmp/somerandom\n\nExtract a sub-part of a document tree\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``sops`` can extract a specific part of a YAML or JSON document, by provided the\npath in the ``--extract`` command line flag. This is useful to extract specific\nvalues, like keys, without needing an extra parser.\n\n.. code:: bash\n\n\t$ sops -d --extract '[\"app2\"][\"key\"]' ~/git/svc/sops/example.yaml\n\t-----BEGIN RSA PRIVATE KEY-----\n\tMIIBPAIBAAJBAPTMNIyHuZtpLYc7VsHQtwOkWYobkUblmHWRmbXzlAX6K8tMf3Wf\n\tImcbNkqAKnELzFAPSBeEMhrBN0PyOC9lYlMCAwEAAQJBALXD4sjuBn1E7Y9aGiMz\n\tbJEBuZJ4wbhYxomVoQKfaCu+kH80uLFZKoSz85/ySauWE8LgZcMLIBoiXNhDKfQL\n\tvHECIQD6tCG9NMFWor69kgbX8vK5Y+QL+kRq+9HK6yZ9a+hsLQIhAPn4Ie6HGTjw\n\tfHSTXWZpGSan7NwTkIu4U5q2SlLjcZh/AiEA78NYRRBwGwAYNUqzutGBqyXKUl4u\n\tErb0xAEyVV7e8J0CIQC8VBY8f8yg+Y7Kxbw4zDYGyb3KkXL10YorpeuZR4LuQQIg\n\tbKGPkMM4w5blyE1tqGN0T7sJwEx+EUOgacRNqM2ljVA=\n\t-----END RSA PRIVATE KEY-----\n\nThe tree path syntax uses regular python dictionary syntax, without the\nvariable name. Extract keys by naming them, and array elements by numbering\nthem.\n\n.. code:: bash\n\n\t$ sops -d --extract '[\"an_array\"][1]' ~/git/svc/sops/example.yaml\n\tsecretuser2\n\nSet a sub-part in a document tree\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``sops`` can set a specific part of a YAML or JSON document, by providing\nthe path and value in the ``--set`` command line flag. This is useful to\nset specific values, like keys, without needing an editor.\n\n.. code:: bash\n\n\t$ sops --set '[\"app2\"][\"key\"] \"app2keystringvalue\"'  ~/git/svc/sops/example.yaml\n\nThe tree path syntax uses regular python dictionary syntax, without the\nvariable name. Set to keys by naming them, and array elements by\nnumbering them.\n\n.. code:: bash\n\n\t$ sops --set '[\"an_array\"][1] \"secretuser2\"' ~/git/svc/sops/example.yaml\n\nThe value must be formatted as json.\n\n.. code:: bash\n\n\t$ sops --set '[\"an_array\"][1] {\"uid1\":null,\"uid2\":1000,\"uid3\":[\"bob\"]}' ~/git/svc/sops/example.yaml\n\nShowing diffs in cleartext in git\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou most likely want to store encrypted files in a version controlled repository.\nSops can be used with git to decrypt files when showing diffs between versions.\nThis is very handy for reviewing changes or visualizing history.\n\nTo configure sops to decrypt files during diff, create a ``.gitattributes`` file\nat the root of your repository that contains a filter and a command.\n\n.. code::\n\n\t*.yaml diff=sopsdiffer\n\nHere we only care about YAML files. ``sopsdiffer`` is an arbitrary name that we map\nto a sops command in the git configuration file of the repository.\n\n.. code:: bash\n\n\t$ git config diff.sopsdiffer.textconv \"sops -d\"\n\n\t$ grep -A 1 sopsdiffer .git/config\n\t[diff \"sopsdiffer\"]\n\t\ttextconv = \"sops -d\"\n\nWith this in place, calls to ``git diff`` will decrypt both previous and current\nversions of the target file prior to displaying the diff. And it even works with\ngit client interfaces, because they call git diff under the hood!\n\nEncrypting only parts of a file\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNote: this only works on YAML and JSON files, not on BINARY files.\n\nBy default, ``sops`` encrypts all the values of a YAML or JSON file and leaves the\nkeys in cleartext. In some instances, you may want to exclude some values from\nbeing encrypted. This can be accomplished by adding the suffix **_unencrypted**\nto any key of a file. When set, all values underneath the key that set the\n**_unencrypted** suffix will be left in cleartext.\n\nNote that, while in cleartext, unencrypted content is still added to the\nchecksum of the file, and thus cannot be modified outside of sops without\nbreaking the file integrity check.\n\nThe unencrypted suffix can be set to a different value using the\n``--unencrypted-suffix`` option.\n\nConversely, you can opt in to only encrypt some values in a YAML or JSON file,\nby adding a chosen suffix to those keys and passing it to the ``--encrypted-suffix`` option.\n\nA third method is to use the ``--encrypted-regex`` which will only encrypt values under\nkeys that match the supplied regular expression.  For example, this command:\n\n.. code:: bash\n\n\t$ sops --encrypt --encrypted-regex '^(data|stringData)$' k8s-secrets.yaml\n\nwill encrypt the values under the ``data`` and ``stringData`` keys in a YAML file\ncontaining kubernetes secrets.  It will not encrypt other values that help you to\nnavigate the file, like ``metadata`` which contains the secrets' names.\n\nConversely, you can opt in to only left certain keys without encrypting by using the \n``--unencrypted-regex`` option, which will leave the values unencrypted of those keys \nthat match the supplied regular expression. For example, this command:\n\n.. code:: bash\n\n  $ sops --encrypt --unencrypted-regex '^(description|metadata)$' k8s-secrets.yaml\n\nwill not encrypt the values under the ``description`` and ``metadata`` keys in a YAML file\ncontaining kubernetes secrets, while encrypting everything else.\n\nYou can also specify these options in the ``.sops.yaml`` config file.\n\nNote: these four options ``--unencrypted-suffix``, ``--encrypted-suffix``, ``--encrypted-regex`` and ``--unencrypted-regex`` are\nmutually exclusive and cannot all be used in the same file.\n\nEncryption Protocol\n-------------------\n\nWhen sops creates a file, it generates a random 256 bit data key and asks each\nKMS and PGP master key to encrypt the data key. The encrypted version of the data\nkey is stored in the ``sops`` metadata under ``sops.kms`` and ``sops.pgp``.\n\nFor KMS:\n\n.. code:: yaml\n\n    sops:\n        kms:\n        -   enc: CiC6yCOtzsnFhkfdIslYZ0bAf//gYLYCmIu87B3sy/5yYxKnAQEBAQB4usgjrc7JxYZH3SLJWGdGwH//4GC2ApiLvOwd7Mv+cmMAAAB+MHwGCSqGSIb3DQEHBqBvMG0CAQAwaAYJKoZIhvcNAQcBMB4GCWCGSAFlAwQBLjARBAyGdRODuYMHbA8Ozj8CARCAO7opMolPJUmBXd39Zlp0L2H9fzMKidHm1vvaF6nNFq0ClRY7FlIZmTm4JfnOebPseffiXFn9tG8cq7oi\n            enc_ts: 1439568549.245995\n            arn: arn:aws:kms:us-east-1:656532927350:key/920aff2e-c5f1-4040-943a-047fa387b27e\n\nFor PGP:\n\n.. code:: yaml\n\n    sops:\n        pgp:\n        -   fp: 85D77543B3D624B63CEA9E6DBC17301B491B3F21\n            created_at: 1441570391.930042\n            enc: |\n                -----BEGIN PGP MESSAGE-----\n                Version: GnuPG v1\n\n                hQIMA0t4uZHfl9qgAQ//UvGAwGePyHuf2/zayWcloGaDs0MzI+zw6CmXvMRNPUsA\n                pAgRKczJmDu4+XzN+cxX5Iq9xEWIbny9B5rOjwTXT3qcUYZ4Gkzbq4MWkjuPp/Iv\n                qO4MJaYzoH5YxC4YORQ2LvzhA2YGsCzYnljmatGEUNg01yJ6r5mwFwDxl4Nc80Cn\n                RwnHuGExK8j1jYJZu/juK1qRbuBOAuruIPPWVdFB845PA7waacG1IdUW3ZtBkOy3\n                O0BIfG2ekRg0Nik6sTOhDUA+l2bewCcECI8FYCEjwHm9Sg5cxmP2V5m1mby+uKAm\n                kewaoOyjbmV1Mh3iI1b/AQMr+/6ZE9MT2KnsoWosYamFyjxV5r1ZZM7cWKnOT+tu\n                KOvGhTV1TeOfVpajNTNwtV/Oyh3mMLQ0F0HgCTqomQVqw5+sj7OWAASuD3CU/dyo\n                pcmY5Qe0TNL1JsMNEH8LJDqSh+E0hsUxdY1ouVsg3ysf6mdM8ciWb3WRGxih1Vmf\n                unfLy8Ly3V7ZIC8EHV8aLJqh32jIZV4i2zXIoO4ZBKrudKcECY1C2+zb/TziVAL8\n                qyPe47q8gi1rIyEv5uirLZjgpP+JkDUgoMnzlX334FZ9pWtQMYW4Y67urAI4xUq6\n                /q1zBAeHoeeeQK+YKDB7Ak/Y22YsiqQbNp2n4CKSKAE4erZLWVtDvSp+49SWmS/S\n                XgGi+13MaXIp0ecPKyNTBjF+NOw/I3muyKr8EbDHrd2XgIT06QXqjYLsCb1TZ0zm\n                xgXsOTY3b+ONQ2zjhcovanDp7/k77B+gFitLYKg4BLZsl7gJB12T8MQnpfSmRT4=\n                =oJgS\n                -----END PGP MESSAGE-----\n\n``sops`` then opens a text editor on the newly created file. The user adds data to the\nfile and saves it when done.\n\nUpon save, sops browses the entire file as a key/value tree. Every time sops\nencounters a leaf value (a value that does not have children), it encrypts the\nvalue with AES256_GCM using the data key and a 256 bit random initialization\nvector.\n\nEach file uses a single data key to encrypt all values of a document, but each\nvalue receives a unique initialization vector and has unique authentication data.\n\nAdditional data is used to guarantee the integrity of the encrypted data\nand of the tree structure: when encrypting the tree, key names are concatenated\ninto a byte string that is used as AEAD additional data (aad) when encrypting\nvalues. We expect that keys do not carry sensitive information, and\nkeeping them in cleartext allows for better diff and overall readability.\n\nAny valid KMS or PGP master key can later decrypt the data key and access the\ndata.\n\nMultiple master keys allow for sharing encrypted files without sharing master\nkeys, and provide a disaster recovery solution. The recommended way to use sops\nis to have two KMS master keys in different regions and one PGP public key with\nthe private key stored offline. If, by any chance, both KMS master keys are\nlost, you can always recover the encrypted data using the PGP private key.\n\nMessage Authentication Code\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn addition to authenticating branches of the tree using keys as additional\ndata, sops computes a MAC on all the values to ensure that no value has been\nadded or removed fraudulently. The MAC is stored encrypted with AES_GCM and\nthe data key under tree->`sops`->`mac`.\n\nMotivation\n----------\n\nAutomating the distribution of secrets and credentials to components of an\ninfrastructure is a hard problem. We know how to encrypt secrets and share them\nbetween humans, but extending that trust to systems is difficult. Particularly\nwhen these systems follow devops principles and are created and destroyed\nwithout human intervention. The issue boils down to establishing the initial\ntrust of a system that just joined the infrastructure, and providing it access\nto the secrets it needs to configure itself.\n\nThe initial trust\n~~~~~~~~~~~~~~~~~\n\nIn many infrastructures, even highly dynamic ones, the initial trust is\nestablished by a human. An example is seen in Puppet by the way certificates are\nissued: when a new system attempts to join a Puppetmaster, an administrator\nmust, by default, manually approve the issuance of the certificate the system\nneeds. This is cumbersome, and many puppetmasters are configured to auto-sign\nnew certificates to work around that issue. This is obviously not recommended\nand far from ideal.\n\nAWS provides a more flexible approach to trusting new systems. It uses a\npowerful mechanism of roles and identities. In AWS, it is possible to verify\nthat a new system has been granted a specific role at creation, and it is\npossible to map that role to specific resources. Instead of trusting new systems\ndirectly, the administrator trusts the AWS permission model and its automation\ninfrastructure. As long as AWS keys are safe, and the AWS API is secure, we can\nassume that trust is maintained and systems are who they say they are.\n\nKMS, Trust and secrets distribution\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nUsing the AWS trust model, we can create fine grained access controls to\nAmazon's Key Management Service (KMS). KMS is a service that encrypts and\ndecrypts data with AES_GCM, using keys that are never visible to users of the\nservice. Each KMS master key has a set of role-based access controls, and\nindividual roles are permitted to encrypt or decrypt using the master key. KMS\nhelps solve the problem of distributing keys, by shifting it into an access\ncontrol problem that can be solved using AWS's trust model.\n\nOperational requirements\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen Mozilla's Services Operations team started revisiting the issue of\ndistributing secrets to EC2 instances, we set a goal to store these secrets\nencrypted until the very last moment, when they need to be decrypted on target\nsystems. Not unlike many other organizations that operate sufficiently complex\nautomation, we found this to be a hard problem with a number of prerequisites:\n\n1. Secrets must be stored in YAML files for easy integration into hiera\n\n2. Secrets must be stored in GIT, and when a new CloudFormation stack is\n   built, the current HEAD is pinned to the stack. (This allows secrets to\n   be changed in GIT without impacting the current stack that may\n   autoscale).\n\n3. Entries must be encrypted separately. Encrypting entire files as blobs makes\n   git conflict resolution almost impossible. Encrypting each entry\n   separately is much easier to manage.\n\n4. Secrets must always be encrypted on disk (admin laptop, upstream\n   git repo, jenkins and S3) and only be decrypted on the target\n   systems\n\nSOPS can be used to encrypt YAML, JSON and BINARY files. In BINARY mode, the\ncontent of the file is treated as a blob, the same way PGP would encrypt an\nentire file. In YAML and JSON modes, however, the content of the file is\nmanipulated as a tree where keys are stored in cleartext, and values are\nencrypted. hiera-eyaml does something similar, and over the years we learned\nto appreciate its benefits, namely:\n\n* diffs are meaningful. If a single value of a file is modified, only that\n  value will show up in the diff. The diff is still limited to only showing\n  encrypted data, but that information is already more granular that\n  indicating that an entire file has changed.\n\n* conflicts are easier to resolve. If multiple users are working on the\n  same encrypted files, as long as they don't modify the same values,\n  changes are easy to merge. This is an improvement over the PGP\n  encryption approach where unsolvable conflicts often happen when\n  multiple users work on the same file.\n\nOpenPGP integration\n~~~~~~~~~~~~~~~~~~~\n\nOpenPGP gets a lot of bad press for being an outdated crypto protocol, and while\ntrue, what really made us look for alternatives is the difficulty of managing and\ndistributing keys to systems. With KMS, we manage permissions to an API, not keys,\nand that's a lot easier to do.\n\nBut PGP is not dead yet, and we still rely on it heavily as a backup solution:\nall our files are encrypted with KMS and with one PGP public key, with its\nprivate key stored securely for emergency decryption in the event that we lose\nall our KMS master keys.\n\nSOPS can be used without KMS entirely, the same way you would use an encrypted\nPGP file: by referencing the pubkeys of each individual who has access to the file.\nIt can easily be done by providing sops with a comma-separated list of public keys\nwhen creating a new file:\n\n.. code:: bash\n\n\t$ sops --pgp \"E60892BB9BD89A69F759A1A0A3D652173B763E8F,84050F1D61AF7C230A12217687DF65059EF093D3,85D77543B3D624B63CEA9E6DBC17301B491B3F21\" mynewfile.yaml\n\nThreat Model\n------------\n\nThe security of the data stored using sops is as strong as the weakest\ncryptographic mechanism. Values are encrypted using AES256_GCM which is the\nstrongest symmetric encryption algorithm known today. Data keys are encrypted\nin either KMS, which also uses AES256_GCM, or PGP which uses either RSA or\nECDSA keys.\n\nGoing from the most likely to the least likely, the threats are as follows:\n\nCompromised AWS credentials grant access to KMS master key\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAn attacker with access to an AWS console can grant itself access to one of\nthe KMS master keys used to encrypt a sops data key. This threat should be\nmitigated by protecting AWS accesses with strong controls, such as multi-factor\nauthentication, and also by performing regular audits of permissions granted\nto AWS users.\n\nCompromised PGP key\n~~~~~~~~~~~~~~~~~~~\n\nPGP keys are routinely mishandled, either because owners copy them from\nmachine to machine, or because the key is left forgotten on an unused machine\nan attacker gains access to. When using PGP encryption, sops users should take\nspecial care of PGP private keys, and store them on smart cards or offline\nas often as possible.\n\nFactorized RSA key\n~~~~~~~~~~~~~~~~~~\n\nsops doesn't apply any restriction on the size or type of PGP keys. A weak PGP\nkeys, for example 512 bits RSA, could be factorized by an attacker to gain\naccess to the private key and decrypt the data key. Users of sops should rely\non strong keys, such as 2048+ bits RSA keys, or 256+ bits ECDSA keys.\n\nWeak AES cryptography\n~~~~~~~~~~~~~~~~~~~~~\n\nA vulnerability in AES256_GCM could potentially leak the data key or the KMS\nmaster key used by a sops encrypted file. While no such vulnerability exists\ntoday, we recommend that users keep their encrypted files reasonably private.\n\nBackward compatibility\n----------------------\n\n``sops`` will remain backward compatible on the major version, meaning that all\nimprovements brought to the 1.X and 2.X branches (current) will maintain the\nfile format introduced in **1.0**.\n\nSecurity\n--------\n\nPlease report security issues to security at mozilla dot org, or by using one\nof the contact method available here: `https://www.mozilla.org/en-US/security/#For_Developers <https://www.mozilla.org/en-US/security/#For_Developers>`_\n\nLicense\n-------\nMozilla Public License Version 2.0\n\nAuthors\n-------\n\nThe core team is composed of:\n\n* AJ Banhken @ajvb\n\nThe original authors were:\n\n* Adrian Utrilla @autrilla\n* Julien Vehent @jvehent\n\nAnd a whole bunch of `contributors <https://github.com/mozilla/sops/graphs/contributors>`_\n\nCredits\n-------\n\n`sops` was inspired by `hiera-eyaml <https://github.com/TomPoulton/hiera-eyaml>`_,\n`credstash <https://github.com/LuminalOSS/credstash>`_ ,\n`sneaker <https://github.com/codahale/sneaker>`_,\n`password store <http://www.passwordstore.org/>`_ and too many years managing\nPGP encrypted files by hand...\n"
},
{
  "name": "messaging-system-personalization-experiment-1-addon",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "build.js",
      "docs",
      "package.json",
      "schemas",
      "seed-remote-settings-dev-server.sh",
      "src",
      "web-ext-config.js",
      "yarn.lock"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md"
    ]
  },
  "makefile": null,
  "readme": "# Messaging System Personalization Experiment 1 Add-on\n\n## What the add-on does\n\nOn a 60 minute schedule, the add-on fetches the latest ml model from remote settings, evaluates features and computes scores for each CFR message in the experiment.\n\nThe scores are persisted via preferences, which affects how often and which messages from the messaging system are displayed.\n\nThe evaluated features are sent via telemetry to a model training job so that the model gets improved over time.\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n## References\n\n- [Experimenter](https://experimenter.services.mozilla.com/experiments/messaging-system-personalization-experiment-1-accounts/)\n- [Bugzilla](https://bugzilla.mozilla.org/show_bug.cgi?id=1594422)\n"
},
{
  "name": "protocol",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".nvmrc",
      ".stylelintignore",
      ".stylelintrc.json",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assets",
      "bin",
      "components",
      "docs",
      "fractal.config.js",
      "main.js",
      "netlify.toml",
      "package-lock.json",
      "package.json",
      "static",
      "tests",
      "theme",
      "webpack.docs.build.config.js",
      "webpack.docs.static.config.js",
      "webpack.package.build.config.js",
      "webpack.package.static.config.js"
    ],
    "/docs": [
      "01-fundamentals",
      "02-usage",
      "03-contributing",
      "_partials",
      "index.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE.md",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Protocol\n\nProtocol is a design system for Mozilla and Firefox websites. It establishes a\ncommon design language, provides reusable coded components, and outlines high\nlevel guidelines for content and accessibility.\n\nhttps://protocol.mozilla.org/\n\nProtocol is still an evolving project. Currently, it\u2019s used primarily by the\nMozilla Marketing Websites team as the front-end for [www.mozilla.org](https://www.mozilla.org).\nThe long term goal is to provide a robust, unified design system that anyone at\nMozilla can use to build an on-brand website.\n\nIf you\u2019re interested in using Protocol on your project, let us know and we can\nhelp you. You can find us in #protocol-design-system on Mozilla\u2019s Slack (for\nMozillians) or in #protocol-design-system on [Matrix](https://chat.mozilla.org/)\n(open to the public). Also feel free to\n[file an issue on GitHub](https://github.com/mozilla/protocol/issues).\n\n![Current npm package version.](https://img.shields.io/npm/v/@mozilla-protocol/core)\n![Total downloads on npm.](https://img.shields.io/npm/dt/@mozilla-protocol/core)\n![Pull requests welcome!](https://img.shields.io/badge/PRs-welcome-brightgreen)\n\n## Getting Started\n\nProtocol is built on the [Node.js](https://nodejs.org/) platform and published\nto [NPM](https://www.npmjs.com/), so be sure to have both installed before\nproceeding.\n\n## Installation\n\nTo use Protocol in your website you can install the core package directly from\nNPM:\n\n```\nnpm install @mozilla-protocol/core --save\n```\n\nAlternatively, you can also [download the latest release](https://github.com/mozilla/protocol/releases/latest)\nfrom GitHub.\n\n## Usage\n\nOnce installed, the relevant CSS, JavaScript, and asset files will be available\nin your project under `./node_modules/@mozilla-protocol/core/`.\n\nThe core CSS file is bundled as `protocol.css`, which contains styling for things\nsuch as basic elements and typography, as well as some global components like\nnavigation and a footer. Other component and layout CSS is bundled as\n`protocol-components.css` for convenience.\n\nHowever, these pre-compiled CSS files include the _entire_ pattern library, which\nyou may not need. We recommend compiling your own styles from the source Sass\nfiles, also included in the published package. That allows you to configure Protocol\nto include just the styles and components you need for each page of your website.\n\n## Make it Run\n\nTo build Protocol from source and run the documentation site locally, you can\nclone the repo from GitHub:\n\n```\n$ git clone https://github.com/mozilla/protocol.git\n$ cd protocol\n$ npm install\n```\n\nRunning `npm install` will install dependencies. Then:\n\n```\n$ npm run webpack-docs\n```\n\nThis will compile the Sass and copy assets into a local folder in preparation to\nrun the server. It also starts a \u201cwatch\u201d process that will watch those files and\nautomatically recompile when they change.\n\nIn another command line console (and still within the Protocol folder), run:\n\n```\n$ npm start\n```\n\nThis will build the site locally and start the development server at\n<http://localhost:3000>.\n\n## Building the website\n\nTo build the Protocol documentation site for deployment, run:\n\n```\n$ npm run build-docs\n```\n\n## Building the NPM package\n\nWe use a [Webpack](https://webpack.js.org/) configuration for building the contents\nof the NPM package ready for publishing. To build the package, run:\n\n```\nnpm run build-package\n```\n\nThis will install dependencies, lint CSS/JS files, and then build the package\ncontent in the `./package/` directory.\n\n## Running tests\n\nTo perform the package build process above and then run front-end JS tests against\nthe processed files:\n\n```\nnpm test\n```\n\n## Publishing to NPM\n\nProtocol is published to NPM under the `@mozilla-protocol/core` namespace/package\nname. To publish a release to NPM, use the following steps:\n\n1. Before you start make sure the project's [CHANGELOG.md](https://github.com/mozilla/protocol/blob/main/CHANGELOG.md)\n    is up to date.\n2. Update the package `version` number in [assets/package/package.json](https://github.com/mozilla/protocol/blob/main/assets/package/package.json)\n    (use [Semantic Versioning](https://semver.org/) to determine what the new version number\n    should be).\n3. Update the package README [assets/package/README.md](https://github.com/mozilla/protocol/blob/main/assets/package/README.md).\n4. Run `npm install` to update the package-lock.json file.\n5. Submit a pull request with your changes (or commit directly to `main` if you\n    have permission). Once the changes have been merged to main:\n6. Tag a new release. You can do this either using [Git tag](https://git-scm.com/book/en/v2/Git-Basics-Tagging),\n    or directly on the [GitHub website](https://github.com/mozilla/protocol/releases/latest).\n7. Run `npm test` to run the build script and front-end tests. The package contents\n    will be located in `./package/`.\n8. If the build is successful and all tests pass, publish to NPM using `npm publish ./package/`.\n\n## Deployment\n\nNote: the following instructions assume the Mozilla repository is the remote\ncalled `origin`.\n\n### Pushing to production\n\nEach time an updated package is published to NPM, https://protocol.mozilla.org/\nshould also be updated so the documentation site matches the NPM package features.\n\n1. Verify all is good on the [staging site](https://protocol-stage.moz.works/).\n2. Make sure your local `main` branch is up to date.\n3. Push the `main` branch to the `prod` branch: `git push origin main:prod`.\n\nA notice will be posted in #www-notify on Slack when the push has completed.\n\n### Pushing to demo\n\nFor previewing new components before they are merged to `main`, two demo instances\nare available.\n\n1. Push your branch to the `demo1` or `demo2` branches e.g.\n    `git push -f origin my-branch-name:demo1`\n2. Your branch will be published:\n  - https://demo1--mozilla-protocol.netlify.com/\n  - https://demo2--mozilla-protocol.netlify.com/\n\nA notice will be posted in #www-notify on Slack when the push has completed.\n"
},
{
  "name": "are-we-triaged-yet",
  "files": {
    "/": [
      ".gitignore",
      ".glitch-assets",
      "README.md",
      "modules-local",
      "package-lock.json",
      "package.json",
      "public",
      "server.js",
      "shrinkwrap.yaml",
      "views"
    ]
  },
  "makefile": null,
  "readme": "# Are We Triaged Yet\n\nTriage stats for Nightly and Beta. Deployed on https://are-we-triaged-yet.herokuapp.com/\n\nUse `npm install` and `npm run start` to start and make a note of the URL returned in the console.\n\n## Query string arguments\n\n* **version**: numeric version of Firefox nightly or beta\n* **report**: `untriaged`, `needinfo`, `affecting`, `fix_or_defer`\n* **all**: by default, the top 10 components are listed in each section, adding `all` to the query string will list all components\n\nMultiple reports can be specified as comma separated values, ie `?report=untriaged,needinfo&all`.\n"
},
{
  "name": "mozci",
  "files": {
    "/": [
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "docs",
      "mozci",
      "poetry.lock",
      "pyproject.toml",
      "requirements.readthedocs.txt",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "api",
      "conf.py",
      "configuration.rst",
      "index.rst",
      "make.bat",
      "regressions.rst",
      "usage.rst"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "[![Task Status](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/coverage-crawler/master/badge.svg)](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/coverage-crawler/master/latest)\n[![PyPI version](https://badge.fury.io/py/mozci.svg)](https://badge.fury.io/py/mozci)\n[![Docs](https://readthedocs.org/projects/mozci/badge/?version=latest)](https://mozci.readthedocs.io/en/latest/?badge=latest)\n\n# mozci\n\nA library for inspecting push and task results in Mozilla's CI.\n\n## Installation\n\nTo install, run:\n\n```bash\n$ pip install mozci\n```\n\n## Usage\n\nBasic usage is to instantiate a `Push` object then start accessing properties and call methods.\nFor example:\n\n```python3\nfrom mozci.push import Push\n\npush = Push(\"79041cab0cc2\", branch=\"autoland\")\nprint(\"\\n\".join([t.label for t in push.tasks if t.failed]))\n```\n\nThis will print all the failing tasks from a given push. See the\n[documentation](https://mozci.readthedocs.io/en/latest/) for more usage details and API docs.\n\n## Contributing\n\nMozci uses [poetry](https://python-poetry.org/) to manage the project. So first make sure that is\ninstalled. Then clone the repo and run:\n\n```bash\n$ poetry install\n```\n\nThis will create a virtualenv and install both project and dev dependencies in it. See the [poetry\ndocumentation](https://python-poetry.org/docs/) to learn how to work within the project.\n\nTo execute tests and linters, run:\n\n```bash\n$ tox\n```\n\nThis should run successfully prior to submitting PRs (unless you need help figuring out the\nproblem).\n\nThere are also some integration tests that will hit live data sources. These are run in a cron task\nand are excluded from the default test run. But if needed, you can run them locally via:\n\n```bash\n$ tox -e integration\n```\n\nSince `tox` installs packages on every invocation, it's much faster to run tests directly with `pytest`:\n\n```bash\n$ poetry run pytest tests\n```\n\nor\n\n```bash\n$ poetry shell\n$ pytest tests\n```\n\nAdditionally, you can install the `pre-commit` hooks by running:\n\n```bash\n$ pre-commit install\n```\n\nLinters and formatters will now run every time you commit.\n"
},
{
  "name": "code-coverage",
  "files": {
    "/": [
      ".dockerignore",
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "addon",
      "backend",
      "bot",
      "events",
      "frontend",
      "report",
      "tools"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Code Coverage\n\nThis project has 4 parts:\n\n* `bot` is a Python script running as a Taskcluster hook, aggregating code coverage data from Mozilla repositories,\n* `backend` is a Python API built with Flask, that serves the aggregated code coverage data, in an efficient way,\n* `frontend` is a vanilla Javascript SPA displaying code coverage data in your browser,\n* `addon` is a Web Extension for Firefox, extending several Mozilla websites with code coverage data. Published at https://addons.mozilla.org/firefox/addon/gecko-code-coverage/.\n\n## Help\n\nYou can reach us on our Matrix instance: [#codecoverage:mozilla.org](https://chat.mozilla.org/#/room/#codecoverage:mozilla.org)\n"
},
{
  "name": "grcov",
  "files": {
    "/": [
      ".cargo",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".markdownlint.yaml",
      ".pre-commit-config.yaml",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE-MPL-2.0",
      "README.md",
      "benches",
      "src",
      "test",
      "tests"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# grcov\n\n[![Build Status](https://github.com/mozilla/grcov/actions/workflows/CICD.yml/badge.svg?branch=master)](https://github.com/mozilla/grcov/actions/workflows/CICD.yml)\n[![codecov](https://codecov.io/gh/mozilla/grcov/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/grcov)\n[![crates.io](https://img.shields.io/crates/v/grcov.svg)](https://crates.io/crates/grcov)\n\ngrcov collects and aggregates code coverage information for multiple source files.\ngrcov processes .profraw and .gcda files which can be generated from llvm/clang or gcc.\ngrcov also processes lcov files (for JS coverage) and JaCoCo files (for Java coverage).\nLinux, macOS and Windows are supported.\n\nThis is a project initiated by Mozilla to gather code coverage results on Firefox.\n\n<!-- omit in toc -->\n## Table of Contents\n\n- [man grcov](#man-grcov)\n- [How to get grcov](#how-to-get-grcov)\n- [Usage](#usage)\n  - [Example: How to generate source-based coverage for a Rust project](#example-how-to-generate-source-based-coverage-for-a-rust-project)\n  - [Example: How to generate .gcda files for C/C++](#example-how-to-generate-gcda-files-for-cc)\n  - [Example: How to generate .gcda files for a Rust project](#example-how-to-generate-gcda-files-for-a-rust-project)\n  - [Generate a coverage report from coverage artifacts](#generate-a-coverage-report-from-coverage-artifacts)\n    - [LCOV output](#lcov-output)\n    - [Coveralls output](#coveralls-output)\n    - [grcov with Travis](#grcov-with-travis)\n  - [Alternative reports](#alternative-reports)\n  - [Hosting HTML reports and using coverage badges](#hosting-html-reports-and-using-coverage-badges)\n    - [Example](#example)\n  - [Enabling symlinks on Windows](#enabling-symlinks-on-windows)\n- [Auto-formatting](#auto-formatting)\n- [Build & Test](#build--test)\n- [Minimum requirements](#minimum-requirements)\n- [License](#license)\n\n## man grcov\n\n```text\nUSAGE:\n    grcov [FLAGS] [OPTIONS] <paths>...\n\nFLAGS:\n        --branch\n            Enables parsing branch coverage information\n\n        --guess-directory-when-missing\n\n\n    -h, --help\n            Prints help information\n\n        --ignore-not-existing\n            Ignore source files that can't be found on the disk\n\n        --llvm\n            Speeds-up parsing, when the code coverage information is exclusively coming from a llvm build\n\n        --parallel\n            Sets the build type to be parallel for 'coveralls' and 'coveralls+' formats\n\n    -V, --version\n            Prints version information\n\n\nOPTIONS:\n    -b, --binary-path <PATH>\n            Sets the path to the directory containing the compiled binaries to be used\n\n        --commit-sha <COMMIT HASH>\n            Sets the hash of the commit used to generate the code coverage data\n\n        --excl-br-line <regex>\n            Lines in covered files containing this marker will be excluded from branch coverage.\n\n        --excl-br-start <regex>\n            Marks the beginning of a section excluded from branch coverage. The current line is part of this section.\n\n        --excl-br-stop <regex>\n            Marks the end of a section excluded from branch coverage. The current line is part of this section.\n\n        --excl-line <regex>\n            Lines in covered files containing this marker will be excluded.\n\n        --excl-start <regex>\n            Marks the beginning of an excluded section. The current line is part of this section.\n\n        --excl-stop <regex>\n            Marks the end of an excluded section. The current line is part of this section.\n\n        --filter <filter>\n            Filters out covered/uncovered files. Use 'covered' to only return covered files, 'uncovered' to only return\n            uncovered files [possible values: covered, uncovered]\n        --ignore <PATH>...\n            Ignore files/directories specified as globs\n\n        --keep-only <PATH>...\n            Keep only files/directories specified as globs\n\n        --log <LOG>\n            Set the file where to log (or stderr or stdout). Defaults to 'stderr' [default: stderr]\n\n        --log-level <LEVEL>\n            Set the log level. [default: ERROR]  [possible values: OFF, ERROR, WARN, INFO, DEBUG, TRACE]\n\n    -o, --output-path <PATH>\n            Specifies the output path\n\n    -t, --output-type <OUTPUT TYPE>\n            Sets a custom output type:\n            - *html* for a HTML coverage report;\n            - *coveralls* for the Coveralls specific format;\n            - *lcov* for the lcov INFO format;\n            - *covdir* for the covdir recursive JSON format;\n            - *coveralls+* for the Coveralls specific format with function information;\n            - *ade* for the ActiveData-ETL specific format;\n            - *cobertura* for a cobertura coverage report;\n            - *files* to only return a list of files.\n             [default: lcov]  [possible values: ade, lcov, coveralls, coveralls+, files, covdir, html, cobertura]\n        --path-mapping <PATH>...\n\n\n    -p, --prefix-dir <PATH>\n            Specifies a prefix to remove from the paths (e.g. if grcov is run on a different machine than the one that\n            generated the code coverage information)\n        --service-job-id <SERVICE JOB ID>\n            Sets the service job id [aliases: service-job-number]\n\n        --service-name <SERVICE NAME>\n            Sets the service name\n\n        --service-number <SERVICE NUMBER>\n            Sets the service number\n\n        --service-pull-request <SERVICE PULL REQUEST>\n            Sets the service pull request number\n\n    -s, --source-dir <DIRECTORY>\n            Specifies the root directory of the source files\n\n        --threads <NUMBER>\n             [default: 11]\n\n        --token <TOKEN>\n            Sets the repository token from Coveralls, required for the 'coveralls' and 'coveralls+' formats\n\n        --vcs-branch <VCS BRANCH>\n            Set the branch for coveralls report. Defaults to 'master' [default: master]\n\n\nARGS:\n    <paths>...\n            Sets the input paths to use\n```\n\n## How to get grcov\n\nGrcov can be downloaded from [releases](https://github.com/mozilla/grcov/releases) or, if you have Rust installed,\nyou can run `cargo install grcov`.\n\n## Usage\n\n### Example: How to generate source-based coverage for a Rust project\n\n1. Install the llvm-tools or llvm-tools-preview component:\n\n   ```sh\n   rustup component add llvm-tools-preview\n   ```\n\n2. Ensure that the following environment variable is set up:\n\n   ```sh\n   export RUSTFLAGS=\"-Cinstrument-coverage\"\n   ```\n\n3. Build your code:\n\n   `cargo build`\n\n4. Ensure each test runs gets its own profile information by defining the LLVM_PROFILE_FILE environment variable (%p will be replaced by the process ID, and %m by the binary signature):\n\n   ```sh\n   export LLVM_PROFILE_FILE=\"your_name-%p-%m.profraw\"\n   ```\n\n5. Run your tests:\n\n   `cargo test`\n\nIn the CWD, you will see a `.profraw` file has been generated. This contains the profiling information that grcov will parse, alongside with your binaries.\n\n### Example: How to generate .gcda files for C/C++\n\nPass `--coverage` to `clang` or `gcc` (or for older gcc versions pass `-ftest-coverage` and `-fprofile-arcs` options (see [gcc docs](https://gcc.gnu.org/onlinedocs/gcc/Gcov-Data-Files.html)).\n\n### Example: How to generate .gcda files for a Rust project\n\n**Nightly Rust is required** to use grcov for Rust gcov-based coverage. Alternatively, you can `export\nRUSTC_BOOTSTRAP=1`, which basically turns your stable rustc into a Nightly one.\n\n1. Ensure that the following environment variables are set up:\n\n   ```sh\n   export CARGO_INCREMENTAL=0\n   export RUSTFLAGS=\"-Zprofile -Ccodegen-units=1 -Copt-level=0 -Clink-dead-code -Coverflow-checks=off -Zpanic_abort_tests -Cpanic=abort\"\n   export RUSTDOCFLAGS=\"-Cpanic=abort\"\n   ```\n\n   These will ensure that things like dead code elimination do not skew the coverage.\n\n2. Build your code:\n\n   `cargo build`\n\n   If you look in `target/debug/deps` dir you will see `.gcno` files have appeared. These are the locations that could be covered.\n\n3. Run your tests:\n\n   `cargo test`\n\n   In the `target/debug/deps/` dir you will now also see `.gcda` files. These contain the hit counts on which of those locations have been reached. Both sets of files are used as inputs to `grcov`.\n\n### Generate a coverage report from coverage artifacts\n\nGenerate a html coverage report like this:\n\n```sh\ngrcov . -s . --binary-path ./target/debug/ -t html --branch --ignore-not-existing -o ./target/debug/coverage/\n```\n\nN.B.: The `--binary-path` argument is only necessary for source-based coverage.\n\nYou can see the report in `target/debug/coverage/index.html`.\n\n(or alternatively with `-t lcov` grcov will output a lcov compatible coverage report that you could then feed into lcov's `genhtml` command).\n\n#### LCOV output\n\nBy passing `-t lcov` you could generate an lcov.info file and pass it to genhtml:\n\n```sh\ngenhtml -o ./target/debug/coverage/ --show-details --highlight --ignore-errors source --legend ./target/debug/lcov.info\n```\n\nLCOV output should be used when uploading to Codecov, with the `--branch` argument for branch coverage support.\n\n#### Coveralls output\n\nCoverage can also be generated in coveralls format:\n\n```sh\ngrcov . --binary-path ./target/debug/ -t coveralls -s . --token YOUR_COVERALLS_TOKEN > coveralls.json\n```\n\n#### grcov with Travis\n\nHere is an example of .travis.yml file for source-based coverage:\n\n```yaml\nlanguage: rust\n\nbefore_install:\n  - curl -L https://github.com/mozilla/grcov/releases/latest/download/grcov-x86_64-unknown-linux-gnu.tar.bz2 | tar jxf -\n\nmatrix:\n  include:\n    - os: linux\n      rust: stable\n\nscript:\n    - rustup component add llvm-tools-preview\n    - export RUSTFLAGS=\"-Cinstrument-coverage\"\n    - cargo build --verbose\n    - LLVM_PROFILE_FILE=\"your_name-%p-%m.profraw\" cargo test --verbose\n    - ./grcov . --binary-path ./target/debug/ -s . -t lcov --branch --ignore-not-existing --ignore \"/*\" -o lcov.info\n    - bash <(curl -s https://codecov.io/bash) -f lcov.info\n```\n\nHere is an example of .travis.yml file:\n\n```yaml\nlanguage: rust\n\nbefore_install:\n  - curl -L https://github.com/mozilla/grcov/releases/latest/download/grcov-x86_64-unknown-linux-gnu.tar.bz2 | tar jxf -\n\nmatrix:\n  include:\n    - os: linux\n      rust: stable\n\nscript:\n    - export CARGO_INCREMENTAL=0\n    - export RUSTFLAGS=\"-Zprofile -Ccodegen-units=1 -Copt-level=0 -Clink-dead-code -Coverflow-checks=off -Zpanic_abort_tests -Cpanic=abort\"\n    - export RUSTDOCFLAGS=\"-Cpanic=abort\"\n    - cargo build --verbose $CARGO_OPTIONS\n    - cargo test --verbose $CARGO_OPTIONS\n    - |\n      zip -0 ccov.zip `find . \\( -name \"YOUR_PROJECT_NAME*.gc*\" \\) -print`;\n      ./grcov ccov.zip -s . -t lcov --llvm --branch --ignore-not-existing --ignore \"/*\" -o lcov.info;\n      bash <(curl -s https://codecov.io/bash) -f lcov.info;\n```\n\n### Alternative reports\n\ngrcov provides the following output types:\n\n| Output Type `-t` | Description                                                               |\n| ---------------- | ------------------------------------------------------------------------- |\n| lcov (default)   | lcov's INFO format that is compatible with the linux coverage project.    |\n| ade              | ActiveData\\-ETL format. Only useful for Mozilla projects.                 |\n| coveralls        | Generates coverage in Coveralls format.                                   |\n| coveralls+       | Like coveralls but with function level information.                       |\n| files            | Output a file list of covered or uncovered source files.                  |\n| covdir           | Provides coverage in a recursive JSON format.                             |\n| html             | Output a HTML coverage report, including coverage badges for your README. |\n| cobertura        | Cobertura XML. Used for coverage analysis in some IDEs and Gitlab CI.     |\n\n### Hosting HTML reports and using coverage badges\n\nThe HTML report can be hosted on static website providers like GitHub Pages, Netlify and others. It\nis common to provide a coverage badge in a project's readme to show the current percentage of\ncovered code.\n\nTo still allow adding the badge when using a static site host, grcov generates coverage badges and\na JSON file with coverage information that can be used with <https://shields.io> to dynamically\ngenerate badges.\n\nThe coverage data for <htttps://shields.io> can be found at `/coverage.json` and the generated\nbagdes are available as SVGs at `/badges/*svg`.\n\nThe design of generated badges is taken from `shields.io` but may not be updated immediately if there\nis any change. Using their endpoint method is recommended if other badges from their service are\nused already.\n\n### Enabling symlinks on Windows\n\n`grcov` uses symbolic links to avoid copying files, when processing directories\nof coverage data. On Windows, by default, creating symbolic links to files\nrequires Administrator privileges. (The reason is to avoid security attacks in\napplications that were designed before Windows added support for symbolic\nlinks.)\n\nWhen running on Windows `grcov` will attempt to create a symbolic link. If that\nfails then `grcov` will fall back to copying the file. Copying is less efficient\nbut at least allows users to run `grcov`. `grcov` will also print a warning\nwhen it falls back to copying a file, advising the user either to enable the\nprivilege for their account or to run as Administrator.\n\nYou can enable the \"Create Symbolic Links\" privilege for your account so that\nyou do not need to run as Administrator to use `grcov`.\n\n1. Click Start, then select \"Local Group Policy Editor\". Or just run\n   `gpedit.msc` to open it directly.\n1. In the navigation tree, select \"Computer Configuration\", \"Windows Settings\",\n   \"Security Settings\", \"Local Policies\".\n1. In the pane on the right, select \"Create symbolic links\" and double-click it.\n1. Click \"Add User or Group\", and add your account.\n1. Log out and then log back in.\n\n#### Example\n\nLet's consider we have a project at with username `sample` and project `awesome` that is hosted with\nGitHub Pages at `https://sample.github.io/awesome`.\n\nBy using the the `shields.io` endpoint we can create a Markdown badge like so:\n\n```md\n[![coverage](https://shields.io/endpoint?url=https://sample.github.io/awesome/coverage.json)](https://sample.github.io/awesome/index.html)\n```\n\nIf we want to avoid using `shields.io` as well, we can use the generated badges as follows (note\nthe different URL for the image):\n\n```md\n[![coverage](https://sample.github.io/awesome/badges/flat.svg)](https://sample.github.io/awesome/index.html)\n```\n\n## Auto-formatting\n\nThis project is using pre-commit. Please run `pre-commit install` to install the git pre-commit hooks on your clone. Instructions on how to install pre-commit can be found [here](https://pre-commit.com/#install).\n\nEvery time you will try to commit, pre-commit will run checks on your files to make sure they follow our style standards and they aren't affected by some simple issues. If the checks fail, pre-commit won't let you commit.\n\n## Build & Test\n\nBuild with:\n\n```sh\ncargo build\n```\n\nTo run unit tests:\n\n```sh\ncargo test --lib\n```\n\nTo run integration tests, it is suggested to use the Docker image defined in tests/Dockerfile. Simply build the image to run them:\n\n```sh\ndocker build -t marcocas/grcov -f tests/Dockerfile .\n```\n\nOtherwise, if you don't want to use Docker, the only prerequisite is to install GCC 7, setting the `GCC_CXX` environment variable to `g++-7` and the `GCOV` environment variable to `gcov-7`. Then run the tests with:\n\n```sh\ncargo test\n```\n\n## Minimum requirements\n\n- GCC 4.9 or higher is required (if parsing coverage artifacts generated by GCC).\n- Rust 1.52\n\n## License\n\nPublished under the MPL 2.0 license.\n"
},
{
  "name": "iris_control_center",
  "files": {
    "/": [
      ".env",
      ".gitignore",
      ".prettierignore",
      ".prettierrc",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "jsconfig.json",
      "package-lock.json",
      "package.json",
      "public",
      "src",
      "targets.json"
    ]
  },
  "makefile": null,
  "readme": "# Welcome to Mozilla Iris!\n\nThis repository is used to create the front-end administrative Control Center for the Mozilla Iris automation tool.\n\nPlease refer to these projects for more information:\n\n* [Iris](https://github.com/mozilla/iris) - a visual automation platform for the desktop, written in Python 3.\n* [Iris for Firefox](https://github.com/mozilla/iris_firefox) - tests written specifically for automation of the Firefox browser.\n\n\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `npm start`\n\nRuns the app in the development mode.<br>\nOpen [http://localhost:3000](http://localhost:3000) to view it in the browser.\n\nThe page will reload if you make edits.<br>\nYou will also see any lint errors in the console.\n\n### `npm test`\n\nLaunches the test runner in the interactive watch mode.<br>\nSee the section about [running tests](https://facebook.github.io/create-react-app/docs/running-tests) for more information.\n\n### `npm run build`\n\nBuilds the app for production to the `build` folder.<br>\nIt correctly bundles React in production mode and optimizes the build for the best performance.\n\nThe build is minified and the filenames include the hashes.<br>\nYour app is ready to be deployed!\n\nSee the section about [deployment](https://facebook.github.io/create-react-app/docs/deployment) for more information.\n\n### `npm run eject`\n\n**Note: this is a one-way operation. Once you `eject`, you can\u2019t go back!**\n\nIf you aren\u2019t satisfied with the build tool and configuration choices, you can `eject` at any time. This command will remove the single build dependency from your project.\n\nInstead, it will copy all the configuration files and the transitive dependencies (Webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except `eject` will still work, but they will point to the copied scripts so you can tweak them. At this point you\u2019re on your own.\n\nYou don\u2019t have to ever use `eject`. The curated feature set is suitable for small and middle deployments, and you shouldn\u2019t feel obligated to use this feature. However we understand that this tool wouldn\u2019t be useful if you couldn\u2019t customize it when you are ready for it.\n\n## Learn More\n\nYou can learn more in the [Create React App documentation](https://facebook.github.io/create-react-app/docs/getting-started).\n\nTo learn React, check out the [React documentation](https://reactjs.org/).\n\n### Code Splitting\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting\n\n### Analyzing the Bundle Size\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size\n\n### Making a Progressive Web App\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app\n\n### Advanced Configuration\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration\n\n### Deployment\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/deployment\n\n### `npm run build` fails to minify\n\nThis section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify\n"
},
{
  "name": "preparatory-survey-for-naar-study-addon",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "ci-builds",
      "dist",
      "docs",
      "karma.conf.js",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "src",
      "survey",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# Preparatory Survey for NAAR Study Add-on\n\n[![CircleCI badge](https://img.shields.io/circleci/project/github/mozilla/preparatory-survey-for-naar-study-addon/master.svg?label=CircleCI)](https://circleci.com/gh/mozilla/preparatory-survey-for-naar-study-addon/)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/preparatory-survey-for-naar-study-addon/badge.svg)](https://coveralls.io/github/mozilla/preparatory-survey-for-naar-study-addon)\n\nThe purpose of this extension is to prompt users to participate in [the preparatory survey for NAAR](https://experimenter.services.mozilla.com/experiments/preparatory-survey-for-naar-needs-aware-add-on-recommendations/).\n\nShipped as a study add-on since the survey requires metadata about some of the add-ons that the user has installed.\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Analyzing data\n\nTelemetry pings are loaded into S3 and re:dash. Sample query:\n\n* [All pings](https://sql.telemetry.mozilla.org/queries/{#your-id}/source#table)\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n## References\n\n* [Experimenter](https://experimenter.services.mozilla.com/experiments/preparatory-survey-for-naar-needs-aware-add-on-recommendations/)\n"
},
{
  "name": "pioneer-participation-prompt",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "docs",
      "karma.conf.js",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# Pioneer Participation Prompt\n\n[![CircleCI badge](https://img.shields.io/circleci/project/github/mozilla/pioneer-participation-prompt/master.svg?label=CircleCI)](https://circleci.com/gh/mozilla/pioneer-participation-prompt/)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/pioneer-participation-prompt/badge.svg)](https://coveralls.io/github/mozilla/pioneer-participation-prompt)\n\nThe purpose of this extension is to prompt users to participate in [Pioneer](https://medium.com/firefox-context-graph/make-firefox-better-with-pioneer-10c82d0f9301).\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Analyzing data\n\nTelemetry pings are loaded into S3 and re:dash. Sample query:\n\n* [All pings](https://sql.telemetry.mozilla.org/queries/{#your-id}/source#table)\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n## References\n\n* [Experimenter](https://experimenter.services.mozilla.com/experiments/firefox-pioneer-enrollment-prompt-add-on/)\n"
},
{
  "name": "bugzilla-dashboard",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      ".neutrinorc.js",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "jest.config.js",
      "netlify.toml",
      "package.json",
      "poetry.lock",
      "pyproject.toml",
      "scripts",
      "src",
      "test",
      "webpack.config.js",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": "# The Bugzilla management dashboard\n\nThis is a Bugzilla dashboard that helps management determine Bugzilla components triaging status plus listing members of their reporting chain.\n\nOnly LDAP users are allowed to use this app. You can do development locally without an LDAP account, however, the app will only\nhave fake org data. See the [Contribute](#contribute) section.\n\nYou can see the deployment in our [Netlify instance](http://bugzilla-management-dashboard.netlify.com/).\n\n## Adding more teams\n\nA team is a collection of components that can span various products and it is shown under the Teams tab.\nYou can add new teams and make them show in the Teams tab by making changes to the [config](https://github.com/mozilla/bugzilla-dashboard/blob/master/src/config.js) file.\n\nTo add a team you need to modify `TEAMS_CONFIG` and an entry similar to this:\n\n```javascript\nexport const TEAMS_CONFIG = {\n  domCore: {\n    label: 'DOM Core',\n    owner: 'someone@mozilla.com',\n    product: ['Core'],\n    component: [\n      'DOM: Core & HTML', 'DOM: Events',\n      'Editor', 'HTML: Parser', 'Selection', 'Serializers',\n      'User events and focus handling',\n    ],\n},\n```\n\nHere's how to configure it:\n\n* `product` and `component` are parameters passed to the Bugzilla queries.\n* `owner` should match someone reporting to you.\n  * Use their Bugzilla email rather than their LDAP\n  * If the person does is not someone showing up on your Reportees tab it won't work\n* `label` is the name of the team\n\n## Generate data\n\nUntil we have a backend, we need to regenerate certain files to bring the app up-to-date.\n\n### Org related data\n\nThe data is stored in Taskcluster Secrets and it's only accessible to moco_team. See [bug 1540823](https://bugzilla.mozilla.org/show_bug.cgi?id=1540823)\n\nTo update the data you will need to take a Phonebook dump, get it reduced and converted to Yaml and upload it to Taskcluster Secrets.\n\nRequirements:\n\n* Python\n* pip (which comes with Python) or [poetry](https://poetry.eustace.io/docs/#installation)\n\nSet up the virtualenv with `poetry`:\n\n```bash\npoetry install\npoetry shell\n```\n\nor:\n\n```bash\npython3 -m venv venv\nsource ./venv/bin/activate\npip install PyYaml\n```\n\nExecute the command:\n\n```bash\npython scripts/processPeopleFile.py --path /path/to/phonebook.json\n```\n\nYou can read in [here](https://github.com/mozilla-iam/cis/issues/402) what changes are needed to get data from CIS.\n\n### triageOwners.json\n\nThis file is checked-in because it makes the app snapier, however, it can fall out of date.\n\nTo regenerate it run this and commit the updated file:\n\n```bash\nnode scripts/generateTriageOwners.js\n```\n\n## Contribute\n\nIf you don't have LDAP access you can start the app with `yarn start:alternativeAuth` and use Google or GitHub to authenticate. This will\nnot give you access to a functioning app, however, it will allow you to make contributions to the authenticated interface.\n\nIssue #66 will add fake data into this alternative auth approach.\n\n## Auth info\n\nThis app authenticates with Mozilla's official [Auth0 domain](https://auth.mozilla.auth0.com).\nIt uses SSO and it only allows authentication of Mozilla staff via LDAP.\n\nThe authentication configuration has the following characteristics:\n\n* There are two different Auth0 clients\n  * An official one (SSO + LDAP) and the other for non-LDAP contributors\n  * Non-LDAP users will receive fake org data\n* After a user authenticates, the auth will also authenticate with Firefox CI Taskcluster (`firefox-ci-tc.services.mozilla.com`)\n  * This is in order to later fetch a Taskcluster secret (only available to LDAP users)\n\n## Running & tests\n\n* [Install Yarn](https://yarnpkg.com/lang/en/docs/install/)\n\n* To install the dependencies:\n\n```bash\nyarn install\n```\n\n* To run the tests:\n\n```bash\nyarn test -u\n```\n\n* To run the linting tests\n\n```bash\nyarn lint\n```\n\n* To run the project\n\n```bash\nyarn install\nyarn start\n```\n"
},
{
  "name": "missioncontrol",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".env-dist",
      ".eslintrc.js",
      ".gitignore",
      ".neutrinorc.js",
      ".pyup.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "contribute.json",
      "docker-compose-ci.yml",
      "docker-compose.yml",
      "docs",
      "fixtures_init.sql",
      "frontend",
      "manage.py",
      "missioncontrol",
      "package.json",
      "pytest.ini",
      "requirements",
      "sample.snappy.parquet",
      "setup.cfg",
      "setup.py",
      "tests",
      "yarn.lock"
    ],
    "/docs": [
      "API.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build migrate shell presto-cli up fixtures tests flake8\n\nhelp:\n\t@echo \"Welcome to the Mission Control Api Service\\n\"\n\t@echo \"The list of commands for local development:\\n\"\n\t@echo \"  build      Builds the docker images for the docker-compose setup\"\n\t@echo \"  migrate    Runs the Django database migrations\"\n\t@echo \"  shell      Opens a Bash shell\"\n\t@echo \"  presto-cli Opens a Presto command line client\"\n\t@echo \"  up         Runs the whole stack, served under http://localhost:8000/\"\n\t@echo \"  fixtures   Generates sample data\"\n\t@echo \"  tests      Run pytest tests using tox\"\n\t@echo \"  flake8     Run flake8 using tox\"\n\nbuild:\n\tSERVICE_DOMAIN=\"\" yarn build\n\tdocker-compose build\n\nmigrate:\n\tdocker-compose run web python manage.py migrate --run-syncdb\n\nshell:\n\tdocker-compose exec web bash\n\ndjango-shell:\n\tdocker-compose exec web ./manage.py shell\n\npresto-cli:\n\tdocker-compose exec presto presto-cli\n\nup:\n\tdocker-compose up\n\nfixtures:\n\t@bin/fixtures_init.sh\n\ntests:\n\tdocker-compose run web tox -etests\n\nflake8:\n\tdocker-compose run web tox -eflake8\n",
  "readme": "Mission Control\n===============\n\n[![CircleCI](https://img.shields.io/circleci/project/github/mozilla/missioncontrol/master.svg)](https://circleci.com/gh/mozilla/missioncontrol)\n[![codecov](https://codecov.io/gh/mozilla/missioncontrol/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/missioncontrol)\n\nMission Control is a monitoring service for Firefox release health, it allows you\nto view in (near) real time the rate of crashes and other quantitative measures of\nquality. It uses the dataset generated by the [telemetry-streaming](https://github.com/mozilla/telemetry-streaming) library.\n\nThe server-side backend is written in Python using Django. The UI is written in [React](https://reactjs.org/), [Redux](http://redux.js.org/) and [metricsgraphics](http://metricsgraphicsjs.org/).\n\nGetting in touch\n----------------\n\nIf you have any questions about Mission Control (either as a user or contributor), the best place to ask is the [#missioncontrol](https://client00.chat.mibbit.com/?server=irc.mozilla.org&channel=%23missioncontrol) channel on irc.mozilla.org ([learn more about irc @ Mozilla](https://wiki.mozilla.org/IRC)).\n\nContributing\n------------\n\nWe welcome contributions to Mission Control! Working on the UI component (see\ninstructions immediately below) does not require any special access to Mozilla's\ninternal systems.\n\nIf you\u2019re looking for a way to jump in and contribute, our list of [good first issues](https://github.com/mozilla/missioncontrol/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) is a great place to start.\n\nInstructions for development (UI only)\n--------------------------------------\n\nIf you only want to hack on the UI, you can set up a local-only of missioncontrol which\npulls data from the current production server. You only need to have\n[yarn](https://yarnpkg.com/) installed.\n\nRun:\n\n```bash\nyarn install\nyarn start\n```\n\nThis should start up a webserver at http://localhost:5000 which you can connect to.\n\nTo run the [Jest](https://jestjs.io/) tests for Mission Control's React components, execute:\n\n```bash\nyarn test\n```\n\nInstructions for development (full stack)\n-----------------------------------------\n\nMake sure you have [docker](https://docker.io), [docker-compose](https://github.com/docker/compose), and [yarn](https://yarnpkg.com/) installed.\n\nThen run:\n\n```bash\nyarn install\ncp .env-dist .env\nmake build\nmake up\nmake fixtures\n```\n\nAfter you have brought the environment up, you can bring up a development version of\nthe server by running `make shell` and then running `./manage.py runserver`\nfrom there. You should then be able to connect to `http://localhost:8000` from\nyour web browser.\n\nBy default the environment uses a rather improverished set of test data, so\nthe environment will not be that interesting. If you have Mozilla credentials,\nyou can set up `PRESTO_URL` and `SECRET_KEY` variables in a `.env` file to have it pull data\nfrom a production dataset. Once you have that set up, you should be able to\ndownload a set of recent data from a shell environment (`make shell`) via the\nload_measure_data subcommand. E.g.:\n\n```bash\n./manage.py load_measure_data linux release main_crashes\n```\n\nThe recommended way of running the tests locally is via the shell environment.\nAfter running `make shell`, execute:\n\n```bash\npytest tests/\n```\n\nBy default all tests and linters are run. Often you just want to run a subset\nof the python tests. You can do this by adding some arguments to your tox\ninvocation:\n\n```bash\ntox -e tests -- -k tests/test_api.py  # run only tests in test_api.py\n```\n\nInstructions for deployment\n---------------------------\n\nThe target environment for this project follows the [dockerflow](https://github.com/mozilla-services/Dockerflow) conventions.\nIn order to run it correctly, a number of environment variables need to be set up.\nThe full list of variables can be found in the web section of the docker-compose.yml file.\nFrom a services standpoint, this project requires:\n - a Postgres DB to store the application data, defined by DATABASE_URL\n - a Presto/Athena service, defined by PRESTO_URL\n - an optional Redis cache service, defined by CACHE_URL\n"
},
{
  "name": "libdweb",
  "files": {
    "/": [
      ".flowconfig",
      ".gitignore",
      ".npmignore",
      ".travis.yml",
      ".vscode",
      "CODE_OF_CONDUCT.md",
      "License.md",
      "Readme.md",
      "demo",
      "flow-typed",
      "package.json",
      "src",
      "www",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "firefox-health-backend",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      ".mocharc.js",
      ".neutrinorc.js",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "app.json",
      "package.json",
      "src",
      "test",
      "webpack.config.js",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Health backend\n\nFirefox metrics & insights backend.\nFor the frontend code visit the [Firefox health dashboard](https://github.com/mozilla/firefox-health-dashboard) repo.\n\n[![Build Status](https://api.travis-ci.org/mozilla/firefox-health-backend.svg?branch=master)](https://travis-ci.org/mozilla/firefox-health-backend)\n\n## Requirements\n\n* Node\n* Yarn\n* Docker OR redis\n\n## Setting project up\n\nIn your console:\n\n```shell\ndocker run -p 6379:6379 -d redis\nyarn // To get the dependencies installed\nyarn start // To start the server\n```\n\n### Enable access to Nimbledroid's data\n\nNimbledroid provides us with performance data for various sites on Android.\nIf you want to make changes to the Nimbledroid APIs on the backend you will need\nto have access to our corporate Nimbledroid account.\n\nOnce you have access you can fetch your personal key (keep private) under your\n[account](https://nimbledroid.com/account). You can re-generate it there if it ever gets leaked.\n\nOnce you have it you can start the backend like this:\n\n```shell\nexport NIMBLEDROID_API_KEY=<API key>\nexport NIMBLEDROID_EMAIL=<your email address>\nexport REDIS_URL=redis://localhost:6379\n// Change DEBUG to script:* if you also want debug output\nDEBUG=script:info,script:error yarn fetchNimbledroidData\nyarn start\n```\n\nLoad [this page](http://localhost:3000/api/android/nimbledroid?product=com.chrome.beta) to verify it works.\n\n### Redis\n\nIf you want to test caching with Redis (there's caching with JS as a fallback) make sure to install Redis and set the REDIS_URL env to `redis://localhost:6379` before starting the server.\n\n## How the Nimbledroid data is fetched\n\nIn order to fetch the data from Nimbledroid's APIs we need an API key, thus, we need to use this backend.\nIn order to handle the inefficient APIs we store the data in Redis.\n\nThis is how we fetch, store & serve the data:\n\n* We schedule a job every hour on Heroku\n* This job executes the [fetchNimbledroidData.js](https://github.com/mozilla/firefox-health-backend/blob/master/src/scripts/fetchNimbledroidData.js) script\n* If any of the profiles fetched are new we store them on Redis\n* When the frontend hits this backend we return data from our Redis storage\n\nThis set up above is accomplished via the \"firefox-health-backend\" app on Heroku.\n\nThese env variables are needed:\n\n* NIMBLEDROID_API_KEY\n* NIMBLEDROID_EMAIL\n\nThe addons involved are:\n\n* Heroku Redis\n* Heroku Scheduler\n\nIn order to report any issues with fetching the data we've also enabled the \"Dead Man's snitch\" (DMS) on Heroku.\nThs add-on expects the Heroku scheduled job to consistently report a successful run.\nIf DMS does not hear back from the script after a couple of hours it will send an email notifying few users.\nYou can adjust who the recipients of the alerts are via the add-on on Heroku.\n\n### Nimbledroid data seems old\n\nFirst check the APKs uploaded dates from [Nimbledroid](https://nimbledroid.com/my_apps). Hover over the apps to verify the match the package IDs (e.g. [org.mozilla.klar](https://nimbledroid.com/my_apps/org.mozilla.klar?a=2ab0db47-8e11-4be3-bd58-cfec06e225e9#summary).\n\nIf everything seems fine, try to run the script as described in the section above to see if there's anything broken.\n\nIf everything works, load the Nimbledroid APKs directly and inspect the output.\n\n## Attributions\n\n* heartbeat icon by Creative Stall from the Noun Project\n"
},
{
  "name": "delivery-dashboard",
  "files": {
    "/": [
      ".flowconfig",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "codecov.yml",
      "config-overrides.js",
      "flow-typed",
      "package.json",
      "public",
      "renovate.json",
      "scripts",
      "src",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": "# Delivery Dashboard\n\n[![Renovate enabled](https://img.shields.io/badge/renovate-enabled-brightgreen.svg)](https://renovateapp.com/)\n\nProduct Delivery's Web Client to Microservices.\n\nThe Product Delivery team is implementing various Microservices such as\n[PollBot](https://github.com/mozilla/PollBot), and this dashboard aims at\ndisplaying information from those in a centralized place.\n\n## Table of Contents\n\n* [Cloning and getting into the Project Dir](#cloning-and-getting-into-the-project-dir)\n* [Setting up the development environment](#setting-up-the-development-environment)\n* [Starting the dev server](#starting-the-dev-server)\n  * [Building](#building)\n  * [Deploying to gh-pages](#deploying-to-gh-pages)\n* [Launching testsuite](#launching-testsuite)\n* [Linting](#linting)\n\n## Cloning and getting into the Project Dir\n\nTo clone this repository simply type\n\n    $ git clone https://github.com/mozilla/delivery-dashboard && cd delivery-dashboard\n    $ cd delivery-dashboard\n\n## Setting up the development environment\n\nYou would need to install `yarn`. You can use:\n\n    $ npm install -g yarn\n\nYou can then use `yarn` to install the project dependencies:\n\n    $ yarn install\n\n## Starting the dev server\n\nTo start the dev server type the following command:\n\n    $ yarn start\n\n### Building\n\nTo build this app, type the command below:\n\n    $ yarn build\n\n### Deploying to gh-pages\n\n    $ yarn deploy\n\nThe app should be deployed to\nhttps://[your-github-username].github.io/delivery-dashboard/\n\n## Launching testsuite\n\nTo run the testsuite, simply type:\n\n    $ yarn test\n\n## Linting\n\nWe use [Prettier](https://prettier.io/) to format all `.js` and `.css` files\naccording to the default configuration of Prettier.\n\nWhen contributing, it is your responsibility to make sure all files you\ntouch conform to the Prettier standard, but there are useful tools to make\nthis easier.\n\nLinting is checked in continuous integration for every pull request and\nbuild of `master`. If any file has any deviation from the Prettier output\nit will \"break the build\" and you're expected to fix it.\n\nTo make it easier to see what the potential linting problems are run:\n\n```sh\n$ yarn lint\n```\n\nIt will report any errors and explain which files need attention. To\nmake this more convenient you can simply run:\n\n```sh\n$ yarn lint-fix\n```\n\nwhich will directly fix the files that didn't pass.\n\n## Using a different Pollbot server\n\nWhen accessing https://mozilla.github.io/delivery-dashboard/, data is fetched\nfrom https://pollbot.services.mozilla.com/v1, which is the production server.\nIf you want to use a different server (for example the stage or dev versions),\nadd a `server` query parameter like so:\n\n    - dev: https://mozilla.github.io/delivery-dashboard/?server=https://pollbot.dev.mozaws.net/v1\n    - stage: https://mozilla.github.io/delivery-dashboard/?server=https://pollbot.stage.mozaws.net/v1\n"
},
{
  "name": "http-observatory-website",
  "files": {
    "/": [
      ".eslintrc",
      ".gitattributes",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "config",
      "dist",
      "package-lock.json",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Observatory :: Website\n\nThe Mozilla Observatory is a set of tools to analyze your website and inform you if you are utilizing the many available methods to secure it.\n\nIt is split into three projects:\n\n* [http-observatory](https://github.com/mozilla/http-observatory) - scanner/grader\n* [http-observatory-cli](https://github.com/mozilla/observatory-cli) - command line interface\n* [http-observatory-website](https://github.com/mozilla/http-observatory-website) - web interface\n\nTLS evaluation relies on external scanners, such as Mozilla's [TLS Observatory](https://github.com/mozilla/tls-observatory).\n\n## Installation\n\nIf you just want to use a local version of the website, you can simply clone the dist directory:\n\n```bash\n$ git clone -b gh-pages https://github.com/mozilla/http-observatory-website.git\n```\n\nHowever, it comes with a built-in web server that will automatically regenerate the SRI hashes:\n\n```bash\n$ npm install\n$ npm run watch\n```\n\nNote that this will still use the global Mozilla Observatory API endpoints; you will need to change `httpobs.js` and\n`httpobs-third-party.js` if you wish to use your own local endpoints.\n\n## Authors\n\n* April King\n\n## License\n\n* Mozilla Public License Version 2.0\n"
},
{
  "name": "cookie-restrictions-strict-list-study",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ]
  },
  "makefile": null,
  "readme": "# Cookie Restrictions Strict List Study\n\nThis addon is a Shield study based on https://github.com/nhnt11/multipreffer.\n\nSee https://bugzilla.mozilla.org/show_bug.cgi?id=1522309 for details on the study.\n"
},
{
  "name": "PollBot",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      ".python-version",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTORS.rst",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "bin",
      "docker-compose.yml",
      "env-dist",
      "pollbot",
      "requirements.in",
      "requirements.txt",
      "scripts",
      "setup.cfg",
      "setup.py",
      "tests"
    ],
    "/.github": [
      "CODEOWNERS",
      "PULL_REQUEST_TEMPLATE.md",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "DC := $(shell which docker-compose)\n\n.PHONY: help\nhelp: default\n\n.PHONY: default\ndefault:\n\t@echo \"Please use 'make <target>' where <target> is one of\"\n\t@echo \"\"\n\t@echo \"  build       - build local dev environment\"\n\t@echo \"  run         - start the kinto server on default port\"\n\t@echo \"  lint        - run the linter\"\n\t@echo \"  test        - run tests\"\n\t@echo \"  shell       - run a bash shell in the container\"\n\t@echo \"  clean       - remove build artifacts\"\n\t@echo \"\"\n\t@echo \"Check the Makefile to know exactly what each target is doing.\"\n\n.docker-build:\n\tmake build\n\n.PHONY: build\nbuild: .env\n\t${DC} build app\n\ttouch .docker-build\n\n.env:\n\t@if [ ! -f .env ]; \\\n\tthen \\\n\techo \"Copying env-dist to .env...\"; \\\n\tcp env-dist .env; \\\n\tfi\n\nSOURCE := $(shell git config remote.origin.url | sed -e 's|git@|https://|g' | sed -e 's|github.com:|github.com/|g')\nVERSION := $(shell git describe --always --tag)\nCOMMIT := $(shell git log --pretty=format:'%H' -n 1)\n\n.PHONY: version-file\nversion-file:\n\techo '{\"build\":\"Manual build\",\"version\":\"$(VERSION)\",\"source\":\"$(SOURCE)\",\"commit\":\"$(COMMIT)\"}' > version.json\n\n.PHONY: install-local\ninstall-local:\n# Need to do this to create the pollbot.egg-info in the repo directory.\n# FIXME(willkg): This is gross, but \"works\".\n\t${DC} run --rm --user 0 app pip install -e /app\n\n.PHONY: run\nrun: .docker-build .env version-file install-local\n\t${DC} up app\n\n.PHONY: test\ntest: .docker-build .env install-local\n\t${DC} run --rm --no-deps app /bin/bash -c \"/usr/local/bin/pytest tests -s\"\n\n.PHONY: lint\nlint: .docker-build .env\n\t${DC} run --rm --no-deps app /bin/bash -c \"/usr/local/bin/flake8 pollbot tests\"\n\n.PHONY: shell\nshell: .docker-build .env\n\t${DC} run --rm --no-deps app /bin/bash\n\nclean:\n\trm .docker-build\n",
  "readme": "PollBot\n=======\n\n.. image:: https://img.shields.io/badge/%E2%9D%A4-code%20of%20conduct-blue.svg\n   :alt: Code of conduct\n   :target: https://github.com/mozilla/PollBot/blob/master/CODE_OF_CONDUCT.md\n.. image:: https://travis-ci.org/mozilla/PollBot.svg?branch=master\n   :alt: Travis CI status\n   :target: https://travis-ci.org/mozilla/PollBot\n.. image:: https://coveralls.io/repos/mozilla/PollBot/badge.svg?branch=master\n   :alt: Coverage\n   :target: https://coveralls.io/r/mozilla/PollBot\n.. image:: https://readthedocs.org/projects/pollbot/badge/?version=latest\n   :alt: Documentation Status\n   :target: https://pollbot.readthedocs.io/en/latest/\n.. image:: https://img.shields.io/pypi/v/pollbot.svg\n   :alg: PyPI\n   :target: https://pypi.python.org/pypi/pollbot\n.. image:: https://img.shields.io/badge/whatsdeployed-stage,prod-green.svg\n   :alt: What's Deployed\n   :target: https://whatsdeployed.io/s-olI\n\nPollBot is an hardworking little robot (microservice) that frees its\nhuman masters from the toilsome task of polling for the state of\nthings during the Firefox release process.\n\n`Version 1.0 <https://github.com/mozilla/PollBot/projects/1>`_ will\nprovide, at a minimum, these API resources:\n\n#. build exists on archive.mozilla.org\n#. release notes published\n#. product-details.mozilla.org JSON contains the release\n#. download links are on mozilla.org and they work\n#. security advisories are published and links work\n\nDevelopment\n-----------\n\nCreate a local dev environment:\n\n.. code-block:: shell\n\n   make build\n\nThen you can run various dev-related tasks. For a list, see:\n\n.. code-block:: shell\n\n   make help\n\nTo start the development server you need this in your ``.env``:\n\n``TELEMETRY_API_KEY`` - See https://sql.telemetry.mozilla.org/users/me\n\nEquipped with these you can now run PollBot:\n\n.. code-block:: shell\n\n   make run\n\nThat should start a server on ``http://localhost:9876``.\n\nDeployment\n----------\n\n* Stage - https://pollbot.stage.mozaws.net/v1/\n* Prod - https://pollbot.services.mozilla.com/v1/\n\nStage is automatically upgraded when new releases are made. A release is\nbasically a tag with the same name. To make a release run:\n\n.. code-block:: shell\n\n    ./bin/make-release.py --help\n\nLicense\n-------\n\nMPL v2 (see `LICENSE <https://github.com/mozilla/PollBot/blob/master/LICENSE>`_)\n\n\nConfiguration\n-------------\n\nPollBot is a currently a stateless service, which means there are no\ndatabase services to configure.\n\nHowever you can configure the following parameters using environment variables:\n\n+-----------------------+-------------------------------------------------+\n| **VARIABLE**          | **Description**                                 |\n+-----------------------+-------------------------------------------------+\n| ``PORT``              | The service PORT, by default runs on 9876       |\n+-----------------------+-------------------------------------------------+\n| ``VERSION_FILE``      | The JSON version file, default PWD/version.json |\n+-----------------------+-------------------------------------------------+\n| ``CACHE_MAX_AGE``     | The Cache-Control max-age value, default to 30  |\n|                       | seconds. Set it to 0 to set it to no-cache      |\n+-----------------------+-------------------------------------------------+\n| ``TELEMETRY_API_KEY`` | API KEY to use to query the Telemetry Service   |\n+-----------------------+-------------------------------------------------+\n"
},
{
  "name": "donate-wagtail",
  "files": {
    "/": [
      ".devcontainer",
      ".eslintignore",
      ".eslintrc.json",
      ".github",
      ".gitignore",
      ".mergify.yml",
      ".prettierignore",
      ".profile",
      ".stylelintrc",
      ".stylelintrc-colors.js",
      ".vscode",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "ISSUE.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "app.json",
      "bin",
      "dev-requirements.in",
      "dev-requirements.txt",
      "docker-compose.yml",
      "dockerfiles",
      "docs",
      "donate",
      "env.default",
      "invoke.yaml",
      "maintenance",
      "manage.py",
      "package-lock.json",
      "package.json",
      "release-steps.sh",
      "requirements.in",
      "requirements.txt",
      "runtime.txt",
      "source",
      "tasks.py",
      "tests",
      "tox.ini",
      "translation-management.sh",
      "travis-scripts",
      "webpack.config.js"
    ],
    "/docs": [
      "local_dev.md",
      "pages.md",
      "pontoon_integration.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "pull_request_template.md",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# donate-wagtail\n\n[![Build Status](https://travis-ci.org/mozilla/donate-wagtail.svg?branch=master)](https://travis-ci.org/mozilla/donate-wagtail)\n\n## Table of contents\n\n- [How to setup your dev environment with Docker](#setup-your-dev-environment-with-docker)\n- [Basket donations queue](#basket)\n\n## Documentation\n\n- [Pages](docs/pages.md)\n- [Pontoon Integration](docs/pontoon_integration.md)\n- [Local dev](./docs/local_dev.md)\n\n## Setup your Dev Environment with Docker\n\n- Install [Docker Desktop](https://www.docker.com/products/docker-desktop) (macOS and Windows). For Linux users: install [Docker CE](https://docs.docker.com/install/#supported-platforms) and [Docker Compose](https://docs.docker.com/compose/install/).\n- [Check your install](https://docs.docker.com/get-started/#test-docker-version) by running `docker run hello-world`.\n- [Install Invoke](https://www.pyinvoke.org/installing.html). We recommend you use [pipx](https://pypi.org/project/pipx/)\n- Run `inv new-env`: it's building docker images, installing dependencies, setting up a populated DB, and configuring your environment variables.\n\nWhen it's done, run `docker-compose up`, wait for the static files to be built, and go to `0.0.0.0:8000`. When you want to stop, do `^C` to shut down your containers. If they don't stop properly, run `docker-compose down`. If you want a new dev environment, stop your containers and run `inv new_env`.\n\nSecrets necessary to enable several features in a local docker development environment can be found in the 1pass, Engagement Vault, in a Note called \"donate-wagtail docker .env file\".  Copy the contents of this file and replace the .env file that came with the repo.  Do not distrubute the .env file.  \n\nIt's possible to connect your IDE to the python virtual env available inside the backend container (tested with pycharm and vscode). If you run into issues, ping patjouk on slack.\n\nTo run commands with Docker, run `docker-compose run [SERVICE] [COMMAND]`. For example, running the python tests is done by `docker-compose run backend ./dockerpythonvenv/bin/python manage.py test --settings=donate.settings_test`. Since it's pretty long, most cases are covered by Invoke commands.\n\nMore information on how to use Docker for local dev is available in the [Local dev](./docs/local_dev.md) documentation.\n\n## Configuration\n\n[Django Configurations](https://django-configurations.readthedocs.io/en/stable/) is used for application configuration. The following Configuration Classes are provided:\n\n| Value                 | Purpose                                                                                                            |\n|-----------------------|--------------------------------------------------------------------------------------------------------------------|\n| Development           |  Base configuration, suitable for local development.                                                               |\n| Staging               | Staging configuration.                                                                                             |\n| Production            | Production configuration.                                                                                          |\n| ReviewApp             | Review App configuration. Use this configuration for Heroku Review apps.                                           |\n| ThunderbirdDevelopment | Base configuration that enables all the Thunderbird template and string overrides. Suitable for local development. |\n| ThunderbirdStaging    | Staging configuration for Thunderbird donation configurations.                                                     |\n| ThunderbirdProduction | Production configuration for Thunderbird donation configurations.                                                  |\n| ThunderbirdReviewApp  | Review App configuration for Thunderbird donation configurations.                                                  |\n\nTo set the Thunderbird settings in local development, simply change your `DJANGO_CONFIGURATION` in your .env file to be one of the choices above (For Thunderbird, use `ThunderbirdDevelopment`)\n\n\n## Braintree configuration\n\nThe following environment variables are required to configure payment processing via Braintree:\n\n- `BRAINTREE_MERCHANT_ID`: the merchant ID for the Braintree account used to process donations.\n- `BRAINTREE_MERCHANT_ACCOUNTS`: a series of key-value pairs that map each supported currency to the corresponding Braintree merchant account that is configured in that currency. For example: `usd=usd-ac,gbp=gbp-ac,eur=eur-ac` where `usd-ac`, `gbp-ac` and `eur-ac` are merchant account IDs.\n- `BRAINTREE_PLANS`: a series of key-value pairs that map each supported currency to the corresponding Braintree subscription plan that is configured in that currency. For example: `usd=usd-plan,gbp=gbp-plan,eur=eur-plan` where `usd-plan`, `gbp-plan` and `eur-plan` are plan IDs.\n- `BRAINTREE_PUBLIC_KEY`: Public API key provided by Braintree.\n- `BRAINTREE_PRIVATE_KEY`: Private API key provided by Braintree.\n- `BRAINTREE_TOKENIZATION_KEY`: Tokenization key provided by Braintree.\n- `BRAINTREE_USE_SANDBOX`: Boolean to configure whether or not to use the Braintree sandbox.\n\n### Webhook Configuration\n\nThere's a webhook endpoint for processing Braintree events. The events it supports are:\n\n* `subscription_charged_successfully`\n* `subscription_charged_unsuccessfully`\n* `dispute_lost`\n\nThe endpoint accepts requests on `/braintree/webhook/` and will verify the payload signature to ensure it's a legitimate event. [Documentation for Braintree webhooks can be found here](https://developers.braintreepayments.com/guides/webhooks/overview).\n\n## Basket\n\n[Basket](https://github.com/mozmeao/basket) is a tool run by MoCo to manage newsletter subscriptions and donations. It's listening for messages (JSON) sent to a SQS queue.\n\n### Basket donations queue:\n\nBasket has [4 event types we can use](https://github.com/mozmeao/basket/blob/master/basket/news/management/commands/process_donations_queue.py#L21-L26):\n\n- `donation`: process a donation and send its data to SFDC,\n- `crm_petition_data`: add petition signature to SFDC,\n- `newsletter_signup_data`: newsletter signup for the foundation site,\n- `DEFAULT`: process a followup Stripe event on a donation.\n\nFor this project, we're only using the `donation` event type.\n\n### Donation event type\n\nExample of a donation message sent to Basket, via SQS:\n\n```\n{\n    'data': {\n        'event_type': 'donation',\n        'last_name': 'alex',\n        'email': 'alex@alex.org',\n        'donation_amount': 50,\n        'currency': 'usd',\n        'created': 1563801762,\n        'recurring': false,\n        'service': 'paypal',\n        'transaction_id': 'ch_1Ez1TSG8Mmx3htnxyShib70n',\n        'project': 'mozillafoundation',\n        'last_4': '4242',\n        'donation_url': 'http://localhost:3000/en-US/',\n        'locale': 'en-US',\n        'conversion_amount': 50,\n        'net_amount': 48.6,\n        'transaction_fee': 1.4\n    }\n}\n```\n\n- `event_type`: Basket event type. Should be donation,\n- `first_name`: first name of the donor (optional),\n- `last_name`: last name of the donor,\n- `email`: email of the donor,\n- `donation_amount`: amount of the donation,\n- `currency`: letter code for the currency,\n- `created`: unix timestamp,\n- `recurring`: `false` for a one-time donation or `true` for a recurring one,\n- `service`: name of the payment processor (ex: `stripe` or `paypal`)\n- `transaction_id`: ID generated by the payment processor,\n- `project`: name of the project that will receive the donation (ex: `thunderbird`, `mozillafoundation`)\n- `last_4`: last 4 digits of the credit card,\n- `donation_url`: url from which the donation was made,\n- `locale`: language code for the donor,\n- `conversion_amount`: donation amount in USD, before transaction fees,\n- `net_amount`: donation amount in USD, after transaction fees,\n- `transaction_fee`: payment processor's transaction fees in USD\n\n\n### Newsletter signup\n\nWe're using Basket newsletter HTTP API to signup people to our newsletter. Specs are available in [Basket's documentation](https://basket.readthedocs.io/newsletter_api.html#news-subscribe). (Note that this is different than the SQS approach used for donation events)\n\nExample from donate.mozilla.org:\n```\n { format: 'html',\n   lang: 'en-US',\n   newsletters: 'mozilla-foundation',\n   trigger_welcome: 'N',\n   source_url: 'https://donate.mozilla.org/',\n   email: 'alex@alex.org',\n   country: undefined }\n```\n\n_Notes_: We want to keep the `trigger_welcome` at `N` and the `format` to `html`. We don't have the country info for now, but from what I understood, it's something we want to change.\n\n## Review App\n\n### Environment variables\n\nNon-secret envs can be added to the `app.json` file. Secrets must be set on Heroku in the `Review Apps` section of the pipelines' `settings` tab.\n\n### Review App for PRs\n\nOpening a PR will automatically create a Review App in the `donate-wagtail` and `thunderbird-donate` pipelines. A slack bot posts credentials and links to Review Apps in to the `mofo-ra-donate-wagtail` and `mofo-ra-thunderbird-donate-wagtail` channels.\n\n*Note:* This only work for Mo-Fo staff: you will need to manually open a Review App on Heroku for PRs opened by external contributors.\n\n### Review App for branches\n\nYou can manually create a review app for any branch pushed to this repo. It's useful if you want to test your code on Heroku without opening a PR yet.\nTo create one:\n- log into Heroku.\n- Go in the `donate-wagtail` or `thunderbird-donate` pipeline.\n- Click on `+ New app` and select the branch you want to use.\n\nThe review app slack bot will post a message in either the `mofo-ra-donate-wagtail` or `mofo-ra-thunderbird-donate-wagtail` with links and credentials as soon as the review app is ready.\n\n## SSO and admin logins for local development\n\nThe default for admin login for local development is the standard Django login. To use Mozilla SSO via OpenID Connect, set the `USE_CONVENTIONAL_AUTH` environment variable to `False`.\n\nTo make sure you can log in using your Mozilla SSO credentials, your will need to create a Django superuser with your mozilla email address, using:\n\n```shell\ndocker-compose exec app python manage.py createsuperuser\n```\n\n## Adding users to the system\n\nThe security model currently requires that an existing admin creates an account for a new user first, tied to that user's Mozilla email account, before that user can can log in using SSO.\n\nFurther more, in order for SSO authentication to succeed, their account must be a member of the donate user group. To request that an account be added to this group, please file [an SSO request bug](https://bugzilla.mozilla.org/enter_bug.cgi?product=Infrastructure%20%26%20Operations&component=SSO:%20Requests), making sure to also `cc` a donate admin in the bug.\n\n## Translations\n\nTranslation is happening on [Pontoon](https://pontoon.mozilla.org), in multiple projects where you can participate:\n\n| Project on Pontoon                          | Source repository                  |\n|---------------------------------------------|------------------------------------|\n[Mozilla & Thunderbird UI strings (Django)](https://pontoon.mozilla.org/projects/mozilla-donate-website/) | [Repository on GitHub](https://github.com/mozilla-l10n/donate-l10n)\n[Mozilla (CMS content)](https://pontoon.mozilla.org/projects/donate-mozilla-content/) | [Repository on GitHub](https://github.com/mozilla-l10n/mozilla-donate-content)\n[Thunderbird (CMS content)](https://pontoon.mozilla.org/projects/donate-thunderbird-content/) | [Repository on GitHub](https://github.com/mozilla-l10n/thunderbird-donate-content)\n\nThe latest UI source strings are regularly exposed to Pontoon by a Localization PM using the process below. The CMS strings are automatically synchronized with the repositories.\n\n### Initial setup:\n- Clone the `donate-l10n` repository locally.\n- Set the `LOCAL_PATH_TO_L10N_REPO` variable in your `.env` file. Use the absolute path to your copy of the `donate-l10n` repository and include the trailing slash. E.g. `LOCAL_PATH_TO_L10N_REPO=/Users/username/Documents/GitHub/donate-l10n/`\n\n### Exposing latest source strings:\n- Make sure your local repositories of `donate-l10n` and `donate-wagtail` are matching the latest revision from master.\n- Run `inv docker-makemessages` from your `donate-wagtail` repository.\n- Files should have been updated in your `donate-l10n` repository. You can now create a pull-request.\n\n### Getting the latest translations for local dev\n\nLatest translations are uploaded to S3. To get them, run:\n- `curl -o translations.tar https://donate-wagtail-translations.s3.amazonaws.com/translations.tar`\n- `tar -C network-api -xvf translations.tar`\n\nYou don't need to run `compilemessages`.\n\nThe `translations_github_commit_[...]` file from the archive is only used for debug purposes on Heroku. It can be safely deleted if needed.\n"
},
{
  "name": "github-org-scripts",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      ".jupyter",
      ".pre-commit-config.yaml",
      ".secrets.baseline",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "audit-log",
      "block_user",
      "cache",
      "check_CoC.py",
      "client.py",
      "close_pull_requests.py",
      "close_pull_requests.yaml",
      "code_search.py",
      "contributing.py",
      "get_PAT_owner",
      "get_active_hooks.py",
      "get_olson_tz.sh",
      "get_org_info.py",
      "manage_invitations.py",
      "notebooks",
      "old_repos.py",
      "owner_actions_from_auditlog",
      "poetry.lock",
      "pyproject.toml",
      "remove_member_from_org.py",
      "repo-admins",
      "repo-pr-stats.py",
      "requirements.in",
      "requirements.txt",
      "set_secrets_in_env.sh",
      "team_update.py",
      "tests"
    ]
  },
  "makefile": "gitrev := $(shell git rev-parse --short=10 HEAD)\nVENV_NAME := venv\nnow := $(shell date --utc +%Y%m%dT%H%MZ)\ngithub3_version:=1.1.0-$(now)-$(gitrev)\nport := 8888\nimage_to_use := offboard-slim\ncontainer_user_name := jovyan\nSOPS_credentials := $(SECOPS_SOPS_PATH)/off-boarding.yaml\n\nDOCKER_OPTS :=\n\n.PHONY: help\nhelp:\n\t@echo \"Targets available\"\n\t@echo \"\"\n\t@echo \"    help          this message\"\n\t@echo \"    build         create a docker image based on working directory\"\n\t@echo \"    run           run a docker image previously created\"\n\t@echo \"    run-edit      run with modifiable current directory\"\n\t@echo \"    jupyter       run local (non docker) jupyter server for development (deprecated)\"\n\t@echo \"    $(VENV_NAME)  create a local virtualenv for old style development (deprecated)\"\n\n$(VENV_NAME):\n\tpython3 -m venv $@\n\t. $(VENV_NAME)/bin/activate && echo req*.txt | xargs -n1 pip install -r\n\t@echo \"Virtualenv created in $(VENV_NAME). You must activate before continuing.\"\n\tfalse\n\nSHELL := /bin/bash\n.PHONY: build debug_build\n# New build\nbuild: Dockerfile .dockerignore Makefile notebooks/*ipynb requirements*.txt\n\tdocker build --tag $(image_to_use):$(github3_version) --tag $(image_to_use):latest .\n# debug the build by not using buildkit - we also assume last one failed, so no need to tag prior\ndebug-build:\n\tDOCKER_BUILDKIT=0 docker build --tag $(image_to_use):debug .\n\n# For `run`, we use the configs baked into the image at the time of\n# the build, so we get what we expect.\n.PHONY: run\nrun:\n\t$(SHELL) -ce ' ( source set_secrets_in_env.sh $(SOPS_credentials); \\\n\t\texport TZ=$$(./get_olson_tz.sh) ; \\\n\t\tdocker run --rm --publish-all \\\n\t\t\t--env \"GITHUB_PAT\" \\\n\t\t\t--env \"CIS_CLIENT_ID\" \\\n\t\t\t--env \"CIS_CLIENT_SECRET\" \\\n\t\t\t--env \"TZ\" \\\n\t\t\t--env \"DOCKER_STACKS_JUPYTER_CMD=notebook\" \\\n\t\t\t--publish $(port):8888 \\\n\t\t\t$(image_to_use):latest \\\n\t\t& \\\n\t\tjob_pid=$$! ; \\\n\t\tsleep 5 ; \\\n\t\tdocker ps --filter \"ancestor=$(image_to_use)\" ; \\\n\t\twait $$job_pid ; \\\n\t) '\n\n# For `run-edit`, we're mapping the local notebooks directory onto the container's notebooks directory\n# This allows for editing the notebook, but still uses the baked in jupyter configuration (in $HOME)\n\n.PHONY: run-edit\nrun-edit:\n\t$(SHELL) -ce ' ( source set_secrets_in_env.sh $(SOPS_credentials); \\\n\t\texport TZ=$$(./get_olson_tz.sh) ; \\\n\t\tdocker run --rm --publish-all \\\n\t\t\t$(DOCKER_OPTS) \\\n\t\t\t--env \"GITHUB_PAT\" \\\n\t\t\t--env \"CIS_CLIENT_ID\" \\\n\t\t\t--env \"CIS_CLIENT_SECRET\" \\\n\t\t\t--env \"TZ\" \\\n\t\t\t--env \"DOCKER_STACKS_JUPYTER_CMD=notebook\" \\\n\t\t\t--publish $(port):8888 \\\n\t\t\t--volume \"$$PWD/notebooks:/home/$(container_user_name)\"/notebooks \\\n\t\t\t$(image_to_use):latest \\\n\t\t& \\\n\t\tjob_pid=$$! ; \\\n\t\tsleep 10 ; \\\n\t\tdocker ps --filter \"ancestor=$(image_to_use):$(github3_version)\" ; \\\n\t\twait $$job_pid ; \\\n\t) '\n\n.PHONY: jupyter\njupyter:\n\t$(SHELL) -ce ' ( source set_secrets_in_env.sh $(SOPS_credentials); \\\n\t\tjupyter-notebook ; \\\n\t) '\n\n.PHONY: debug-edit\ndebug-edit:\n\t$(MAKE) DOCKER_OPTS=\"--security-opt=seccomp:unconfined\" run-edit\n\n\n# vim: noet ts=8\n",
  "readme": "# github org helper scripts\n\nCurrent minimums: python 3.7; GitHub3.py 1.3.0\n\n***Please use [poetry](https://pypi.org/project/poetry/) to manage virtual environments - `requirements.txt` may be out of date, and does not include development dependencies.***\n\nThese are some API helper scripts for sanely managing a github org. For now this is somewhat hardcoded for the mozilla org; no need for it to remain that way though. Many scripts support the `--help` option. That information should be more up to date than information in this document.\n\n## Credentials\n\nSupplying credentials for execution is done by passing a PAT token as the value\nof the environment variable `GITHUB_TOKEN` (preferred) or `GITHUB_PAT`.\n\nThe recommended way to set `GITHUB_TOKEN` is via cli access to your password\nmanager. For example, using [pass]:\n```bash\nGITHUB_TOKEN=$(pass show myPAT) script args\n```\n[pass]: https://www.passwordstore.org/\n\n## Jupyter Notebooks\n### Docker Images\n\nOur Jupyter Notebooks have a farely simple environment as regards dependencies. The recommended way to\ndeal with this is by using a docker container.\n\nThe Makefile contains targets for building and running the docker images. Invoke\n`make` without arguments to see those targets\n\n- **NOTE**: the docker image allows credentials to be supplied via [sops].\n  The environment variable \"`SECOPS_SOPS_PATH`\" must be set appropriately.\n\n[sops]: https://github.com/mozilla/sops\n\nWhen started, the docker container will serve notebooks from the `notebooks/`\ndirectory, but they will be available at the top level. Current notebooks include:\n\n- **`User Search.ipynb`** --\n    Given a set of possible GitHub logins, determine if they might have any\n    permissions in various organizations. Links are provided for hits, so easy\n    to examine more closely.\n\n    N.B.: Both this script and the GitHub search interface make assumptions. It\n    is *your* responsibility to ensure any possible match is a valid match.\n\n    There is now a section which will search for usernames in any non-documentation source file. The intent is to spot cases where app, login, or other permissions may have been granted via that file. Since such authorization usage is adhoc, there are likely to be many false positives. (However teams may choose to use the list for \"cleanup\" of unmaintained documents.) Typically, the user will want to supply both ldap and GitHub logins to be the search targets.\n\n## Scripts\n\nScripts should now work with Python 3. Please open issues for any problems you\nencounter.\n\n### auditlog.py\nDownload audit log for $ORG via headless firefox via selenium\n([`geckodriver`][gd_url] must be installed). Credentials as environment\nvariables, and 2FA token passed as input when requested.\n\n### contributing.py\nAnalyze all the \"sources\" repositories (i.e., those that aren't forks) in a github org and list the repositories that do *NOT* have a CONTRIBUTING file.\n\n### get_active_hooks.py\nFind all hooks configured for an organization -- see --help for details\n\n### get_org_info.py\nOutput basic info about an org, more if you have permissions. See --help for details\n\n### manage_invitations.py\nCancel all org & repository invitations older than a specified age (default 2\nweeks). See --help for details.\n\n### team_update.py\nUpdate administrative teams so they can be used for the new GitHub discussion\nfeature. Use the ``--help`` option for more information.\n\n#### hooks.py\nAnalyzes a list of audit log export files (from the JS script) for hook/service creation/deletion and provides a summary. Use it to show commonly used apps/services/webhooks across the org.\n\n### old_repos.py\nGenerate a list of empty (should be deleted) repositories as well as untouched repos (might need to be archived).\n\n## BUGS\n\n- Some of these scripts are no longer relevent.\n\n## License\nThis code is free software and licensed under an MPL-2.0 license. &copy; 2015-2021 Fred Wenzel and others. For more information read the file ``LICENSE``.\n\n[gd_url]: https://github.com/mozilla/geckodriver/releases\n"
},
{
  "name": "trackertest",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bug1108017.html",
      "cookie_access_test.html",
      "fxui-test.css",
      "fxui-test.jpg",
      "get_referrer.html",
      "green-tick.png",
      "index.html",
      "set_cookie.html",
      "storage_access_api.html",
      "tracker.js"
    ]
  },
  "makefile": null,
  "readme": "# trackertest\n\nThis repo contains the code behind <https://trackertest.org>.\n\n<https://itisatracker.org> and <https://itisatracker.com> are aliases which also point here.\n"
},
{
  "name": "donate-wagtail-locust",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.md",
      "dockerfiles",
      "k8s",
      "requirements.txt",
      "sample.env",
      "scripts",
      "stress_test.py"
    ]
  },
  "makefile": ".PHONY: all build_base build_web build_worker push_base push_web push_worker\n\nall: build_base push_base build_web push_web build_worker push_worker\n\nbuild_base:\n\tdocker build -f ./dockerfiles/Dockerfile.base -t cade/donate-locust:base-latest .\n\npush_base:\n\tdocker push cade/donate-locust:base-latest\n\nbuild_web:\n\tdocker build -f ./dockerfiles/Dockerfile.web -t cade/donate-locust:web-latest .\n\npush_web:\n\tdocker push cade/donate-locust:web-latest\n\nbuild_worker:\n\tdocker build -f ./dockerfiles/Dockerfile.worker -t cade/donate-locust:worker-latest .\n\npush_worker:\n\tdocker push cade/donate-locust:worker-latest",
  "readme": "donate-wagtail-locust\n---------------------\n\nThis project aims to help us in capacity planning for the new wagtail based donate platform we're launching in late 2019.\n\n### Running with Docker locally\n\n1. `docker build -t donate-locust .`\n2. Copy the sample environment file, and customize it to your needs: `cp sample.env .env`.\n3. `docker run --env-file .env -p 8089:8089 --add-host \"localhost:$HOST_IP_ADDR\" -it donate-locust`\n \n#### Environment variables\n\n| Variable           |                                                                             Description                                                                            |\n|--------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n| TARGET_URL         |  The hostname (including protocol) of the wagtail-donate stack to test. Required.                                                                                  |\n| LOCUST_MODE        | Start locust in the specified mode. One of `standalone`, `leader`, or `follower`. Optional, defaults to `standalone`                                               |\n| LOCUST_LEADER_HOST |  When running in distributed mode, this variable tells followers where to connect to the leader node. Required when `LOCUST_MODE` is `standalone`                  |\n| LOCUST_LEADER_PORT |  When running in distributed mode, this variable tells followers what port number the leader mode is listening for follower nodes on. optional, defaults to `5557` |"
},
{
  "name": "network-pulse",
  "files": {
    "/": [
      ".eslintrc.json",
      ".github",
      ".gitignore",
      ".mergify.yml",
      ".stylelintrc",
      ".stylelintrc-colors.js",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "PULL_REQUEST_TEMPLATE.md",
      "Procfile",
      "README.md",
      "app.json",
      "app.jsx",
      "appveyor.yml",
      "assets",
      "components",
      "config",
      "contribute.json",
      "js",
      "main.jsx",
      "manifest.json",
      "package-lock.json",
      "package.json",
      "pages",
      "scss",
      "server.js",
      "webpack.client.common.js",
      "webpack.client.dev.js",
      "webpack.client.prod.js",
      "webpack.server.common.js",
      "webpack.server.dev.js",
      "webpack.server.prod.js"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Pulse\n\n[![Travis Build Status](https://travis-ci.org/mozilla/network-pulse.svg?branch=master)](https://travis-ci.org/mozilla/network-pulse) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/mozilla/network-pulse?svg=true)](https://ci.appveyor.com/project/mozillafoundation/network-pulse)\n\nPulse is a platform that helps the Mozilla Network capture and broadcast its projects and activities. [mozillapulse.org](https://www.mozillapulse.org/featured)\n\nAround the globe, teachers, engineers, activists, and others collaborate to protect and extend the internet as a public resource. They create amazing apps, art, tools, games, and campaigns. Their impact is tremendous, but decentralized and difficult to track, even within the network.\n\nPulse reveals the network's footprint, it fosters collaboration and amplifies the big wins.\n\n## Contribute\n\nWe love contributors, but the team maintaining this project is small and not structured to significantly support new and inexperienced contributors. If there's an unassigned issue that catches your eye, feel free to open a PR for it, but keep in mind our support will be limited. We usually don't have the capacity to walk you through the process of spinning up the project, opening a PR or describing what the solution to the issue could be.\n\n## Development\n\n### Requirements\n\n- `node`\n- `npm`\n\n### Setup\n\n```bash\n$> git clone https://github.com/mozilla/network-pulse.git\n$> cd network-pulse\n$> npm install\n$> npm start\n```\n\n### Environments\n\nIf you would like to override default environment variables... create a `.env` file on the root directory and set your env vars there. See [environment variables section](https://github.com/mozilla/network-pulse#environment-variables) for details.\n\nThere are two files that you can expressly copy depending on your use:\n\n1. `default.env` has all the environment variables needed to run the pulse site without running a local pulse API instance.\n2. `default.localhost.env` has all the environment variables needed to run the pulse site in a way that it uses a locally running pulse API instance.\n\nIf you need (2), make sure to follow the instructions in the network pulse api README.md that explain how to set up Google Authentication.\n\n### Key scripts to run\n\n#### `npm start`\nThis starts server in development mode. See [environment variables section](https://github.com/mozilla/network-pulse#environment-variables) for `PORT` number.\n\n#### `npm test`\nThis starts a few test scripts. Don't forget to run this command and fix errors (if any) before you git push your changes.\n\n##### Fixing linting errors\n\nIf `npm test` yields linting errors, you can run `npm run fix` to have the style linters try to automatically fix any linting issues. This should almost always be enough to fix any linting errors.\n\n#### `npm optimize`\nThis starts a few image optimization scripts.\n\n### Environment variables\n\n   Name | Description\n------------------|---------------------------------------------\n`PORT` | Default: `process.env.PORT`(falls back to `3000` if `process.env.PORT` cannot be found)<br><br>The port number you are running the server on.\n`PULSE_API_HOST` | Default: `https://pulse-api.mofostaging.net`<br><br>Host of the Pulse API URL.\n`PULSE_API` | Default: `https://pulse-api.mofostaging.net/api/pulse`<br><br>URL to Pulse API. e.g., `http://test.example.com:8000/api/pulse`. <br>To set up a local instance of Pulse API, follow instructions on [Pulse API README doc](https://github.com/mozilla/network-pulse-api/blob/master/README.md).\n`PULSE_LOGIN_URL` | Default: `https://pulse-api.mofostaging.net/accounts/login/`<br><br>URL to use to login to Pulse. This needs to be a Pulse API login url.\n`PULSE_LOGOUT_URL` | Default: `https://pulse-api.mofostaging.net/accounts/logout/`<br><br>URL to use to logout of Pulse. This needs to be a Pulse API logout url.\n`USE_RECAPTCHA` | Default: `true`<br><br>Whether or not to have recaptcha securing the sign up/sign in action.\n`RECAPTCHA_KEY` | Default: empty string<br><br>The recaptcha site key to use, when recaptcha is enabled.\n`PROJECT_BATCH_SIZE`| Default: `24`<br><br>Number of projects you want to display as a batch. Make sure this number is divisible by 2 AND 3 so rows display evenly for different screen sizes.\n`PROFILE_BATCH_SIZE`| Default: `10`<br><br>Number of profiles you want to display as a batch.\n`LEARN_MORE_LINK` | Default: `https://www.mozillapulse.org/entry/120`<br><br>Link to learn more about what Pulse project is about.\n`NODE_ENV` | Default: `development`<br><br>When this is set to `production`, it enables production specific express settings and middleware\n`APP_HOST` | Default: `localhost`<br><br>The domain which this app should serve. It's only used when `NODE_ENV` is set to `production`\n`HEROKU_APP_NAME` | Default: `\"\"`<br><br>The name of the review app (generated by Heroku).\n`GITHUB_TOKEN` | Default: `\"\"`<br><br>GitHub token used by the review app slack webhook.\n`SLACK_WEBHOOK` | Default: `\"\"`<br><br>Webhook of the Slack channel where the bot is posting.\n\n## Deployment\n\n### Staging\n\nURL: https://network-pulse-staging.herokuapp.com/\n\nUpdates to `master` branch automatically triggers staging deployment.\n\n### Production\n\nURL: https://mozillapulse.org\n\nDeployment is done using the Heroku pipeline. Simply click \"Promote To Production\".\n\n### Review Apps\n\n#### Review App for PRs\n\nOpening a PR will automatically create a Review App in the `network-pulse` pipeline. A slack bot posts credentials and links to Review Apps in to the `mofo-ra-pulse` Slack channel.\n\n*Note:* This only work for Mo-Fo staff: you will need to manually open a Review App on Heroku for PRs opened by external contributors.\n\n#### Review App for branches\n\nYou can manually create a review app for any branch pushed to this repo. It's useful if you want to test your code on Heroku without opening a PR yet.\n\nTo create one:\n- log into Heroku.\n- Go to the `network-pulse` pipeline.\n- Click on `+ New app` and select the branch you want to use.\n\nThe review app slack bot will post a message in the `mofo-ra-pulse` with links and credentials as soon as the review app is ready.\n\n#### Environment variables:\n\n- `GITHUB_TOKEN`: GITHUB API authentication.\n- `SLACK_WEBHOOK_RA`: Webhook to `mofo-ra-pulse`.\n- `STAGING_SERVER`: Boolean, set this to `True` on the staging server.\n\nNon-secret envs can be added to the `app.json` file. Secrets must be set on Heroku in the `Review Apps` (pipelines' `settings` tab).\n"
},
{
  "name": "firefox-translations-models",
  "files": {
    "/": [
      ".circleci",
      ".gitattributes",
      ".gitignore",
      "LICENSE",
      "README.md",
      "evaluation",
      "models",
      "registry.json",
      "scripts"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Translations models\nCPU-optimized NMT models for [Firefox Translations](https://github.com/mozilla-extensions/firefox-translations).\n\nThe model files are hosted using [Git LFS](https://docs.github.com/en/github/managing-large-files/versioning-large-files/about-git-large-file-storage).\n\n[prod](models/prod) - production quality models \n\n[dev](models/dev) - test models under development (can be of low quality or speed). \n\nWhen a dev model has satisfactory quality, it is moved to prod.\n\n# Automatic quality evaluation\n\n[Results for prod models](evaluation/prod/results.md)\n\n[Resutls for dev models](evaluation/dev/results.md)\n\nAutomatic evaluation is a part of pull request CI. \nIt uses Microsoft and Google translation APIs and pushes results back to the branch (not available for forks).\nIt is performed using [firefox-translations-evaluation](https://github.com/mozilla/firefox-translations-evaluation) tool.\n\n# Model training\n\nUse [Firefox Translations training pipeline](https://github.com/mozilla/firefox-translations-training) or [browsermt/students](https://github.com/browsermt/students/tree/master/train-student) recipe to train CPU-optimized models. They should have similar size and inference speed to already submitted models.\n\n## Training data\n\nDo not use [SacreBLEU](https://github.com/mjpost/sacrebleu) or [Flores](https://github.com/facebookresearch/flores) datasets as a part of training data, otherwise evaluation will not be correct.\n\nTo see SacreBLEU datasets run `sacrebleu --list`.\n\n# Model contribution\n\nAll models should be contributed to `dev` folder first.\n\n## By maintainers\n\nCreate a pull Request to `main` branch from another branch in this repo.\n\n## From forks\n\nCreate a Pull Request to `contrib` branch. \nWhen it is reviewed and merged, another pull request to `main` branch will be created by a maintainer to kick off automatic evaluation.\n\n## Local testing\n\nYou can run model evaluation locally by running `bash scripts/update-results.sh`. \nMake sure to set environment variables `GCP_CREDS_PATH` and `AZURE_TRANSLATOR_KEY` to use Google and Microsoft APIs.\nIf you want to run it with `bergamot` only, remove mentions of those variables from `bash scripts/update-results.sh` and remove `microsoft,google` from `scripts/eval.sh`. \n\n# Model deployment\n\nCreate a new release with a version tag `x.y.z` following semantic versioning.\n\nThe models will be automatically uploaded to GCS bucket `gs://bergamot-models-sandbox/x.y.z/`. \n\n# Model types\n\n## Vocabulary\n\nPrefix of the vocabulary file in the model registry:\n- `vocab.` - vocabulary is reused for the source and target languages\n- `srcvocab.` and `trgvocab.` - different vocabularies for the source and target languages\n\n## GEMM precision\n\nSuffix of the model file in the registry:\n- `intgemm8.bin`  - supports `gemm-precision: int8shiftAll` inference setting\n- `intgemm.alphas.bin` - supports `gemm-precision: int8shiftAlphaAll` inference setting\n\n## \n\n# Currently supported Languages\n\n## Prod\n- Spanish <-> English\n- Estonian <-> English\n- English <-> German\n- Czech <-> English\n- Bulgarian <-> English\n- Norwegian Bokm\u00e5l -> English\n- Portuguese <-> English\n- Italian <-> English\n- Polish <-> English\n- French <-> English\n\n## Dev\n- Russian <-> English\n- Persian (Farsi) <-> English\n- Icelandic -> English\n- Norwegian Nynorsk -> English\n- Ukrainian <-> English\n\n## Upcoming\n- Dutch <-> English\n"
},
{
  "name": "probe-scraper",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".flake8",
      ".gce_boto",
      ".gcloudignore",
      ".github",
      ".gitignore",
      ".yamllint",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "conftest.py",
      "docker-compose.yml",
      "docs.png",
      "main.py",
      "netlify.toml",
      "notebooks",
      "probe_scraper",
      "probeinfo_api.yaml",
      "pytest.ini",
      "repositories.yaml",
      "requirements.txt",
      "schemas",
      "setup.py",
      "test_requirements.txt",
      "tests"
    ],
    "/.github": [
      "CODEOWNERS",
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: help clean lint test build docker-rm shell run stop apidoc\n\nhelp:\n\t@echo \"  apidoc                 Render the API documentation locally to index.html\"\n\t@echo \"  clean                  Remove build artifacts\"\n\t@echo \"  check-repos            Verify all repositories in repositories.yaml are scrapable\"\n\t@echo \"  lint                   Check style with flake8\"\n\t@echo \"  format                 Format code with black and isort\"\n\t@echo \"  test                   Run tests quickly with the default Python\"\n\t@echo \"  build                  Builds the docker images for the docker-compose setup\"\n\t@echo \"  docker-rm              Stops and removes all docker containers\"\n\t@echo \"  shell                  Opens a Bash shell\"\n\t@echo \"  run                    Run a command. Can run scripts, e.g. make run COMMAND=\\\"./scripts/schema_generator.sh\\\"\"\n\t@echo \"  stop                   Stop docker compose\"\n\nclean: clean-build clean-pyc docker-rm\n\nclean-build:\n\trm -fr build/\n\trm -fr dist/\n\trm -fr *.egg-info\n\nclean-pyc:\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\napidoc:\n\t# Keep in sync with doc task in .circleci/config.yml\n\tdocker run --rm \\\n\t\t-v ${PWD}:/local \\\n\t\tnode:15.5.1-alpine3.12 \\\n\t\tsh -c \"npm install -g redoc-cli; redoc-cli bundle --options.expandResponses=200,201 --options.jsonSampleExpandLevel=2 /local/probeinfo_api.yaml generate -o /local/index.html\"\n\nformat:\n\tpython3 -m black probe_scraper tests ./*.py\n\tpython3 -m isort --profile black probe_scraper tests ./*.py\n\nlint: build\n\tdocker-compose run app flake8 --max-line-length 100 .\n\tdocker-compose run app yamllint repositories.yaml .circleci\n\tdocker-compose run app python -m black --check probe_scraper tests ./*.py\n\tdocker-compose run app python -m isort --profile black --check-only probe_scraper tests ./*.py\n\ncheck-repos:\n\tdocker-compose run app python -m probe_scraper.check_repositories\n\ntest: build\n\tdocker-compose run app pytest tests/ --run-web-tests\n\n# For this test, we scrape glean-deprecated so we can also test the dependency resolution logic\n# (burnham depends on glean-deprecated) which we use to validate against duplicate metrics\n# and failed in mozilla/probe-scraper#283\nburnham-dryrun:\n\tdocker-compose run app python -m probe_scraper.runner --glean --glean-repo glean-core --glean-repo glean-android --glean-repo burnham --dry-run\n\nbuild:\n\tdocker-compose build\n\ndocker-rm: stop\n\tdocker-compose rm -f\n\nshell:\n\tdocker-compose run --entrypoint \"/bin/bash\" app\n\nrun: build\n\tdocker-compose run app $(COMMAND)\n\nstop:\n\tdocker-compose down\n\tdocker-compose stop\n",
  "readme": "# probe-scraper\nScrape Telemetry probe data from Firefox repositories.\n\nThis extracts per-version Telemetry probe data for Firefox and other Mozilla products from registry files like Histograms.json and Scalars.yaml.\nThe data allows answering questions like \"which Firefox versions is this Telemetry probe in anyway?\".\nAlso, probes outside of Histograms.json - like the CSS use counters - are included in the output data.\n\nThe data is pulled from two different sources:\n- From [`hg.mozilla.org`](https://hg.mozilla.org) for Firefox data.\n- From a [configurable set of Github repositories](repositories.yaml) that use [Glean](https://github.com/mozilla-mobile/android-components/tree/master/components/service/glean).\n\nProbe Scraper outputs JSON to https://probeinfo.telemetry.mozilla.org.\nEffectively, this creates a REST API which can be used by downstream tools like\n[mozilla-schema-generator](https://github.com/mozilla/mozilla-schema-generator)\nand various data dictionary type applications (see below).\n\nAn [OpenAPI reference](https://mozilla.github.io/probe-scraper/) to this API is available:\n\n<a href=\"https://mozilla.github.io/probe-scraper/\" rel=\"probeinfo API docs\">![probeinfo API docs](docs.png)</a>\n\nA web tool to explore the Firefox-related data is available at [probes.telemetry.mozilla.org](https://probes.telemetry.mozilla.org/). A project to develop a similar view for Glean-based data\nis under development in the [Glean Dictionary](https://github.com/mozilla/glean-dictionary).\n\n\n## Adding a New Glean Repository\n\nTo scrape a git repository for probe definitions, an entry needs to be added in `repositories.yaml`.\nThe exact format of the entry depends on whether you are adding an application or a library. See below for details.\n\n### Adding an application\n\nFor a given application, Glean metrics are emitted by the application itself, any libraries it uses\nthat also use Glean, as well as the Glean library proper. Therefore, probe scraper needs a way to\nfind all of the dependencies to determine all of the metrics emitted by\nthat application.\n\nTherefore, each application should specify a `dependencies` parameter, which is a list of Glean-using libraries used by the application.  Each entry should be a library name as specified by the library's `library_names` parameter.\n\nFor Android applications, if you're not sure what the dependencies of the application are, you can run the following command at the root of the project folder:\n\n```bash\n$ ./gradlew :app:dependencies\n```\n\nSee the full [application schema documentation](https://mozilla.github.io/probe-scraper/#tag/application)\nfor descriptions of all the available parameters.\n\n### Adding a library\n\nProbe scraper also needs a way to map dependencies back to an entry in the\n`repositories.yaml` file. Therefore, any libraries defined should also include\ntheir build-system-specific library names in the `library_names` parameter.\n\nSee the full [library schema documentation](https://mozilla.github.io/probe-scraper/#tag/library)\nfor descriptions of all the available parameters.\n\n## Developing the probe-scraper\n\nYou can choose to develop using the container, or locally. Using the container will be slower, since changes will trigger a rebuild of the container.\nBut using the container method will ensure that your PR passes CircleCI build/test phases.\n\n### Local development\n\nYou may wish to,\ninstead of installing all these requirements in your global Python environment,\nstart by generating and activating a\n[Python virtual environment](https://docs.python.org/3/library/venv.html).\nThe `.gitignore` expects it to be called `ENV` or `venv`:\n```console\npython -m venv venv\n. venv/bin/activate\n```\n\nInstall the requirements:\n```\npip install -r requirements.txt\npip install -r test_requirements.txt\npython setup.py develop\n```\n\nRun tests. This by default does not run tests that require a web connection:\n```\npytest tests/\n```\n\nTo run all tests, including those that require a web connection:\n```\npytest tests/ --run-web-tests\n```\n\nTo test whether the code conforms to the style rules, you can run:\n```\npython -m black --check probe_scraper tests ./*.py\nflake8 --max-line-length 100 probe_scraper tests ./*.py\nyamllint repositories.yaml .circleci\npython -m isort --profile black --check-only probe_scraper tests ./*.py\n```\n\nTo render API documentation locally to `index.html`:\n```\nmake apidoc\n```\n\n### Developing using the container\n\nRun tests in container. This does not run tests that require a web connection:\n```\nexport COMMAND='pytest tests/'\nmake run\n```\n\nTo run all tests, including those that require a web connection:\n```\nmake test\n```\n\nTo test whether the code conforms to the style rules, you can run:\n```\nmake lint\n```\n\n### Tests with Web Dependencies\n\nAny tests that require a web connection to run should be marked with `@pytest.mark.web_dependency`.\n\nThese will not run by default, but will run on CI.\n\n### Performing a Dry-Run\n\nBefore opening a PR, it's good to test the code you wrote on the production data. You can specify a specific Firefox\nversion to run on by using `first-version`:\n```\nexport COMMAND='python -m probe_scraper.runner --firefox-version 65 --dry-run'\nmake run\n```\nor locally via:\n```\npython -m probe_scraper.runner --firefox-version 65 --dry-run\n```\n\nIncluding `--dry-run` means emails will not be sent.\n\nAdditionally, you can test just on Glean repositories:\n```\nexport COMMAND='python -m probe_scraper.runner --glean --dry-run'\nmake run\n```\n\nBy default that will test against every Glean repository, which might take a while. If you want to test against just one (e.g. a new repository you're adding), you can use the `--glean-repo` argument to just test the repositories you care about:\n```\nexport COMMAND='python -m probe_scraper.runner --glean --glean-repo glean-core --glean-repo glean-android --glean-repo burnham --dry-run'\nmake run\n```\n\nReplace burnham in the example above with your repository and its dependencies.\n\nYou can also do the dry-run locally:\n\n```\npython -m probe_scraper.runner --glean --glean-repo glean-core --glean-repo glean-android --glean-repo burnham --dry-run\n```\n\n## Module overview\n\nThe module is built around the following data flow:\n\n- scrape registry files from mozilla-central, clone files from repositories directory\n- extract probe data from the files\n- transform probe data into output formats\n- save to disk\n\nThe code layout consists mainly of:\n\n- `probe_scraper`\n  - `runner.py` - the central script, ties the other pieces together\n  - `scrapers`\n     - `buildhub.py` - pull build info from the [BuildHub](https://buildhub.moz.tools) service\n     - `moz_central_scraper.py` - loads probe registry files for multiple versions from mozilla-central\n     - `git_scraper.py` - loads probe registry files from a git repository (no version or channel support yet, just per-commit)\n  - `parsers/` - extract probe data from the registry files\n     - `third_party` - these are imported parser scripts from [mozilla-central](https://dxr.mozilla.org/mozilla-central/source/toolkit/components/telemetry/)\n   - `transform_*.py` - transform the extracted raw data into output formats\n- `tests/` - the unit tests\n\n## Accessing the data files\nThe processed probe data is serialized to the disk in a directory hierarchy starting from the provided output directory. The directory layout resembles a REST-friendly structure.\n\n    |-- product\n        |-- general\n        |-- revisions\n        |-- channel (or \"all\")\n            |-- ping type\n                |-- probe type (or \"all_probes\")\n\nFor example, all the JSON probe data in the [main ping]() for the *Firefox Nightly* channel can be accessed with the following path: `firefox/nightly/main/all_probes`. The probe data for all the channels (same product and ping) can be accessed instead using `firefox/all/main/all_probes`.\n\nThe root directory for the output generated from the scheduled job can be found at <https://probeinfo.telemetry.mozilla.org/>.\nAll the probe data for Firefox coming from the main ping can be found at <https://probeinfo.telemetry.mozilla.org/firefox/all/main/all_probes>.\n\n## Accessing `Glean` metrics data\nGlean data is generally laid out as follows:\n\n```\n| -- glean\n    | -- repositories\n    | -- general\n    | -- repository-name\n        | -- general\n        | -- metrics\n```\n\nFor example, the data for a repository called `fenix` would be found at [`/glean/fenix/metrics`](https://probeinfo.telemetry.mozilla.org/glean/fenix/metrics). The time the data was last updated for that project can be found at [`glean/fenix/general`](https://probeinfo.telemetry.mozilla.org/glean/fenix/general).\n\nA list of available repositories is at [`/glean/repositories`](https://probeinfo.telemetry.mozilla.org/glean/repositories).\n"
},
{
  "name": "opmon-config",
  "files": {
    "/": [
      ".circleci",
      "LICENSE",
      "README.md",
      "bug-1660366-pref-ongoing-fission-nightly-experiment-nightly-83-100.toml",
      "bug-1732206-rollout-fission-release-rollout-release-94-95.toml",
      "bug-1751039-pref-win32k-experiment-nightly-99-99.toml",
      "bug-1751307-pref-tab-unloading-on-low-memory-for-linux-release-97-98.toml",
      "bug-1751308-pref-tab-unloading-on-low-memory-for-macos-release-97-98.toml",
      "bug-1751309-pref-tab-unloading-on-low-memory-for-windows-release-97-98.toml",
      "bug-1759000-rollout-initial-rollout-of-tcp-release-91-99.toml",
      "bug-1759171-pref-win32k-experiment-v2-beta-99-100.toml",
      "defaults",
      "definitions",
      "mission-control-desktop-nightly.toml",
      "tcp-rollout-phase-2.toml",
      "tcp-rollout.toml",
      "telemetry-alerts-prototype.toml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# opmon-config\n\nCustom configs operational monitoring projects run via [opmon](https://github.com/mozilla/opmon).\n\n## Adding Custom Configurations\n\nCustom configuration files are written in [TOML](https://toml.io/en/).\nMore information on adding or changing configurations is available on [dtmo](https://docs.telemetry.mozilla.org/cookbooks/operational_monitoring.html).\n\nTo add or update a custom configuration, open a pull request.\nCI checks will validate the columns, data sources, and SQL syntax.\n\n"
},
{
  "name": "quitter",
  "files": {
    "/": [
      ".cron.yml",
      ".gitignore",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "background.js",
      "contentscript.js",
      "manifest.json",
      "parent.js",
      "schema.json",
      "taskcluster"
    ]
  },
  "makefile": null,
  "readme": "# quitter\nTest extension used to quit firefox\n"
},
{
  "name": "authenticator-rs",
  "files": {
    "/": [
      ".clippy.toml",
      ".flake8",
      ".gitignore",
      ".pre-commit-config.yaml",
      ".travis.yml",
      "Cargo.toml",
      "Cross.toml",
      "LICENSE",
      "README.md",
      "build.rs",
      "examples",
      "fuzz",
      "rustfmt.toml",
      "src",
      "testing",
      "webdriver-tools"
    ]
  },
  "makefile": null,
  "readme": "# A Rust library for interacting with CTAP1/CTAP2 Security Keys\n\n[![Build Status](https://travis-ci.org/mozilla/authenticator-rs.svg?branch=master)](https://travis-ci.org/mozilla/authenticator-rs)\n![Maturity Level](https://img.shields.io/badge/maturity-release-green.svg)\n\nThis is a cross-platform library for interacting with Security Key-type devices via Rust.\n\n* **Supported Platforms**: Windows, Linux, FreeBSD, NetBSD, OpenBSD, and macOS.\n* **Supported Transports**: USB HID.\n* **Supported Protocols**: [FIDO U2F over USB](https://fidoalliance.org/specs/fido-u2f-v1.1-id-20160915/fido-u2f-raw-message-formats-v1.1-id-20160915.html). [CTAP2 support](https://fidoalliance.org/specs/fido-v2.0-ps-20190130/fido-client-to-authenticator-protocol-v2.0-ps-20190130.html) is forthcoming, with work being done in the **unstable** [`ctap2` branch](https://github.com/mozilla/authenticator-rs/tree/ctap2).\n\nThis library currently focuses on USB security keys, but is expected to be extended to\nsupport additional transports.\n\n## Usage\n\nThere's only a simple example function that tries to register and sign right now. It uses\n[env_logger](http://rust-lang-nursery.github.io/log/env_logger/) for logging, which you\nconfigure with the `RUST_LOG` environment variable:\n\n```\ncargo build --example main\nRUST_LOG=debug cargo run --example main\n```\n\nProper usage should be to call into this library from something else - e.g., Firefox. There are\nsome [C headers exposed for the purpose](./src/u2fhid-capi.h).\n\n## Tests\n\nThere are some tests of the cross-platform runloop logic and the protocol decoder:\n\n```\ncargo test\n```\n\n## Fuzzing\n\nThere are fuzzers for the USB protocol reader, basically fuzzing inputs from the HID layer.\nThere are not (yet) fuzzers for the C API used by callers (such as Gecko).\n\nTo fuzz, you will need cargo-fuzz (the latest version from GitHub) as well as Rust Nightly.\n\n```\nrustup install nightly\ncargo install cargo-fuzz\n\ncargo +nightly fuzz run u2f_read -- -max_len=512\ncargo +nightly fuzz run u2f_read_write -- -max_len=512\n```\n"
},
{
  "name": "audioipc",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "Cargo.toml",
      "README.md",
      "audioipc",
      "client",
      "ipctest",
      "rustfmt.toml",
      "server"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Cubeb Audio Remoting Prototype\n"
},
{
  "name": "rust-code-analysis",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".pre-commit-audit-config.yaml",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "Cargo.lock",
      "Cargo.toml",
      "README.md",
      "check-grammar-crate.py",
      "check-grammars-crates.sh",
      "enums",
      "generate-grammars",
      "recreate-grammars.sh",
      "rust-code-analysis-book",
      "rust-code-analysis-cli",
      "rust-code-analysis-web",
      "split-minimal-tests.py",
      "src",
      "tests",
      "tree-sitter-ccomment",
      "tree-sitter-mozcpp",
      "tree-sitter-mozjs",
      "tree-sitter-preproc"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# rust-code-analysis\n\n[![Task Status](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/rust-code-analysis/master/badge.svg)](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/rust-code-analysis/master/latest)\n[![codecov](https://codecov.io/gh/mozilla/rust-code-analysis/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/rust-code-analysis)\n<a href=\"https://chat.mozilla.org/#/room/#rust-code-analysis:mozilla.org\" target=\"_blank\">\n   <img src=\"https://img.shields.io/badge/chat%20on%20[m]-%23rust--code--analysis%3Amozilla.org-blue\">\n</a>\n\n**rust-code-analysis** is a Rust library to analyze and extract information\nfrom source code written in many different programming languages.\nIt is based on a parser generator tool and an incremental parsing library\ncalled\n<a href=\"https://tree-sitter.github.io/tree-sitter/\" target=\"_blank\">Tree Sitter</a>.\n\n\nA command line tool called **rust-code-analysis-cli** is provided to interact with the API of the library in an easy way.\n\nThis tool can be used to:\n\n- Call **rust-code-analysis** API\n- Print nodes and metrics information\n- Export metrics in different formats\n\nIn addition, we provide a **rust-code-analysis-web** tool to use the library through a REST API.\n\n\n# Usage\n\n**rust-code-analysis** supports many types of programming languages and\ncomputes a great variety of metrics. You can find up to date documentation at\n<a href=\"https://mozilla.github.io/rust-code-analysis/index.html\" target=\"_blank\">Documentation</a>.\n\nOn the\n<a href=\"https://mozilla.github.io/rust-code-analysis/commands/index.html\" target=\"_blank\">\n    Commands\n</a> page, there is a list of commands that can be run to get information\nabout metrics, nodes, and other general data provided by this software.\n\n## Building\n\nTo build the `rust-code-analysis` library, you need to run the following\ncommand:\n\n```console\ncargo build\n```\n\nIf you want to build the `cli`:\n\n```console\ncargo build -p rust-code-analysis-cli\n```\n\nIf you want to build the `web` server:\n\n```console\ncargo build -p rust-code-analysis-web\n```\n\nIf you want to build everything in one fell swoop:\n\n```console\ncargo build --workspace\n```\n\n## Testing\n\nTo verify whether all tests pass, run the `cargo test` command.\n\n```console\ncargo test --workspace --all-features --verbose\n```\n\n# Contributing\n\nIf you want to contribute to the development of this software, have a look at the\nguidelines contained in our\n<a href=\"https://mozilla.github.io/rust-code-analysis/developers/index.html\" target=\"_blank\">Developers Guide</a>.\n\n\n# How to cite rust-code-analysis\n\n```\n@article{ARDITO2020100635,\n    title = {rust-code-analysis: A Rust library to analyze and extract maintainability information from source codes},\n    journal = {SoftwareX},\n    volume = {12},\n    pages = {100635},\n    year = {2020},\n    issn = {2352-7110},\n    doi = {https://doi.org/10.1016/j.softx.2020.100635},\n    url = {https://www.sciencedirect.com/science/article/pii/S2352711020303484},\n    author = {Luca Ardito and Luca Barbato and Marco Castelluccio and Riccardo Coppola and Calixte Denizet and Sylvestre Ledru and Michele Valsesia},\n    keywords = {Algorithm, Software metrics, Software maintainability, Software quality},\n    abstract = {The literature proposes many software metrics for evaluating the source code non-functional properties, such as its complexity and maintainability. The literature also proposes several tools to compute those properties on source codes developed with many different software languages. However, the Rust language emergence has not been paired by the community\u2019s effort in developing parsers and tools able to compute metrics for the Rust source code. Also, metrics tools often fall short in providing immediate means of comparing maintainability metrics between different algorithms or coding languages. We hence introduce rust-code-analysis, a Rust library that allows the extraction of a set of eleven maintainability metrics for ten different languages, including Rust. rust-code-analysis, through the Abstract Syntax Tree (AST) of a source file, allows the inspection of the code structure, analyzing source code metrics at different levels of granularity, and finding code syntax errors before compiling time. The tool also offers a command-line interface that allows exporting the results in different formats. The possibility of analyzing source codes written in different programming languages enables simple and systematic comparisons between the metrics produced from different empirical and large-scale analysis sources.}\n}\n```\n\n\n# Licenses\n\n- Mozilla-defined grammars are released under the MIT license.\n\n- **rust-code-analysis**, **rust-code-analysis-cli** and **rust-code-analysis-web**\nare released under the\n<a href=\"https://www.mozilla.org/MPL/2.0/\" target=\"_blank\">Mozilla Public License v2.0</a>.\n"
},
{
  "name": "addons",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE",
      "README.md",
      "default-github-labels.json",
      "docs",
      "releases",
      "requirements"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "browser",
      "conf.py",
      "images",
      "index.rst",
      "l10n.rst",
      "make.bat",
      "random",
      "repositories.rst",
      "server",
      "ux"
    ],
    "/.github": [
      "CODE_OF_CONDUCT.md",
      "ISSUE_TEMPLATE.md",
      "stale.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Add-ons\n\nThis is the general root project for add-ons at Mozilla.\n\n## Filing bugs\n\nIf you're looking to file a bug relating to the addons.mozilla.org (AMO)\nwebsite then please [file an issue on this project](https://github.com/mozilla/addons/issues/new).\n\n## Security Bug Reports\n\nThe associated production website at addons.mozilla.org is included in Mozilla\u2019s web and services [bug bounty program]. If you find a security vulnerability, please submit it via the process outlined in the program and [FAQ pages]. Further technical details about this application are available from the [Bug Bounty Onramp page].\n\nPlease submit all security-related bugs through Bugzilla using the [web security bug form].\n\nNever submit security-related bugs through a Github Issue or by email.\n\n  [bug bounty program]: https://www.mozilla.org/en-US/security/web-bug-bounty/\n  [FAQ pages]: https://www.mozilla.org/en-US/security/bug-bounty/faq-webapp/\n  [Bug Bounty Onramp page]: https://wiki.mozilla.org/Security/BugBountyOnramp/\n  [web security bug form]: https://bugzilla.mozilla.org/form.web.bounty\n\n## Documentation\n\nThis project contains general documentation relating to the Add-ons project.\n[You can view the docs here](https://addons.readthedocs.io).\n\nIf you would like to contribute to the docs, here's how to set up a development\nenvironment:\n\nUsing [Python](https://www.python.org/)\nand optionally [virtualenv](https://virtualenv.pypa.io/en/latest/),\ninstall the dependencies:\n\n    pip install -r requirements/docs.txt\n\nMake some changes then build the docs:\n\n    make -C docs/ html\n\nOpen `docs/_build/html/index.html` to preview your changes before creating a\npull request.\n"
},
{
  "name": "check-auth0-applications-missing-authorization",
  "files": {
    "/": [
      "Makefile",
      "README.md",
      "check-auth0-applications-missing-authorization.yml",
      "deploy.sh",
      "functions"
    ]
  },
  "makefile": "STACK_NAME\t:= CheckAuth0Apps\nPROD_LAMBDA_CODE_STORAGE_S3_BUCKET_NAME\t:= public.us-west-2.iam.mozilla.com\nCODE_STORAGE_S3_PREFIX\t:= check-auth0-applications-missing-authorization\n\n.PHONE: deploy\ndeploy:\n\t./deploy.sh \\\n\t\t check-auth0-applications-missing-authorization.yml \\\n\t\t $(PROD_LAMBDA_CODE_STORAGE_S3_BUCKET_NAME) \\\n\t\t $(STACK_NAME) \\\n\t\t $(CODE_STORAGE_S3_PREFIX)\n",
  "readme": "# check-auth0-applications-missing-authorization\n\nThis is an AWS Lambda tool that runs daily to detect Auth0 clients that\nwere created but are mistakenly missing authorization information in [`apps.yml`](https://github.com/mozilla-iam/sso-dashboard-configuration/blob/master/apps.yml) \n\nIf any misconfigured clients are found, the tool sends a Pagerduty alert.\n\n# Deployment\n\nAuthenticate to AWS on the command line. For Mozilla's deployment this can be\ndone with [`maws`](https://github.com/mozilla-iam/mozilla-aws-cli).\n\nRun `make deploy` which will spawn the `deploy.sh` script and pass it the\narguments for Mozilla's deployment of the tool. This script will deploy a\nCloudFormation stack which provisions the AWS Lambda function, the AWS\nEventBridge (CloudWatch Events) Rule to trigger the function every day at noon \npacific time, as well as the required IAM Role and permissions to enable\nEventBridge to invoke the function and for the function to read configuration\nparameters from AWS SSM ParameterStore.\n\nTo update the already deployed tool if you make code changes, just run\n`make deploy` which will update the already deployed CloudFormation stack.\n\n# Configuration\n\nThe tool is hard coded to \n\n* use the production Mozilla Auth0 tenant (`auth.mozilla.auth0.com`)\n* trigger every day at noon pacific time\n* compare against the Mozilla deployed `apps.yml` file at https://cdn.sso.mozilla.com/apps.yml\n* only consider specific clients/applications in Auth0 that meet these \n  requirements\n  * are regular web applications or single page applications in Auth0\n    (ignoring machine to machine and native integrations)\n  * have at least one callback/redirect_uri configured (if no callback is\n    configured, then no logins would work and so it can be ignored)\n  * are not present in the `temp_client_id_ignore_list` configuration values\n    which list out existing clients which are missing authorization.\n\nThe following settings can be configured in AWS SSM ParameterStore. The Mozilla\ndeployment of the tool runs in the `mozilla-iam` AWS account in `us-west-2`\nwhich is where these configuration parameters can be found.\n\n* `/iam/check_auth0_applications_missing_authorization/production/auth0_management_api_client_id`\n  * The Auth0 client ID for the `Management API - check_auth0_applications_missing_authorization`\n    Auth0 client which is used to query the [Auth0 management API](https://auth0.com/docs/api/management/v2#!/Clients/get_clients)\n    for the list of clients/applications.\n* `/iam/check_auth0_applications_missing_authorization/production/auth0_management_api_client_secret`\n  * The Auth0 client secret for the `Management API - check_auth0_applications_missing_authorization`\n    Auth0 client.\n* `/iam/check_auth0_applications_missing_authorization/production/pagerduty_integration_key`\n  * The PagerDuty API Key for the [PagerDuty Events API v2](https://developer.pagerduty.com/docs/ZG9jOjExMDI5NTgw-events-api-v2-overview)\n    which the tool uses to alert when it discovers Auth0 clients/applications\n    that are missing an authorization entry in `apps.yml`\n* `/iam/check_auth0_applications_missing_authorization/production/temp_client_id_ignore_list/*`\n  * A collection of parameters (e.g. `/iam/check_auth0_applications_missing_authorization/production/temp_client_id_ignore_list/1`)\n    which contain a comma delimted list of Auth0 client IDs to ignore. These are\n    existing Auth0 clients/applications that are missing authorization entries\n    in `apps.yml`. Once these clients are all addressed and added into \n    `apps.yml` the `temp_client_id_ignore_list` parameters can be deleted.\n\n# Current PagerDuty Service\n\n[IAM Relying Party Check](https://mozilla.pagerduty.com/service-directory/P2DSVSF)\n\n# PagerDuty Alert\n\nEach pagerduty alert has a deduplication key made up of a hash of the sorted\nclient IDs of the discovered misconfigured clients/applications.\n\nOnce the problem is fixed by adding authorization entries to `apps.yml`, the\nPagerDuty alert will need to be manually resolved in PagerDuty by the PagerDuty\nresponder as it can't automatically be cleared.\n\n## Example PagerDuty alert\n\n* Title \n  * New Auth0 client(s)/app(s) created but missing authorization configuration\n* Body \n  * The following new Auth0 client(s)/app(s) have been found but are missing \n    from \"apps.yml\" authorization configuration. Entries must be created in \n    \"apps.yml\" to assert what groups should have access to these relying \n    parties.\n  * Example RP : abcdefghijklmnopqrstuvwxyz012345\n\n# Logs\n\nLogs of the most recent daily run of the tool can be found in an AWS CloudWatch\nLogGroup with a name beginning with `/aws/lambda/CheckAuth0Apps-LambdaFunction-`\n"
},
{
  "name": "docker-test-mozilla-django-oidc",
  "files": {
    "/": [
      ".circleci",
      "CODE_OF_CONDUCT.md",
      "HISTORY.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "dockerfiles",
      "testprovider",
      "testrp"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "DEFAULT_GOAL := help\n\nNS ?= mozilla/oidc-testprovider\nIMAGES := oidc_testprovider oidc_testrunner oidc_testrp_py3 oidc_e2e_setup_py37 oidc_e2e_setup_py38 oidc_e2e_setup_py39 oidc_e2e_setup_py310\nBUILD := $(addprefix build-,${IMAGES})\nPULL := $(addprefix pull-,$(IMAGES))\nCLEAN := $(addprefix clean-,$(IMAGES))\n\n.PHONY: help\nhelp:\n\t@fgrep -h \"##\" Makefile | fgrep -v fgrep | sed 's/\\(.*\\):.*##/\\1:/'\n\n.PHONY: build\nbuild: ${BUILD} ## Build all images\n\n.PHONY: pull\npull: ${PULL} ## Pull all -latest images\n\n.PHONY: clean\nclean: ${CLEAN} ## Clean images and other artifacts\n\n.PHONY: ${BUILD}\n${BUILD}: build-%:\n\tdocker build -t $* -f dockerfiles/$* .\n\n.PHONY: ${PULL}\n${PULL}: pull-%:\n\tdocker pull ${NS}:$*-latest\n\n.PHONY: ${CLEAN}\n${CLEAN}: clean-%:\n\tdocker rmi ${NS}/$(subst _py,:py,$(*))\n\tdocker rmi $(subst _py,:py,$(*))\n",
  "readme": "# docker-test-mozilla-django-oidc\n\nThe purpose of these docker images is to setup a local environment to develop and test\n`mozilla-django-oidc`.\n\n\n## oidc-testprovider\n\nhttps://hub.docker.com/r/mozilla/oidc-testprovider/tags?name=testprovider\n\n* Provides a docker image for an OIDC OP with preconfigured OIDC client IDs and secrets\n* OIDC provider endpoint is exposed in port `8080`\n* Provides a Django management command for creating users\n* Uses `django-oidc-provider`\n* Great for local development environments!\n\n\n### Usage\n\nIn order for this setup to work `testprovider`, `testrp` hostnames should resolve to the\nIP of the docker image (for local development it's `127.0.0.1`).\n\nYou can add the resolution to your `/etc/hosts` file.\n\nYou can also use [nip.io](http://nip.io/). For example, if you name the service\n\"oidcprovider\", then you could have these three variables:\n\n```\nOIDC_OP_AUTHORIZATION_ENDPOINT=http://oidcprovider.127.0.0.1.nip.io:8080/openid/authorize\nOIDC_OP_TOKEN_ENDPOINT=http://oidcprovider.127.0.0.1.nip.io:8080/openid/token\nOIDC_OP_USER_ENDPOINT=http://oidcprovider.127.0.0.1.nip.io:8080/openid/userinfo\n```\n\n### Example setup\n\n`docker-compose.yml`\n\n```\nversion: '3'\nservices:\n  testprovider:\n    image: mozilla/oidc-testprovider:oidc_testprovider-v0.9.3\n    ports:\n      - \"8080:8080\"\n```\n\n\n### Creating users in the container\n\nThe `testprovider` image has a Django management command for creating users in\nthe OIDC provider. This lets you create users on the command line.\n\nWith an already running `testprovider` container run:\n\n```\ndocker-compose exec testprovider manage.py createuser USERNAME PASSWORD EMAIL\n```\n\n\n## oidc_e2e_setup_py*\n\nhttps://hub.docker.com/r/mozilla/oidc-testprovider/tags?name=e2e&page=1\n\nThese images are used for the end-to-end testing for [mozilla-django-oidc](https://github.com/mozilla/mozilla-django-oidc).\n\nWe will maintain an image for each supported Python version. Currently, that's:\n\n* Python 3.7\n* Python 3.8\n* Python 3.9\n* Python 3.10\n\n\n## Other images generated by this project\n\nAll images are pushed to: https://hub.docker.com/r/mozilla/oidc-testprovider\n\n* `oidc_testrunner`\n* `oidc_testrp_py3`\n    * Test django project preconfigured to work with `testprovider`\n    * Uses `mozilla-django-oidc` as an authentication backend\n    * Test RP is exposed in port `8081`\n    * Environment variables\n        * `TEST_OIDC_ALGO={hs,rs}`\n\n\n### Example setup for oidc_testrp\n\n`docker-compose.yml`\n\n```\nversion: '3'\nservices:\n  testrp:\n    image: mozilla/oidc-testprovider:oidc_testrp_py3-v0.9.3\n    ports:\n      - \"8081:8081\"\n    environment:\n      - TEST_OIDC_ALGO=hs\n```\n\n\n## Development\n\nWe use `make` to automate the docker image workflow.\n\nFor more info run `make help`.\n\nPushing a tag to GitHub will trigger building images and uploading them\nto Dockerhub.\n"
},
{
  "name": "nucleus",
  "files": {
    "/": [
      ".adr-dir",
      ".coveragerc",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".gitlab-ci.yml",
      ".pre-commit-config.yaml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "contribute.json",
      "docker-compose.yml",
      "docker",
      "docs",
      "jenkins",
      "manage.py",
      "newrelic.ini",
      "nucleus",
      "pyproject.toml",
      "requirements",
      "root_files",
      "setup.cfg",
      "setup.py"
    ],
    "/docs": [
      "Makefile",
      "architecture",
      "conf.py",
      "index.rst",
      "make.bat"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": "DC_CI = bin/dc.sh\nDC = docker-compose\n\nall: help\n\n.env:\n\t@touch .env\n\n.make.docker.build:\n\t${MAKE} build\n\n.make.docker.pull:\n\t${MAKE} pull\n\nbuild: .make.docker.pull\n\t${DC} build --pull web\n\t@touch .make.docker.build\n\npull: .env\n\t-GIT_COMMIT= ${DC} pull db web builder\n\t@touch .make.docker.pull\n\nrun: .make.docker.pull\n\t${DC} up web worker\n\nrun-shell:\n\t${DC} run --rm web bash\n\nshell:\n\t${DC} exec web bash\n\ndjshell:\n\t${DC} exec web python manage.py shell_plus\n\nstop:\n\t${DC} stop\n\nkill:\n\t${DC} kill\n\nclean:\n#\tpython related things\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -rf {} +\n#\ttest related things\n\t-rm -f .coverage\n#\tdocs files\n\t-rm -rf docs/_build/\n#\tstate files\n\t-rm -f .make.*\n\nlint: .make.docker.pull\n\t${DC} run test flake8\n\ntest: .make.docker.pull\n\t${DC} run --rm test\n\ntest-image: .make.docker.build\n\t${DC} run --rm test-image\n\ndocs: .make.docker.pull\n\t${DC} run --rm web make -C docs/ clean html\n\ncheck-requirements: .make.docker.pull\n\t${DC} run --rm test pip list -o\n\ncompile-requirements: .make.docker.pull\n\t${DC} run --rm compile-requirements\n\ninstall-local-python-deps:\n\tpip install -r requirements/dev.txt\n\n###############\n# For use in CI\n###############\n.make.docker.build.ci:\n\t${MAKE} build-ci\n\nclean-ci: clean .env\n\t${DC_CI} down -v\n\nbuild-ci: .make.docker.pull\n\t${DC_CI} build --pull web\n#\ttag intermediate images using cache\n\t${DC_CI} build builder\n\t@touch .make.docker.build.ci\n\ntest-ci: .make.docker.build.ci\n\t${DC_CI} run --rm test-image\n\npush-ci: .make.docker.build.ci\n\tdocker/bin/push2dockerhub.sh\n\n######################################################\n# For use in local-machine development (not in Docker)\n######################################################\n\ninstall-local-python-deps:\n\t# Dev requirements are a superset of prod requirements\n\tpip install -r requirements/dev.txt\n\n\nhelp:\n\t@echo \"Please use \\`make <target>' where <target> is one of\"\n\t@echo \"  run                       - docker-compose up the entire system for dev\"\n\t@echo \"  build                     - build docker images for dev\"\n\t@echo \"  pull                      - pull the latest production images from Docker Hub\"\n\t@echo \"  run-shell                 - open a bash shell in a fresh container\"\n\t@echo \"  shell                     - open a bash shell in the running app\"\n\t@echo \"  djshell                   - start the Django Python shell in the running app\"\n\t@echo \"  clean                     - remove all build, test, coverage and Python artifacts\"\n\t@echo \"  lint                      - check style with flake8, jshint, and stylelint\"\n\t@echo \"  test                      - run tests against local files\"\n\t@echo \"  test-image                - run tests against files in docker image\"\n\t@echo \"  docs                      - generate Sphinx HTML documentation\"\n\t@echo \"  build-ci                  - build docker images for use in our CI pipeline\"\n\t@echo \"  test-ci                   - run tests against files in docker image built by CI\"\n\t@echo \"  check-requirements        - output list of outdated Python requirements\"\n\t@echo \"  compile-requirements      - compile pip requirements using pip-compile-multi\"\n\t@echo \"  install-local-python-deps - install Python dependencies onto your development machine\"\n\n.PHONY: all clean clean-ci build pull docs lint run run-shell shell test test-image build-ci test-ci push-ci djshell stop kill check-requirements compile-requirements install-local-python-deps\n",
  "readme": "Nucleus\n=======\n\nThe publication platform for Mozilla's marketing websites.\n\nDocker for development\n----------------------\n\nMake sure you have [docker](https://www.docker.com/products/docker-desktop) and \n[docker-compose](https://github.com/docker/compose). After those are setup and running\nyou can use the following commands:\n\n```bash\n$ # This file must exist and you can customize environment variables for local dev in it\n$ touch .env\n$ # this pulls our latest builds from the docker hub.\n$ # it's optional but will speed up your builds considerably.\n$ docker-compose pull\n$ # get the site up and running\n$ docker-compose up web\n```\n\nIf you've made changes to the `Dockerfile` or the `requirements/*.txt` files you'll need to rebuild\nthe image to run the app and tests:\n\n```bash\n$ docker-compose build web\n```\n\nThen to run the app you run the `docker-compose up web` command again, or for running tests against your local changes you run:\n\n```bash\n$ docker-compose run --rm test\n```\n\nWe use pytest for running tests. So if you'd like to craft your own pytest command to run individual test files or something\nyou can do so by passing in a command to the above:\n\n```bash\n$ docker-compose run --rm test py.test nucleus/base/tests.py\n```\n\nAnd if you need to debug a running container, you can open another terminal to your nucleus code and run the following:\n\n```bash\n$ docker-compose exec web bash\n$ # or\n$ docker-compose exec web python manage.py shell\n```\n\nManaging Python dependencies\n----------------------------\n\nFor Python we use [pip-compile-multi](https://pypi.org/project/pip-compile-multi/) to manage dependencies expressed in our requirements\nfiles. `pip-compile-multi` is wrapped up in Makefile commands, to ensure we use it consistently.\n\nIf you add a new Python dependency (e.g. to `requirements/prod.in` or `requirements/dev.in`) you can generate a pinned and hash-marked\naddition to our requirements files by running:\n\n```bash\n$ make compile-requirements\n```\n\nand committing any changes that are made. Please re-build your docker image and test it with `make build test` to be sure the dependency\ndoes not cause a regression.\n\nSimilarly, if you *upgrade* a pinned dependency in an `*.in` file, run `make compile-requirements` then rebuild, test and commit the results.\n\nTo check for stale Python dependencies (basically `pip list -o` but in the Docker container):\n\n```bash\n$ make check-requirements\n```\n\nInstall Python requirements locally\n-----------------------------------\n\nIdeally, do this in a virtual environment (eg a `venv` or `virtualenv`)\n\n```bash\n$ make install-local-python-deps\n```\n\nDocker for deploying to production\n-----------------------------------\n\n1. Add your project in [Docker Registry](https://registry.hub.docker.com/) as [Automated Build](http://docs.docker.com/docker-hub/builds/)\n2. Prepare a 'env' file with all the variables needed by dev, stage or production.\n3. Run the image:\n\n```bash\n$ docker run --env-file env -p 80:8000 mozilla/nucleus\n```\n\nHeroku\n------\n1. heroku create\n2. heroku config:set DEBUG=False ALLOWED_HOSTS=<foobar>.herokuapp.com, SECRET_KEY=something_secret\n   DATABASE_URL gets populated by heroku once you setup a database.\n3. git push heroku master\n\n\nNewRelic Monitoring\n-------------------\n\nA newrelic.ini file is already included. To enable NewRelic monitoring\nadd two environment variables:\n\n - NEW_RELIC_LICENSE_KEY\n - NEW_RELIC_APP_NAME\n\nSee the [full list of supported environment variables](https://docs.newrelic.com/docs/agents/python-agent/installation-configuration/python-agent-configuration#environment-variables).\n\n\n## Kubernetes\n\nhttps://github.com/mozmeao/nucleus-config/ has public examples of deployments in k8s clusters in AWS & GCP.\n\n\n## Gitlab CI/CD\n\nWe have https://gitlab.com/mozmeao/nucleus/pipelines [set up as CI/CD for](https://gitlab.com/mozmeao/infra/blob/master/docs/gitlab_ci.md)  https://github.com/mozilla/nucleus via this [.gitlab-ci.yml](https://github.com/mozilla/nucleus/blob/gitlab/.gitlab-ci.yml), which [updates the config repo](https://github.com/mozilla/nucleus/blob/gitlab/bin/update-config.sh) triggering https://gitlab.com/mozmeao/nucleus/pipelines configured by [.gitlab-ci.yml in the config repo](https://github.com/mozilla/nucleus-config/blob/master/.gitlab-ci.yml).\n"
},
{
  "name": "mlhackweek2021",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "searchapp"
    ]
  },
  "makefile": null,
  "readme": "# mlhackweek2021\n\n## Search WebApp\nThe intent is to collect search data and train a ranking model.\nThe application can be run from the project, build and run a docker image or deploy to Google App Engine\n### Docker\n1. `cd ./searchapp/mysite`\n2. Create a file named `.env` and add `export DJANGO_SECRET_KEY=<key>`\n3. `cd ..`\n4. From `searchapp/` Run `docker build -t mysite .` (Yes I used the default project name, naming is hard).\n5. Run the following command, replacing <port> with the external port you wish to use.\n   ```\n   docker run -it -p <port>:8020 -e DJANGO_SUPERUSER_USERNAME=admin -e DJANGO_SUPERUSER_PASSWORD=admin -e DJANGO_SUPERUSER_EMAIL=gleonard@mozilla.com mysite\n   ```\n6. Go to `http://127.0.0.1:<port>/` to use the app.\n7. Once a search is performed **and a result selected**, the ping will be listed in the debug view (https://debug-ping-preview.firebaseapp.com/)\n\n### Non Docker\nTo run the application:\n1. `cd ./searchapp/mysite`\n2. Create a file named `.env` and add `export DJANGO_SECRET_KEY=<key>`\n3. Run `virtualenv venv && venv/bin/pip install -r requirements.txt` to create the venv.\n4. Run `source venv/bin/activate`\n5. Run `python manage.py makemigrations`\n6. Run `python manage.py migrate`\n7. Currently the application's Glean metrics are not stored in BigQuery (will be added once we settle on the required metrics).  \n   For now the custom ping(s) can be view by setting the following environment variables:\n   - `export GLEAN_LOG_PINGS=true`\n   - `export GLEAN_DEBUG_VIEW_TAG=mlhackweek-search` or any other tag you want to use to filter pings.\n8. Run `python manage.py runserver`\n9. Go to http://127.0.0.1:8000/ to use the app.\n10. Once a search is performed **and a result selected**, the ping will be listed in the debug view (https://debug-ping-preview.firebaseapp.com/)\n\n### Deploy to Google App Engine\n1. `cd ./searchapp/mysite`\n2. Run `gcloud init` and set the project to **srg-team-sandbox**\n3. Run `gcloud auth login`\n4. Run `gcloud app deploy`\n5. Go to https://console.cloud.google.com/appengine/services?project=srg-team-sandbox&folder=&organizationId= to verify deploy\n6. Go to https://hackweeksearch-dot-srg-team-sandbox.uc.r.appspot.com/ to use the application\n"
},
{
  "name": "docker-etl",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".flake8",
      ".github",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "docker_etl",
      "jobs",
      "pytest.ini",
      "requirements.in",
      "requirements.txt",
      "script",
      "setup.py",
      "templates",
      "tests"
    ],
    "/.github": [
      "dependabot.yml",
      "pull_request_template.md"
    ],
    "/.circleci": [
      "config.template.yml",
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Docker ETL\n\nThis repo is a collection of dockerized ETL jobs to increase discoverability \nof the source code of scheduled ETL.\nThere are also tools here that automate the common steps involved with creating and\nscheduling an ETL job.\nThis includes defining a Docker image, setting up CI, and language boilerplate.\nThe primary use of this repo is to create Dockerized jobs that are pushed to GCR\nso they can be scheduled via the Airflow GKE pod operator.\n\n## Project Structure\n\n### Jobs\n\nEach job is located in its own directory in the `jobs/` directory, \ne.g. the contents of a job named `my-job` would go into `jobs/my-job`\n\nAll job directories should have a `Dockerfile`, a `ci_job.yaml`, \na `ci_workflow.yaml`, and a `README.md` in the root directory.          \n`ci_job.yaml` and `ci_workflow.yaml` contain the yaml structure that will be placed\nin the `- jobs:` and `- workflows:` sections of the CircleCI `config.yml` respectively.\n\n### Templates\n\nTemplates for job creation and the CI config file are located in `templates/`.\n\nThe CI config template is in `.circleci/config.template.yml`.\nThis is the file that should be modified instead of the `circleci/config.yml`.\n\nEach job template is located in a directory in `templates/` that is the name of the template, \ne.g. a `python` template is in `templates/python/`.\nWithin the directory of a template is a directory named `job/` that contains\nall the contents that will be copied when the template is used.\nOther files in the directory of a particular template are used for\njob creation, e.g. `ci_job.template.yaml`.\n\n### Example Directory Structure:\n\n```\n+--docker-etl/\n|  +--jobs/\n|     +--example-python-1/\n|        +--ci_job.yaml\n|        +--ci_workflow.yaml\n|        +--Dockerfile\n|        +--README.md\n|        +--script\n|  +--templates/\n|     +--python/\n|        +--job/\n|           +--module/\n|           +--tests/\n|           +--Dockerfile\n|           +--README.md\n|           +--requirements.txt\n|        +--ci_job.template.yaml\n|        +--ci_workflow.template.yaml\n\n```\n\n## Development\n\nThe tools in this repository are intended for python 3.8+.\n\nTo install dependencies:\n\n```sh\npip install -r requirements.txt\n```\n\nThis project uses `pip-tools` to pin dependencies.  New dependencies go in\n`requirements.in` and `pip-compile` is used to generate `requirements.txt`:\n\n```sh\npip install pip-tools\npip-compile --generate-hashes requirements.in\n```\n\nTo run tests:\n\n```sh\npytest --flake8 --black tests/\n```\n\n### Adding a new job\n\nTo add a new job:\n\n```sh\n./script/create_job --job-name example-job --template python\n```\n\n`job-name` is the name of the directory that will be created in `jobs/`.\n\n`template` is an optional argument that will populate the created directory\nwith the contents of a template.\nIf no template is given, a directory with only the required files is created.\n\n#### Available Templates:\n\n| Template name | Description |\n| ------------- | ----------- |\n| default       | Base directory with readme, Dockerfile, and CI config files |\n| python        | Simple Python module with unit test and lint config |\n\n### Modifying the CI config\n\nThis repo uses CircleCI which only allows a single global config file.\nIn order to simplify adding and removing jobs to CI, the config file is \ngenerated using templates.\nThis means the `config.yml` in `.circleci/` should not be modified directly.\n\nGenerate `.circleci/config.yml`:\n\n```sh\n./script/update_ci_config\n```\n\nTo make changes to the config that are not ETL job specific \n(e.g. add a command), changes should be made to `templates/config.template.yml` \nand the output config should be re-generated.\n\nEach job has a `ci_job.yaml` and a `ci_workflow.yaml` which define the steps \nthat will go into the jobs and workflow sections of the CircleCI config.\nAny changes to these files should be followed by updating the global config\nvia `scripts/update_ci_config`.\nWhen a job is created, the CI files are created based on the \n`ci_*.template.yaml` files in the template directory.\n\n### Adding a template\n\nTo add a new template, create a new directory in `templates/` with the name\nof the template.\nThis directory must have a `ci_job.template.yaml`, a `ci_workflow.template.yaml`,\nand a `job/` directory which contains all the files that will be copied to \nany job that uses this template.\n "
},
{
  "name": "srg-python_mozetl",
  "files": {
    "/": [
      "CHANGELOG",
      "Dockerfile",
      "Makefile",
      "README.md",
      "docker-compose.yml",
      "etl-requirements.txt",
      "install-java.sh"
    ]
  },
  "makefile": "bash:\n\tdocker-compose run --service-ports  devshell\n\nbuild:\n\tdocker-compose build devshell\n\nroot_shell:\n\tdocker-compose run --user root --service-ports devshell\n",
  "readme": "# srg-python_mozetl\n\nStop futzing with setting up your python_mozetl dependencies.  \n\nYou'll go mad.\n\nThis sets up a stock Ubuntu image with a working PySpark toolchain\nthat will pass all the mozilla/python_mozetl testsuite.\n\nRun `make build` to build the image\n\nRun `make bash` to run bash with ~/dev/ mounted into /app/src\n\nThe container has an enviroment variable set for HOST_IP which is your\nhost machine's IP address for en0\n\nThe default user is 'app' with a password set to 'badpass' with sudo\nrights\n\nThe docker-compose.yml file specifies mount points to point to your\nsource directory.\n\nMy own `python_mozetl` repository is typically located at\n`/Users/vng/dev/python_mozetl`, and is made available to the container\nunder `/app/src/python_mozetl`.  \n\nYou will need to update the volumes entry so that the `python_mozetl`\nrepository is available to the container on your machine.\n"
},
{
  "name": "python_mozaggregator",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".flake8",
      ".gitignore",
      ".pyup.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Dockerfile.dev",
      "Makefile",
      "README.md",
      "bin",
      "docker-compose.yml",
      "mozaggregator",
      "queries",
      "requirements",
      "script",
      "setup.py",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build clean test shell stop up\n\nhelp:\n\t@echo \"Welcome to the Python Mozaggregator\\n\"\n\t@echo \"The list of commands for local development:\\n\"\n\t@echo \"  build      Builds the docker images for the docker-compose setup\"\n\t@echo \"  clean      Stops and removes all docker containers\"\n\t@echo \"  shell      Opens a Bash shell\"\n\t@echo \"  stop       Stops the docker containers\"\n\t@echo \"  test       Runs the Python test suite\"\n\t@echo \"  up         Runs the whole stack, served at http://localhost:5000/\"\n\nbuild:\n\tdocker-compose build\n\nclean: stop\n\tdocker-compose rm -f\n\nshell: \n\tdocker-compose run --service-ports web bash\n\nstop:\n\tdocker-compose stop\n\ntest:\n\tdocker-compose run web test\n\nup:\n\tdocker-compose up\n",
  "readme": "# python_mozaggregator\n\nAggregator job for Telemetry. See this [blog](http://robertovitillo.com/2015/07/02/telemetry-metrics-roll-ups/) post for details. \n\n[![CircleCI](https://circleci.com/gh/mozilla/python_mozaggregator/tree/main.svg?style=svg)](https://circleci.com/gh/mozilla/python_mozaggregator/tree/main)\n\n## Development and deployment\n\nTo clean, build, and run all containers:\n```\nmake up\n```\n\nTo build containers and ssh into web container:\n```\nmake shell\n```\n\nTo manually ssh into running web container (e.g. after a 'make up'):\n```\ndocker ps\ndocker exec -it <CONTAINER_ID of web container> /bin/bash\n```\n\nTo build and run tests inside dev container:\n```\nmake test\n```\n\nTo manually run tests on running web container:\n```\nmake shell\n./bin/run test\n```\n\n## Deployment\nThe following Env vars need to be set up in hiera-sops POSTGRES_HOST, POSTGRES_RO_HOST, POSTGRES_PASS\nThere are jenkins pipeline jobs to deploy this.  See cloudops-deployment/projects/mozaggregator for details\n\n## Enabling and Disabling Metrics\nTo completely disable viewing of a metric, add it to the `METRICS_BLACKLIST`. No matter how this is deployed, users will\nnever be able to see that metric. Any regex can be matched against with the blacklist, for example to disable all metrics\nprefixed with \"USER_DATA\", put in `r\"USER_DATA.*\"`.\n\n### Release Metrics\nBy default, release metrics are not allowed by the service. To enable a specific release metric, add it to `PUBLIC_RELEASE_METRICS`.\nIt will then be viewable publicly.\n\nTo enable all release metrics (except those in `METRICS_BLACKLIST`), set the envvar `ALLOW_ALL_RELEASE_METRICS` to \"True\".\n\n## API\nAggregates are made available through a HTTP API. There are two kinds of aggregates: per submission date (date a ping is received by the server) and per build-id (date the submitting product was built).\n\nTo access the aggregates use the ```aggregates_by/build_id/``` and ```aggregates_by/submission_date/``` prefix respectively.\n\nIn the URLs below, replace `SERVICE` with the origin of this service's instance. The official service is `https://aggregates.telemetry.mozilla.org`.\n\nThe following examples are based on build-id aggregates. Replace `build_id` with `submission_date` to use aggregates per submission date instead.\n\n##### Get available channels:\n```bash\ncurl -X GET https://SERVICE/aggregates_by/build_id/channels/\n[\"nightly\",\"beta\",\"release\"]\n```\n\n##### Get a list of options for the available dimensions on a given channel and version:\n```bash\ncurl -X GET \"https://SERVICE/filters/?channel=nightly&version=42\"\n{\"metric\":[\"A11Y_CONSUMERS\",\"A11Y_IATABLE_USAGE_FLAG\",...], \n \"application\":[\"Fennec\",\"Firefox\"],\n ...}\n```\n\n##### Get a list of available build-ids for a given channel:\n```bash\ncurl -X GET \"https://SERVICE/aggregates_by/build_id/channels/nightly/dates/\"\n[{\"date\":\"20150630\",\"version\":\"42\"}, {\"date\":\"20150629\",\"version\":\"42\"}]\n```\n\n##### Given a set of build-ids, retrieve for each of build-id the aggregated histogram that complies with the requested filters:\n```bash\ncurl -X GET \"https://SERVICE/aggregates_by/build_id/channels/nightly/?version=41&dates=20150615,20150616&metric=GC_MS&os=Windows_NT\"\n{\"buckets\":[0, ..., 10000],\n \"data\":[{\"date\":\"20150615\",\n          \"count\":239459,\n          \"sum\": 412346123,\n          \"histogram\":[309, ..., 5047],\n          \"label\":\"\"},\n         {\"date\":\"20150616\",\n          \"count\":233688,\n          \"sum\": 402241121,\n          \"histogram\":[306, ..., 7875],\n          \"label\":\"\"}],\n \"kind\":\"exponential\",\n \"description\":\"Time spent running JS GC (ms)\"}\n```\n\nThe available filters are:\n- `metric`, e.g. JS_TELEMETRY_ADDON_EXCEPTIONS\n- `application`, e.g. Firefox\n- `architecture`, e.g. x86\n- `os`, e.g. Windows_NT\n- `osVersion`, e.g. 6.1\n- `label`, e.g Adblock-Plus\n- `child`, e.g. true, meaningful only if e10s is enabled\n\nA reply has the following attributes:\n- `buckets`, which represents the bucket labels of the histogram\n- `kind`, the kind of histogram (e.g. exponential)\n- `data`, which is an array of metric objects with the following attributes:\n  - `date`: a build-id\n  - `count`: number of metrics aggregated\n  - `sum`: sum of accumulated values\n  - `histogram`: bucket values\n  - `description`: histogram description\n  - `label`: for keyed histograms, the key the entry belongs to, or otherwise a blank string\n\nKeyed histograms have the same format as unkeyed histograms, but there can possibly be multiple metric objects with the same date, each with a different key (`label`).\n"
},
{
  "name": "openvpn",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      ".mailmap",
      ".svncommitters",
      "AUTHORS",
      "COPYING",
      "COPYRIGHT.GPL",
      "ChangeLog",
      "INSTALL",
      "INSTALL-win32.txt",
      "Makefile.am",
      "NEWS",
      "PORTS",
      "README",
      "README.IPv6",
      "README.ec",
      "README.polarssl",
      "TODO.IPv6",
      "build",
      "compat.m4",
      "config-msvc-version.h.in",
      "config-msvc.h",
      "configure.ac",
      "contrib",
      "debug",
      "distro",
      "doc",
      "include",
      "m4",
      "msvc-build.bat",
      "msvc-dev.bat",
      "msvc-env.bat",
      "openvpn.sln",
      "sample",
      "src",
      "tests",
      "version.m4",
      "version.sh.in"
    ]
  },
  "makefile": null,
  "readme": "OpenVPN -- A Secure tunneling daemon\n\nCopyright (C) 2002-2010 OpenVPN Technologies, Inc. This program is free software;\nyou can redistribute it and/or modify\nit under the terms of the GNU General Public License version 2\nas published by the Free Software Foundation.\n\n*************************************************************************\n\nFor the latest version of OpenVPN, go to:\n\n\thttp://openvpn.net/\n\nTo Build and Install,\n\n\t./configure\n\tmake\n\tmake install\n\nor see the file INSTALL for more info.\n\n*************************************************************************\n\nFor detailed information on OpenVPN, including examples, see the man page\n  http://openvpn.net/man.html\n\nFor a sample VPN configuration, see\n  http://openvpn.net/howto.html\n\nFor a description of OpenVPN's underlying protocol,\n  see the file ssl.h included in the source distribution.\n\n*************************************************************************\n\nOther Files & Directories:\n\n* INSTALL-win32.txt -- installation instructions\n  for Windows\n\n* configure.ac -- script to rebuild our configure\n  script and makefile.\n\n* sample/sample-scripts/verify-cn\n\n  A sample perl script which can be used with OpenVPN's\n  --tls-verify option to provide a customized authentication\n  test on embedded X509 certificate fields.\n\n* sample/sample-keys/\n\n  Sample RSA keys and certificates.  DON'T USE THESE FILES\n  FOR ANYTHING OTHER THAN TESTING BECAUSE THEY ARE TOTALLY INSECURE.\n\n* sample/sample-config-files/\n\n  A collection of OpenVPN config files and scripts from\n  the HOWTO at http://openvpn.net/howto.html\n\n*************************************************************************\n\nNote that easy-rsa and tap-windows are now maintained in their own subprojects.\nTheir source code is available here:\n\n  https://github.com/OpenVPN/easy-rsa\n  https://github.com/OpenVPN/tap-windows\n\nThe old cross-compilation environment (domake-win) and the Python-based\nbuildsystem have been replaced with openvpn-build:\n\n  https://github.com/OpenVPN/openvpn-build\n\nSee the INSTALL file for usage information.\n"
},
{
  "name": "policy-templates",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "_config.yml",
      "index.html",
      "mac",
      "windows"
    ]
  },
  "makefile": null,
  "readme": "**These policies are in active development and so might contain changes that do not work with current versions of Firefox.**\n\n**You should use the [officially released versions](https://github.com/mozilla/policy-templates/releases) if you are deploying changes.**\n\nPolicies can be specified using the [Group Policy templates on Windows](https://github.com/mozilla/policy-templates/tree/master/windows), [Intune on Windows](https://support.mozilla.org/kb/managing-firefox-intune), [configuration profiles on macOS](https://github.com/mozilla/policy-templates/tree/master/mac), or by creating a file called `policies.json`. On Windows, create a directory called `distribution` where the EXE is located and place the file there. On Mac, the file goes into `Firefox.app/Contents/Resources/distribution`.  On Linux, the file goes into `firefox/distribution`, where `firefox` is the installation directory for firefox, which varies by distribution or you can specify system-wide policy by placing the file in `/etc/firefox/policies`.\n\n| Policy Name | Description\n| --- | --- |\n| **[`3rdparty`](#3rdparty)** | Set policies that WebExtensions can access via chrome.storage.managed.\n| **[`AllowedDomainsForApps`](#alloweddomainsforapps)** | Define domains allowed to access Google Workspace.\n| **[`AppAutoUpdate`](#appautoupdate)** | Enable or disable automatic application update.\n| **[`AppUpdateURL`](#appupdateurl)** | Change the URL for application update.\n| **[`Authentication`](#authentication)** | Configure sites that support integrated authentication.\n| **[`AutoLaunchProtocolsFromOrigins`](#autolaunchprotocolsfromorigins)** | Define a list of external protocols that can be used from listed origins without prompting the user.\n| **[`BackgroundAppUpdate`](#backgroundappupdate)** | Enable or disable the background updater (Windows only).\n| **[`BlockAboutAddons`](#blockaboutaddons)** | Block access to the Add-ons Manager (about:addons).\n| **[`BlockAboutConfig`](#blockaboutconfig)** | Block access to about:config.\n| **[`BlockAboutProfiles`](#blockaboutprofiles)** | Block access to About Profiles (about:profiles).\n| **[`BlockAboutSupport`](#blockaboutsupport)** | Block access to Troubleshooting Information (about:support).\n| **[`Bookmarks`](#bookmarks)** | Add bookmarks in either the bookmarks toolbar or menu.\n| **[`CaptivePortal`](#captiveportal)** | Enable or disable the detection of captive portals.\n| **[`Certificates`](#certificates)** |\n| **[`Certificates -> ImportEnterpriseRoots`](#certificates--importenterpriseroots)** | Trust certificates that have been added to the operating system certificate store by a user or administrator.\n| **[`Certificates -> Install`](#certificates--install)** | Install certificates into the Firefox certificate store.\n| **[`Cookies`](#cookies)** | Configure cookie preferences.\n| **[`DefaultDownloadDirectory`](#defaultdownloaddirectory)** | Set the default download directory.\n| **[`DisableAppUpdate`](#disableappupdate)** | Turn off application updates.\n| **[`DisableBuiltinPDFViewer`](#disablebuiltinpdfviewer)** | Disable the built in PDF viewer.\n| **[`DisabledCiphers`](#disabledciphers)** | Disable ciphers.\n| **[`DisableDefaultBrowserAgent`](#disabledefaultbrowseragent)** | Prevent the default browser agent from taking any actions (Windows only).\n| **[`DisableDeveloperTools`](#disabledevelopertools)** | Remove access to all developer tools.\n| **[`DisableFeedbackCommands`](#disablefeedbackcommands)** | Disable the menus for reporting sites.\n| **[`DisableFirefoxAccounts`](#disablefirefoxaccounts)** | Disable Firefox Accounts integration (Sync).\n| **[`DisableFirefoxScreenshots`](#disablefirefoxscreenshots)** | Remove access to Firefox Screenshots.\n| **[`DisableFirefoxStudies`](#disablefirefoxstudies)** | Disable Firefox studies (Shield).\n| **[`DisableForgetButton`](#disableforgetbutton)** | Disable the \"Forget\" button.\n| **[`DisableFormHistory`](#disableformhistory)** | Turn off saving information on web forms and the search bar.\n| **[`DisableMasterPasswordCreation`](#disablemasterpasswordcreation)** | Remove the master password functionality.\n| **[`DisablePasswordReveal`](#disablepasswordreveal)** | Do not allow passwords to be revealed in saved logins.\n| **[`DisablePocket`](#disablepocket)** | Remove Pocket in the Firefox UI.\n| **[`DisablePrivateBrowsing`](#disableprivatebrowsing)** | Remove access to private browsing.\n| **[`DisableProfileImport`](#disableprofileimport)** | Disables the \"Import data from another browser\" option in the bookmarks window.\n| **[`DisableProfileRefresh`](#disableprofilerefresh)** | Disable the Refresh Firefox button on about:support and support.mozilla.org\n| **[`DisableSafeMode`](#disablesafemode)** | Disable safe mode within the browser.\n| **[`DisableSecurityBypass`](#disablesecuritybypass)** | Prevent the user from bypassing security in certain cases.\n| **[`DisableSetDesktopBackground`](#disablesetdesktopbackground)** | Remove the \"Set As Desktop Background...\" menuitem when right clicking on an image.\n| **[`DisableSystemAddonUpdate`](#disablesystemaddonupdate)** | Prevent system add-ons from being installed or updated.\n| **[`DisableTelemetry`](#disabletelemetry)** | DisableTelemetry\n| **[`DisplayBookmarksToolbar`](#displaybookmarkstoolbar)** | Set the initial state of the bookmarks toolbar.\n| **[`DisplayMenuBar`](#displaymenubar)** | Set the state of the menubar.\n| **[`DisplayMenuBar (Deprecated)`](#displaymenubar-deprecated)** | Set the initial state of the menubar.\n| **[`DNSOverHTTPS`](#dnsoverhttps)** | Configure DNS over HTTPS.\n| **[`DontCheckDefaultBrowser`](#dontcheckdefaultbrowser)** | Don't check if Firefox is the default browser at startup.\n| **[`DownloadDirectory`](#downloaddirectory)** | Set and lock the download directory.\n| **[`EnableTrackingProtection`](#enabletrackingprotection)** | Configure tracking protection.\n| **[`EncryptedMediaExtensions`](#encryptedmediaextensions)** | Enable or disable Encrypted Media Extensions and optionally lock it.\n| **[`EnterprisePoliciesEnabled`](#enterprisepoliciesenabled)** | Enable policy support on macOS.\n| **[`ExemptDomainFileTypePairsFromFileTypeDownloadWarnings`](#exemptdomainfiletypepairsfromfiletypedownloadwarnings)** | Disable warnings based on file extension for specific file types on domains.\n| **[`Extensions`](#extensions)** | Control the installation, uninstallation and locking of extensions.\n| **[`ExtensionSettings`](#extensionsettings)** | Manage all aspects of extensions.\n| **[`ExtensionUpdate`](#extensionupdate)** | Control extension updates.\n| **[`FirefoxHome`](#firefoxhome)** | Customize the Firefox Home page.\n| **[`FlashPlugin (Deprecated)`](#flashplugin-deprecated)** | Configure the default Flash plugin policy as well as origins for which Flash is allowed.\n| **[`Handlers`](#handlers)** | Configure default application handlers.\n| **[`HardwareAcceleration`](#hardwareacceleration)** | Control hardware acceleration.\n| **[`Homepage`](#homepage)** | Configure the default homepage and how Firefox starts.\n| **[`InstallAddonsPermission`](#installaddonspermission)** | Configure the default extension install policy as well as origins for extension installs are allowed.\n| **[`LegacyProfiles`](#legacyprofiles)** | Disable the feature enforcing a separate profile for each installation.\n| **[`LegacySameSiteCookieBehaviorEnabled`](#legacysamesitecookiebehaviorenabled)** | Enable default legacy SameSite cookie behavior setting.\n| **[`LegacySameSiteCookieBehaviorEnabledForDomainList`](#legacysamesitecookiebehaviorenabledfordomainlist)** | Revert to legacy SameSite behavior for cookies on specified sites.\n| **[`LocalFileLinks`](#localfilelinks)** | Enable linking to local files by origin.\n| **[`ManagedBookmarks`](#managedbookmarks)** | Configures a list of bookmarks managed by an administrator that cannot be changed by the user.\n| **[`ManualAppUpdateOnly`](#manualappupdateonly)** | Allow manual updates only and do not notify the user about updates.\n| **[`NetworkPrediction`](#networkprediction)** | Enable or disable network prediction (DNS prefetching).\n| **[`NewTabPage`](#newtabpage)** | Enable or disable the New Tab page.\n| **[`NoDefaultBookmarks`](#nodefaultbookmarks)** | Disable the creation of default bookmarks.\n| **[`OfferToSaveLogins`](#offertosavelogins)** | Control whether or not Firefox offers to save passwords.\n| **[`OfferToSaveLoginsDefault`](#offertosaveloginsdefault)** | Set the default value for whether or not Firefox offers to save passwords.\n| **[`OverrideFirstRunPage`](#overridefirstrunpage)** | Override the first run page.\n| **[`OverridePostUpdatePage`](#overridepostupdatepage)** | Override the upgrade page.\n| **[`PasswordManagerEnabled`](#passwordmanagerenabled)** | Remove (some) access to the password manager.\n| **[`PasswordManagerExceptions`](#passwordmanagerexceptions)** | Prevent Firefox from saving passwords for specific sites.\n| **[`PDFjs`](#pdfjs)** | Disable or configure PDF.js, the built-in PDF viewer.\n| **[`Permissions`](#permissions)** | Set permissions associated with camera, microphone, location, and notifications.\n| **[`PictureInPicture`](#pictureinpicture)** | Enable or disable Picture-in-Picture.\n| **[`PopupBlocking`](#popupblocking)** | Configure the default pop-up window policy as well as origins for which pop-up windows are allowed.\n| **[`Preferences`](#preferences)** | Set and lock preferences.\n| **[`Preferences (Deprecated)`](#preferences-deprecated)** | Set and lock some preferences.\n| **[`PrimaryPassword`](#primarypassword)** | Require or prevent using a primary (formerly master) password.\n| **[`PromptForDownloadLocation`](#promptfordownloadlocation)** | Ask where to save each file before downloading.\n| **[`Proxy`](#proxy)** | Configure proxy settings.\n| **[`RequestedLocales`](#requestedlocales)** | Set the the list of requested locales for the application in order of preference.\n| **[`SanitizeOnShutdown` (All)](#sanitizeonshutdown-all)** | Clear all data on shutdown.\n| **[`SanitizeOnShutdown` (Selective)](#sanitizeonshutdown-selective)** | Clear data on shutdown.\n| **[`SearchBar`](#searchbar)** | Set whether or not search bar is displayed.\n| **[`SearchEngines`](#searchengines-this-policy-is-only-available-on-the-esr)** |\n| **[`SearchEngines -> Add`](#searchengines--add)** | Add new search engines.\n| **[`SearchEngines -> Default`](#searchengines--default)** | Set the default search engine.\n| **[`SearchEngines -> PreventInstalls`](#searchengines--preventinstalls)** | Prevent installing search engines from webpages.\n| **[`SearchEngines -> Remove`](#searchengines--remove)** | Hide built-in search engines.\n| **[`SearchSuggestEnabled`](#searchsuggestenabled)** | Enable search suggestions.\n| **[`SecurityDevices`](#securitydevices)** | Install PKCS #11 modules.\n| **[`ShowHomeButton`](#showhomebutton)** | Show the home button on the toolbar.\n| **[`SSLVersionMax`](#sslversionmax)** | Set and lock the maximum version of TLS.\n| **[`SSLVersionMin`](#sslversionmin)** | Set and lock the minimum version of TLS.\n| **[`SupportMenu`](#supportmenu)** | Add a menuitem to the help menu for specifying support information.\n| **[`UserMessaging`](#usermessaging)** | Don't show certain messages to the user.\n| **[`UseSystemPrintDialog`](#usesystemprintdialog)** | Print using the system print dialog instead of print preview.\n| **[`WebsiteFilter`](#websitefilter)** | Block websites from being visited.\n| **[`WindowsSSO`](#windowssso)** | Allow Windows single sign-on for Microsoft, work, and school accounts.\n\n### 3rdparty\n\nAllow WebExtensions to configure policy. For more information, see [Adding policy support to your extension](https://extensionworkshop.com/documentation/enterprise/adding-policy-support-to-your-extension/).\n\nFor GPO and Intune, the extension developer should provide an ADMX file.\n\n**Compatibility:** Firefox 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### macOS\n```\n<dict>\n  <key>3rdparty</key>\n  <dict>\n    <key>Extensions</key>\n    <dict>\n      <key>uBlock0@raymondhill.net</key>\n      <dict>\n        <key>adminSettings</key>\n        <dict>\n          <key>selectedFilterLists</key>\n          <array>\n            <string>ublock-privacy</string>\n            <string>ublock-badware</string>\n            <string>ublock-filters</string>\n            <string>user-filters</string>\n          </array>\n        </dict>\n      </dict>\n    </dict>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"3rdparty\": {\n      \"Extensions\": {\n        \"uBlock0@raymondhill.net\": {\n          \"adminSettings\": {\n            \"selectedFilterLists\": [\n              \"ublock-privacy\",\n              \"ublock-badware\",\n              \"ublock-filters\",\n              \"user-filters\"\n            ]\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n### AllowedDomainsForApps\n\nDefine domains allowed to access Google Workspace.\n\nThis policy is based on the [Chrome policy](https://chromeenterprise.google/policies/#AllowedDomainsForApps) of the same name.\n\nIf this policy is enabled, users can only access Google Workspace using accounts from the specified domains. If you want to allow Gmail, you can add ```consumer_accounts``` to the list.\n\n**Compatibility:** Firefox 89, Firefox ESR 78.11\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\AllowedDomainsForApps = \"managedfirefox.com,example.com\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/AllowedDomainsForApps\n```\nValue (string):\n```\n<enabled/>\n<data id=\"AllowedDomainsForApps\" value=\"managedfirefox.com,example.com\"/>\n```\n#### macOS\n```\n<dict>\n  <key>AllowedDomainsForApps</key>\n  <string>managedfirefox.com,example.com</string>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"AllowedDomainsForApps\": \"managedfirefox.com,example.com\"\n  }\n}\n```\n### AppAutoUpdate\n\nEnable or disable **automatic** application update.\n\nIf set to true, application updates are installed without user approval within Firefox. The operating system might still require approval.\n\nIf set to false, application updates are downloaded but the user can choose when to install the update.\n\nIf you have disabled updates via `DisableAppUpdate`, this policy has no effect.\n\n**Compatibility:** Firefox 75, Firefox ESR 68.7\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `app.update.auto`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\AppAutoUpdate = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/AppAutoUpdate\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>AppAutoUpdate</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"AppAutoUpdate\": true | false\n  }\n}\n```\n### AppUpdateURL\n\nChange the URL for application update if you are providing Firefox updates from a custom update server.\n\n**Compatibility:** Firefox 62, Firefox ESR 60.2\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `app.update.url`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\AppUpdateURL = \"https://yoursite.com\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/AppUpdateURL\n```\nValue (string):\n```\n<enabled/>\n<data id=\"AppUpdateURL\" value=\"https://yoursite.com\"/>\n```\n#### macOS\n```\n<dict>\n  <key>AppUpdateURL</key>\n  <string>https://yoursite.com</string>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"AppUpdateURL\": \"https://yoursite.com\"\n  }\n}\n```\n### Authentication\n\nConfigure sites that support integrated authentication.\n\nSee [Integrated authentication](https://htmlpreview.github.io/?https://github.com/mdn/archived-content/blob/main/files/en-us/mozilla/integrated_authentication/raw.html) for more information.\n\n`PrivateBrowsing` enables integrated authentication in private browsing.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (AllowNonFQDN added in 62/60.2, AllowProxies added in 70/68.2, Locked added in 71/68.3, PrivateBrowsing added in 77/68.9)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.negotiate-auth.trusted-uris`,`network.negotiate-auth.delegation-uris`,`network.automatic-ntlm-auth.trusted-uris`,`network.automatic-ntlm-auth.allow-non-fqdn`,`network.negotiate-auth.allow-non-fqdn`,`network.automatic-ntlm-auth.allow-proxies`,`network.negotiate-auth.allow-proxies`,`network.auth.private-browsing-sso`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\SPNEGO\\1 = \"mydomain.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\SPNEGO\\2 = \"https://myotherdomain.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\Delegated\\1 = \"mydomain.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\Delegated\\2 = \"https://myotherdomain.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\NTLM\\1 = \"mydomain.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\NTLM\\2 = \"https://myotherdomain.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\AllowNonFQDN\\SPNEGO = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\AllowNonFQDN\\NTLM = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\AllowProxies\\SPNEGO = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\AllowProxies\\NTLM = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Authentication\\PrivateBrowsing = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Authentication/Authentication_SPNEGO\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Authentication\" value=\"1&#xF000;mydomain&#xF000;2&#xF000;https://myotherdomain.com\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Authentication/Authentication_Delegated\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Authentication\" value=\"1&#xF000;mydomain&#xF000;2&#xF000;https://myotherdomain.com\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Authentication/Authentication_NTLM\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Authentication\" value=\"1&#xF000;mydomain&#xF000;2&#xF000;https://myotherdomain.com\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Authentication/Authentication_AllowNonFQDN\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Authentication_AllowNonFQDN_NTLM\" value=\"true | false\"/>\n<data id=\"Authentication_AllowNonFQDN_SPNEGO\" value=\"true | false\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Authentication/Authentication_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Authentication/Authentication_PrivateBrowsing\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>Authentication</key>\n  <dict>\n    <key>SPNEGO</key>\n    <array>\n      <string>mydomain.com</string>\n      <string>https://myotherdomain.com</string>\n    </array>\n    <key>Delegated</key>\n    <array>\n      <string>mydomain.com</string>\n      <string>https://myotherdomain.com</string>\n    </array>\n    <key>NTLM</key>\n    <array>\n      <string>mydomain.com</string>\n      <string>https://myotherdomain.com</string>\n    </array>\n    <key>AllowNonFQDN</key>\n      <dict>\n      <key>SPNEGO</key>\n      <true/> | <false/>\n      <key>NTLM</key>\n      <true/> | <false/>\n    </dict>\n    <key>AllowProxies</key>\n      <dict>\n      <key>SPNEGO</key>\n      <true/> | <false/>\n      <key>NTLM</key>\n      <true/> | <false/>\n    </dict>\n    <key>Locked</key>\n    <true/> | <false/>\n    <key>PrivateBrowsing</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Authentication\": {\n      \"SPNEGO\": [\"mydomain.com\", \"https://myotherdomain.com\"],\n      \"Delegated\": [\"mydomain.com\", \"https://myotherdomain.com\"],\n      \"NTLM\": [\"mydomain.com\", \"https://myotherdomain.com\"],\n      \"AllowNonFQDN\": {\n        \"SPNEGO\": true | false,\n        \"NTLM\": true | false\n      },\n      \"AllowProxies\": {\n        \"SPNEGO\": true | false,\n        \"NTLM\": true | false\n      },\n      \"Locked\": true | false,\n      \"PrivateBrowsing\": true | false\n    }\n  }\n}\n```\n### AutoLaunchProtocolsFromOrigins\nDefine a list of external protocols that can be used from listed origins without prompting the user. The origin is the scheme plus the hostname.\n\nThe syntax of this policy is exactly the same as the [Chrome AutoLaunchProtocolsFromOrigins policy](https://cloud.google.com/docs/chrome-enterprise/policies/?policy=AutoLaunchProtocolsFromOrigins) except that you can only use valid origins (not just hostnames). This also means that you cannot specify an asterisk for all origins.\n\nThe schema is:\n```\n{\n \"items\": {\n  \"properties\": {\n   \"allowed_origins\": {\n    \"items\": {\n     \"type\": \"string\"\n    },\n    \"type\": \"array\"\n   },\n   \"protocol\": {\n    \"type\": \"string\"\n   }\n  },\n  \"required\": [\n   \"protocol\",\n   \"allowed_origins\"\n  ],\n  \"type\": \"object\"\n },\n \"type\": \"array\"\n}\n```\n**Compatibility:** Firefox 90, Firefox ESR 78.12\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\nSoftware\\Policies\\Mozilla\\Firefox\\AutoLaunchProtocolsFromOrigins (REG_MULTI_SZ) =\n```\n[\n  {\n    \"protocol\": \"zoommtg\",\n    \"allowed_origins\": [\n      \"https://somesite.zoom.us\"\n    ]\n  }\n]\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/AutoLaunchProtocolsFromOrigins\n```\nValue (string):\n```\n<enabled/>\n<data id=\"JSON\" value='\n[\n  {\n    \"protocol\": \"zoommtg\",\n    \"allowed_origins\": [\n      \"https://somesite.zoom.us\"\n    ]\n  }\n]'/>\n```\n#### macOS\n```\n<dict>\n  <key>AutoLaunchProtocolsFromOrigins</key>\n  <array>\n    <dict>\n      <key>protocol</key>\n      <string>zoommtg</string>\n      <key>allowed_origins</key>\n      <array>\n        <string>https://somesite.zoom.us</string>\n      </array>\n    </dict>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"AutoLaunchProtocolsFromOrigins\": [{\n      \"protocol\": \"zoommtg\",\n      \"allowed_origins\": [\n        \"https://somesite.zoom.us\"\n      ]\n    }]\n  }\n}\n```\n### BackgroundAppUpdate\n\nEnable or disable **automatic** application update **in the background**, when the application is not running.\n\nIf set to true, application updates may be installed (without user approval) in the background, even when the application is not running. The operating system might still require approval.\n\nIf set to false, the application will not try to install updates when the application is not running.\n\nIf you have disabled updates via `DisableAppUpdate` or disabled automatic updates via `AppAutoUpdate`, this policy has no effect.\n\n**Compatibility:** Firefox 90 (Windows only)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `app.update.background.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\BackgroundAppUpdate = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/BackgroundAppUpdate\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>BackgroundAppUpdate</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"BackgroundAppUpdate\": true | false\n  }\n}\n```\n### BlockAboutAddons\n\nBlock access to the Add-ons Manager (about:addons).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableAddonsManager`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\BlockAboutAddons = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/BlockAboutAddons\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>BlockAboutAddons</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"BlockAboutAddons\": true | false\n  }\n}\n```\n### BlockAboutConfig\n\nBlock access to about:config.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableAboutConfig`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\BlockAboutConfig = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/BlockAboutConfig\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>BlockAboutConfig</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"BlockAboutConfig\": true | false\n  }\n}\n```\n### BlockAboutProfiles\n\nBlock access to About Profiles (about:profiles).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableAboutProfiles`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\BlockAboutProfiles = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/BlockAboutProfiles\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>BlockAboutProfiles</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"BlockAboutProfiles\": true | false\n  }\n}\n```\n### BlockAboutSupport\n\nBlock access to Troubleshooting Information (about:support).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableAboutSupport`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\BlockAboutSupport = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/BlockAboutSupport\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>BlockAboutSupport</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"BlockAboutSupport\": true | false\n  }\n}\n```\n### Bookmarks\n\nNote: [`ManagedBookmarks`](#managedbookmarks) is the new recommended way to add bookmarks. This policy will continue to be supported.\n\nAdd bookmarks in either the bookmarks toolbar or menu. Only `Title` and `URL` are required. If `Placement` is not specified, the bookmark will be placed on the toolbar. If `Folder` is specified, it is automatically created and bookmarks with the same folder name are grouped together.\n\nIf you want to clear all bookmarks set with this policy, you can set the value to an empty array (```[]```). This can be on Windows via the new Bookmarks (JSON) policy available with GPO and Intune.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `bookmarks.toolbar`,`bookmarks.menu`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Bookmarks\\1\\Title = \"Example\"\nSoftware\\Policies\\Mozilla\\Firefox\\Bookmarks\\1\\URL = \"https://example.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Bookmarks\\1\\Favicon = \"https://example.com/favicon.ico\"\nSoftware\\Policies\\Mozilla\\Firefox\\Bookmarks\\1\\Placement = \"toolbar\" | \"menu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Bookmarks\\1\\Folder = \"FolderName\"\n\nSoftware\\Policies\\Mozilla\\Firefox\\Bookmarks (REG_MULTI_SZ) =\n```\n[]\n```\n\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Bookmarks/Bookmark01\n```\nValue (string):\n```\n<enabled/>\n<data id=\"BookmarkTitle\" value=\"Example\"/>\n<data id=\"BookmarkURL\" value=\"https://example.com\"/>\n<data id=\"BookmarkFavicon\" value=\"https://example.com/favicon.ico\"/>\n<data id=\"BookmarkPlacement\" value=\"toolbar | menu\"/>\n<data id=\"BookmarkFolder\" value=\"FolderName\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/Bookmarks\n```\nValue (string):\n```\n<enabled/>\n<data id=\"JSON\" value='[]'/>\n```\n#### macOS\n```\n<dict>\n  <key>Bookmarks</key>\n  <array>\n    <dict>\n      <key>Title</key>\n      <string>Example</string>\n      <key>URL</key>\n      <string>https://example.com</string>\n      <key>Favicon</key>\n      <string>https://example.com/favicon.ico</string>\n      <key>Placement</key>\n      <string>toolbar | menu</string>\n      <key>Folder</key>\n      <string>FolderName</string>\n    </dict>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Bookmarks\": [\n      {\n        \"Title\": \"Example\",\n        \"URL\": \"https://example.com\",\n        \"Favicon\": \"https://example.com/favicon.ico\",\n        \"Placement\": \"toolbar\" | \"menu\",\n        \"Folder\": \"FolderName\"\n      }\n    ]\n  }\n}\n```\n### CaptivePortal\nEnable or disable the detection of captive portals.\n\n**Compatibility:** Firefox 67, Firefox ESR 60.7\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.captive-portal-service.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\CaptivePortal = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/CaptivePortal\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>CaptivePortal</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"CaptivePortal\": true | false\n  }\n}\n```\n### Certificates\n\n### Certificates | ImportEnterpriseRoots\n\nTrust certificates that have been added to the operating system certificate store by a user or administrator.\n\nNote: This policy only works on Windows and macOS. For Linux discussion, see [bug 1600509](https://bugzilla.mozilla.org/show_bug.cgi?id=1600509).\n\nSee https://support.mozilla.org/kb/setting-certificate-authorities-firefox for more detail.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (macOS support in Firefox 63, Firefox ESR 68)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `security.enterprise_roots.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Certificates\\ImportEnterpriseRoots = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Certificates/Certificates_ImportEnterpriseRoots\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>Certificates</key>\n  <dict>\n    <key>ImportEnterpriseRoots</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Certificates\": {\n      \"ImportEnterpriseRoots\": true | false\n    }\n  }\n}\n```\n### Certificates | Install\n\nInstall certificates into the Firefox certificate store. If only a filename is specified, Firefox searches for the file in the following locations:\n\n- Windows\n  - %USERPROFILE%\\AppData\\Local\\Mozilla\\Certificates\n  - %USERPROFILE%\\AppData\\Roaming\\Mozilla\\Certificates\n- macOS\n  - /Library/Application Support/Mozilla/Certificates\n  - ~/Library/Application Support/Mozilla/Certificates\n- Linux\n  - /usr/lib/mozilla/certificates\n  - /usr/lib64/mozilla/certificates\n  - ~/.mozilla/certificates\n\nStarting with Firefox 65, Firefox 60.5 ESR, a fully qualified path can be used, including UNC paths. You should use the native path style for your operating system. We do not support using %USERPROFILE% or other environment variables on Windows.\n\nIf you are specifying the path in the policies.json file on Windows, you need to escape your backslashes (`\\\\`) which means that for UNC paths, you need to escape both (`\\\\\\\\`). If you use group policy, you only need one backslash.\n\nCertificates are installed using the trust string `CT,CT,`.\n\nBinary (DER) and ASCII (PEM) certificates are both supported.\n\n**Compatibility:** Firefox 64, Firefox ESR 64\\\n**CCK2 Equivalent:** `certs.ca`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Certificates\\Install\\1 = \"cert1.der\"\nSoftware\\Policies\\Mozilla\\Firefox\\Certificates\\Install\\2 = \"C:\\Users\\username\\cert2.pem\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Certificates/Certificates_Install\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Certificates_Install\" value=\"1&#xF000;cert1.der&#xF000;2&#xF000;C:\\Users\\username\\cert2.pem\"/>\n```\n#### macOS\n```\n<dict>\n  <key>Certificates</key>\n  <dict>\n    <key>Install</key>\n    <array>\n      <string>cert1.der</string>\n      <string>/Users/username/cert2.pem</string>\n    </array>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Certificates\": {\n      \"Install\": [\"cert1.der\", \"/home/username/cert2.pem\"]\n    }\n  }\n}\n```\n### Cookies\nConfigure cookie preferences.\n\n`Allow` is a list of origins (not domains) where cookies are always allowed. You must include http or https.\n\n`AllowSession` is a list of origins (not domains) where cookies are only allowed for the current session. You must include http or https.\n\n`Block` is a list of origins (not domains) where cookies are always blocked. You must include http or https.\n\n`Behavior` sets the default behavior for cookies based on the values below.\n\n`BehaviorPrivateBrowsing` sets the default behavior for cookies in private browsing based on the values below.\n\n| Value | Description\n| --- | ---\n| accept | Accept all cookies\n| reject-foreign | Reject third party cookies\n| reject | Reject all cookies\n| limit-foreign | Reject third party cookies for sites you haven't visited\n| reject-tracker | Reject cookies for known trackers (default)\n| reject-tracker-and-partition-foreign | Reject cookies for known trackers and partition third-party cookies (Total Cookie Protection) (default for private browsing)\n\n`Default` (Deprecated) determines whether cookies are accepted at all.\n\n`AcceptThirdParty` (Deprecated) determines how third-party cookies are handled.\n\n`ExpireAtSessionEnd` determines when cookies expire.\n\n`RejectTracker` (Deprecated) only rejects cookies for trackers.\n\n`Locked` prevents the user from changing cookie preferences.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (RejectTracker added in Firefox 63, AllowSession added in Firefox 79/78.1, Behavior added in Firefox 95/91.4)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.cookie.cookieBehavior`, `network.cookie.cookieBehavior.pbmode`, `network.cookie.lifetimePolicy`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\Allow\\1 = \"https://example.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\AllowSession\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\Block\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\Default = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\AcceptThirdParty = \"always\" | \"never\" | \"from-visited\"\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\ExpireAtSessionEnd = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\RejectTracker = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\Behavior = \"accept\" | \"reject-foreign\" | \"reject\" | \"limit-foreign\" | \"reject-tracker\" | \"reject-tracker-and-partition-foreign\"\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\BehaviorPrivateBrowsing = \"accept\" | \"reject-foreign\" | \"reject\" | \"limit-foreign\" | \"reject-tracker\" | \"reject-tracker-and-partition-foreign\"\nSoftware\\Policies\\Mozilla\\Firefox\\Cookies\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.com\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_AllowSession\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_Block\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_Default\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_AcceptThirdParty\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Cookies_AcceptThirdParty\" value=\"always | never | from-visited\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_ExpireAtSessionEnd\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_RejectTracker\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_Behavior\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Cookies_Behavior\" value=\"accept | reject-foreign | reject | limit-foreign | reject-tracker | reject-tracker-and-partition-foreign\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Cookies/Cookies_BehaviorPrivateBrowsing\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Cookies_BehaviorPrivateBrowsing\" value=\"accept | reject-foreign | reject | limit-foreign | reject-tracker | reject-tracker-and-partition-foreign\"/>\n```\n#### macOS\n```\n<dict>\n  <key>Cookies</key>\n  <dict>\n    <key>Allow</key>\n    <array>\n      <string>http://example.com</string>\n    </array>\n    <key>AllowSession</key>\n    <array>\n      <string>http://example.edu</string>\n    </array>\n    <key>Block</key>\n    <array>\n      <string>http://example.org</string>\n    </array>\n    <key>Default</key>\n    <true/> | <false/>\n    <key>AcceptThirdParty</key>\n    <string>always | never | from-visited</string>\n    <key>ExpireAtSessionEnd</key>\n    <true/> | <false/>\n    <key>RejectTracker</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n    <key>Behavior</key>\n    <string>accept | reject-foreign | reject | limit-foreign | reject-tracker | reject-tracker-and-partition-foreign</string>\n    <key>BehaviorPrivateBrowsing</key>\n    <string>accept | reject-foreign | reject | limit-foreign | reject-tracker | reject-tracker-and-partition-foreign</string>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Cookies\": {\n      \"Allow\": [\"http://example.org/\"],\n      \"AllowSession\": [\"http://example.edu/\"],\n      \"Block\": [\"http://example.edu/\"],\n      \"Default\": true | false,\n      \"AcceptThirdParty\": \"always\" | \"never\" | \"from-visited\",\n      \"ExpireAtSessionEnd\": true | false,\n      \"RejectTracker\": true | false,\n      \"Locked\": true | false,\n      \"Behavior\": \"accept\" | \"reject-foreign\" | \"reject\" | \"limit-foreign\" | \"reject-tracker\" | \"reject-tracker-and-partition-foreign\",\n      \"BehaviorPrivateBrowsing\": \"accept\" | \"reject-foreign\" | \"reject\" | \"limit-foreign\" | \"reject-tracker\" | \"reject-tracker-and-partition-foreign\",\n    }\n  }\n}\n```\n### DefaultDownloadDirectory\nSet the default download directory.\n\nYou can use ${home} for the native home directory.\n\n**Compatibility:** Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.download.dir`, `browser.download.folderList`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DefaultDownloadDirectory = \"${home}\\Downloads\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DefaultDownloadDirectory\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Preferences_String\" value=\"${home}\\Downloads\"/>\n```\n#### macOS\n```\n<dict>\n  <key>DefaultDownloadDirectory</key>\n  <string>${home}/Downloads</string>\n</dict>\n```\n#### policies.json (macOS and Linux)\n```\n{\n  \"policies\": {\n    \"DefaultDownloadDirectory\": \"${home}/Downloads\"\n}\n```\n#### policies.json (Windows)\n```\n{\n  \"policies\": {\n    \"DefaultDownloadDirectory\": \"${home}\\\\Downloads\"\n}\n```\n### DisableAppUpdate\nTurn off application updates within Firefox.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableFirefoxUpdates`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableAppUpdate = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableAppUpdate\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableAppUpdate</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableAppUpdate\": true | false\n  }\n}\n```\n### DisableBuiltinPDFViewer\nDisable the built in PDF viewer. PDF files are downloaded and sent externally.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disablePDFjs`\\\n**Preferences Affected:** `pdfjs.disabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableBuiltinPDFViewer = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableBuiltinPDFViewer\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableBuiltinPDFViewer</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableBuiltinPDFViewer\": true | false\n  }\n}\n```\n### DisabledCiphers\nDisable specific cryptographic ciphers, listed below.\n\n```\nTLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\nTLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\nTLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256\nTLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256\nTLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\nTLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\nTLS_ECDHE_RSA_WITH_AES_128_CBC_SHA\nTLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA\nTLS_ECDHE_RSA_WITH_AES_256_CBC_SHA\nTLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA\nTLS_DHE_RSA_WITH_AES_128_CBC_SHA\nTLS_DHE_RSA_WITH_AES_256_CBC_SHA\nTLS_RSA_WITH_AES_128_GCM_SHA256\nTLS_RSA_WITH_AES_256_GCM_SHA384\nTLS_RSA_WITH_AES_128_CBC_SHA\nTLS_RSA_WITH_AES_256_CBC_SHA\nTLS_RSA_WITH_3DES_EDE_CBC_SHA\n```\n\n**Preferences Affected:** `security.ssl3.ecdhe_rsa_aes_128_gcm_sha256`, `security.ssl3.ecdhe_ecdsa_aes_128_gcm_sha256`, `security.ssl3.ecdhe_ecdsa_chacha20_poly1305_sha256`, `security.ssl3.ecdhe_rsa_chacha20_poly1305_sha256`, `security.ssl3.ecdhe_ecdsa_aes_256_gcm_sha384`, `security.ssl3.ecdhe_rsa_aes_256_gcm_sha384`, `security.ssl3.ecdhe_rsa_aes_128_sha`, `security.ssl3.ecdhe_ecdsa_aes_128_sha`, `security.ssl3.ecdhe_rsa_aes_256_sha`, `security.ssl3.ecdhe_ecdsa_aes_256_sha`, `security.ssl3.dhe_rsa_aes_128_sha`, `security.ssl3.dhe_rsa_aes_256_sha`, `security.ssl3.rsa_aes_128_gcm_sha256`, `security.ssl3.rsa_aes_256_gcm_sha384`, `security.ssl3.rsa_aes_128_sha`, `security.ssl3.rsa_aes_256_sha`, `security.ssl3.deprecated.rsa_des_ede3_sha`\n\n---\n**Note:**\n\nThis policy was updated in Firefox 78 to allow enabling ciphers as well. Setting the value to true disables the cipher, setting the value to false enables the cipher. Previously setting the value to true or false disabled the cipher.\n\n---\n**Compatibility:** Firefox 76, Firefox ESR 68.8 (TLS_RSA_WITH_AES_128_GCM_SHA256 and TLS_RSA_WITH_AES_256_GCM_SHA384 were added in Firefox 78, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA38, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, and TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 were added in Firefox 97 and Firefox 91.6)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisabledCiphers\\CIPHER_NAME = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~DisabledCiphers/DisabledCiphers_CIPHER_NAME\n\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisabledCiphers</key>\n    <dict>\n      <key>CIPHER_NAME</key>\n      <true/> | <false/>\n    </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisabledCiphers\": {\n      \"CIPHER_NAME\": true | false,\n    }\n  }\n}\n```\n### DisableDefaultBrowserAgent\nPrevent the default browser agent from taking any actions. Only applicable to Windows; other platforms don\u2019t have the agent.\n\nThe browser agent is a Windows-only scheduled task which runs in the background to collect and submit data about the browser that the user has set as their OS default. More information is available [here](https://firefox-source-docs.mozilla.org/toolkit/mozapps/defaultagent/default-browser-agent/index.html).\n\n**Compatibility:** Firefox 75, Firefox ESR 68.7 (Windows only)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableDefaultBrowserAgent = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableDefaultBrowserAgent\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableDefaultBrowserAgent\": true | false\n  }\n}\n```\n### DisableDeveloperTools\nRemove access to all developer tools.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `removeDeveloperTools`\\\n**Preferences Affected:** `devtools.policy.disabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableDeveloperTools = 0x1 | 0x0`\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableDeveloperTools\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableDeveloperTools</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableDeveloperTools\": true | false\n  }\n}\n```\n### DisableFeedbackCommands\nDisable the menus for reporting sites (Submit Feedback, Report Deceptive Site).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableFeedbackCommands = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableFeedbackCommands\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableFeedbackCommands</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableFeedbackCommands\": true | false\n  }\n}\n```\n### DisableFirefoxAccounts\nDisable Firefox Accounts integration (Sync).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableSync`\\\n**Preferences Affected:** `identity.fxaccounts.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableFirefoxAccounts = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableFirefoxAccounts\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableFirefoxAccounts</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableFirefoxAccounts\": true | false\n  }\n}\n```\n### DisableFirefoxScreenshots\nRemove access to Firefox Screenshots.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `extensions.screenshots.disabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableFirefoxScreenshots = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableFirefoxScreenshots\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableFirefoxScreenshots</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableFirefoxScreenshots\": true | false\n  }\n}\n```\n### DisableFirefoxStudies\nDisable Firefox studies (Shield).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableFirefoxStudies = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableFirefoxStudies\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableFirefoxStudies</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableFirefoxStudies\": true | false\n  }\n}\n```\n### DisableForgetButton\nDisable the \"Forget\" button.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableForget`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableForgetButton = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableForgetButton\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableForgetButton</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableForgetButton\": true | false\n  }\n}\n```\n### DisableFormHistory\nTurn off saving information on web forms and the search bar.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableFormFill`\\\n**Preferences Affected:** `browser.formfill.enable`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableFormHistory = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableFormHistory\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableFormHistory</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableFormHistory\": true | false\n  }\n}\n```\n### DisableMasterPasswordCreation\nRemove the master password functionality.\n\nIf this value is true, it works the same as setting [`PrimaryPassword`](#primarypassword) to false and removes the primary password functionality.\n\nIf both `DisableMasterPasswordCreation` and `PrimaryPassword` are used, `DisableMasterPasswordCreation` takes precedent.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `noMasterPassword`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableMasterPasswordCreation = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableMasterPasswordCreation\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableMasterPasswordCreation</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableMasterPasswordCreation\": true | false\n  }\n}\n```\n### DisablePasswordReveal\nDo not allow passwords to be shown in saved logins\n\n**Compatibility:** Firefox 71, Firefox ESR 68.3\\\n**CCK2 Equivalent:** N/A\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisablePasswordReveal = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisablePasswordReveal\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisablePasswordReveal</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisablePasswordReveal\": true | false\n  }\n}\n```\n### DisablePocket\nRemove Pocket in the Firefox UI. It does not remove it from the new tab page.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disablePocket`\\\n**Preferences Affected:** `extensions.pocket.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisablePocket = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisablePocket\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisablePocket</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisablePocket\": true | false\n  }\n}\n```\n### DisablePrivateBrowsing\nRemove access to private browsing.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disablePrivateBrowsing`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisablePrivateBrowsing = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisablePrivateBrowsing\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisablePrivateBrowsing</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisablePrivateBrowsing\": true | false\n  }\n}\n```\n### DisableProfileImport\nDisables the \"Import data from another browser\" option in the bookmarks window.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableProfileImport = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableProfileImport\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableProfileImport</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableProfileImport\": true | false\n  }\n}\n```\n### DisableProfileRefresh\nDisable the Refresh Firefox button on about:support and support.mozilla.org, as well as the prompt that displays offering to refresh Firefox when you haven't used it in a while.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableResetFirefox`\\\n**Preferences Affected:** `browser.disableResetPrompt`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableProfileRefresh = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableProfileRefresh\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableProfileRefresh</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableProfileRefresh\": true | false\n  }\n}\n```\n### DisableSafeMode\nDisable safe mode within the browser.\n\nOn Windows, this disables safe mode via the command line as well.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (Windows, macOS)\\\n**CCK2 Equivalent:** `disableSafeMode`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableSafeMode = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableSafeMode\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableSafeMode</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableSafeMode\": true | false\n  }\n}\n```\n### DisableSecurityBypass\nPrevent the user from bypassing security in certain cases.\n\n`InvalidCertificate` prevents adding an exception when an invalid certificate is shown.\n\n`SafeBrowsing` prevents selecting \"ignore the risk\" and visiting a harmful site anyway.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `security.certerror.hideAddException`, `browser.safebrowsing.allowOverride`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableSecurityBypass\\InvalidCertificate = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\DisableSecurityBypass\\SafeBrowsing = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/P_DisableSecurityBypass_InvalidCertificate\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/P_DisableSecurityBypass_SafeBrowsing\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n\n#### macOS\n```\n<dict>\n  <key>DisableSecurityBypass</key>\n  <dict>\n    <key>InvalidCertificate</key>\n    <true/> | <false/>\n    <key>SafeBrowsing</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableSecurityBypass\": {\n      \"InvalidCertificate\": true | false,\n      \"SafeBrowsing\": true | false\n    }\n  }\n}\n```\n### DisableSetDesktopBackground\nRemove the \"Set As Desktop Background...\" menuitem when right clicking on an image.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `removeSetDesktopBackground`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableSetDesktopBackground = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableSetDesktopBackground\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableSetDesktopBackground</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableSetDesktopBackground\": true | false\n  }\n}\n```\n### DisableSystemAddonUpdate\nPrevent system add-ons from being installed or updated.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableSystemAddonUpdate = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableSystemAddonUpdate\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableSystemAddonUpdate</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableSystemAddonUpdate\": true | false\n  }\n}\n```\n### DisableTelemetry\nPrevent the upload of telemetry data.\n\nAs of Firefox 83 and Firefox ESR 78.5, local storage of telemetry data is disabled as well.\n\nMozilla recommends that you do not disable telemetry. Information collected through telemetry helps us build a better product for businesses like yours.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `disableTelemetry`\\\n**Preferences Affected:** `datareporting.healthreport.uploadEnabled`, `datareporting.policy.dataSubmissionEnabled`, `toolkit.telemetry.archive.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisableTelemetry = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisableTelemetry\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisableTelemetry</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisableTelemetry\": true | false\n  }\n}\n```\n### DisplayBookmarksToolbar\nSet the initial state of the bookmarks toolbar. A user can still hide it and it will stay hidden.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `displayBookmarksToolbar`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisplayBookmarksToolbar = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisplayBookmarksToolbar\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DisplayBookmarksToolbar</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisplayBookmarksToolbar\": true | false\n  }\n}\n```\n### DisplayMenuBar\nSet the state of the menubar.\n\n`always` means the menubar is shown and cannot be hidden.\n\n`never` means the menubar is hidden and cannot be shown.\n\n`default-on` means the menubar is on by default but can be hidden.\n\n`default-off` means the menubar is off by default but can be shown.\n\n**Compatibility:** Firefox 73, Firefox ESR 68.5 (Windows, some Linux)\\\n**CCK2 Equivalent:** `displayMenuBar`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisplayMenuBar = \"always\", \"never\", \"default-on\", \"default-off\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DisplayMenuBar_Enum\n```\nValue (string):\n```\n<enabled/>\n<data id=\"DisplayMenuBar\" value=\"always | never | default-on | default-off\"/>\n```\n#### macOS\n```\n<dict>\n  <key>DisplayMenuBar</key>\n  <string>always | never | default-on | default-off</string>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisplayMenuBar\": \"always\", \"never\", \"default-on\", \"default-off\"\n  }\n}\n```\n### DisplayMenuBar (Deprecated)\nSet the initial state of the menubar. A user can still hide it and it will stay hidden.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (Windows, some Linux)\\\n**CCK2 Equivalent:** `displayMenuBar`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DisplayMenuBar = 0x1 | 0x0\n```\n#### macOS\n```\n<dict>\n  <key>DisplayMenuBar</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DisplayMenuBar\": true | false\n  }\n}\n```\n### DNSOverHTTPS\nConfigure DNS over HTTPS.\n\n`Enabled` determines whether DNS over HTTPS is enabled\n\n`ProviderURL` is a URL to another provider.\n\n`Locked` prevents the user from changing DNS over HTTPS preferences.\n\n`ExcludedDomains` excludes domains from DNS over HTTPS.\n\n**Compatibility:** Firefox 63, Firefox ESR 68 (ExcludedDomains added in 75/68.7)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.trr.mode`, `network.trr.uri`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DNSOverHTTPS\\Enabled = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\DNSOverHTTPS\\ProviderURL = \"URL_TO_ALTERNATE_PROVIDER\"\nSoftware\\Policies\\Mozilla\\Firefox\\DNSOverHTTPS\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\DNSOverHTTPS\\ExcludedDomains\\1 = \"example.com\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~DNSOverHTTPS/DNSOverHTTPS_Enabled\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~DNSOverHTTPS/DNSOverHTTPS_ProviderURL\n```\nValue (string):\n```\n<enabled/>\n<data id=\"String\" value=\"URL_TO_ALTERNATE_PROVIDER\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~DNSOverHTTPS/DNSOverHTTPS_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~DNSOverHTTPS/DNSOverHTTPS_ExcludedDomains\n```\nValue (string):\n```\n<enabled/>\n<data id=\"List\" value=\"1&#xF000;example.com\"/>\n```\n#### macOS\n```\n<dict>\n  <key>DNSOverHTTPS</key>\n  <dict>\n    <key>Enabled</key>\n    <true/> | <false/>\n    <key>ProviderURL</key>\n    <string>URL_TO_ALTERNATE_PROVIDER</string>\n    <key>Locked</key>\n    <true/> | <false/>\n    <key>ExcludedDomains</key>\n    <array>\n      <string>example.com</string>\n    </array>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DNSOverHTTPS\": {\n      \"Enabled\":  true | false,\n      \"ProviderURL\": \"URL_TO_ALTERNATE_PROVIDER\",\n      \"Locked\": true | false,\n      \"ExcludedDomains\": [\"example.com\"]\n    }\n  }\n}\n```\n### DontCheckDefaultBrowser\nDon't check if Firefox is the default browser at startup.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `dontCheckDefaultBrowser`\\\n**Preferences Affected:** `browser.shell.checkDefaultBrowser`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DontCheckDefaultBrowser = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DontCheckDefaultBrowser\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>DontCheckDefaultBrowser</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"DontCheckDefaultBrowser\": true | false\n  }\n}\n```\n### DownloadDirectory\nSet and lock the download directory.\n\nYou can use ${home} for the native home directory.\n\n**Compatibility:** Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.download.dir`, `browser.download.folderList`, `browser.download.useDownloadDir`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\DownloadDirectory = \"${home}\\Downloads\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/DownloadDirectory\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Preferences_String\" value=\"${home}\\Downloads\"/>\n```\n#### macOS\n```\n<dict>\n  <key>DownloadDirectory</key>\n  <string>${home}/Downloads</string>\n</dict>\n```\n#### policies.json (macOS and Linux)\n```\n{\n  \"policies\": {\n    \"DownloadDirectory\": \"${home}/Downloads\"\n}\n```\n#### policies.json (Windows)\n```\n{\n  \"policies\": {\n    \"DownloadDirectory\": \"${home}\\\\Downloads\"\n}\n```\n### EnableTrackingProtection\nConfigure tracking protection.\n\nIf this policy is not configured, tracking protection is not enabled by default in the browser, but it is enabled by default in private browsing and the user can change it.\n\nIf `Value` is set to false, tracking protection is disabled and locked in both the regular browser and private browsing.\n\nIf `Value` is set to true, tracking protection is enabled by default in both the regular browser and private browsing and the `Locked` value determines whether or not a user can change it.\n\nIf `Cryptomining` is set to true, cryptomining scripts on websites are blocked.\n\nIf `Fingerprinting` is set to true, fingerprinting scripts on websites are blocked.\n\n`Exceptions` are origins for which tracking protection is not enabled.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (Cryptomining and Fingerprinting added in 70/68.2, Exceptions added in 73/68.5)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `privacy.trackingprotection.enabled`, `privacy.trackingprotection.pbmode.enabled`, `privacy.trackingprotection.cryptomining.enabled`, `privacy.trackingprotection.fingerprinting.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\EnableTrackingProtection\\Value = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\EnableTrackingProtection\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\EnableTrackingProtection\\Cryptomining = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\EnableTrackingProtection\\Fingerprinting = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\EnableTrackingProtection\\Exceptions\\1 = \"https://example.com\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~TrackingProtection/A_TrackingProtection_Value\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~TrackingProtection/B_TrackingProtection_Cryptomining\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~TrackingProtection/C_TrackingProtection_Fingerprinting\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~TrackingProtection/D_TrackingProtection_Exceptions\n```\nValue (string):\n```\n<data id=\"TrackingProtection_Exceptions\" value=\"1&#xF000;https://example.com\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~TrackingProtection/E_TrackingProtection_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>EnableTrackingProtection</key>\n  <dict>\n    <key>Value</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n    <key>Cryptomining</key>\n    <true/> | <false/>\n    <key>Fingerprinting</key>\n    <true/> | <false/>\n    <key>Exceptions</key>\n    <array>\n      <string>https://example.com</string>\n    </array>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"EnableTrackingProtection\": {\n      \"Value\": true | false,\n      \"Locked\": true | false,\n      \"Cryptomining\": true | false,\n      \"Fingerprinting\": true | false,\n      \"Exceptions\": [\"https://example.com\"]\n    }\n  }\n}\n```\n### EncryptedMediaExtensions\nEnable or disable Encrypted Media Extensions and optionally lock it.\n\nIf `Enabled` is set to false, encrypted media extensions (like Widevine) are not downloaded by Firefox unless the user consents to installing them.\n\nIf `Locked` is set to true and `Enabled` is set to false, Firefox will not download encrypted media extensions (like Widevine) or ask the user to install them.\n\n**Compatibility:** Firefox 77, Firefox ESR 68.9\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `media.eme.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\EncryptedMediaExtensions\\Enabled = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\EncryptedMediaExtensions\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~EncryptedMediaExtensions/EncryptedMediaExtensions_Enabled\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~EncryptedMediaExtensions/EncryptedMediaExtensions_Locked\n```\nValue (string):\n```\n<enabled/>or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>EncryptedMediaExtensions</key>\n  <dict>\n    <key>Enabled</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"EncryptedMediaExtensions\": {\n      \"Enabled\": true | false,\n      \"Locked\": true | false\n    }\n  }\n}\n```\n### EnterprisePoliciesEnabled\nEnable policy support on macOS.\n\n**Compatibility:** Firefox 63, Firefox ESR 60.3 (macOS only)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### macOS\n```\n<dict>\n  <key>EnterprisePoliciesEnabled</key>\n  <true/>\n</dict>\n```\n### ExemptDomainFileTypePairsFromFileTypeDownloadWarnings\n\nDisable warnings based on file extension for specific file types on domains.\n\nThis policy is based on the [Chrome policy](https://chromeenterprise.google/policies/#ExemptDomainFileTypePairsFromFileTypeDownloadWarnings) of the same name.\n\nImportant: The documentation for the policy for both Edge and Chrome is incorrect. The ```domains``` value must be a domain, not a URL pattern. Also, we do not support using ```*``` to mean all domains.\n\n**Compatibility:** Firefox 102\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\nSoftware\\Policies\\Mozilla\\Firefox\\ExemptDomainFileTypePairsFromFileTypeDownloadWarnings (REG_MULTI_SZ) =\n```\n[\n  {\n     \"file_extension\": \"jnlp\",\n     \"domains\": [\"example.com\"]\n  }\n]\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/ExemptDomainFileTypePairsFromFileTypeDownloadWarnings\n```\nValue (string):\n```\n<enabled/>\n<data id=\"JSON\" value='\n[\n  {\n     \"file_extension\": \"jnlp\",\n     \"domains\": [\"example.com\"]\n  }\n]\n'/>\n```\n#### macOS\n```\n<dict>\n  <key>ExemptDomainFileTypePairsFromFileTypeDownloadWarnings</key>\n  <array>\n    <dict>\n      <key>file_extension</key>\n      <string>jnlp</string>\n      <key>domains</key>\n      <array>\n        <string>example.com</string>\n      </array>\n    </dict>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"ExemptDomainFileTypePairsFromFileTypeDownloadWarnings\": [{\n      \"file_extension\": \"jnlp\",\n      \"domains\": [\"example.com\"]\n    }]\n  }\n}\n```\n### Extensions\nControl the installation, uninstallation and locking of extensions.\n\nWhile this policy is not technically deprecated, it is recommended that you use the **[`ExtensionSettings`](#extensionsettings)** policy. It has the same functionality and adds more. It does not support native paths, though, so you'll have to use file:/// URLs.\n\n`Install` is a list of URLs or native paths for extensions to be installed.\n\n`Uninstall` is a list of extension IDs that should be uninstalled if found.\n\n`Locked` is a list of extension IDs that the user cannot disable or uninstall.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `addons`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Extensions\\Install\\1 = \"https://addons.mozilla.org/firefox/downloads/somefile.xpi\"\nSoftware\\Policies\\Mozilla\\Firefox\\Extensions\\Install\\2 = \"//path/to/xpi\"\nSoftware\\Policies\\Mozilla\\Firefox\\Extensions\\Uninstall\\1 = \"bad_addon_id@mozilla.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Extensions\\Locked\\1 = \"addon_id@mozilla.org\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Extensions/Extensions_Install\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Extensions\" value=\"1&#xF000;https://addons.mozilla.org/firefox/downloads/somefile.xpi&#xF000;2&#xF000;//path/to/xpi\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Extensions/Extensions_Uninstall\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Extensions\" value=\"1&#xF000;bad_addon_id@mozilla.org\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Extensions/Extensions_Locked\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Extensions\" value=\"1&#xF000;addon_id@mozilla.org\"/>\n```\n#### macOS\n```\n<dict>\n  <key>Extensions</key>\n  <dict>\n    <key>Install</key>\n    <array>\n      <string>https://addons.mozilla.org/firefox/downloads/somefile.xpi</string>\n      <string>//path/to/xpi</string>\n    </array>\n    <key>Uninstall</key>\n    <array>\n      <string>bad_addon_id@mozilla.org</string>\n    </array>\n    <key>Locked</key>\n    <array>\n      <string>addon_id@mozilla.org</string>\n    </array>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Extensions\": {\n      \"Install\": [\"https://addons.mozilla.org/firefox/downloads/somefile.xpi\", \"//path/to/xpi\"],\n      \"Uninstall\": [\"bad_addon_id@mozilla.org\"],\n      \"Locked\":  [\"addon_id@mozilla.org\"]\n    }\n  }\n}\n```\n### ExtensionSettings\nManage all aspects of extensions. This policy is based heavily on the [Chrome policy](https://dev.chromium.org/administrators/policy-list-3/extension-settings-full) of the same name.\n\nThis policy maps an extension ID to its configuration. With an extension ID, the configuration will be applied to the specified extension only. A default configuration can be set for the special ID \"*\", which will apply to all extensions that don't have a custom configuration set in this policy.\n\nTo obtain an extension ID, install the extension and go to about:support. You will see the ID in the Extensions section. I've also created an extension that makes it easy to find the ID of extensions on AMO. You can download it [here](https://github.com/mkaply/queryamoid/releases/tag/v0.1).\n\nThe configuration for each extension is another dictionary that can contain the fields documented below.\n\n| Name | Description |\n| --- | --- |\n| `installation_mode` | Maps to a string indicating the installation mode for the extension. The valid strings are `allowed`,`blocked`,`force_installed`, and `normal_installed`.\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`allowed` | Allows the extension to be installed by the user. This is the default behavior. There is no need for an install_url; it will automatically be allowed based on the ID.\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`blocked`| Blocks installation of the extension and removes it from the device if already installed.\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`force_installed`| The extension is automatically installed and can't be removed by the user. This option is not valid for the default configuration and requires an install_url.\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`normal_installed`| The extension is automatically installed but can be disabled by the user. This option is not valid for the default configuration and requires an install_url.\n| `install_url`| Maps to a URL indicating where Firefox can download a force_installed or normal_installed extension.  If installing from the local file system, use a [```file:///``` URL](https://en.wikipedia.org/wiki/File_URI_scheme). If installing from the addons.mozilla.org, use the following URL (substituting SHORT_NAME from the URL on AMO), https://addons.mozilla.org/firefox/downloads/latest/SHORT_NAME/latest.xpi. Languages packs are available from https://releases.mozilla.org/pub/firefox/releases/VERSION/PLATFORM/xpi/LANGUAGE.xpi. If you need to update the extension, you can change the name of the extension and it will be automatically updated. Extensions installed from file URLs will additional be updated when their internal version changes.\n| `install_sources` | A list of sources from which installing extensions is allowed. **This is unnecessary if you are only allowing the installation of certain extensions by ID.** Each item in this list is an extension-style match pattern. Users will be able to easily install items from any URL that matches an item in this list. Both the location of the *.xpi file and the page where the download is started from (i.e.  the referrer) must be allowed by these patterns. This setting can be used only for the default configuration.\n| `allowed_types` | This setting whitelists the allowed types of extension/apps that can be installed in Firefox. The value is a list of strings, each of which should be one of the following: \"extension\", \"theme\", \"dictionary\", \"locale\" This setting can be used only for the default configuration.\n| `blocked_install_message` | This maps to a string specifying the error message to display to users if they're blocked from installing an extension. This setting allows you to append text to the generic error message displayed when the extension is blocked. This could be be used to direct users to your help desk, explain why a particular extension is blocked, or something else. This setting can be used only for the default configuration.\n| `restricted_domains` | An array of domains on which content scripts can't be run. This setting can be used only for the default configuration.\n| `updates_disabled` | (Firefox 89, Firefox ESR 78.11) Boolean that indicates whether or not to disable automatic updates for an individual extension.\n\n**Compatibility:** Firefox 69, Firefox ESR 68.1 (As of Firefox 85, Firefox ESR 78.7, installing a theme makes it the default.)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\nSoftware\\Policies\\Mozilla\\Firefox\\ExtensionSettings (REG_MULTI_SZ) =\n```\n{\n  \"*\": {\n    \"blocked_install_message\": \"Custom error message.\",\n    \"install_sources\": [\"about:addons\",\"https://addons.mozilla.org/\"],\n    \"installation_mode\": \"blocked\",\n    \"allowed_types\": [\"extension\"]\n  },\n  \"uBlock0@raymondhill.net\": {\n    \"installation_mode\": \"force_installed\",\n    \"install_url\": \"https://addons.mozilla.org/firefox/downloads/latest/ublock-origin/latest.xpi\"\n  },\n  \"https-everywhere@eff.org\": {\n    \"installation_mode\": \"allowed\"\n  }\n}\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Extensions/ExtensionSettings\n```\nValue (string):\n```\n<enabled/>\n<data id=\"ExtensionSettings\" value='\n{\n  \"*\": {\n    \"blocked_install_message\": \"Custom error message.\",\n    \"install_sources\": [\"about:addons\",\"https://addons.mozilla.org/\"],\n    \"installation_mode\": \"blocked\",\n    \"allowed_types\": [\"extension\"]\n  },\n  \"uBlock0@raymondhill.net\": {\n    \"installation_mode\": \"force_installed\",\n    \"install_url\": \"https://addons.mozilla.org/firefox/downloads/latest/ublock-origin/latest.xpi\"\n  },\n    \"https-everywhere@eff.org\": {\n    \"installation_mode\": \"allowed\"\n  }\n}'/>\n```\n#### macOS\n```\n<dict>\n  <key>ExtensionSettings</key>\n  <dict>\n    <key>*</key>\n    <dict>\n      <key>blocked_install_message</key>\n      <string>Custom error message.</string>\n      <key>install_sources</key>\n      <array>\n        <string>about:addons</string>\n        <string>https://addons.mozilla.org/</string>\n      </array>\n      <key>installation_mode</key>\n      <string>blocked</string>\n      <key>allowed_types</key>\n      <array>\n        <string>extension</string>\n      </array>\n    </dict>\n    <key>uBlock0@raymondhill.net</key>\n    <dict>\n      <key>installation_mode</key>\n       <string>force_installed</string>\n      <key>install_url</key>\n      <string>https://addons.mozilla.org/firefox/downloads/latest/ublock-origin/latest.xpi</string>\n    </dict>\n    <key>https-everywhere@eff.org</key>\n    <dict>\n      <key>installation_mode</key>\n       <string>allowed</string>\n    </dict>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"ExtensionSettings\": {\n      \"*\": {\n        \"blocked_install_message\": \"Custom error message.\",\n        \"install_sources\": [\"about:addons\",\"https://addons.mozilla.org/\"],\n        \"installation_mode\": \"blocked\",\n        \"allowed_types\": [\"extension\"]\n      },\n      \"uBlock0@raymondhill.net\": {\n        \"installation_mode\": \"force_installed\",\n        \"install_url\": \"https://addons.mozilla.org/firefox/downloads/latest/ublock-origin/latest.xpi\"\n      },\n      \"https-everywhere@eff.org\": {\n        \"installation_mode\": \"allowed\"\n      }\n    }\n  }\n}\n```\n### ExtensionUpdate\nControl extension updates.\n\n**Compatibility:** Firefox 67, Firefox ESR 60.7\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `extensions.update.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\ExtensionUpdate = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Extensions/ExtensionUpdate\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>ExtensionUpdate</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"ExtensionUpdate\": true | false\n  }\n}\n```\n### FirefoxHome\nCustomize the Firefox Home page.\n\n**Compatibility:** Firefox 68, Firefox ESR 68 (SponsoredTopSites and SponsoredPocket were added in Firefox 95, Firefox ESR 91.4)\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.newtabpage.activity-stream.showSearch`, `browser.newtabpage.activity-stream.feeds.topsites`, `browser.newtabpage.activity-stream.feeds.section.highlights`, `browser.newtabpage.activity-stream.feeds.section.topstories`, `browser.newtabpage.activity-stream.feeds.snippets`, `browser.newtabpage.activity-stream.showSponsoredTopSites`, `browser.newtabpage.activity-stream.showSponsored`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\Search = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\TopSites = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\SponsoredTopSites = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\Highlights = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\Pocket = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\SponsoredPocket = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\Snippets = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FirefoxHome\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/CustomizeFirefoxHome\n```\nValue (string):\n```\n<enabled/>\n<data id=\"FirefoxHome_Search\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_TopSites\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_SponsoredTopSites\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_Highlights\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_Pocket\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_SponsoredPocket\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_Snippets\"  value=\"true | false\"/>\n<data id=\"FirefoxHome_Locked\"  value=\"true | false\"/>\n```\n#### macOS\n```\n<dict>\n  <key>FirefoxHome</key>\n  <dict>\n    <key>Search</key>\n    <true/> | <false/>\n    <key>TopSites</key>\n    <true/> | <false/>\n    <key>SponsoredTopSites</key>\n    <true/> | <false/>\n    <key>Highlights</key>\n    <true/> | <false/>\n    <key>Pocket</key>\n    <true/> | <false/>\n    <key>SponsoredPocket</key>\n    <true/> | <false/>\n    <key>Snippets</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"FirefoxHome\": {\n      \"Search\": true | false,\n      \"TopSites\": true | false,\n      \"SponsoredTopSites\": true | false,\n      \"Highlights\": true | false,\n      \"Pocket\": true | false,\n      \"SponsoredPocket\": true | false,\n      \"Snippets\": true | false,\n      \"Locked\": true | false\n    }\n  }\n}\n```\n### FlashPlugin (Deprecated)\nConfigure the default Flash plugin policy as well as origins for which Flash is allowed.\n\n`Allow` is a list of origins where Flash are allowed.\n\n`Block` is a list of origins where Flash is not allowed.\n\n`Default` determines whether or not Flash is allowed by default.\n\n`Locked` prevents the user from changing Flash preferences.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `permissions.plugin`\\\n**Preferences Affected:** `plugin.state.flash`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\FlashPlugin\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\FlashPlugin\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\FlashPlugin\\Default = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\FlashPlugin\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Flash/FlashPlugin_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org&#xF000;2&#xF000;https://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Flash/FlashPlugin_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Flash/FlashPlugin_Default\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>FlashPlugin</key>\n  <dict>\n    <key>Allow</key>\n    <array>\n      <string>http://example.org</string>\n    </array>\n    <key>Block</key>\n    <array>\n      <string>http://example.edu</string>\n    </array>\n    <key>Default</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"FlashPlugin\": {\n      \"Allow\": [\"http://example.org/\"],\n      \"Block\": [\"http://example.edu/\"],\n      \"Default\": true | false,\n      \"Locked\": true | false\n    }\n  }\n}\n```\n### Handlers\nConfigure default application handlers. This policy is based on the internal format of `handlers.json`.\n\nYou can configure handlers based on a mime type (`mimeTypes`), a file's extension (`extensions`), or a protocol (`schemes`).\n\nWithin each handler type, you specify the given mimeType/extension/scheme as a key and use the following subkeys to describe how it is handled.\n\n| Name | Description |\n| --- | --- |\n| `action`| Can be either `saveToDisk`, `useHelperApp`, `useSystemDefault`.\n| `ask` | If `true`, the user is asked if what they want to do with the file. If `false`, the action is taken without user intervention.\n| `handlers` | An array of handlers with the first one being the default. If you don't want to have a default handler, use an empty object for the first handler. Choose between path or uriTemplate.\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`name` | The display name of the handler (might not be used).\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`path`| The native path to the executable to be used.\n| &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`uriTemplate`| A url to a web based application handler. The URL must be https and contain a %s to be used for substitution.\n\n**Compatibility:** Firefox 78, Firefox ESR 78\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\nSoftware\\Policies\\Mozilla\\Firefox\\Handlers (REG_MULTI_SZ) =\n```\n{\n  \"mimeTypes\": {\n    \"application/msword\": {\n      \"action\": \"useSystemDefault\",\n      \"ask\": true | false\n    }\n  },\n  \"schemes\": {\n    \"mailto\": {\n      \"action\": \"useHelperApp\",\n      \"ask\": true | false,\n      \"handlers\": [{\n        \"name\": \"Gmail\",\n        \"uriTemplate\": \"https://mail.google.com/mail/?extsrc=mailto&url=%s\"\n      }]\n    }\n  },\n  \"extensions\": {\n    \"pdf\": {\n      \"action\": \"useHelperApp\",\n      \"ask\": true | false,\n      \"handlers\": [{\n        \"name\": \"Adobe Acrobat\",\n        \"path\": \"C:\\\\Program Files (x86)\\\\Adobe\\\\Acrobat Reader DC\\\\Reader\\\\AcroRd32.exe\"\n      }]\n    }\n  }\n}\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/Handlers\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Handlers\" value='\n{\n  \"mimeTypes\": {\n    \"application/msword\": {\n      \"action\": \"useSystemDefault\",\n      \"ask\": true | false\n    }\n  },\n  \"schemes\": {\n    \"mailto\": {\n      \"action\": \"useHelperApp\",\n      \"ask\": true | false,\n      \"handlers\": [{\n        \"name\": \"Gmail\",\n        \"uriTemplate\": \"https://mail.google.com/mail/?extsrc=mailto&amp;url=%s\"\n      }]\n    }\n  },\n  \"extensions\": {\n    \"pdf\": {\n      \"action\": \"useHelperApp\",\n      \"ask\": true | false,\n      \"handlers\": [{\n        \"name\": \"Adobe Acrobat\",\n        \"path\": \"C:\\\\Program Files (x86)\\\\Adobe\\\\Acrobat Reader DC\\\\Reader\\\\AcroRd32.exe\"\n      }]\n    }\n  }\n}\n'/>\n```\n#### macOS\n```\n<dict>\n  <key>Handlers</key>\n  <dict>\n    <key>mimeTypes</key>\n    <dict>\n      <key>application/msword</key>\n      <dict>\n        <key>action</key>\n        <string>useSystemDefault</string>\n        <key>ask</key>\n        <true/> | <false/>\n      </dict>\n    </dict>\n    <key>schemes</key>\n    <dict>\n      <key>mailto</key>\n      <dict>\n        <key>action</key>\n        <string>useHelperApp</string>\n        <key>ask</key>\n        <true/> | <false/>\n        <key>handlers</key>\n        <array>\n          <dict>\n            <key>name</key>\n            <string>Gmail</string>\n            <key>uriTemplate</key>\n            <string>https://mail.google.com/mail/?extsrc=mailto&url=%s</string>\n          </dict>\n        </array>\n      </dict>\n    </dict>\n    <key>extensions</key>\n    <dict>\n      <key>pdf</key>\n      <dict>\n        <key>action</key>\n        <string>useHelperApp</string>\n        <key>ask</key>\n        <true/> | <false/>\n        <key>handlers</key>\n        <array>\n          <dict>\n            <key>name</key>\n            <string>Adobe Acrobat</string>\n            <key>path</key>\n            <string>/System/Applications/Preview.app</string>\n          </dict>\n        </array>\n      </dict>\n    </dict>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Handlers\": {\n      \"mimeTypes\": {\n        \"application/msword\": {\n          \"action\": \"useSystemDefault\",\n          \"ask\": false\n        }\n      },\n      \"schemes\": {\n        \"mailto\": {\n          \"action\": \"useHelperApp\",\n          \"ask\": true | false,\n          \"handlers\": [{\n            \"name\": \"Gmail\",\n            \"uriTemplate\": \"https://mail.google.com/mail/?extsrc=mailto&url=%s\"\n          }]\n        }\n      },\n      \"extensions\": {\n        \"pdf\": {\n          \"action\": \"useHelperApp\",\n          \"ask\": true | false,\n          \"handlers\": [{\n            \"name\": \"Adobe Acrobat\",\n            \"path\": \"/usr/bin/acroread\"\n          }]\n        }\n      }\n    }\n  }\n}\n```\n### HardwareAcceleration\nControl hardware acceleration.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `layers.acceleration.disabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\HardwareAcceleration = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/HardwareAcceleration\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>HardwareAcceleration</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"HardwareAcceleration\": true | false\n  }\n}\n```\n### Homepage\nConfigure the default homepage and how Firefox starts.\n\n`URL` is the default homepage.\n\n`Locked` prevents the user from changing homepage preferences.\n\n`Additional` allows for more than one homepage.\n\n`StartPage` is how Firefox starts. The choices are no homepage, the default homepage or the previous session.\n\nWith Firefox 78, an additional option as added for `Startpage`, `homepage-locked`. If this is value is set for the Startpage, the user will always get the homepage at startup and cannot choose to restore their session.\n\n**Compatibility:** Firefox 60, Firefox ESR 60 (StartPage was added in Firefox 60, Firefox ESR 60.4, homepage-locked added in Firefox 78)\\\n**CCK2 Equivalent:** `homePage`,`lockHomePage`\\\n**Preferences Affected:** `browser.startup.homepage`, `browser.startup.page`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Homepage\\URL = \"https://example.com\"\nSoftware\\Policies\\Mozilla\\Firefox\\Homepage\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Homepage\\Additional\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Homepage\\Additional\\2 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Homepage\\StartPage = \"none\" | \"homepage\" | \"previous-session\" | \"homepage-locked\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Homepage/HomepageURL\n```\nValue (string):\n```\n<enabled/>\n\n<data id=\"HomepageURL\" value=\"https://example.com\"/>\n<data id=\"HomepageLocked\" value=\"true | false\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Homepage/HomepageAdditional\n```\nValue (string):\n```\n<enabled/>\n\n<data id=\"HomepageAdditional\" value=\"1&#xF000;http://example.org&#xF000;2&#xF000;http://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Homepage/HomepageStartPage\n```\nValue (string):\n```\n<enabled/>\n\n<data id=\"StartPage\" value=\"none | homepage | previous-session\"/>\n```\n#### macOS\n```\n<dict>\n  <key>Homepage</key>\n  <dict>\n    <key>URL</key>\n    <string>http://example.com</string>\n    <key>Locked</key>\n    <true/> | <false/>\n    <key>Additional</key>\n    <array>\n      <string>http://example.org</string>\n      <string>http://example.edu</string>\n    </array>\n    <key>StartPage</key>\n    <string>none | homepage | previous-session | homepage-locked</string>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Homepage\": {\n      \"URL\": \"http://example.com/\",\n      \"Locked\": true | false,\n      \"Additional\": [\"http://example.org/\",\n                     \"http://example.edu/\"],\n      \"StartPage\": \"none\" | \"homepage\" | \"previous-session\" | \"homepage-locked\"\n    }\n  }\n}\n```\n### InstallAddonsPermission\nConfigure the default extension install policy as well as origins for extension installs are allowed. This policy does not override turning off all extension installs.\n\n`Allow` is a list of origins where extension installs are allowed.\n\n`Default` determines whether or not extension installs are allowed by default.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `permissions.install`\\\n**Preferences Affected:** `xpinstall.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\InstallAddonsPermission\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\InstallAddonsPermission\\Allow\\2 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\InstallAddonsPermission\\Default = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Addons/InstallAddonsPermission_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org&#xF000;2&#xF000;https://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Addons/InstallAddonsPermission_Default\n```\nValue (string):\n```\n<enabled/>\n```\n#### macOS\n```\n<dict>\n  <key>InstallAddonsPermission</key>\n  <dict>\n    <key>Allow</key>\n    <array>\n      <string>http://example.org</string>\n      <string>http://example.edu</string>\n    </array>\n    <key>Default</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"InstallAddonsPermission\": {\n      \"Allow\": [\"http://example.org/\",\n                \"http://example.edu/\"],\n      \"Default\": true | false\n    }\n  }\n}\n```\n### LegacyProfiles\nDisable the feature enforcing a separate profile for each installation.\n\nIf this policy set to true, Firefox will not try to create different profiles for installations of Firefox in different directories. This is the equivalent of the MOZ_LEGACY_PROFILES environment variable.\n\nIf this policy set to false, Firefox will create a new profile for each unique installation of Firefox.\n\nThis policy only work on Windows via GPO (not policies.json).\n\n**Compatibility:** Firefox 70, Firefox ESR 68.2 (Windows only, GPO only)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\LegacyProfiles = = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/LegacyProfiles\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n### LegacySameSiteCookieBehaviorEnabled\nEnable default legacy SameSite cookie behavior setting.\n\nIf this policy is set to true, it reverts all cookies to legacy SameSite behavior which means that cookies that don't explicitly specify a ```SameSite``` attribute are treated as if they were ```SameSite=None```.\n\n**Compatibility:** Firefox 96\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.cookie.sameSite.laxByDefault`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\LegacySameSiteCookieBehaviorEnabled = = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/LegacySameSiteCookieBehaviorEnabled\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>LegacySameSiteCookieBehaviorEnabled</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"LegacySameSiteCookieBehaviorEnabled\": true | false\n}\n```\n### LegacySameSiteCookieBehaviorEnabledForDomainList\nRevert to legacy SameSite behavior for cookies on specified sites.\n\nIf this policy is set to true, cookies set for domains in this list will revert to legacy SameSite behavior which means that cookies that don't explicitly specify a ```SameSite``` attribute are treated as if they were ```SameSite=None```.\n\n**Compatibility:** Firefox 96\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.cookie.sameSite.laxByDefault.disabledHosts`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\LegacySameSiteCookieBehaviorEnabledForDomainList\\1 = \"example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\LegacySameSiteCookieBehaviorEnabledForDomainList\\2 = \"example.edu\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/LegacySameSiteCookieBehaviorEnabledForDomainList\n```\nValue (string):\n```\n<enabled/>\n<data id=\"LegacySameSiteCookieBehaviorEnabledForDomainList\" value=\"1&#xF000;example.org&#xF000;2&#xF000;example.edu\"/>\n```\n#### macOS\n```\n<dict>\n  <key>LegacySameSiteCookieBehaviorEnabledForDomainList</key>\n  <array>\n    <string>example.org</string>\n    <string>example.edu</string>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"LegacySameSiteCookieBehaviorEnabledForDomainList\": [\"example.org\",\n                                                         \"example.edu\"]\n  }\n}\n```\n### LocalFileLinks\nEnable linking to local files by origin.\n\n**Compatibility:** Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `capability.policy.localfilelinks.*`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\LocalFileLinks\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\LocalFileLinks\\2 = \"https://example.edu\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/LocalFileLinks\n```\nValue (string):\n```\n<enabled/>\n<data id=\"LocalFileLinks\" value=\"1&#xF000;https://example.org&#xF000;2&#xF000;https://example.edu\"/>\n```\n#### macOS\n```\n<dict>\n  <key>LocalFileLinks</key>\n  <array>\n    <string>http://example.org</string>\n    <string>http://example.edu</string>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"LocalFileLinks\": [\"http://example.org/\",\n                       \"http://example.edu/\"]\n  }\n}\n```\n### ManagedBookmarks\nConfigures a list of bookmarks managed by an administrator that cannot be changed by the user.\n\nThe bookmarks are only added as a button on the personal toolbar. They are not in the bookmarks folder.\n\nThe syntax of this policy is exactly the same as the [Chrome ManagedBookmarks policy](https://cloud.google.com/docs/chrome-enterprise/policies/?policy=ManagedBookmarks). The schema is:\n```\n{\n \"items\": {\n  \"id\": \"BookmarkType\",\n  \"properties\": {\n   \"children\": {\n    \"items\": {\n     \"$ref\": \"BookmarkType\"\n    },\n    \"type\": \"array\"\n   },\n   \"name\": {\n    \"type\": \"string\"\n   },\n   \"toplevel_name\": {\n    \"type\": \"string\"\n   },\n   \"url\": {\n    \"type\": \"string\"\n   }\n  },\n  \"type\": \"object\"\n },\n \"type\": \"array\"\n}\n```\n**Compatibility:** Firefox 83, Firefox ESR 78.5\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\nSoftware\\Policies\\Mozilla\\Firefox\\ManagedBookmarks (REG_MULTI_SZ) =\n```\n[\n  {\n    \"toplevel_name\": \"My managed bookmarks folder\"\n  },\n  {\n    \"url\": \"example.com\",\n    \"name\": \"Example\"\n  },\n  {\n    \"name\": \"Mozilla links\",\n    \"children\": [\n      {\n        \"url\": \"https://mozilla.org\",\n        \"name\": \"Mozilla.org\"\n      },\n      {\n        \"url\": \"https://support.mozilla.org/\",\n        \"name\": \"SUMO\"\n      }\n    ]\n  }\n]\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/ManagedBookmarks\n```\nValue (string):\n```\n<enabled/>\n<data id=\"JSON\" value='\n[\n  {\n    \"toplevel_name\": \"My managed bookmarks folder\"\n  },\n  {\n    \"url\": \"example.com\",\n    \"name\": \"Example\"\n  },\n  {\n    \"name\": \"Mozilla links\",\n    \"children\": [\n      {\n        \"url\": \"https://mozilla.org\",\n        \"name\": \"Mozilla.org\"\n      },\n      {\n        \"url\": \"https://support.mozilla.org/\",\n        \"name\": \"SUMO\"\n      }\n    ]\n  }\n]'/>\n```\n#### macOS\n```\n<dict>\n  <key>ManagedBookmarks</key>\n  <array>\n    <dict>\n      <key>toplevel_name</key>\n      <string>My managed bookmarks folder</string>\n      <dict>\n        <key>url</key>\n        <string>example.com</string>\n        <key>name</key>\n        <string>Example</string>\n      </dict>\n      <dict>\n      <key>name</key>\n      <string>Mozilla links</string>\n      <key>children</key>\n      <array>\n        <dict>\n          <key>url</key>\n          <string>https://mozilla.org</string>\n          <key>name</key>\n          <string>Mozilla</string>\n        </dict>\n        <dict>\n          <key>url</key>\n          <string>https://support.mozilla.org/</string>\n          <key>name</key>\n          <string>SUMO</string>\n        </dict>\n      </array>\n    </dict>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"ManagedBookmarks\": [\n      {\n        \"toplevel_name\": \"My managed bookmarks folder\"\n      },\n      {\n        \"url\": \"example.com\",\n        \"name\": \"Example\"\n      },\n      {\n        \"name\": \"Mozilla links\",\n        \"children\": [\n          {\n            \"url\": \"https://mozilla.org\",\n            \"name\": \"Mozilla.org\"\n          },\n          {\n            \"url\": \"https://support.mozilla.org/\",\n            \"name\": \"SUMO\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n### ManualAppUpdateOnly\n\nSwitch to manual updates only.\n\nIf this policy is enabled:\n 1. The user will never be prompted to install updates\n 2. Firefox will not check for updates in the background, though it will check automatically when an update UI is displayed (such as the one in the About dialog). This check will be used to show \"Update to version X\" in the UI, but will not automatically download the update or prompt the user to update in any other way.\n 3. The update UI will work as expected, unlike when using DisableAppUpdate.\n\nThis policy is primarily intended for advanced end users, not for enterprises.\n\n**Compatibility:** Firefox 87\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### policies.json\n```\n{\n  \"policies\": {\n    \"ManualAppUpdateOnly\": true | false\n  }\n}\n```\n### NetworkPrediction\nEnable or disable network prediction (DNS prefetching).\n\n**Compatibility:** Firefox 67, Firefox ESR 60.7\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.dns.disablePrefetch`, `network.dns.disablePrefetchFromHTTPS`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\NetworkPrediction = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/NetworkPrediction\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>NetworkPrediction</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"NetworkPrediction\": true | false\n}\n```\n### NewTabPage\nEnable or disable the New Tab page.\n\n**Compatibility:** Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.newtabpage.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\NewTabPage = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/NewTabPage\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>NewTabPage</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"NewTabPage\": true | false\n}\n```\n### NoDefaultBookmarks\nDisable the creation of default bookmarks.\n\nThis policy is only effective if the user profile has not been created yet.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `removeDefaultBookmarks`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\NoDefaultBookmarks = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/NoDefaultBookmarks\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>NoDefaultBookmarks</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"NoDefaultBookmarks\": true | false\n  }\n}\n```\n### OfferToSaveLogins\nControl whether or not Firefox offers to save passwords.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `dontRememberPasswords`\\\n**Preferences Affected:** `signon.rememberSignons`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\OfferToSaveLogins = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/OfferToSaveLogins\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>OfferToSaveLogins</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"OfferToSaveLogins\": true | false\n  }\n}\n```\n### OfferToSaveLoginsDefault\nSets the default value of signon.rememberSignons without locking it.\n\n**Compatibility:** Firefox 70, Firefox ESR 60.2\\\n**CCK2 Equivalent:** `dontRememberPasswords`\\\n**Preferences Affected:** `signon.rememberSignons`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\OfferToSaveLoginsDefault = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/OfferToSaveLoginsDefault\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>OfferToSaveLoginsDefault</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"OfferToSaveLoginsDefault\": true | false\n  }\n}\n```\n### OverrideFirstRunPage\nOverride the first run page. If the value is an empty string (\"\"), the first run page is not displayed.\n\nStarting with Firefox 83, Firefox ESR 78.5, you can also specify multiple URLS separated by a vertical bar (|).\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `welcomePage`,`noWelcomePage`\\\n**Preferences Affected:** `startup.homepage_welcome_url`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\OverrideFirstRunPage = \"http://example.org\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/OverrideFirstRunPage\n```\nValue (string):\n```\n<enabled/>\n<data id=\"OverridePage\" value=\"https://example.com\"/>\n```\n#### macOS\n```\n<dict>\n  <key>OverrideFirstRunPage</key>\n  <string>http://example.org</string>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"OverrideFirstRunPage\": \"http://example.org\"\n  }\n}\n```\n### OverridePostUpdatePage\nOverride the upgrade page. If the value is an empty string (\"\"), no extra pages are displayed when Firefox is upgraded.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `upgradePage`,`noUpgradePage`\\\n**Preferences Affected:** `startup.homepage_override_url`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\OverridePostUpdatePage = \"http://example.org\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/OverridePostUpdatePage\n```\nValue (string):\n```\n<enabled/>\n<data id=\"OverridePage\" value=\"https://example.com\"/>\n```\n#### macOS\n```\n<dict>\n  <key>OverridePostUpdatePage</key>\n  <string>http://example.org</string>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"OverridePostUpdatePage\": \"http://example.org\"\n  }\n}\n```\n### PasswordManagerEnabled\nRemove access to the password manager via preferences and blocks about:logins on Firefox 70.\n\n**Compatibility:** Firefox 70, Firefox ESR 60.2\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `pref.privacy.disable_button.view_passwords`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PasswordManagerEnabled = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/PasswordManagerEnabled\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>PasswordManagerEnabled</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PasswordManagerEnabled\": true | false\n  }\n}\n```\n### PasswordManagerExceptions\nPrevent Firefox from saving passwords for specific sites.\n\nThe sites are specified as a list of origins.\n\n**Compatibility:** Firefox 101\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PasswordManagerExceptions\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\PasswordManagerExceptions\\2 = \"https://example.edu\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/PasswordManagerExceptions\n```\nValue (string):\n```\n<enabled/>\n<data id=\"List\" value=\"1&#xF000;https://example.org&#xF000;2&#xF000;https://example.edu\"/>\n```\n#### macOS\n```\n<dict>\n  <key>PasswordManagerExceptions</key>\n  <array>\n    <string>https://example.org</string>\n    <string>https://example.edu</string>\n  </array>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PasswordManagerExceptions\": [\"https://example.org\",\n                                  \"https://example.edu\"]\n  }\n}\n```\n\n### PDFjs\nDisable or configure PDF.js, the built-in PDF viewer.\n\nIf `Enabled` is set to false, the built-in PDF viewer is disabled.\n\nIf `EnablePermissions` is set to true, the built-in PDF viewer will honor document permissions like preventing the copying of text.\n\nNote: DisableBuiltinPDFViewer has not been deprecated. You can either continue to use it, or switch to using PDFjs->Enabled to disable the built-in PDF viewer. This new permission was added because we needed a place for PDFjs->EnabledPermissions.\n\n**Compatibility:** Firefox 77, Firefox ESR 68.9\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `pdfjs.diabled`, `pdfjs.enablePermissions`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PDFjs\\Enabled = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\PDFjs\\EnablePermissions = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~PDFjs/PDFjs_Enabled\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~PDFjs/PDFjs_EnablePermissions\n```\nValue (string):\n```\n<enabled/>or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>PDFjs</key>\n  <dict>\n    <key>Enabled</key>\n    <true/> | <false/>\n    <key>EnablePermissions</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PDFjs\": {\n      \"Enabled\": true | false,\n      \"EnablePermissions\": true | false\n    }\n  }\n}\n```\n### Permissions\nSet permissions associated with camera, microphone, location, notifications, autoplay, and virtual reality. Because these are origins, not domains, entries with unique ports must be specified separately. This explicitly means that it is not possible to add wildcards. See examples below.\n\n`Allow` is a list of origins where the feature is allowed.\n\n`Block` is a list of origins where the feature is not allowed.\n\n`BlockNewRequests` determines whether or not new requests can be made for the feature.\n\n`Locked` prevents the user from changing preferences for the feature.\n\n`Default` specifies the default value for Autoplay. block-audio-video is not supported on Firefox ESR 68.\n\n**Compatibility:** Firefox 62, Firefox ESR 60.2 (Autoplay added in Firefox 74, Firefox ESR 68.6, Autoplay Default/Locked added in Firefox 76, Firefox ESR 68.8, VirtualReality added in Firefox 80, Firefox ESR 78.2)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `permissions.default.camera`, `permissions.default.microphone`, `permissions.default.geo`, `permissions.default.desktop-notification`, `media.autoplay.default`, `permissions.default.xr`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Camera\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Camera\\Allow\\2 = \"https://example.org:1234\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Camera\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Camera\\BlockNewRequests = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Camera\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Microphone\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Microphone\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Microphone\\BlockNewRequests = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Microphone\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Location\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Location\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Location\\BlockNewRequests = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Location\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Notifications\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Notifications\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Notifications\\BlockNewRequests = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Notifications\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Autoplay\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Autoplay\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Autoplay\\Default = \"allow-audio-video\" | \"block-audio\" | \"block-audio-video\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\Autoplay\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\VirtualReality\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\VirtualReality\\Block\\1 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\VirtualReality\\BlockNewRequests = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Permissions\\VirtualReality\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Location/Location_BlockNewRequests\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Location/Location_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/Notifications_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/Notifications_BlockNewRequests\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/Notifications_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Autoplay/Autoplay_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Autoplay/Autoplay_Block\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Autoplay/Autoplay_Default\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Autoplay_Default\" value=\"allow-audio-video | block-audio | block-audio-video\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Autoplay/Autoplay_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/VirtualReality_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/VirtualReality_Block\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/VirtualReality_BlockNewRequests\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Permissions~Notifications/VirtualReality_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>Permissions</key>\n  <dict>\n    <key>Camera</key>\n    <dict>\n      <key>Allow</key>\n      <array>\n        <string>https://example.org</string>\n        <string>https://example.org:1234</string>\n      </array>\n      <key>Block</key>\n      <array>\n        <string>https://example.edu</string>\n      </array>\n      <key>BlockNewRequests</key>\n      <true/> | <false/>\n      <key>Locked</key>\n      <true/> | <false/>\n    </dict>\n    <key>Microphone</key>\n    <dict>\n      <key>Allow</key>\n      <array>\n        <string>https://example.org</string>\n      </array>\n      <key>Block</key>\n      <array>\n        <string>https://example.edu</string>\n      </array>\n      <key>BlockNewRequests</key>\n      <true/> | <false/>\n      <key>Locked</key>\n      <true/> | <false/>\n    </dict>\n    <key>Location</key>\n    <dict>\n      <key>Allow</key>\n      <array>\n        <string>https://example.org</string>\n      </array>\n      <key>Block</key>\n      <array>\n        <string>https://example.edu</string>\n      </array>\n      <key>BlockNewRequests</key>\n      <true/> | <false/>\n      <key>Locked</key>\n      <true/> | <false/>\n    </dict>\n    <key>Notifications</key>\n    <dict>\n      <key>Allow</key>\n      <array>\n        <string>https://example.org</string>\n      </array>\n      <key>Block</key>\n      <array>\n        <string>https://example.edu</string>\n      </array>\n      <key>BlockNewRequests</key>\n      <true/>\n      <key>Locked</key>\n      <true/>\n    </dict>\n    <key>Autoplay</key>\n    <dict>\n      <key>Allow</key>\n      <array>\n        <string>https://example.org</string>\n      </array>\n      <key>Block</key>\n      <array>\n        <string>https://example.edu</string>\n      </array>\n      <key>Default</key>\n      <string>allow-audio-video | block-audio | block-audio-video</string>\n      <key>Locked</key>\n      <true/> | <false/>\n    </dict>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Permissions\": {\n      \"Camera\": {\n        \"Allow\": [\"https://example.org\",\"https://example.org:1234\"],\n        \"Block\": [\"https://example.edu\"],\n        \"BlockNewRequests\": true | false,\n        \"Locked\": true | false\n      },\n      \"Microphone\": {\n        \"Allow\": [\"https://example.org\"],\n        \"Block\": [\"https://example.edu\"],\n        \"BlockNewRequests\": true | false,\n        \"Locked\": true | false\n      },\n      \"Location\": {\n        \"Allow\": [\"https://example.org\"],\n        \"Block\": [\"https://example.edu\"],\n        \"BlockNewRequests\": true | false,\n        \"Locked\": true | false\n      },\n      \"Notifications\": {\n        \"Allow\": [\"https://example.org\"],\n        \"Block\": [\"https://example.edu\"],\n        \"BlockNewRequests\": true | false,\n        \"Locked\": true | false\n      },\n      \"Autoplay\": {\n        \"Allow\": [\"https://example.org\"],\n        \"Block\": [\"https://example.edu\"],\n        \"Default\": \"allow-audio-video\" | \"block-audio\" | \"block-audio-video\",\n        \"Locked\": true | false\n      }\n    }\n  }\n}\n```\n### PictureInPicture\n\nEnable or disable Picture-in-Picture as well as prevent the user from enabling or disabling it (Locked).\n\n**Compatibility:** Firefox 78, Firefox ESR 78\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `media.videocontrols.picture-in-picture.video-toggle.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PictureInPicture\\Enabled = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\PictureInPicture\\Locked = 0x1 | 0x0\n\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~PictureInPicture/PictureInPicture_Enabled\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~PictureInPicture/PictureInPicture_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>PictureInPicture</key>\n  <dict>\n    <key>Enabled</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PictureInPicture\": {\n      \"Enabled\": true | false,\n      \"Locked\": true | false\n    }\n  }\n}\n```\n### PopupBlocking\nConfigure the default pop-up window policy as well as origins for which pop-up windows are allowed.\n\n`Allow` is a list of origins where popup-windows are allowed.\n\n`Default` determines whether or not pop-up windows are allowed by default.\n\n`Locked` prevents the user from changing pop-up preferences.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `permissions.popup`\\\n**Preferences Affected:** `dom.disable_open_during_load`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PopupBlocking\\Allow\\1 = \"https://example.org\"\nSoftware\\Policies\\Mozilla\\Firefox\\PopupBlocking\\Allow\\2 = \"https://example.edu\"\nSoftware\\Policies\\Mozilla\\Firefox\\PopupBlocking\\Default = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\PopupBlocking\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Popups/PopupBlocking_Allow\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Permissions\" value=\"1&#xF000;https://example.org&#xF000;2&#xF000;https://example.edu\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Popups/PopupBlocking_Default\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Popups/PopupBlocking_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>PopupBlocking</key>\n  <dict>\n    <key>Allow</key>\n    <array>\n      <string>http://example.org</string>\n      <string>http://example.edu</string>\n    </array>\n    <key>Default</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PopupBlocking\": {\n      \"Allow\": [\"http://example.org/\",\n                \"http://example.edu/\"],\n      \"Default\": true | false,\n      \"Locked\": true | false\n    }\n  }\n}\n```\n### Preferences\nSet and lock preferences.\n\n**NOTE** On Windows, in order to use this policy, you must clear all settings in the old **Preferences (Deprecated)** section.\n\nPreviously you could only set and lock a subset of preferences. Starting with Firefox 81 and Firefox ESR 78.3 you can set many more preferences. You can also set default preferences, user preferences and you can clear preferences.\n\nPreferences that start with the following prefixes are supported:\n```\naccessibility.\napp.update.* (Firefox 86, Firefox 78.8)\nbrowser.\ndatareporting.policy.\ndom.\nextensions.\ngeneral.autoScroll (Firefox 83, Firefox ESR 78.5)\ngeneral.smoothScroll (Firefox 83, Firefox ESR 78.5)\ngeo.\ngfx.\nintl.\nkeyword.enabled (Firefox 95, Firefox ESR 91.4)\nlayers.\nlayout.\nmedia.\nnetwork.\npdfjs. (Firefox 84, Firefox ESR 78.6)\nplaces.\nprint.\nsignon. (Firefox 83, Firefox ESR 78.5)\nspellchecker. (Firefox 84, Firefox ESR 78.6)\ntoolkit.legacyUserProfileCustomizations.stylesheets (Firefox 95, Firefox ESR 91.4)\nui.\nwidget.\n```\nas well as the following security preferences:\n| Preference | Type | Default\n| --- | --- | ---\n| security.default_personal_cert | string | Ask Every Time\n| &nbsp;&nbsp;&nbsp;&nbsp;If set to Select Automatically, Firefox automatically chooses the default personal certificate.\n| security.insecure_connection_text.enabled | bool | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If set to true, adds the words \"Not Secure\" for insecure sites.\n| security.insecure_connection_text.pbmode.enabled | bool | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If set to true, adds the words \"Not Secure\" for insecure sites in private browsing.\n| security.insecure_field_warning.contextual.enabled | bool | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If set to false, remove the warning for inscure login fields.\n| security.mixed_content.block_active_content | boolean | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, mixed active content (HTTP and HTTPS) is not blocked.\n| security.osclientcerts.autoload | boolean | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, client certificates are loaded from the operating system certificate store.\n| security.ssl.errorReporting.enabled | boolean | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, SSL errors cannot be sent to Mozilla.\n| security.tls.enable_0rtt_data | boolean | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, TLS early data is turned off (Firefox 93, Firefox 91.2, Firefox 78.15).\n| security.tls.hello_downgrade_check | boolean | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the TLS 1.3 downgrade check is disabled.\n| security.tls.version.enable-deprecated | boolean | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, browser will accept TLS 1.0. and TLS 1.1 (Firefox 86, Firefox 78.8).\n| security.warn_submit_secure_to_insecure | boolean | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, no warning is shown when submitting a form from https to http.\n&nbsp;\n\nUsing the preference as the key, set the `Value` to the corresponding preference value.\n\n`Status` can be \"default\", \"locked\", \"user\" or \"clear\"\n\n* `\"default\"`: Read/Write: Settings appear as default even if factory default differs.\n* `\"locked\"`: Read-Only: Settings appear as default even if factory default differs.\n* `\"user\"`: Read/Write: Settings appear as changed if it differs from factory default.\n* `\"clear\"`: Read/Write: `Value` has no effect. Resets to factory defaults on each startup.\n\n`\"user\"` preferences persist across invocations of Firefox. It is the equivalent of a user setting the preference. They are most useful when a preference is needed very early in startup so it can't be set as default by policy. An example of this is ```toolkit.legacyUserProfileCustomizations.stylesheets```.\n\n`\"user\"` preferences persist even if the policy is removed, so if you need to remove them, you should use the clear policy.\n\nSee the examples below for more detail.\n\nIMPORTANT: Make sure you're only setting a particular preference using this mechanism and not some other way.\n\nStatus\n**Compatibility:** Firefox 81, Firefox ESR 78.3\\\n**CCK2 Equivalent:** `preferences`\\\n**Preferences Affected:** Many\n\n#### Windows (GPO)\nSoftware\\Policies\\Mozilla\\Firefox\\Preferences (REG_MULTI_SZ) =\n```\n{\n  \"accessibility.force_disabled\": {\n    \"Value\": 1,\n    \"Status\": \"default\"\n  },\n  \"browser.cache.disk.parent_directory\": {\n    \"Value\": \"SOME_NATIVE_PATH\",\n    \"Status\": \"user\"\n  },\n  \"browser.tabs.warnOnClose\": {\n    \"Value\": false,\n    \"Status\": \"locked\"\n  }\n}\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/Preferences\n```\nValue (string):\n```\n<enabled/>\n<data id=\"JSON\" value='\n{\n  \"accessibility.force_disabled\": {\n    \"Value\": 1,\n    \"Status\": \"default\"\n  },\n  \"browser.cache.disk.parent_directory\": {\n    \"Value\": \"SOME_NATIVE_PATH\",\n    \"Status\": \"user\"\n  },\n  \"browser.tabs.warnOnClose\": {\n    \"Value\": false,\n    \"Status\": \"locked\"\n  }\n}'/>\n```\n#### macOS\n```\n<dict>\n  <key>Preferences</key>\n  <dict>\n    <key>accessibility.force_disabled</key>\n    <dict>\n      <key>Value</key>\n      <integer>1</integer>\n      <key>Status</key>\n      <string>default</string>\n    </dict>\n    <key>browser.cache.disk.parent_directory</key>\n    <dict>\n      <key>Value</key>\n      <string>SOME_NATIVE_PATH</string>\n      <key>Status</key>\n      <string>user</string>\n    </dict>\n    <key>browser.tabs.warnOnClose</key>\n    <dict>\n      <key>Value</key>\n      <false/>\n      <key>Status</key>\n      <string>locked</string>\n    </dict>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Preferences\": {\n      \"accessibility.force_disabled\": {\n        \"Value\": 1,\n        \"Status\": \"default\"\n      },\n      \"browser.cache.disk.parent_directory\": {\n        \"Value\": \"SOME_NATIVE_PATH\",\n        \"Status\": \"user\"\n      },\n      \"browser.tabs.warnOnClose\": {\n        \"Value\": false,\n        \"Status\": \"locked\"\n      }\n    }\n  }\n}\n```\n### Preferences (Deprecated)\nSet and lock certain preferences.\n\n**Compatibility:** See below\\\n**CCK2 Equivalent:** `preferences`\\\n**Preferences Affected:** See below\n\n| Preference | Type | Compatibility | Default\n| --- | --- | --- | ---\n| accessibility.force_disabled | integer | Firefox 70, Firefox ESR 68.2 | 0\n| &nbsp;&nbsp;&nbsp;&nbsp;If set to 1, platform accessibility is disabled.\n| app.update.auto (Deprecated - Switch to AppAutoUpdate policy) | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, Firefox doesn't automatically install update.\n| browser.bookmarks.autoExportHTML | boolean | Firefox 70, Firefox ESR 68.2 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, bookmarks are exported on shutdown.\n| browser.bookmarks.file | string | Firefox 70, Firefox ESR 68.2 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;If set, the name of the file where bookmarks are exported and imported.\n| browser.bookmarks.restore_default_bookmarks | boolean | Firefox 70, Firefox ESR 68.2 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, bookmarks are restored to their defaults.\n| browser.cache.disk.enable | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, don't store cache on the hard drive.\n| ~browser.cache.disk.parent_directory~ | string | Firefox 68, Firefox ESR 68 | Profile temporary directory\n| &nbsp;&nbsp;&nbsp;&nbsp;~If set, changes the location of the disk cache.~ This policy doesn't work. It's being worked on.\n| browser.fixup.dns_first_for_single_words | boolean | Firefox 68, Firefox ESR 68 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, single words are sent to DNS, not directly to search.\n| browser.newtabpage.activity-stream.default.sites | string | Firefox 72, ESR 68.4 | Locale dependent\n| &nbsp;&nbsp;&nbsp;&nbsp;If set, a list of URLs to use as the default top sites on the new tab page. Due to Firefox limitations, search sites can't be added. In addition, sites with the same name but different TLDs (example.org/example.com) will not display properly.\n| browser.places.importBookmarksHTML | boolean | Firefox 70, Firefox ESR 68.2\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, bookmarks are always imported on startup.\n| browser.safebrowsing.phishing.enabled | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, phishing protection is not enabled (Not recommended)\n| browser.safebrowsing.malware.enabled | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, malware protection is not enabled (Not recommended)\n| browser.search.update | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, updates for search engines are not checked.\n| browser.slowStartup.notificationDisabled | boolean | Firefox 70, Firefox ESR 68.2 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, a notification isn't shown if startup is slow.\n| browser.tabs.warnOnClose | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, there is no warning when the browser is closed.\n| browser.taskbar.previews.enable | boolean | Firefox 70, Firefox ESR 68.2 (Windows only) | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, tab previews are shown in the Windows taskbar.\n| browser.urlbar.suggest.bookmark | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, bookmarks aren't suggested when typing in the URL bar.\n| browser.urlbar.suggest.history | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, history isn't suggested when typing in the URL bar.\n| browser.urlbar.suggest.openpage | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, open tabs aren't suggested when typing in the URL bar.\n| datareporting.policy.dataSubmissionPolicyBypassNotification | boolean | Firefox 68, Firefox ESR 68 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, don't show the privacy policy tab on first run.\n| dom.allow_scripts_to_close_windows | boolean | Firefox 70, Firefox ESR 68.2 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, web page can close windows.\n| dom.disable_window_flip | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, web pages can focus and activate windows.\n| dom.disable_window_move_resize | boolean | Firefox 68, Firefox ESR 68 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, web pages can't move or resize windows.\n| dom.event.contextmenu.enabled | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, web pages can't override context menus.\n| dom.keyboardevent.keypress.hack.dispatch_non_printable_keys.addl | string | Firefox 68, Firefox ESR 68 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;See https://support.mozilla.org/en-US/kb/dom-events-changes-introduced-firefox-66\n| dom.keyboardevent.keypress.hack.use_legacy_keycode_and_charcode.addl | string | Firefox 68, Firefox ESR 68 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;See https://support.mozilla.org/en-US/kb/dom-events-changes-introduced-firefox-66\n| dom.xmldocument.load.enabled | boolean | Firefox ESR 68.5 | true.\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, XMLDocument.load is not available.\n| dom.xmldocument.async.enabled | boolean | Firefox ESR 68.5 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, XMLDocument.async is not available.\n| extensions.blocklist.enabled | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the extensions blocklist is not used (Not recommended)\n| extensions.getAddons.showPane | boolean | Firefox 68, Firefox ESR 68 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the Recommendations tab is not displayed in the Add-ons Manager.\n| extensions.htmlaboutaddons.recommendations.enabled | boolean | Firefox 72, Firefox ESR 68.4 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, recommendations are not shown on the Extensions tab in the Add-ons Manager.\n| geo.enabled | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the geolocation API is disabled. | Language dependent\n| intl.accept_languages | string | Firefox 70, Firefox ESR 68.2\n| &nbsp;&nbsp;&nbsp;&nbsp;If set, preferred language for web pages.\n| media.eme.enabled (Deprecated - Switch to EncryptedMediaExtensions policy) | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, Encrypted Media Extensions are not enabled.\n| media.gmp-gmpopenh264.enabled | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the OpenH264  plugin is not downloaded.\n| media.gmp-widevinecdm.enabled | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the Widevine plugin is not downloaded.\n| media.peerconnection.enabled | boolean | Firefox 72, Firefox ESR 68.4 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, WebRTC is disabled\n| media.peerconnection.ice.obfuscate_host_addresses.whitelist (Deprecated) | string | Firefox 72, Firefox ESR 68.4 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;If set, a list of domains for which mDNS hostname obfuscation is\ndisabled\n| media.peerconnection.ice.obfuscate_host_addresses.blocklist | string | Firefox 79, Firefox ESR 78.1 | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;If set, a list of domains for which mDNS hostname obfuscation is\ndisabled\n| network.dns.disableIPv6 | boolean | Firefox 68, Firefox ESR 68 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, IPv6 DNS lokoups are disabled.\n| network.IDN_show_punycode | boolean | Firefox 68, Firefox ESR 68 | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, display the punycode version of internationalized domain names.\n| places.history.enabled | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, history is not enabled.\n| print.save_print_settings | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, print settings are not saved between jobs.\n| security.default_personal_cert | string | Firefox 68, Firefox ESR 68 | Ask Every Time\n| &nbsp;&nbsp;&nbsp;&nbsp;If set to Select Automatically, Firefox automatically chooses the default personal certificate.\n| security.mixed_content.block_active_content | boolean | Firefox 70, Firefox ESR 68.2 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, mixed active content (HTTP and HTTPS) is not blocked.\n| security.osclientcerts.autoload | boolean | Firefox 72 (Windows), Firefox 75 (macOS)  | false\n| &nbsp;&nbsp;&nbsp;&nbsp;If true, client certificates are loaded from the operating system certificate store.\n| security.ssl.errorReporting.enabled | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, SSL errors cannot be sent to Mozilla.\n| security.tls.hello_downgrade_check | boolean | Firefox 72, Firefox ESR 68.4 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the TLS 1.3 downgrade check is disabled.\n| ui.key.menuAccessKeyFocuses | boolean | Firefox 68, Firefox ESR 68 | true\n| &nbsp;&nbsp;&nbsp;&nbsp;If false, the Alt key doesn't show the menubar on Windows.\n| widget.content.gtk-theme-override | string | Firefox 72, Firefox ESR 68.4 (Linux only) | N/A\n| &nbsp;&nbsp;&nbsp;&nbsp;If set, overrides the GTK theme for widgets.\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Preferences\\boolean_preference_name = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Preferences\\string_preference_name = \"string_value\"\n```\n#### Windows (Intune)\nOMA-URI: (periods are replaced by underscores)\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Preferences/boolean_preference_name\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI: (periods are replaced by underscores)\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Preferences/string_preference_name\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Preferences_String\" value=\"string_value\"/>\n```\n#### macOS\n```\n<dict>\n  <key>Preferences</key>\n  <dict>\n    <key>boolean_preference_name</key>\n    <true/> | <false/>\n    <key>string_preference_name</key>\n    <string>string_value</string>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Preferences\": {\n      \"boolean_preference_name\": true | false,\n      \"string_preference_name\": \"string_value\"\n    }\n  }\n}\n```\n### PrimaryPassword\nRequire or prevent using a primary (formerly master) password.\n\nIf this value is true, a primary password is required. If this value is false, it works the same as if [`DisableMasterPasswordCreation`](#disablemasterpasswordcreation) was true and removes the primary password functionality.\n\nIf both DisableMasterPasswordCreation and PrimaryPassword are used, DisableMasterPasswordCreation takes precedent.\n\n**Compatibility:** Firefox 79, Firefox ESR 78.1\\\n**CCK2 Equivalent:** `noMasterPassword`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PrimaryPassword = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/PrimaryPassword\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>PrimaryPassword</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PrimaryPassword\": true | false\n  }\n}\n```\n### PromptForDownloadLocation\nAsk where to save each file before downloading.\n\n**Compatibility:** Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.download.useDownloadDir`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\PromptForDownloadLocation = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/PromptForDownloadLocation\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>PromptForDownloadLocation</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"PromptForDownloadLocation\": true | false\n  }\n}\n```\n### Proxy\nConfigure proxy settings. These settings correspond to the connection settings in Firefox preferences.\nTo specify ports, append them to the hostnames with a colon (:).\n\n`Mode` is the proxy method being used.\n\n`Locked` is whether or not proxy settings can be changed.\n\n`HTTPProxy` is the HTTP proxy server.\n\n`UseHTTPProxyForAllProtocols` is whether or not the HTTP proxy should be used for all other proxies.\n\n`SSLProxy` is the SSL proxy server.\n\n`FTPProxy` is the FTP proxy server.\n\n`SOCKSProxy` is the SOCKS proxy server\n\n`SOCKSVersion` is the SOCKS version (4 or 5)\n\n`Passthrough` is list of hostnames or IP addresses that will not be proxied. Use `<local>` to bypass proxying for all hostnames which do not contain periods.\n\n`AutoConfigURL` is a  URL for proxy configuration (only used if Mode is autoConfig).\n\n`AutoLogin` means do not prompt for authentication if password is saved.\n\n`UseProxyForDNS` to use proxy DNS when using SOCKS v5.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `networkProxy*`\\\n**Preferences Affected:** `network.proxy.type`, `network.proxy.autoconfig_url`, `network.proxy.socks_remote_dns`, `signon.autologin.proxy`, `network.proxy.socks_version`, `network.proxy.no_proxies_on`, `network.proxy.share_proxy_settings`, `network.proxy.http`, `network.proxy.http_port`, `network.proxy.ftp`, `network.proxy.ftp_port`, `network.proxy.ssl`, `network.proxy.ssl_port`, `network.proxy.socks`, `network.proxy.socks_port`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\Mode = \"none\" | \"system\" | \"manual\" | \"autoDetect\" | \"autoConfig\"\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\Locked = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\=Proxy\\HTTPProxy = https://httpproxy.example.com\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\UseHTTPProxyForAllProtocols = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\SSLProxy = https://sslproxy.example.com\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\FTPProxy = https://ftpproxy.example.com\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\SOCKSProxy = https://socksproxy.example.com\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\SOCKSVersion = 0x4 | 0x5\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\Passthrough = <local>\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\AutoConfigURL = URL_TO_AUTOCONFIG\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\AutoLogin = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\Proxy\\UseProxyForDNS = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/Proxy\n```\nValue (string):\n```\n<enabled/>\n<data id=\"ProxyLocked\" value=\"true | false\"/>\n<data id=\"ConnectionType\" value=\"none | system | manual | autoDetect | autoConfig\"/>\n<data id=\"HTTPProxy\" value=\"https://httpproxy.example.com\"/>\n<data id=\"UseHTTPProxyForAllProtocols\" value=\"true | false\"/>\n<data id=\"SSLProxy\" value=\"https://sslproxy.example.com\"/>\n<data id=\"FTPProxy\" value=\"https://ftpproxy.example.com\"/>\n<data id=\"SOCKSProxy\" value=\"https://socksproxy.example.com\"/>\n<data id=\"SOCKSVersion\" value=\"4 | 5\"/>\n<data id=\"AutoConfigURL\" value=\"URL_TO_AUTOCONFIG\"/>\n<data id=\"Passthrough\" value=\"<local>\"/>\n<data id=\"AutoLogin\" value=\"true | false\"/>\n<data id=\"UseProxyForDNS\" value=\"true | false\"/>\n```\n#### macOS\n```\n<dict>\n  <key>Proxy</key>\n  <dict>\n    <key>Mode</key>\n    <string>none | system | manual | autoDetect | autoConfig</string>\n    <key>Locked</key>\n    <true> | </false>\n    <key>HTTPProxy</key>\n    <string>https://httpproxy.example.com</string>\n    <key>UseHTTPProxyForAllProtocols</key>\n    <true> | </false>\n    <key>SSLProxy</key>\n    <string>https://sslproxy.example.com</string>\n    <key>FTPProxy</key>\n    <string>https://ftpproxy.example.com</string>\n    <key>SOCKSProxy</key>\n    <string>https://socksproxy.example.com</string>\n    <key>SOCKSVersion</key>\n    <string>4 | 5</string>\n    <key>Passthrough</key>\n    <string>&lt;local>&gt;</string>\n    <key>AutoConfigURL</key>\n    <string>URL_TO_AUTOCONFIG</string>\n    <key>AutoLogin</key>\n    <true> | </false>\n    <key>UseProxyForDNS</key>\n    <true> | </false>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"Proxy\": {\n      \"Mode\": \"none\" | \"system\" | \"manual\" | \"autoDetect\" | \"autoConfig\",\n      \"Locked\": true | false,\n      \"HTTPProxy\": \"hostname\",\n      \"UseHTTPProxyForAllProtocols\": true | false,\n      \"SSLProxy\": \"hostname\",\n      \"FTPProxy\": \"hostname\",\n      \"SOCKSProxy\": \"hostname\",\n      \"SOCKSVersion\": 4 | 5,\n      \"Passthrough\": \"<local>\",\n      \"AutoConfigURL\": \"URL_TO_AUTOCONFIG\",\n      \"AutoLogin\": true | false,\n      \"UseProxyForDNS\": true | false\n    }\n  }\n}\n```\n### RequestedLocales\nSet the the list of requested locales for the application in order of preference. It will cause the corresponding language pack to become active.\n\nNote: For Firefox 68, this can now be a string so that you can specify an empty value.\n\n**Compatibility:** Firefox 64, Firefox ESR 60.4, Updated in Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\RequestedLocales\\1 = \"de\"\nSoftware\\Policies\\Mozilla\\Firefox\\RequestedLocales\\2 = \"en-US\"\n\nor\n\nSoftware\\Policies\\Mozilla\\Firefox\\RequestedLocales = \"de,en-US\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/RequestedLocalesString\n```\nValue (string):\n```\n<enabled/>\n<data id=\"Preferences_String\" value=\"de,en-US\"/>\n```\n#### macOS\n```\n<dict>\n  <key>RequestedLocales</key>\n  <array>\n    <string>de</string>\n    <string>en-US</string>\n  </array>\n</dict>\n\nor\n\n<dict>\n  <key>RequestedLocales</key>\n  <string>de,en-US</string>\n</dict>\n\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"RequestedLocales\": [\"de\", \"en-US\"]\n  }\n}\n\nor\n\n{\n  \"policies\": {\n    \"RequestedLocales\": \"de,en-US\"\n  }\n}\n```\n<a name=\"SanitizeOnShutdown\"></a>\n\n### SanitizeOnShutdown (Selective)\nClear data on shutdown. Choose from Cache, Cookies, Download History, Form & Search History, Browsing History, Active Logins, Site Preferences and Offline Website Data.\n\nPreviously, these values were always locked. Starting with Firefox 74 and Firefox ESR 68.6, you can use the `Locked` option to either keep the values unlocked (set it to false), or lock only the values you set (set it to true). If you want the old behavior of locking everything, do not set `Locked` at all.\n\n**Compatibility:** Firefox 68, Firefox ESR 68 (Locked added in 74/68.6)\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `privacy.sanitize.sanitizeOnShutdown`, `privacy.clearOnShutdown.cache`, `privacy.clearOnShutdown.cookies`, `privacy.clearOnShutdown.downloads`, `privacy.clearOnShutdown.formdata`, `privacy.clearOnShutdown.history`, `privacy.clearOnShutdown.sessions`, `privacy.clearOnShutdown.siteSettings`, `privacy.clearOnShutdown.offlineApps`\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\Cache = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\Cookies = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\Downloads = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\FormData = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\History = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\Sessions = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\SiteSettings = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\OfflineApps = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown\\Locked = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/A_SanitizeOnShutdown_Cache\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/B_SanitizeOnShutdown_Cookies\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/C_SanitizeOnShutdown_Downloads\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/D_SanitizeOnShutdown_FormData\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/E_SanitizeOnShutdown_History\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/F_SanitizeOnShutdown_Sessions\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/G_SanitizeOnShutdown_SiteSettings\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/H_SanitizeOnShutdown_OfflineApps\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~SanitizeOnShutdown/I_SanitizeOnShutdown_Locked\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>SanitizeOnShutdown</key>\n  <dict>\n    <key>Cache</key>\n    <true/> | <false/>\n    <key>Cookies</key>\n    <true/> | <false/>\n    <key>Downloads</key>\n    <true/> | <false/>\n    <key>FormData</key>\n    <true/> | <false/>\n    <key>History</key>\n    <true/> | <false/>\n    <key>Sessions</key>\n    <true/> | <false/>\n    <key>SiteSettings</key>\n    <true/> | <false/>\n    <key>OfflineApps</key>\n    <true/> | <false/>\n    <key>Locked</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SanitizeOnShutdown\": {\n      \"Cache\": true | false,\n      \"Cookies\": true | false,\n      \"Downloads\": true | false,\n      \"FormData\": true | false,\n      \"History\": true | false,\n      \"Sessions\": true | false,\n      \"SiteSettings\": true | false,\n      \"OfflineApps\": true | false,\n      \"Locked\": true | false\n    }\n  }\n}\n```\n### SanitizeOnShutdown (All)\nClear all data on shutdown, including Browsing & Download History, Cookies, Active Logins, Cache, Form & Search History, Site Preferences and Offline Website Data.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `privacy.sanitize.sanitizeOnShutdown`, `privacy.clearOnShutdown.cache`, `privacy.clearOnShutdown.cookies`, `privacy.clearOnShutdown.downloads`, `privacy.clearOnShutdown.formdata`, `privacy.clearOnShutdown.history`, `privacy.clearOnShutdown.sessions`, `privacy.clearOnShutdown.siteSettings`, `privacy.clearOnShutdown.offlineApps`\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SanitizeOnShutdown = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/C_SanitizeOnShutdown\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>SanitizeOnShutdown</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SanitizeOnShutdown\": true | false\n  }\n}\n```\n### SearchBar\nSet whether or not search bar is displayed.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** `showSearchBar`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SearchBar = \"unified\" | \"separate\"\n```\n\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/SearchBar\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SearchBar\" value=\"unified | separate\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SearchBar</key>\n  <string>unified | separate</string>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SearchBar\": \"unified\" | \"separate\"\n  }\n}\n```\n<a name=\"SearchEngines\"></a>\n\n### SearchEngines (This policy is only available on the ESR.)\n\n### SearchEngines | Add\n\nAdd new search engines. Although there are only five engines available in the ADMX template, there is no limit. To add more in the ADMX template, you can duplicate the XML.\n\nThis policy is only available on the ESR. `Name` and `URLTemplate` are required.\n\n`Name` is the name of the search engine.\n\n`URLTemplate` is the search URL with {searchTerms} to substitute for the search term.\n\n`Method` is either GET or POST\n\n`IconURL` is a URL for the icon to use.\n\n`Alias` is a keyword to use for the engine.\n\n`Description` is a description of the search engine.\n\n`PostData` is the POST data as name value pairs separated by &.\n\n`SuggestURLTemplate` is a search suggestions URL with {searchTerms} to substitute for the search term.\n\n`Encoding` is the query charset for the engine. It defaults to UTF-8.\n\n**Compatibility:** Firefox ESR 60 (POST support in Firefox ESR 68, Encoding support in Firefox 91)\\\n**CCK2 Equivalent:** `searchplugins`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\Name = \"Example1\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\URLTemplate = \"https://www.example.org/q={searchTerms}\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\Method = \"GET\" | \"POST\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\IconURL = \"https://www.example.org/favicon.ico\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\Alias = \"example\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\Description = \"Example Description\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\SuggestURLTemplate = \"https://www.example.org/suggestions/q={searchTerms}\"\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Add\\1\\PostData = \"name=value&q={searchTerms}\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Search/SearchEngines_1\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SearchEngine_Name\" value=\"Example1\"/>\n<data id=\"SearchEngine_URLTemplate\" value=\"https://www.example.org/q={searchTerms\"/>\n<data id=\"SearchEngine_Method\" value=\"GET | POST\"/>\n<data id=\"SearchEngine_IconURL\" value=\"https://www.example.org/favicon.ico\"/>\n<data id=\"SearchEngine_Alias\" value=\"example\"/>\n<data id=\"SearchEngine_Description\" value=\"Example Description\"/>\n<data id=\"SearchEngine_SuggestURLTemplate\" value=\"https://www.example.org/suggestions/q={searchTerms}\"/>\n<data id=\"SearchEngine_PostData\" value=\"name=value&amp;q={searchTerms}\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SearchEngines</key>\n  <dict>\n    <key>Add</key>\n    <array>\n      <dict>\n        <key>Name</key>\n        <string>Example1</string>\n        <key>URLTemplate</key>\n        <string>https://www.example.org/q={searchTerms}</string>\n        <key>Method</key>\n        <string>GET | POST </string>\n        <key>IconURL</key>\n        <string>https://www.example.org/favicon.ico</string>\n        <key>Alias</key>\n        <string>example</string>\n        <key>Description</key>\n        <string>Example Description</string>\n        <key>SuggestURLTemplate</key>\n        <string>https://www.example.org/suggestions/q={searchTerms}</string>\n        <key>PostData</key>\n        <string>name=value&q={searchTerms}</string>\n      </dict>\n    <array>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SearchEngines\": {\n      \"Add\": [\n        {\n          \"Name\": \"Example1\",\n          \"URLTemplate\": \"https://www.example.org/q={searchTerms}\",\n          \"Method\": \"GET\" | \"POST\",\n          \"IconURL\": \"https://www.example.org/favicon.ico\",\n          \"Alias\": \"example\",\n          \"Description\": \"Description\",\n          \"PostData\": \"name=value&q={searchTerms}\",\n          \"SuggestURLTemplate\": \"https://www.example.org/suggestions/q={searchTerms}\"\n        }\n      ]\n    }\n  }\n}\n```\n### SearchEngines | Default\n\nSet the default search engine. This policy is only available on the ESR.\n\n**Compatibility:** Firefox ESR 60\\\n**CCK2 Equivalent:** `defaultSearchEngine`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Default = NAME_OF_SEARCH_ENGINE\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Search/SearchEngines_Default\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SearchEngines_Default\" value=\"NAME_OF_SEARCH_ENGINE\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SearchEngines</key>\n  <dict>\n    <key>Default</key>\n    <string>NAME_OF_SEARCH_ENGINE</string>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SearchEngines\": {\n      \"Default\": \"NAME_OF_SEARCH_ENGINE\"\n    }\n  }\n}\n```\n### SearchEngines | PreventInstalls\n\nPrevent installing search engines from webpages.\n\n**Compatibility:** Firefox ESR 60\\\n**CCK2 Equivalent:** `disableSearchEngineInstall`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\PreventInstalls = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Search/SearchEngines_PreventInstalls\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>SearchEngines</key>\n  <dict>\n    <key>PreventInstalls</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SearchEngines\": {\n      \"PreventInstalls\": true | false\n    }\n  }\n}\n```\n### SearchEngines | Remove\n\nHide built-in search engines. This policy is only available on the ESR.\n\n**Compatibility:** Firefox ESR 60.2\\\n**CCK2 Equivalent:** `removeDefaultSearchEngines` (removed all built-in engines)\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SearchEngines\\Remove\\1 = NAME_OF_SEARCH_ENGINE\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Search/SearchEngines_Remove\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SearchEngines_Remove\" value=\"1&#xF000;NAME_OF_SEARCH_ENGINE\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SearchEngines</key>\n  <dict>\n    <key>Remove</key>\n    <array>\n      <string>NAME_OF_SEARCH_ENGINE</string>\n    </array>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SearchEngines\": {\n      \"Remove\": [\"NAME_OF_SEARCH_ENGINE\"]\n    }\n  }\n}\n```\n### SearchSuggestEnabled\n\nEnable search suggestions.\n\n**Compatibility:** Firefox 68, Firefox ESR 68\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.urlbar.suggest.searches`, `browser.search.suggest.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SearchSuggestEnabled = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Search/SearchSuggestEnabled\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>SearchSuggestEnabled</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SearchSuggestEnabled\": true | false\n  }\n}\n```\n### SecurityDevices\n\nInstall PKCS #11 modules.\n\n**Compatibility:** Firefox 64, Firefox ESR 60.4\\\n**CCK2 Equivalent:** `certs.devices`\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SecurityDevices\\NAME_OF_DEVICE = PATH_TO_LIBRARY_FOR_DEVICE\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/SecurityDevices\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SecurityDevices\" value=\"NAME_OF_DEVICE&#xF000;PATH_TO_LIBRARY_FOR_DEVICE\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SecurityDevices</key>\n  <dict>\n    <key>NAME_OF_DEVICE</key>\n    <string>PATH_TO_LIBRARY_FOR_DEVICE</string>\n  </dict>\n</dict>\n```\n\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SecurityDevices\": {\n      \"NAME_OF_DEVICE\": \"PATH_TO_LIBRARY_FOR_DEVICE\"\n    }\n  }\n}\n```\n### ShowHomeButton\nShow the home button on the toolbar.\n\nFuture versions of Firefox will not show the home button by default.\n\n**Compatibility:** Firefox 88, Firefox ESR 78.10\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\ShowHomeButton = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~Homepage/Homepage_ShowHomeButton\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>ShowHomeButton</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"ShowHomeButton\": true | false\n  }\n}\n```\n### SSLVersionMax\n\nSet and lock the maximum version of TLS.\n\n**Compatibility:** Firefox 66, Firefox ESR 60.6\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `security.tls.version.max`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SSLVersionMax = \"tls1\" | \"tls1.1\" | \"tls1.2\" | \"tls1.3\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/SSLVersionMax\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SSLVersion\" value=\"tls1 | tls1.2 | tls1.3\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SSLVersionMax</key>\n  <string>tls1 | tls1.1 | tls1.2 | tls1.3</string>\n</dict>\n```\n\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SSLVersionMax\": \"tls1\" | \"tls1.1\" | \"tls1.2\" | \"tls1.3\"\n  }\n}\n```\n### SSLVersionMin\n\nSet and lock the minimum version of TLS.\n\n**Compatibility:** Firefox 66, Firefox ESR 60.6\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `security.tls.version.min`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SSLVersionMin = \"tls1\" | \"tls1.1\" | \"tls1.2\" | \"tls1.3\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/SSLVersionMin\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SSLVersion\" value=\"tls1 | tls1.2 | tls1.3\"/>\n```\n#### macOS\n```\n<dict>\n  <key>SSLVersionMin</key>\n  <string>tls1 | tls1.1 | tls1.2 | tls1.3</string>\n</dict>\n```\n\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SSLVersionMin\": \"tls1\" | \"tls1.1\" | \"tls1.2\" | \"tls1.3\"\n  }\n}\n```\n### SupportMenu\nAdd a menuitem to the help menu for specifying support information.\n\n**Compatibility:** Firefox 68.0.1, Firefox ESR 68.0.1\\\n**CCK2 Equivalent:** helpMenu\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\SupportMenu\\Title = \"Support Menu\"\nSoftware\\Policies\\Mozilla\\Firefox\\SupportMenu\\URL = \"http://example.com/support\"\nSoftware\\Policies\\Mozilla\\Firefox\\SupportMenu\\AccessKey = \"S\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/SupportMenu\n```\nValue (string):\n```\n<enabled/>\n<data id=\"SupportMenuTitle\" value=\"Support Menu\"/>\n<data id=\"SupportMenuURL\" value=\"http://example.com/support\"/>\n<data id=\"SupportMenuAccessKey\" value=\"S\">\n```\n#### macOS\n```\n<dict>\n  <key>SupportMenu</key>\n  <dict>\n    <key>Title</key>\n    <string>SupportMenu</string>\n    <key>URL</key>\n    <string>http://example.com/support</string>\n    <key>AccessKey</key>\n    <string>S</string>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"SupportMenu\": {\n      \"Title\": \"Support Menu\",\n      \"URL\": \"http://example.com/support\",\n      \"AccessKey\": \"S\"\n    }\n  }\n}\n```\n### UserMessaging\n\nPrevent Firefox from messaging the user in certain situations.\n\n`WhatsNew` Remove the \"What's New\" icon and menuitem.\n\n`ExtensionRecommendations` If false, don't recommend extensions while the user is visiting web pages.\n\n`FeatureRecommendations` If false, don't recommend browser features.\n\n`UrlbarInterventions` If false, Don't offer Firefox specific suggestions in the URL bar.\n\n`SkipOnboarding` If true, don't show onboarding messages on the new tab page.\n\n`MoreFromMozilla` If false, don't show the \"More from Mozilla\" section in Preferences. (Firefox 98)\n\n**Compatibility:** Firefox 75, Firefox ESR 68.7\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `browser.messaging-system.whatsNewPanel.enabled`, `browser.newtabpage.activity-stream.asrouter.userprefs.cfr.addons`, `browser.newtabpage.activity-stream.asrouter.userprefs.cfr.features`, `browser.aboutwelcome.enabled`, `browser.preferences.moreFromMozilla`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\UserMessaging\\WhatsNew = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\UserMessaging\\ExtensionRecommendations = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\UserMessaging\\FeatureRecommendations = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\UserMessaging\\UrlbarInterventions = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\UserMessaging\\SkipOnboarding = 0x1 | 0x0\nSoftware\\Policies\\Mozilla\\Firefox\\UserMessaging\\MoreFromMozilla = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~UserMessaging/UserMessaging_WhatsNew\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~UserMessaging/UserMessaging_ExtensionRecommendations\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~UserMessaging/UserMessaging_FeatureRecommendations\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~UserMessaging/UserMessaging_UrlbarInterventions\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~UserMessaging/UserMessaging_SkipOnboarding\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox~UserMessaging/UserMessaging_MoreFromMozilla\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>UserMessaging</key>\n  <dict>\n    <key>WhatsNew</key>\n    <true/> | <false/>\n    <key>ExtensionRecommendations</key>\n    <true/> | <false/>\n    <key>FeatureRecommendations</key>\n    <true/> | <false/>\n    <key>UrlbarInterventions</key>\n    <true/> | <false/>\n    <key>SkipOnboarding</key>\n    <true/> | <false/>\n    <key>MoreFromMozilla</key>\n    <true/> | <false/>\n  </dict>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"UserMessaging\": {\n      \"WhatsNew\": true | false,\n      \"ExtensionRecommendations\": true | false,\n      \"FeatureRecommendations\": true | false,\n      \"UrlbarInterventions\": true | false,\n      \"SkipOnboarding\": true | false,\n      \"MoreFromMozilla\": true | false\n    }\n  }\n}\n```\n### UseSystemPrintDialog\nUse the system print dialog instead of the print preview window.\n\n**Compatibility:** Firefox 102\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `print.prefer_system_dialog`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\UseSystemPrintDialog = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/UseSystemPrintDialog\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### macOS\n```\n<dict>\n  <key>UseSystemPrintDialog</key>\n  <true/> | <false/>\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"UseSystemPrintDialog\": true | false\n  }\n}\n```\n### WebsiteFilter\nBlock websites from being visited. The parameters take an array of Match Patterns, as documented in https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Match_patterns.\nThe arrays are limited to 1000 entries each.\n\nIf you want to block all URLs, you can use `<all_urls>` or `*://*/*`. You can't have just a `*` on the right side.\n\nFor specific protocols, use `https://*/*` or `http://*/*`.\n\nAs of Firefox 83 and Firefox ESR 78.5, file URLs are supported.\n\n**Compatibility:** Firefox 60, Firefox ESR 60\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** N/A\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\WebsiteFilter\\Block\\1 = \"<all_urls>\"\nSoftware\\Policies\\Mozilla\\Firefox\\WebsiteFilter\\Exceptions\\1 = \"http://example.org/*\"\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/B_WebsiteFilter_Block\n```\nValue (string):\n```\n<enabled/> <data id=\"WebsiteFilter\" value=\"1&#xF000;&#60;all_urls&#62;\"/>\n```\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/B_WebsiteFilter_Exceptions\n```\nValue (string):\n```\n<enabled/>\n<data id=\"WebsiteFilter\" value=\"1&#xF000;http://example.org/*\"/>\n```\n#### macOS\n```\n<dict>\n  <key>WebsiteFilter</key>\n  <dict>\n    <key>Block</key>\n    <array>\n      <string><all_urls></string>\n    </array>\n    <key>Exceptions</key>\n    <array>\n      <string>http://example.org/*</string>\n    </array>\n  </dict>\n\n</dict>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"WebsiteFilter\": {\n      \"Block\": [\"<all_urls>\"],\n      \"Exceptions\": [\"http://example.org/*\"]\n    }\n  }\n}\n```\n### WindowsSSO\nAllow Windows single sign-on for Microsoft, work, and school accounts.\n\nIf this policy is set to true, Firefox will use credentials stored in Windows to sign in to Microsoft, work, and school accounts.\n\n**Compatibility:** Firefox 91\\\n**CCK2 Equivalent:** N/A\\\n**Preferences Affected:** `network.http.windows-sso.enabled`\n\n#### Windows (GPO)\n```\nSoftware\\Policies\\Mozilla\\Firefox\\WindowsSSO = 0x1 | 0x0\n```\n#### Windows (Intune)\nOMA-URI:\n```\n./Device/Vendor/MSFT/Policy/Config/Firefox~Policy~firefox/WindowsSSO\n```\nValue (string):\n```\n<enabled/> or <disabled/>\n```\n#### policies.json\n```\n{\n  \"policies\": {\n    \"WindowsSSO\": true | false\n  }\n}\n```\n"
},
{
  "name": "telemetry-airflow",
  "files": {
    "/": [
      ".add_credentials.md",
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "GRAVEYARD.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "airflow.cfg",
      "airflow.sh",
      "bin",
      "config",
      "dags",
      "dataproc_bootstrap",
      "dev_webserver_config.py",
      "docker-compose.yml",
      "examples",
      "jobs",
      "newrelic.ini",
      "plugins",
      "requirements.in",
      "requirements.txt",
      "tests",
      "webserver_config.py"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build clean migrate redis-cli run secret shell stop up\n\nhelp:\n\t@echo \"Welcome to the Telemetry Airflow\\n\"\n\t@echo \"The list of commands for local development:\\n\"\n\t@echo \"  build      Builds the docker images for the docker-compose setup\"\n\t@echo \"  clean      Stops and removes all docker containers\"\n\t@echo \"  migrate    Runs the Django database migrations\"\n\t@echo \"  redis-cli  Opens a Redis CLI\"\n\t@echo \"  run        Run a airflow command\"\n\t@echo \"  secret     Create a secret to be used for a config variable\"\n\t@echo \"  shell      Opens a Bash shell\"\n\t@echo \"  up         Runs the whole stack, served under http://localhost:8000/\"\n\t@echo \"  gke        Create a sandbox gke cluster for testing\"\n\t@echo \"  clean-gke  Delete the sandbox gke cluster\"\n\t@echo \"  stop       Stops the docker containers\"\n\nbuild:\n\tdocker-compose build\n\nclean:\tstop\n\tdocker-compose rm -f\n\trm -rf logs/*\n\tif [ -f airflow-worker.pid ]; then rm airflow-worker.pid; fi\n\nshell:\n\tdocker-compose run web bash\n\nredis-cli:\n\tdocker-compose run redis redis-cli -h redis\n\nrun:\n\tdocker-compose run -e DEV_USERNAME -e AWS_SECRET_ACCESS_KEY -e AWS_ACCESS_KEY_ID web airflow $(COMMAND)\n\nsecret:\n\t@docker-compose run web python -c \\\n\t\"from cryptography.fernet import Fernet; print Fernet.generate_key()\"\n\nstop:\n\tdocker-compose down\n\tdocker-compose stop\n\nup:\n\tdocker-compose up\n\ngke:\n\tbin/start_gke\n\nclean-gke:\n\tbin/stop_gke\n\ntest:\n\ttox\n\ncompile-requirements:\n\tvenv/bin/pip-compile requirements.in --output-file requirements.txt\n",
  "readme": "# Telemetry-Airflow\n\n[![CircleCi](https://circleci.com/gh/mozilla/telemetry-airflow.svg?style=shield&circle-token=62f4c1be98e5c9f36bd667edb7545fa736eed3ae)](https://circleci.com/gh/mozilla/telemetry-airflow)\n\n[Apache Airflow](https://airflow.apache.org/) is a platform to programmatically\nauthor, schedule and monitor workflows.\n\nThis repository codifies the Airflow cluster that is deployed at\n[workflow.telemetry.mozilla.org](https://workflow.telemetry.mozilla.org)\n(behind SSO) and commonly referred to as \"WTMO\" or simply \"Airflow\".\n\nSome links relevant to users and developers of WTMO:\n\n- The `dags` directory in this repository contains some custom DAG definitions\n- Many of the DAGs registered with WTMO don't live in this repository, but are\n  instead generated from ETL task definitions in\n  [bigquery-etl](https://github.com/mozilla/bigquery-etl)\n- The Data SRE team maintains a\n  [WTMO Developer Guide](https://mana.mozilla.org/wiki/display/DOPS/WTMO+Developer+Guide)\n  (behind SSO)\n\n## Prerequisites\n\nThis app is built and deployed with\n[docker](https://docs.docker.com/) and\n[docker-compose](https://docs.docker.com/compose/).\n\n### Updating Python dependencies\n\nAdd new Python dependencies into `requirements.in`. Run the following commands with the same Python\nversion specified by the Dockerfile.\n\n```bash\n# As of time of writing, python3.7\npip install pip-tools\npip-compile\n```\n\n### Build Container\n\nAn Airflow container can be built with\n\n```bash\nmake build\n```\n\n### Migrate Database\n\nAirflow database migration is no longer a separate step for dev but is run by the web container if necessary on first run. That means, however, that you should run the web container (and the database container, of course) and wait for the database migrations to complete before running individual test commands per below. The easiest way to do this is to run `make up` and let it run until the migrations complete.\n\n## Testing\n\nA single task, e.g. `spark`, of an Airflow dag, e.g. `example`, can be run with an execution date, e.g. `2018-01-01`, in the `dev` environment with:\n\n```bash\nmake run COMMAND=\"test example spark 20180101\"\n```\n\n```bash\ndocker logs -f telemetryairflow_scheduler_1\n```\n\n### Adding dummy credentials\n\nTasks often require credentials to access external credentials. For example, one may choose to store\nAPI keys in an Airflow connection or variable. These variables are sure to exist in production but\nare often not mirrored locally for logistical reasons. Providing a dummy variable is the preferred\nway to keep the local development environment up to date.\n\nIn `bin/run`, please update the `init_connections` and `init_variables` with appropriate strings to\nprevent broken workflows. To test this, run `bin/test-parse` to check for errors. You may manually\ntest this by restarting the orchestrated containers and checking for error messages within the main\nadministration UI at `localhost:8000`.\n\n### Local Deployment\n\nAssuming you're using macOS and Docker for macOS, start the docker service,\nclick the docker icon in the menu bar, click on preferences and change the\navailable memory to 4GB.\n\nTo deploy the Airflow container on the docker engine, with its required dependencies, run:\n\n```bash\nmake up\n```\n\nYou can now connect to your local Airflow web console at\n`http://localhost:8000/`.\n\nAll DAGs are paused by default for local instances and our staging instance of Airflow.\nIn order to submit a DAG via the UI, you'll need to toggle the DAG from \"Off\" to \"On\".\nYou'll likely want to toggle the DAG back to \"Off\" as soon as your desired task starts running.\n\n#### Workaround for permission issues\n\nUsers on Linux distributions will encounter permission issues with `docker-compose`.\nThis is because the local application folder is mounted as a volume into the running container.\nThe Airflow user and group in the container is set to `10001`.\n\nTo work around this, replace all instances of `10001` in `Dockerfile.dev` with the host user id.\n\n```bash\nsed -i \"s/10001/$(id -u)/g\" Dockerfile.dev\n\n```\n\n### Testing GKE Jobs (including BigQuery-etl changes)\n\nSee https://go.corp.mozilla.com/wtmodev for more details.\n\n```\nmake build && make up\nmake gke\n\nWhen done:\nmake clean-gke\n```\n\nFrom there, [connect to Airflow](localhost:8000) and enable your job.\n\n### Testing Dataproc Jobs\n\nDataproc jobs run on a self-contained Dataproc cluster, created by Airflow.\n\nTo test these, jobs, you'll need a sandbox account and corresponding service account.\nFor information on creating that, see \"Testing GKE Jobs\". Your service account\nwill need Dataproc and GCS permissions (and BigQuery, if you're connecting to it). _Note_: Dataproc requires \"Dataproc/Dataproc Worker\"\nas well as Compute Admin permissions.\nYou'll need to ensure that the Dataproc API is [enabled in your sandbox project.](https://console.developers.google.com/apis/api/dataproc.googleapis.com)\n\nEnsure that your dataproc job has a configurable project to write to.\nSet the project in the DAG entry to be configured based on development environment;\nsee the `ltv.py` job for an example of that.\n\nFrom there, run the following:\n\n```bash\nmake build && make up\n./bin/add_gcp_creds $GOOGLE_APPLICATION_CREDENTIALS google_cloud_airflow_dataproc\n```\n\nYou can then connect to Airflow [locally](localhost:8000). Enable your DAG and see that it runs correctly.\n\n### Production Setup\n\nNote: the canonical reference for production environment variables lives\nin [a private repository](https://github.com/mozilla-services/cloudops-deployment/blob/master/projects/data/puppet/yaml/app/data.prod.wtmo.yaml).\n\nWhen deploying to production make sure to set up the following environment\nvariables:\n\n- `AWS_ACCESS_KEY_ID` -- The AWS access key ID to spin up the Spark clusters\n- `AWS_SECRET_ACCESS_KEY` -- The AWS secret access key\n- `AIRFLOW_DATABASE_URL` -- The connection URI for the Airflow database, e.g.\n  `mysql://username:password@hostname:port/database`\n- `AIRFLOW_BROKER_URL` -- The connection URI for the Airflow worker queue, e.g.\n  `redis://hostname:6379/0`\n- `AIRFLOW_BROKER_URL` -- The connection URI for the Airflow result backend, e.g.\n  `redis://hostname:6379/1`\n- `AIRFLOW_GOOGLE_CLIENT_ID` -- The Google Auth client id used for\n  authentication.\n- `AIRFLOW_GOOGLE_CLIENT_SECRET` -- The Google Auth client secret used for\n  authentication.\n- `AIRFLOW_GOOGLE_APPS_DOMAIN` -- The domain(s) to restrict Google Auth login\n  to e.g. `mozilla.com`\n- `AIRFLOW_SMTP_HOST` -- The SMTP server to use to send emails e.g.\n  `email-smtp.us-west-2.amazonaws.com`\n- `AIRFLOW_SMTP_USER` -- The SMTP user name\n- `AIRFLOW_SMTP_PASSWORD` -- The SMTP password\n- `AIRFLOW_SMTP_FROM` -- The email address to send emails from e.g.\n  `telemetry-alerts@workflow.telemetry.mozilla.org`\n- `URL` -- The base URL of the website e.g.\n  `https://workflow.telemetry.mozilla.org`\n- `DEPLOY_ENVIRONMENT` -- The environment currently running, e.g.\n  `stage` or `prod`\n\n\nAlso, please set\n\n- `AIRFLOW_SECRET_KEY` -- A secret key for Airflow's Flask based webserver\n- `AIRFLOW__CORE__FERNET_KEY` -- A secret key to saving connection passwords in the DB\n\nBoth values should be set by using the cryptography module's fernet tool that\nwe've wrapped in a docker-compose call:\n\n```bash\nmake secret\n```\n\nRun this for each key config variable, and **don't use the same for both!**\n\n### Debugging\n\nSome useful docker tricks for development and debugging:\n\n```bash\n# Stop all docker containers:\ndocker stop $(docker ps -aq)\n\n# Remove any leftover docker volumes:\ndocker volume rm $(docker volume ls -qf dangling=true)\n\n# Purge docker volumes (helps with mysql container failing to start)\n# Careful as this will purge all local volumes not used by at least one container.\ndocker volume prune\n```\n\nFailing CircleCI 'test-environment' check:\n\n```bash\n# These commands are from the bin/test-parse script (get_errors_in_listing)\n# If --detach is unavailable,  make sure you are running the latest version of docker-compose\ndocker-compose up --detach\n\ndocker-compose logs --follow --tail 0 | sed -n '/\\[testing_stage_0\\]/q'\n\n# Don't pipe to grep to see the full output including your errors\ndocker-compose exec web airflow list_dags\n```\n\n### Triggering a task to re-run within the Airflow UI\n\n- Check if the task / run you want to re-run is visible in the DAG's Tree View UI\n  - For example, [the `main_summary` DAG tree view](http://workflow.telemetry.mozilla.org/admin/airflow/tree?num_runs=25&root=&dag_id=main_summary).\n  - Hover over the little squares to find the scheduled dag run you're looking for.\n- If the dag run is not showing in the Dag Tree View UI (maybe deleted)\n  - Browse -> Dag Runs\n  - Create (you can look at another dag run of the same dag for example values too)\n    - Dag Id: the name of the dag, for example, `main_summary`\n    - Execution Date: The date the dag should have run, for example, `2018-05-14 00:00:00`\n    - Start Date: Some date between the execution date and \"now\", for example, `2018-05-20 00:00:05`\n    - End Date: Leave it blank\n    - State: success\n    - Run Id: `scheduled__2018-05-14T00:00:00`\n    - External Trigger: unchecked\n  - Click Save\n  - Click on the Graph view for the dag in question. From the main DAGs view, click the name of the DAG\n  - Select the \"Run Id\" you just entered from the drop-down list\n  - Click \"Go\"\n  - Click each element of the DAG and \"Mark Success\"\n  - The tasks should now show in the Tree View UI\n- If the dag run is showing in the DAG's Tree View UI\n  - Click on the small square for the task you want to re-run\n  - **Uncheck** the \"Downstream\" toggle\n  - Click the \"Clear\" button\n  - Confirm that you want to clear it\n  - The task should be scheduled to run again straight away.\n\n### Triggering backfill tasks using the CLI\n\n- SSH into the ECS container instance\n- List docker containers using `docker ps`\n- Log in to one of the docker containers using `docker exec -it <container_id> bash`. The web server instance is a good choice.\n- Run the desired backfill command, something like `$ airflow backfill main_summary -s 2018-05-20 -e 2018-05-26`\n\n### CircleCI\n\n- Commits to forked repo PRs will trigger CircleCI builds that build the docker container and test python dag compilation. This should pass prior to merging.\n- Every commit to main or tag will trigger a CircleCI build that will build and push the container to dockerhub\n"
},
{
  "name": "CCADB-Tools",
  "files": {
    "/": [
      ".gitignore",
      "API_AddUpdateIntermediateCert",
      "EVChecker",
      "README.md",
      "cacheck",
      "cacompliance",
      "capi",
      "certdataDiffCCADB",
      "crlVerification",
      "kinto_integrity",
      "oneCRLDiffCCADB",
      "oneCRLViewer"
    ]
  },
  "makefile": null,
  "readme": "# CCADB-Tools\nServices called by the [CCADB](https://www.ccadb.org) Salesforce instance to perform specific tasks\n"
},
{
  "name": "ichnaea",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".readthedocs.yml",
      "CHANGES.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.rst",
      "alembic.ini",
      "bin",
      "contribute.json",
      "data",
      "docker-compose.yml",
      "docker.make",
      "docker",
      "docs",
      "geocalclib",
      "ichnaea",
      "mobile_codes",
      "node.make",
      "requirements-docs.txt",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "vendor",
      "version.json"
    ],
    "/docs": [
      "Makefile",
      "algo",
      "api",
      "architecture.rst",
      "changes",
      "conf.py",
      "configure.rst",
      "dataflows.rst",
      "debug.rst",
      "deploy.png",
      "deploy.rst",
      "devdeploy_index.rst",
      "glossary.rst",
      "images",
      "import_export.rst",
      "index.rst",
      "libraries.rst",
      "local_dev.rst",
      "metrics.rst",
      "python_requirements.rst",
      "rate_control.rst",
      "user_index.rst"
    ],
    "/.github": [
      "CODEOWNERS",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# Include my.env and export it so variables set in there are available\n# in the Makefile.\ninclude my.env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user\n# in the container doesn't match the user on your host.\nICHNAEA_UID ?= 10001\nICHNAEA_GID ?= 10001\nICHNAEA_DOCKER_DB_ENGINE ?= mysql_5_7\n\n# Set this in the environment to force --no-cache docker builds.\nDOCKER_BUILD_OPTS :=\nifeq (1, ${NOCACHE})\nDOCKER_BUILD_OPTS := --no-cache\nendif\n\nDC := $(shell which docker-compose)\n\n.PHONY: help\nhelp: default\n\n.PHONY: default\ndefault:\n\t@echo \"Usage: make RULE\"\n\t@echo \"\"\n\t@echo \"Ichnaea make rules:\"\n\t@echo \"\"\n\t@echo \"  build            - build docker containers\"\n\t@echo \"  setup            - drop and recreate service state\"\n\t@echo \"  run              - run webapp\"\n\t@echo \"  runcelery        - run scheduler and worker\"\n\t@echo \"  runservices      - run service containers (database and redis)\"\n\t@echo \"  stop             - stop all service containers\"\n\t@echo \"\"\n\t@echo \"  shell            - open a shell in the app container\"\n\t@echo \"  dbshell          - open a database shell in the database container\"\n\t@echo \"  clean            - remove all build, test, coverage and Python artifacts\"\n\t@echo \"  lint             - lint code\"\n\t@echo \"  lintfix          - reformat code\"\n\t@echo \"  test             - run unit tests\"\n\t@echo \"  testcoverage     - run unit tests with coverage report\"\n\t@echo \"  testshell        - open a shell in the test environment\"\n\t@echo \"  docs             - generate Sphinx HTML documentation, including API docs\"\n\t@echo \"  assets           - build all generated static assets\"\n\t@echo \"  clean-assets     - remove generated static assets\"\n\t@echo \"  update-vendored  - re-download vendor source and test data\"\n\t@echo \"  update-reqs      - regenerate Python requirements\"\n\t@echo \"  local-map        - generate local map tiles\"\n\t@echo \"\"\n\t@echo \"  help             - see this text\"\n\t@echo \"\"\n\t@echo \"See https://ichnaea.readthedocs.io/ for more documentation.\"\n\n.PHONY: clean\nclean:\n\trm .docker-build* || true\n\nmy.env:\n\t@if [ ! -f my.env ]; \\\n\tthen \\\n\techo \"Copying my.env.dist to my.env...\"; \\\n\tcp docker/config/my.env.dist my.env; \\\n\tfi\n\n.docker-build:\n\tmake build\n\n.PHONY: build\nbuild: my.env\n\t@if [ ! -f docker/node/npm-shrinkwrap.json ]; \\\n\tthen \\\n\techo \"{}\" > docker/node/npm-shrinkwrap.json; \\\n\tfi\n\t${DC} build ${DOCKER_BUILD_OPTS} \\\n\t    --build-arg userid=${ICHNAEA_UID} \\\n\t    --build-arg groupid=${ICHNAEA_GID} \\\n\t    node app\n\t${DC} build ${DOCKER_BUILD_OPTS} \\\n\t    redis db\n\ttouch .docker-build\n\n.PHONY: setup\nsetup: my.env\n\t${DC} run app shell ./docker/run_setup.sh\n\n.PHONY: shell\nshell: my.env .docker-build\n\t${DC} run --rm app shell\n\n.PHONY: dbshell\ndbshell: my.env .docker-build\n\t${DC} up -d db\n\t${DC} exec db mysql --user root --password=location location\n\n.PHONY: test\ntest: my.env .docker-build\n\t./bin/test_env.sh\n\n.PHONY: testcoverage\ntestcoverage: my.env .docker-build\n\t./bin/test_env.sh --cov=ichnaea --cov-branch\n\n.PHONY: testshell\ntestshell: my.env .docker-build\n\t./bin/test_env.sh --shell\n\n.PHONY: docs\ndocs: my.env .docker-build\n\t${DC} run --rm --no-deps app shell ./docker/run_build_docs.sh\n\n.PHONY: assets\nassets: my.env .docker-build\n\t${DC} run --rm --user ${ICHNAEA_UID} node make -f node.make\n\n.PHONY: clean-assets\nclean-assets: my.env .docker-build\n\t${DC} run --rm --user ${ICHNAEA_UID} node make -f node.make clean\n\n.PHONY: lint\nlint: my.env .docker-build\n\t${DC} run --rm --no-deps app shell ./docker/run_lint.sh\n\n.PHONY: lintfix\nlintfix: my.env .docker-build\n\t${DC} run --rm --no-deps app shell ./docker/run_lint.sh --fix\n\n.PHONY: run\nrun: my.env .docker-build\n\t${DC} up web\n\n.PHONY: runcelery\nruncelery: my.env .docker-build\n\t${DC} up scheduler worker\n\n.PHONY: runservices\nrunservices: my.env .docker-build\n\t${DC} up -d redis db\n\n.PHONY: stop\nstop: my.env\n\t${DC} stop\n\n.PHONY: update-vendored\nupdate-vendored: my.env\n\t${DC} run --rm --no-deps app shell make -f docker.make update_vendored\n\n.PHONY: update-reqs\nupdate-reqs: my.env\n\t${DC} run --rm --no-deps app shell ./docker/run_update_requirements.sh\n\n.PHONY: local-map\nlocal-map: my.env .docker-build\n\t${DC} run --rm app shell ./docker/run_local_map.sh\n",
  "readme": ".. image:: https://circleci.com/gh/mozilla/ichnaea.svg?style=svg\n    :target: https://circleci.com/gh/mozilla/ichnaea\n\n=======\nIchnaea\n=======\n\n``Ichnaea`` is an application that provides geolocation coordinates\nfrom other sources of data (Bluetooth, cell or WiFi networks, GeoIP, etc.).\n\nFor more information look at\n`the full docs <https://ichnaea.readthedocs.io/>`_.\n\nPlease use `our github tracker <https://github.com/mozilla/ichnaea/issues>`_\nto report issues.\n\n\nLicense\n=======\n\n``ichnaea`` is offered under the Apache License 2.0.\n"
},
{
  "name": "irlpodcast",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintrc.js",
      ".gitignore",
      ".gitlab-ci.yml",
      ".nvmrc",
      ".stylelintrc",
      "CNAME",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "archetypes",
      "bin",
      "config-dev.toml",
      "config-prod.toml",
      "config-stage.toml",
      "config.toml",
      "content",
      "docker-compose.yml",
      "docker-compose_bkup.yml",
      "docs",
      "gulpfile.js",
      "layouts",
      "netlify.toml",
      "package-lock.json",
      "package.json",
      "src",
      "static"
    ],
    "/docs": [
      "CNAME"
    ]
  },
  "makefile": null,
  "readme": "## Running Locally\n\n1. Install dependencies: `npm install`\n2. Start Gulp for asset compilation: `gulp`\n3. In a new terminal window, start the server: `docker-compose up`\n4. Open `http://localhost:1313` in a browser\n\nEdit the `.scss` and `.js` files in the `/src` directory (*NOT* in `/static`).\n\n## Adding new episodes\n\n1. `docker exec irlpodcast_hugo_1 hugo new episodes/201X-XX-XX-SXXEXX-episode-title-here.md`\n\nYou can create this file by hand in the filesystem, but the above command will use the\n`archetypes/episodes.md` template to populate all the necessary front matter, saving\nyou precious, precious time.\n\n## Adding new pages\n\nThis probably won't happen often, but if needed, run:\n\n1. `docker exec irlpodcast_hugo_1 hugo new somepage.md`\n\n## Pushing to dev\n\nAny merge to the `master` branch will automatically update the dev site:\n\n`https://master.irlpodcast.org/`\n\nA notice will be posted in `#irlpodcast-notify` on Slack when the push has completed.\n\n## Pushing to staging\n\n1. Push your changes to the `stage` branch: `git push origin my-branch-name:stage`\n2. Test on `https://stage.irlpodcast.org`\n\n## Pushing to production\n\nEpisode deployments typically happen at 6AM PT on the publish date. Please\npost in #irl-site-redesign (private channel) on Slack, or DM @michaela when you push\nso she know's it's been successful.\n\nIt is responsibility of the person who codes the episode to find someone to push\nif 6AM PT is too early for them. All members of MozMEAO are happy to do this.\n\n1. Verify all is good on [the staging site](https://stage.irlpodcast.org/)\n  - If the episode was merged before the publish date it will not be visible on\n  staging. You can see it on staging by either re-running the Jenkins job (if\n  you have permission) or by merging a new pull request. If there are no\n  reviewed pull requests that are ready you can submit one that changes the\n  publish date to 1 minute earlier.\n2. Make sure your local `master` branch is up to date\n3. Push the `master` branch to the `prod` branch: `git push origin master:prod`\n\nA notice will be posted in `#irlpodcast-notify` on Slack when the push has completed.\n\n### Kudos\n\n- http://danbahrami.io/articles/building-a-production-website-with-hugo-and-gulp-js/\n- https://regisphilibert.com/blog/2018/02/hugo-the-scope-the-context-and-the-dot/\n"
},
{
  "name": "srihash.org",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintrc.json",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".stylelintrc",
      "CNAME",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "app.js",
      "badge",
      "clipboard.js",
      "favicons",
      "index.html",
      "package-lock.json",
      "package.json",
      "robots.txt",
      "style.css"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# SRI Hash Generator\n\nThis is the code behind the <https://www.srihash.org/> website. It generates [subresource integrity](https://www.w3.org/TR/SRI/) hashes.\n\n[![Build Status](https://travis-ci.org/mozilla/srihash.org.svg?branch=master)](https://travis-ci.org/mozilla/srihash.org)\n[![devDependencies Status](https://david-dm.org/mozilla/srihash.org/dev-status.svg)](https://david-dm.org/mozilla/srihash.org?type=dev)\n\n## Install\n\nYou'll need Node.js 10.x and npm for linting and a local server for testing\n\nClone the git repository and install dependencies:\n\n```shell\ngit clone git://github.com/mozilla/srihash.org.git\ncd srihash.org\nnpm install\n```\n\n## Linters\n\nRun tests with:\n\n```shell\nnpm run lint\n```\n\n## Code of Conduct\nThis repository is governed by Mozilla's code of conduct and etiquette guidelines.\nFor more details, please read the\n[Mozilla Community Participation Guidelines](https://www.mozilla.org/about/governance/policies/participation/) or [CODE_OF_CONDUCT.md].\n"
},
{
  "name": "sanitizer-polyfill",
  "files": {
    "/": [
      ".eslintrc.json",
      ".github",
      ".gitignore",
      ".prettierignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "demo",
      "dist",
      "package-lock.json",
      "package.json",
      "rollup.config.browser-min.js",
      "rollup.config.browser.js",
      "rollup.config.cjs.js",
      "rollup.config.esm.js",
      "src"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Polyfill for the [Sanitizer API](https://github.com/WICG/sanitizer-api/) specification.\n\nThe polyfill might provide a shim on top of\n[DOMPurify](https://github.com/cure53/DOMPurify/), that mainly rewrites the\nspecified configuration object into a DOMPurify configuration.\n\nDOMPurify is more interesting than other libraries, as it relies on the\ncurrent browser's HTML parsing behavior (it is built on top of the\n[NodeIterator](https://developer.mozilla.org/en-US/docs/Web/API/NodeIterator)\ninterface).\n\n### [Demo](https://mozilla.github.io/sanitizer-polyfill/demo/)\n\nThere's a [Demo](https://mozilla.github.io/sanitizer-polyfill/demo/) page that loads all of the polyfill scripts and then does nothing.\nBy default, the polyfill will bail out if you already have a Sanitizer object defined.\nBut that can be easily overridden by clicking the **\u00b6**.\n\nIt might useful to test `Element.setHTML`\n"
},
{
  "name": "perf-looker",
  "files": {
    "/": [
      ".gitkeep",
      "README.md",
      "models",
      "views"
    ]
  },
  "makefile": null,
  "readme": "# PerfHerder Looker\n\nThis repository contains all the Looker resources (views, models) as part of the perf-looker LookML Project. It is owned and used primary by [the Firefox Performance Program](https://mana.mozilla.org/wiki/display/FIREFOX/Firefox+Performance).\n\nThis perf-looker project & primary model use a connection to the Perfherder MySQL database.\n"
},
{
  "name": "glean-annotations",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "annotations",
      "docs",
      "mkdocs.yml",
      "netlify.toml",
      "requirements.in",
      "requirements.txt",
      "scripts"
    ],
    "/docs": [
      "contributing",
      "index.md"
    ],
    "/.github": [
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Glean Annotations\n\n[![CircleCI](https://circleci.com/gh/mozilla/glean-annotations.svg?style=svg)](https://circleci.com/gh/mozilla/glean-annotations)\n\nThis repository stores user-defined annotations against glean metrics: both those defined in applications (for example, \"fenix\" aka \"Firefox for Android\") as well as libraries like [android-components].\n\nThe published set of annotations is available at: https://mozilla.github.io/glean-annotations/api.json\n\nDocumentation on how this repository works, as well as contribution information, is available at: https://mozilla.github.io/glean-annotations/\n\n[android-components]: https://github.com/mozilla-mobile/android-components\n\n## Local development\n\nIn general, the idea behind this repository is that most people will edit annotations from the Glean Dictionary.\nHowever, if you want to do local development the recommended setup process is to create a virtual environment\nand install the dependencies inside it. This requires a recent version of python (3.7+):\n\n```bash\npython3 -m venv venv/\nsource venv/bin/activate\npip install -r requirements.txt\n```\n\nAfter creating activating a virtual environment and installing the dependencies, run:\n\n```bash\n./scripts/create-api.py > api.json\n```\n\nThis will create a JSON file which should be the same as the published set of annotations above.\n\nTo exit the virtual environment, use: \n\n```bash\ndeactivate venv\n```\n"
},
{
  "name": "experimenter-docs",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".prettierrc",
      "README.md",
      "babel.config.js",
      "docs",
      "docusaurus.config.js",
      "package.json",
      "sidebars.js",
      "src",
      "static",
      "yarn.lock"
    ],
    "/docs": [
      "cookbook",
      "data-scientists",
      "deep-dives",
      "faq",
      "getting-started",
      "homepage",
      "integration-tests.md",
      "jetstream",
      "messaging",
      "whats-news",
      "workflow"
    ],
    "/.github": [
      "PULL_REQUEST_TEMPLATE.md",
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# \ud83c\udf29 Documentation Hub for Nimbus & Experimenter Users\n\nCheck out the docs at: **https://experimenter.info/**\n\n**Please file issues for this repository in [Experimenter](https://github.com/mozilla/experimenter/issues) and add the `experimenter-docs` label.**\n\n## About\n\nThis repository is the documentation hub for [Experimenter](https://github.com/mozilla/experimenter). Its purpose is to be a single source of truth for and house user, developer, and data documentation.\n\nThe site is built using [Docusaurus v2](https://v2.docusaurus.io/) and is automatically deployed from the `main` branch to GitHub Pages using GitHub Actions.\n\nRelevant ADRs:\n\n- [Use Docusaurus + GH Pages for the Nimbus User Doc Hub](https://github.com/mozilla/experimenter/blob/main/app/experimenter/docs/adrs/0005-doc-hub.md)\n- [Location for the new Experiment \"Docs Hub\" codebase + docs](https://github.com/mozilla/experimenter/blob/main/app/experimenter/docs/adrs/0006-doc-hub-repo.md)\n\n## Contributing\n\nPages are written in Markdown and can be found under [`docs/`](/docs).\n\nCheck out [the Contributing page](https://experimenter.info/contributing) for helpful Docusaurus and GitHub UI tips to learn how to create a new document, edit an existing one, and/or how to adjust the sidebar. You don't have to check out the repository locally to contribute.\n\n### Working locally\n\nTo build and run this project locally, clone the repository and run:\n\n```\nyarn install\nyarn start\n```\n\nThat should open a new browser window automatically, or you can manually browse to http://localhost:3000/experimenter-docs/ to view the docs.\n"
},
{
  "name": "coverage-crawler",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "coverage_crawler",
      "download_artifacts.py",
      "requirements.txt",
      "run_crawler.py",
      "setup.cfg",
      "setup.py",
      "style.css",
      "test-requirements.txt",
      "tests",
      "websites.txt"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# coverage-crawler\n[![Task Status](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/coverage-crawler/master/badge.svg)](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/coverage-crawler/master/latest)\n\nA crawler to find websites that exercise code in Firefox that is not covered by unit tests\n\n## Software requirements\n- Python 3.6+\n- [Mercurial](https://www.mercurial-scm.org/)\n\n## Usage as a script\n\n- Install requirements with `pip install -r requirements.txt`;\n- Install development requirements with `pip install -r test-requirements.txt`;\n- Run the `download_artifacts.py` script with the desired revision passed as argument to download the latest Firefox coverage build;\n- Run the `run_crawler.py` script.\n\n## Usage as a module\n\n- Add this project's repository to your requirements file as a Git dependency;\n- Import `coverage_crawler`;\n- Use function `download_artifacts` from `latest_cov_build.py` with the desired revision passed as argument to download the latest Firefox coverage build and other artifacts;\n- Run function `run` from `crawler.py` with the desired website passed as an argument.\n"
},
{
  "name": "task-boot",
  "files": {
    "/": [
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "VERSION",
      "requirements-tests.txt",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "taskboot",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": "ROOT_DIR:=$(shell dirname $(realpath $(lastword $(MAKEFILE_LIST))))\nTAG=mozilla/taskboot\nVERSION=$(shell cat $(ROOT_DIR)/VERSION)\n\nbuild:\n\timg build -t $(TAG):latest -t $(TAG):$(VERSION) $(ROOT_DIR)\n\npublish:\n\timg push $(TAG):latest\n",
  "readme": "TaskBoot\n========\n\n> A Python 3 tool to bootstrap CI/CD workflows in [Taskcluster](https://docs.taskcluster.net) by building and publishing Docker images.\n\nFeatures\n--------\n\n* clone a git repo\n* build a docker image in a Taskcluster task without `dind`, by using the excellent project [img](https://github.com/genuinetools/img/)\n* push that docker image to a Docker repo, reading credentials from a Taskcluster secret\n* build multiple docker images using a `docker-compose.yml` file\n* build/update a Taskcluster hook\n* write docker images as Taskcluster artifacts\n* use those artifacts in another task to push them (allow for workflows like: build on pull-request and build/push on tag/prod branch)\n\nDemo\n----\n\nTaskBoot is used by [bugbug](https://github.com/mozilla/bugbug/) to produce Docker images on pull requests and branch pushes, pushing them only when a tag is created.\n\nDocumentation\n-------------\n\nA more detailed documentation is available in this [project's wiki](https://github.com/mozilla/task-boot/wiki).\n\nSystem requirements\n-------------------\n\n* `img` must be at least in version 0.5.2 to support multiple tags\n"
},
{
  "name": "rhino",
  "files": {
    "/": [
      ".circleci",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "NOTICE-tools.txt",
      "NOTICE.txt",
      "README.md",
      "RELEASE-NOTES.md",
      "RELEASE-STEPS.md",
      "benchmarks",
      "build.gradle",
      "checkstyle.xml",
      "docs",
      "examples",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "man",
      "spotbugs-exclude.xml",
      "src",
      "test262",
      "testsrc",
      "tools",
      "toolsrc",
      "xmlimplsrc"
    ],
    "/docs": [
      "_config.yml",
      "compat",
      "index.md"
    ],
    "/.github": [
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Rhino: JavaScript in Java\n\n<a title=\"Rodrigo J De Marco, CC0, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Rhino_(234581759).jpeg\"><img width=\"384\" alt=\"Rhino (234581759)\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4f/Rhino_%28234581759%29.jpeg/512px-Rhino_%28234581759%29.jpeg\"></a>\n\nRhino is an implementation of JavaScript in Java.\n\n## License\n\nRhino is licensed under the [MPL 2.0](./LICENSE.txt).\n\n## Releases\n\n<table>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7R5_RELEASE\">Rhino 1.7R5</a></td><td>January 29, 2015</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_6_RELEASE\">Rhino 1.7.6</a></td><td>April 15, 2015</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_7_RELEASE\">Rhino 1.7.7</a></td><td>June 17, 2015</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_7_1_RELEASE\">Rhino 1.7.7.1</a></td><td>February 2, 2016</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_7_2_Release\">Rhino 1.7.7.2</a></td><td>August 24, 2017</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_8_Release\">Rhino 1.7.8</a></td><td>January 22, 2018</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_9_Release\">Rhino 1.7.9</a></td><td>March 15, 2018</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_10_Release\">Rhino 1.7.10</a></td><td>April 9, 2018</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_11_Release\">Rhino 1.7.11</a></td><td>May 30, 2019</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_12_Release\">Rhino 1.7.12</a></td><td>January 13, 2020</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_13_Release\">Rhino 1.7.13</a></td><td>September 2, 2020</td></tr>\n<tr><td><a href=\"https://github.com/mozilla/rhino/releases/tag/Rhino1_7_14_Release\">Rhino 1.7.14</a></td><td>January 6, 2022</td></tr>\n</table>\n\n[Release Notes](./RELEASE-NOTES.md) for recent releases.\n\n[Compatibility table](https://mozilla.github.io/rhino/compat/engines.html) which shows which advanced JavaScript\nfeatures from ES6, and ES2016+ are implemented in Rhino.\n\n[![GitHub Action Status](https://github.com/mozilla/rhino/actions/workflows/gradle.yml/badge.svg)](https://github.com/mozilla/rhino/actions/workflows/gradle.yml)\n\n[![Mozilla](https://circleci.com/gh/mozilla/rhino.svg?style=shield)](https://app.circleci.com/pipelines/github/mozilla/rhino)\n\n## Documentation\n\nInformation for script builders and embedders:\n\n[Archived](http://web.archive.org/web/20210304081342/https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino/Documentation)\n\nJavaDoc for all the APIs:\n\n[https://javadoc.io/doc/org.mozilla/rhino](https://javadoc.io/doc/org.mozilla/rhino)\n\nMore resources if you get stuck:\n\n[https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino/Community](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/Rhino/Community)\n\n## Building\n\n### How to Build\n\nRhino builds with `Gradle`. Here are some useful tasks:\n```\n./gradlew jar\n```\nBuild and create `Rhino` jar in the `buildGradle/libs` directory.\n```\ngit submodule init\ngit submodule update\n./gradlew test\n```\nBuild and run all the tests, including the official [ECMAScript Test Suite](https://github.com/tc39/test262).\nSee [Running tests](testsrc/README.md) for more detailed info about running tests.\n```\n./gradlew testBenchmark\n```\nBuild and run benchmark tests.\n\n## Releasing and publishing new version\n\n1. Ensure all tests are passing\n2. Remove `-SNAPSHOT` from version in `gradle.properties` in project root folder\n3. Create file `gradle.properties` in `$HOME/.gradle` folder with following properties. Populate them with maven repo credentials and repo location.\n```\nmavenUser=\nmavenPassword=\nmavenSnapshotRepo=\nmavenReleaseRepo=\n```\n\n4. Run `Gradle` task to publish artifacts to Maven Central.\n```\n./gradlew publish\n```\n5. Increase version and add `-SNAPSHOT` to it in `gradle.properties` in project root folder.\n6. Push `gradle.properties` to `GitHub`\n\n## Running\n\nRhino can run as a stand-alone interpreter from the command line:\n```\njava -jar buildGradle/libs/rhino-1.7.12.jar -debug -version 200\nRhino 1.7.9 2018 03 15\njs> print('Hello, World!');\nHello, World!\njs>\n```\nThere is also a \"rhino\" package for many Linux distributions as well as Homebrew for the Mac.\n\nYou can also embed it, as most people do. See below for more docs.\n\n### Java 16 and later\n\nIf you are using a modular JDK that disallows the reflective access to\nnon-public fields (16 and later), you may need to configure the JVM with the\n[`--add-opens`](https://docs.oracle.com/en/java/javase/17/migrate/migrating-jdk-8-later-jdk-releases.html#GUID-12F945EB-71D6-46AF-8C3D-D354FD0B1781)\noption to authorize the packages that your scripts shall use, for example:\n```\n--add-opens java.desktop/javax.swing.table=ALL-UNNAMED\n```\n\n## Issues\n\nMost issues are managed on GitHub:\n\n[https://github.com/mozilla/rhino/issues](https://github.com/mozilla/rhino/issues)\n\n## Contributing PRs\n\nTo submit a new PR, please use the following process:\n\n* Ensure that your entire build passes \"./gradlew check\". This will include\ncode formatting and style checks and runs the tests.\n* Please write tests for what you fixed, unless you can show us that existing\ntests cover the changes. Use existing tests, such as those in\n\"testsrc/org/mozilla/javascript/tests\", as a guide.\n* If you fixed ECMAScript spec compatibility, take a look at test262.properties and see\nif you can un-disable some tests.\n* Push your change to GitHub and open a pull request.\n* Please be patient as Rhino is only maintained by volunteers and we may need\nsome time to get back to you.\n* Thank you for contributing!\n\n### Code Formatting\n\nCode formatting was introduced in 2021. The \"spotless\" plugin will fail your\nbuild if you have changed any files that have not yet been reformatted.\nPlease use \"spotlessApply\" to reformat the necessary files.\n\nIf you are the first person to touch a big file that spotless wants to make\nhundreds of lines of changes to, please try to put the reformatting changes\nalone into a single Git commit so that we can separate reformatting changes\nfrom more substantive changes.\n\n> **Warning:** If you build with Java 16 or later, you need to apply a\n> workaround for a \"spotless\" issue. Otherwise, the task will be disabled\n> and your PR may fail.\n> \n> The following must be added to your `gradle.properties`.\n> ```\n> org.gradle.jvmargs=--add-exports jdk.compiler/com.sun.tools.javac.api=ALL-UNNAMED \\\n>  --add-exports jdk.compiler/com.sun.tools.javac.file=ALL-UNNAMED \\\n>  --add-exports jdk.compiler/com.sun.tools.javac.parser=ALL-UNNAMED \\\n>  --add-exports jdk.compiler/com.sun.tools.javac.tree=ALL-UNNAMED \\\n>  --add-exports jdk.compiler/com.sun.tools.javac.util=ALL-UNNAMED\n> ```\n> For more details, see https://github.com/diffplug/spotless/issues/834#issuecomment-819118761\n\n## More Help\n\nThe Google group is the best place to go with questions:\n\n[https://groups.google.com/forum/#!forum/mozilla-rhino](https://groups.google.com/forum/#!forum/mozilla-rhino)\n"
},
{
  "name": "burnham",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "application",
      "bigquery",
      "docker-compose.yml",
      "fake-data-platform",
      "labels.toml"
    ],
    "/.github": [
      "CODEOWNERS",
      "ISSUE_TEMPLATE",
      "dependabot.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# burnham \ud83d\udc69\u200d\ud83d\ude80\ud83d\udcc8\ud83e\udd16\n\nThe burnham project is an end-to-end test suite that aims to automatically\nverify that Glean-based products correctly measure, collect, and submit\nnon-personal information to the GCP-based Data Platform and that the received\ntelemetry data is then correctly processed, stored to the respective tables\nand made available in BigQuery.\n\nBlog post about the proof of concept:\nhttps://raphael.codes/blog/automated-end-to-end-tests-for-glean/\n\n## Overview\n\n### Glean SDK\n\nFor Mozilla, getting reliable data from our products is critical to inform\nour decision making. Glean is a product analytics & telemetry solution that\nprovides a consistent experience and behavior across all of our products.\n\nFind out more in our [Data Documentation][data_documentation]. \ud83d\udcdd\n\n### burnham\n\nWe have developed a command-line application based on the [Glean SDK Python\nbindings][glean_sdk] for producing test data as part of the automated\nend-to-end test suite. The burnham application submits Glean pings to the\nData Platform which validates and stores these pings in BigQuery tables.\n\nYou can find the code for the burnham application in the [application\ndirectory][application]. \ud83d\udc69\u200d\ud83d\ude80\n\n### burnham-bigquery\n\nWe also developed a test suite based on the [pytest][pytest] framework that\ndynamically generates tests. Each test runs a specific query on BigQuery to\nverify a certain test scenario. After the test session finished, we then\nstore the results in a designated BigQuery table with ID\n`burnham_derived.test_results_v1`.\n\nThe test suite code is located in the [bigquery][bigquery] directory. \ud83d\udcca\n\n### telemetry-airflow\n\nWe build and push Docker images for both burnham and burnham-bigquery on CI\nfor pushes to the `main` branch of this repository. The end-to-end test suite\nis configured as a DAG on [telemetry-airflow][telemetry-airflow] on the Data\nPlatform and scheduled to run daily. It runs several instances of a burnham\nDocker container to produce Glean telemetry, uses Airflow sensors to wait for\nthe data to be available in the various burnham live tables, and then runs\nburnham-bigquery to verify the results.\n\nPlease see the [burnham DAG][airflow_dag] for more information. \ud83d\udccb\n\n### Redash\n\nWe created two scheduled queries that read the results from the\n`burnham_derived.test_results_v1` table from the past 4 days. The queries run\ndaily at 02:00 UTC after telemetry-airflow ran the burnham DAG.\n\nMozilla employees can find this information in the [burnham\ndashboard][redash].\n\n## Development status\n\nWe successfully completed the proof of concept and are now running burnham\nand burnham-bigquery in production. We are now actively working on developing\nadditional test scenarios. \ud83d\ude80\n\n## Requirements\n\nBoth burnham and burnham-bigquery run on Python 3.7 on Debian.\n\n## Integration points\n\nThe burnham project has integration points with the Glean SDK team, the Data\nPlatform team and the Ecosystem Test Engineering team.\n\n## Community Participation Guidelines\n\nThis repository is governed by Mozilla's code of conduct and etiquette. Please\nread the burnham [Code of Conduct][code_of_conduct].\n\n## Project theme\n\nThe project theme is inspired by Michael Burnham, the fictional protagonist\non the web television series Star Trek: Discovery portrayed by Sonequa\nMartin-Green. Burnham is a science specialist on the Discovery. She and her\ncrew do research on spore drive technology and complete missions in outer\nspace and these themes of scientific exploration and space travel are a\nperfect fit for this project. \ud83d\udc69\u200d\ud83d\ude80\n\n[application]: /application\n[code_of_conduct]: /CODE_OF_CONDUCT.md\n[bigquery]: /bigquery\n[airflow_dag]: https://github.com/mozilla/telemetry-airflow/blob/master/dags/burnham.py\n[data_documentation]: https://docs.telemetry.mozilla.org/concepts/glean/glean.html\n[pytest]: https://pypi.org/project/pytest/\n[glean_sdk]: https://pypi.org/project/glean-sdk/\n[telemetry-airflow]: https://github.com/mozilla/telemetry-airflow\n[redash]: https://sql.telemetry.mozilla.org/dashboard/burnham\n"
},
{
  "name": "ci-docker-bases",
  "files": {
    "/": [
      ".circleci",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bin",
      "bors.toml",
      "docker",
      "firefox",
      "renovate.json",
      "rust",
      "therapist"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# CI Bases\n\nThis is a collection of Docker images designed to be useful as base images in\nCircle CI tests for Mozilla projects. They are automatically kept up to date,\nand endeavor to provide a useful set of tools to run tests on.\n\nUnless otherwise noted, all images are based on Ubuntu 20.04.\n\n## Status: Deprecated\nWe plan to stop updating these images on October 1, 2022, and archive this repository.\nWe recommend using one of the\n[Pre-Built CircleCI Docker Images](https://circleci.com/docs/2.0/circleci-images/)\ninstead. See https://github.com/mozilla/ci-docker-bases/issues/322 for more information.\n\nWe intend to keep the automated updates running until October 1, 2022, but do not plan\nto add new images or features, or update to later OS versions.\n\n## Available images\n\nAll images are in the [mozilla/cidockerbases repository][dockerhub].\nDifferent images are available as tags within that repository. Images are\nversioned by date, for example: `image-2018-08-27`. Additionally, for each\nimage type there is a `image-latest` tag.\n\nThere is no bare `latest` tag, so referring to `mozilla/cidockerbases` without\na version tag won't work. An explicit tag must be specified, like\n`docker-latest`.\n\n[dockerhub]: https://hub.docker.com/r/mozilla/cidockerbases/\n\n### Docker\n\n`mozilla/cidockerbases:docker-latest`\n\nA modern version of Docker, Docker Compose, and other tools to make running\nCI easier. A version of this image is used to build all the images in this\nrepository, including itself.\n\n### Firefox\n\n`mozilla/cidockerbases:firefox-latest`\n\nThe latest stable version of Firefox and Node.js. A great base for running JS\nintegration tests in a browser.\n\n### Therapist\n\n`mozilla/cidockerbases:therapist-latest`\n\nThe latest Python 3 and Node.js 10 with \n[Therapist](https://github.com/rehandalal/therapist) pre-installed.\nA great base for linting jobs.\n\nThis is based on the ``python:3.9`` image, which is built on Debian 11 (bullseye).\n\n### Rust\n\n`mozilla/cidockerbases:rust-latest`\n\nThe latest stable version of Rust. Includes:\n\n- rustfmt\n- cargo-audit\n- cargo-kcov for code coverage\n- sccache for faster builds (requires set up)\n- cargo-hack\n\nThis is based on the ``rust:buster`` image, which is built on Debian 10 (buster).\n"
},
{
  "name": "cubeb-rs",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".gitmodules",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "cubeb-api",
      "cubeb-backend",
      "cubeb-core",
      "cubeb-sys",
      "systest"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# cubeb-rs [![ISC License](https://img.shields.io/crates/l/cubeb.svg)](https://github.com/djg/cubeb-rs/blob/master/LICENSE)\n\nA cross-platform audio library in Rust.\n\n## Features\n\nProvides access to the following:\n\n- Multiple audio backends across multiple platforms. See\n  [here](https://github.com/mozilla/cubeb/wiki/Backend-Support) for details.\n- Enumeration of available audio devices.\n- Opening input, output and duplex audio streams with control over latency,\n  sample rate, channel layout, state transitions, data handling and more.\n\n## Goals\n\nCurrently, **cubeb-rs** is based on a set of bindings to the original [**cubeb**\nC++ library](https://github.com/mozilla/cubeb) most notable for its use as the\naudio backend within\n[Gecko](https://github.com/mozilla/gecko-dev/search?q=cubeb&unscoped_q=cubeb),\nMozilla's browser engine. The long-term goal for **cubeb-rs** is to become\nindependent of the C++ library and provide a pure-Rust implementation down to\nthe level of the platform API eventually replacing the original within Gecko\nwhere possible.\n\nIn order to achieve this goal **cubeb-rs** is structured in a manner that\nsupports backend implementations in both pure-Rust and via bindings to the C++\nimplementation, allowing for progressive replacement. So far, pure-Rust\nimplementations exist for:\n\n- CoreAudio https://github.com/mozilla/cubeb-coreaudio-rs\n- PulseAudio https://github.com/mozilla/cubeb-pulse-rs\n\nThe plan is to consolidate all **cubeb**-related projects (including backend\nimplementations) under a single repository\n[here](https://github.com/mozilla/cubeb) in the near future.\n\nWhile **cubeb** is primarily renown for its use within Gecko, contributions and\nuse from projects outside of Gecko is very welcome.\n\n## Crates\n\nThe following crates are included within this repository:\n\n| Crate | Links | Description |\n| --- | --- | --- |\n| `cubeb` | [![crates.io](https://img.shields.io/crates/v/cubeb.svg)](https://crates.io/crates/cubeb) [![docs.rs](https://docs.rs/cubeb/badge.svg)](https://docs.rs/cubeb) | The top-level user API for **cubeb-rs**. See the `cubeb-api` subdirectory. Depends on `cubeb-core`. |\n| `cubeb-core` | [![crates.io](https://img.shields.io/crates/v/cubeb-core.svg)](https://crates.io/crates/cubeb-core) [![docs.rs](https://docs.rs/cubeb-core/badge.svg)](https://docs.rs/cubeb-core) | Common types and definitions for cubeb rust and C bindings. Not intended for direct use. Depends on `cubeb-sys`. |\n| `cubeb-sys` | [![crates.io](https://img.shields.io/crates/v/cubeb-sys.svg)](https://crates.io/crates/cubeb-sys) [![docs.rs](https://docs.rs/cubeb-sys/badge.svg)](https://docs.rs/cubeb-sys) | Native bindings to the cubeb C++ library. Requires `pkg-config` and `cmake` |\n| `cubeb-backend` | [![crates.io](https://img.shields.io/crates/v/cubeb-backend.svg)](https://crates.io/crates/cubeb-backend) [![docs.rs](https://docs.rs/cubeb-backend/badge.svg)](https://docs.rs/cubeb-backend) | Bindings to libcubeb internals to facilitate implementing cubeb backends in Rust. Depends on `cubeb-core`. |\n"
},
{
  "name": "mp4parse-rust",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "mp4parse",
      "mp4parse_capi"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "This is an mp4 track metadata parser.\n\n[![Latest crate version](https://img.shields.io/crates/v/mp4parse.svg)](https://crates.io/crates/mp4parse)\n[![Build status](https://github.com/mozilla/mp4parse-rust/actions/workflows/build.yml/badge.svg)](https://github.com/mozilla/mp4parse-rust/actions)\n\nOur primary interest is writing a pure-rust replacement for the\ntrack metadata parser needed by Firefox.\n\n[API documentation](https://docs.rs/mp4parse/)\n\n# Project structure\n\n`mp4parse` is a parser for ISO base media file format (mp4) written in rust.\n\n`mp4parse-capi` is a C API that exposes the functionality of `mp4parse`. The C\nAPI is intended to wrap the rust parser. As such, features should primarily\nbe implemented in the rust parser and exposed via the C API, rather than the C\nAPI implementing features on its own.\n\n## Tests\n\nTest coverage comes from several sources:\n- Conventional tests exist in `mp4parse/src/lib.rs` and\n`mp4parse_capi/src/lib.rs` as well as under `mp4parse/tests` and\n`mp4parse_capi/tests`. These tests can be run via `cargo test`.\n- Examples are included under `mp4parse_capi/examples`. These programs should\ncontinue to build and run after changes are made. Note, these programs are not\ntypically run by `cargo test`, so manual verification is required.\n\n# Versioning\n\nPrior to Firefox 95, versions of this library have been updated sporadically,\nand uploaded to crates.io even less frequently. Going forward, there will be\na new release on github and crates.io whenever the version of the code used\nin Firefox (see [toolkit/library/rust/shared/Cargo.toml](https://searchfox.org/mozilla-central/source/toolkit/library/rust/shared/Cargo.toml#12))\nis updated. For convenience, tags will be added to indicate what version of\nthe mp4parse is used in what version of Firefox.\n"
},
{
  "name": "terraform-modules",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".pre-commit-config.yaml",
      ".secrets.baseline",
      "CODEOWNERS",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "aws_itse-roles",
      "google_cloudsql_mysql",
      "google_cloudsql_postgres",
      "google_deployment_accounts",
      "google_gar",
      "google_gke",
      "google_memcache",
      "google_project-dns",
      "google_project",
      "google_redis",
      "google_tfstate",
      "google_workload_identity"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla SRE Terraform Modules\n\nThese are Terraform modules used across Mozilla SRE teams and related infrastructure partners.\n\nThese modules are intended for Mozilla usage internally. They are not built or supported with external users' usage in mind.\n\n## Automation\n\nThis repository uses [pre-commit](https://pre-commit.com/) for running some pre-git-commit checks. Install pre-commit locally (see link for instructions) for your own workspace to also run these checks on every git commit. Pipenv files are included optionally if you use such tooling for managing your pre-commit (or other Python packages) installation.\n\nThis repository also uses [GitHub Actions](.github/workflows/ci.yaml) to run stateless (e.g. no Terraform state or provider connections required) automated checks, including the following:\n* terraform fmt --recursive & terraform validate for each module directory;\n* for each module (top level subdirectory):\n  * the module subdirectory name follows the format `provider_name`;\n  * a `README.md` exists (use terraform-docs, detailed below, to generate);\n  * a `main.tf` file exists;\n  * a `versions.tf` file exists & contains value `terraform.required_version`;\n* security checks are run, including validating providers used are allowed & secrets scanning (via same pre-commit tooling).\n\n## Creating modules\n\nEach module should follow the given structure within the top level of this directory:\n\n```\n\u251c\u2500\u2500 aws_asg-lifecycle  # 1 top-level directory per module, named provider_name\n\u2502   \u251c\u2500\u2500 README.md  # required, generated via terraform-docs\n\u2502   \u251c\u2500\u2500 main.tf  # required\n\u2502   \u251c\u2500\u2500 outputs.tf\n\u2502   \u251c\u2500\u2500 variables.tf\n\u2502   \u2514\u2500\u2500 versions.tf  # required, validated to contain terraform.required_version\n```\n\n### Naming\n\nThe name should follow the format `provider_module-name` like `google_gke-cluster` or `newrelic_synthetic-checks`. The providers should follow the names listed here (this is to support future work on publishing our Terraform modules): https://registry.terraform.io/browse/providers\n\n### Ownership\n\nBy default, anyone who is an owner or contributor on this repository can review, approve & merge PRs on any module.\n\nIf you want to ensure specific people or teams have to review changes to certain modules, put a `CODEOWNERS` file in your module's subdirectory.\n\n### Documentation\n\nA `README.md` is required for each module, however its contents are not currently validated. It is strongly recommended that you at least start your `README.md` using [terraform-docs](https://github.com/terraform-docs/terraform-docs).\n\nIf you need to generate documentation run:\n\n```\nfor directory in */\ndo\n  terraform-docs --sort-by required markdown \"$directory\" > \"${directory}README.md\"\ndone\n```\n\nAlternatively, `pre-commit install` on this repository to automatically format the docs on commit.\n\n### Versioning\n\nCurrently, for Mozilla SRE Terraform modules versioning, we simply use git commits or git tags, using a `module-name_tag-version` structure, and the GitHub repository for the Terraform GitHub source. So:\n\n1. Person pushes Terraform modules changes to mozilla/terraform-modules;\n2. Person optionally tags their mozilla/terraform-modules module git commit following structure `module-name_tag-version`;\n3. Person who always wants to work on the latest version of that module pins to main (softly not recommended);\n4. Person who wants to stay on a pinned \u201cversion\u201d of that module pins to a git commit or git tag (see usage instructions below).\n\nThis does not give us version filtering, nor a clear mapping of Terraform modules source code to versions - however, we do have some backlog tickets to address fully-featured Terraform modules versioning & publication in future sprints.\n\n## Using these modules\n\nSome examples on using modules in this repository follow.\n\nUsing the (imaginary) `google_gke-cluster` module with always the latest version (e.g. following main branch):\n\n```terraform\n module \"gke\" {\n  source      = \"git@github.com:mozilla/terraform-modules.git//google_gke-cluster?ref=main\"\n}\n```\n\nUsing the (imaginary) `google_gke-cluster` module based on a specific (imaginary) git commit:\n```terraform\n module \"gke\" {\n  source      = \"git@github.com:mozilla/terraform-modules.git//google_gke-cluster?ref=69ad17030bfa4ea46f68f8cc449102d446658851\"\n}\n```\n\nUsing the (imaginary) `google_gke-cluster` module based on a specific (imaginary) git tag:\n```terraform\n module \"gke\" {\n  source      = \"git@github.com:mozilla/terraform-modules.git//google_gke-cluster?ref=google_gke-cluster_v1.0.1\"\n}\n```\n"
},
{
  "name": "mofo-redirector",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Procfile",
      "README.md",
      "app.py",
      "config.py",
      "default.env",
      "dev-requirements.in",
      "dev-requirements.txt",
      "requirements.in",
      "requirements.txt",
      "runtime.txt",
      "test_app.py",
      "tox.ini"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# MoFo-Redirector\r\n\r\n[![Build Status](https://travis-ci.org/mozilla/mofo-redirector.svg?branch=master)](https://travis-ci.org/mozilla/mofo-redirector)\r\n\r\nThe MoFo-Redirector is a small Flask application that serves to redirect deprecated and unused MoFo Domains.\r\n\r\nThe domains served by the redirect are defined as Tuples in config.py:\r\n\r\n`('example.com', 'https://foundation.mozilla.org', 301)`\r\n\r\nThe first value is the Host header value to match in an incoming request. The second is the target of the redirect. The third value is the redirect code to use.\r\n\r\n## How to setup local dev\r\n\r\n- Create a virtualenv: `python -m venv venv`. Activate it.\r\n- Install pip-tools: `pip install pip-tools`.\r\n- Install python dependencies by running `pip-sync requirements.txt dev-requirements.txt`.\r\n- Run the tests with `pytest`.\r\n\r\n## How to add a new redirect\r\n\r\n- Create a PR (instructions in `config.py`),\r\n- Wait for review and merge,\r\n- Detach the domain from current heroku app, attach it to mofo-redirector,\r\n- in Route53 update the Hosted Zone record for redirected domain to point to the redirector heroku app.\r\n"
},
{
  "name": "community-tc-config",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      ".yamllint",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "config",
      "deployment-details.md",
      "generate",
      "imagesets",
      "misc",
      "pyproject.toml",
      "setup.py",
      "tc-admin.py"
    ],
    "/.github": [
      "CODEOWNERS"
    ]
  },
  "makefile": null,
  "readme": "# community-tc-config\n\nThis repository defines a tool to manage the runtime configuration for the Taskcluster deployment at <https://community-tc.services.mozilla.com/>.\nIt uses [tc-admin](https://github.com/taskcluster/tc-admin) to examine and update the deployment.\nSee that library's documentation for background on how the process works.\n\n## Background\n\nA Taskcluster deployment has a collection of resources such a [roles](https://community-tc.services.mozilla.com/docs/manual/design/apis/hawk/roles), [hooks](https://community-tc.services.mozilla.com/docs/reference/core/hooks), and [worker pools](https://community-tc.services.mozilla.com/docs/reference/core/worker-manager), that define its behavior.\nThese can all be managed via the Taskcluster API, but managing them by hand is error-prone and difficult to track over time.\nThis tool exists to manage those resources in a controlled, observable way.\nIt does so by making API calls to determine the current state, examining this repository to determine the desired state, and then \"applying\" the necessary changes to get from the former to the latter.\n\nA deployment is also defined by a number of back-end settings that are not available in the API.\nThese are defined by the [service configuration](https://community-tc.services.mozilla.com/docs/manual/deploying).\nWhile this tool cannot change those settings, it does depend on them, and they are described [here](deployment-details.md).\n\n## Quick Start\n\nIf you would like to propose a change to the configuration of the Community-TC deployment, you are in the right spot.\nYou should already have an understanding of the resources you would like to modify.\nSee the [Taskcluster Documentation](https://community-tc.services.mozilla.com/docs) or consult with the Taskcluster team -- we are responsible for managing this deployment, and happy to help -- if you need assistance.\n\nBegin by installing this app by running `pip install -e .` in this directory.\nThen, run\n\n```shell\nTASKCLUSTER_ROOT_URL=https://community-tc.services.mozilla.com tc-admin diff --without-secrets\n```\n\nThis will show you the current difference between what's defined in your local repository and the runtime configuration of the deployment.\nMost of the time, there should be no difference.\n\nThen, change the configuration in this repository, using the comments in the relevant files as a guide.\nAfter making a change to the configuration, you can examine the results by running `tc-admin diff` again.\nIf you are adding or removing a number of resources, you can use `--ids-only` to show only the names of the added or removed resources.\nSee `tc-admin --help` for more useful command-line tricks.\n\n## Applying changes (community-tc administrators only)\n\nWhen the `main` branch is updated, typically a community-tc administrator will apply the changes.\n\nNote to community-tc administrators: you may need to use the `root` client to run `tc-admin apply <options>...` successfully, i.e.\n\n```\nexport TASKCLUSTER_ROOT_URL=https://community-tc.services.mozilla.com\nexport TASKCLUSTER_CLIENT_ID=static/taskcluster/root\nexport TASKCLUSTER_ACCESS_TOKEN=<root access token>\nunset TASKCLUSTER_CERTIFICATE\n```\n\nwhere the value for `<root access token>` can be shown by running the command `pass show community-tc/root`.\n\n## Conventions\n\nThis deployment follows the convention of [project namespaces](https://docs.taskcluster.net/docs/manual/using/namespaces#projects).\nEach project has a its own worker pools, and user roles can be given \"admin\" access to the project.\n\nRepositories are granted scopes via the [Taskcluster-github scheme](https://docs.taskcluster.net/docs/reference/integrations/github/taskcluster-yml-v1#scopes-and-roles).\nEach repository is associated with a project, and scopes granted to the repository should be associated with that project.\nThis occurs within `config/projects.yml`.\n\n### Secrets\n\nThe tool can manage secrets directly, but this requires access to secret values, and is thus limited to a smaller group of people: the Taskcluster team.\nThose people use `--with-secrets`, which automatically reads from the team's password storage repository.\n\nIn general, per-project secrets can either be managed by this tool, or managed directly by the project admins.\nSee the comments in `projects.yml` for details.\n\n### Externally Managed Projects and Resources\n\nThis repository manages all resources in the deployment *except* those associated with \"externally managed\" projects.\nProjects that manage their own resources, either by hand or via their own automation, should have the `externallyManaged` attribute set in `config/projects.yml`, otherwise the next run of `tc-admin apply` will delete the project's resources!\nNote that externally managed projects can still define other resources in their `projects.yml` stanza.\nSuch resources will be created and managed by this repository, but if they are removed from `projects.yml`, this repository cannot delete them.\n\nThe `externallyManaged` attribute can be set to `true` (only resources explicitly mentioned should be managed by this repo; unknown resources will not be deleted) or `false` (all resources are managed by this repo; unknown resources will be deleted), or to a regular expression or list of regular expressions.\nThese regular expressions describe resource IDs of resources that are managed externally.  For example, if the project `darjeeling` dynamically creates hooks with prefix `project-darjeeling/dynamic-`, it it would set\n\n```yaml\n  externallyManaged:\n    - \"Hook=project-darjeeling/dynamic-.*\"\n```\n\n### Image Sets\n\nTo build a set of machine images in GCP/AWS, see the [imagesets](/imagesets) subdirectory.\n\n### Code Style\n\nThe Python code here follows [Black](https://black.readthedocs.io/en/stable/).\n\n```shell\npip install black\nblack generate\n```\n\nThe YAML in `config/` is linted with `yamllint`'s \"relaxed\" preset, and without checking line length.\n\n```shell\npip install yamllint\nyamllint config\n```\n\n### Pre-commit checks\n\nIf you would like your staged files to be checked for errors you can do so by installing `pre-commit` with: `pip install pre-commit` and `pre-commit install`.\n"
},
{
  "name": "build-relengdocs",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Makefile",
      "README.md",
      "adding_docs_to_existing_code.rst",
      "adding_repo_to_releng_rtfd_account.rst",
      "addons",
      "architecture",
      "balrog",
      "best-practices",
      "conf.py",
      "explanations",
      "future",
      "gecko_tests",
      "hosts.rst",
      "how-tos",
      "index.rst",
      "logs",
      "machine-users.rst",
      "make.bat",
      "procedures",
      "reference",
      "releng_changelog.md",
      "releng_wordlist.txt",
      "requirements",
      "signing",
      "software.rst",
      "taskcluster",
      "tobewritten.rst",
      "troubleshooting.rst",
      "tutorials"
    ]
  },
  "makefile": "# Minimal makefile for Sphinx documentation\n#\n\n# You can set these variables from the command line, and also\n# from the environment for the first two.\nSPHINXOPTS    ?=\nSPHINXBUILD   ?= sphinx-build\nSOURCEDIR     = .\nBUILDDIR      = _build\n\n# Put it first so that \"make\" without argument is like \"make help\".\nhelp:\n\t@$(SPHINXBUILD) -M help \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)\n\n.PHONY: help Makefile\n\nlivehtml:\n\tsphinx-autobuild \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(0)\n\n# Catch-all target: route all unknown targets to Sphinx using the new\n# \"make mode\" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).\n%: Makefile\n\t@$(SPHINXBUILD) -M $@ \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)\n",
  "readme": "[![Docs](https://readthedocs.org/projects/moz-releng-docs/badge/?version=latest)](https://moz-releng-docs.readthedocs.io/en/latest/?badge=latest)\n\n# Mozilla Release Engineering Documentation\n\nThis repository contains some of the more technical internal documentation for Mozilla Release\nEngineering.\n\nRendered versions of the documentation may be viewed at:<br/>\n[http://moz-releng-docs.readthedocs.io/en/latest/](http://moz-releng-docs.readthedocs.io/en/latest/)\n\n## Contributing\n\n1. Create a Python [virtualenv](https://docs.python.org/3/tutorial/venv.html)\n2. Run:\n\n        pip install -r requirements/dev.txt\n\n3. To build the docs locally and start a [livereload](https://github.com/lepture/python-livereload)\n   server, run:\n\n        make livehtml\n\n   Verify changes by opening the linked URL in your browser. Further changes will automatically\n   rebuild and refresh your page.\n\n   Alternatively you can run:\n\n        make html\n\n   to build static docs. They will be generated under the `_build/html` directory.\n\nNote: Any new docs should be directly or indirectly linked to from `index.rst`. (For example, if\n`index.rst` contains `balrog/index.rst` in its\n[toctree](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-toctree),\nand the new doc is in the `balrog/index.rst` toctree, then the new doc is successfully indirectly\nlinked.)\n\n## Reference\n\nThese docs use [reStructuredText](https://en.wikipedia.org/wiki/ReStructuredText) and\n[Sphinx](https://www.sphinx-doc.org/en/master/index.html). Here are some reference materials:\n\n* [reStructuredText primer](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n* [Sphinx directives](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html)\n* [Sphinx roles](https://www.sphinx-doc.org/en/master/usage/restructuredtext/roles.html)\n* [Sphinx domains](https://www.sphinx-doc.org/en/master/usage/restructuredtext/domains.html)\n\n## Motivation\n\nFor the original motivation behind these docs see [RELENG RFC\n0007](https://github.com/mozilla-releng/releng-rfcs/blob/master/rfcs/0007-docs-location.md).\n"
},
{
  "name": "contain-facebook",
  "files": {
    "/": [
      ".DS_Store",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "docs",
      "package-lock.json",
      "package.json",
      "src",
      "test"
    ],
    "/docs": [
      "CODEOWNERS",
      "container-management.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE.md",
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "# Facebook Container\n\n**Prevent Facebook from tracking your visits to other websites**\n\nFacebook Container is an add-on you can install on Firefox to prevent Facebook from tracking your activity on other websites, so you can continue to use Facebook while protecting your privacy.\n\n**Note:** To learn more about Containers in general, see [Firefox Multi-Account Containers](https://support.mozilla.org/kb/containers).\n\n## How does Facebook Container work?\n\nThe Add-on keeps Facebook in a separate Container to prevent it from following your activity on other websites. When you first install the add-on, it signs you out of Facebook and deletes the cookies that Facebook uses to track you on other websites. \n\nEvery time you visit Facebook, it will open in its own container, separate from other websites you visit.  You can login to Facebook within its container.  When browsing outside the container, Facebook won\u2019t be able to easily collect your browsing data and connect it to your Facebook identity.\n\n## How do I enable Facebook Container?\n\nWe\u2019ve made it easy to take steps to protect your privacy so you can go on with your day.\n\n1. [Install Facebook Container](https://addons.mozilla.org/firefox/addon/facebook-container/). This will log you out of Facebook and delete the cookies it\u2019s been using to track you.\n2. Open Facebook and use it like you normally would.  Firefox will automatically switch to the Facebook Container tab for you.\n3. If you click on a link to a page outside of Facebook or type in another website in the address bar, Firefox will load them outside of the Facebook Container\n\n## How does this affect Facebook\u2019s features?\n\nFacebook Containers prevents Facebook from linking your activity on other websites to your Facebook identity. Therefore, the following will not work:\n\n### \u201cLike\u201d buttons and embedded Facebook comments on other websites.\n\nBecause you are logged into Facebook only in the Container, \u201cLike\u201d buttons and embedded Facebook comments on other websites will not work.\n\n### Logging in or creating accounts on other websites using Facebook\n\nWebsites that allow you to create an account or log in using Facebook will generally not work properly.\n\n## Will this protect me from Facebook completely?\n\nThis add-on does not prevent Facebook from mishandling the data it already has or permitted others to obtain about you. Facebook still will have access to everything that you do while you are on facebook.com or on the Facebook app, including your Facebook comments, photo uploads, likes, and any data you share with Facebook connected apps, etc.  \n\nOther ad networks may try to link your Facebook activities with your regular browsing. In addition to this add-on, there are other things you can do to maximize your protection, including changing your Facebook settings, using Private Browsing and Tracking Protection, blocking third-party cookies, and/or using [Firefox Multi-Account Containers](https://addons.mozilla.org/firefox/addon/multi-account-containers/ ) extension to further limit tracking.\n\n## How do I use Containers for other websites?\n\nGood news! Containers aren\u2019t just for Facebook. You can use Containers to prevent websites from linking your identities across the Web by installing [Firefox Multi-Account Containers](https://addons.mozilla.org/firefox/addon/multi-account-containers/).\n\nTo learn more about how Multi-Account Containers work, see our support page at [Firefox Multi-Account Containers](https://addons.mozilla.org/firefox/addon/multi-account-containers/).\n\n## Development\n\n1. `npm install`\n2. `./node_modules/.bin/web-ext run -s src/`\n\n### Testing\n`npm run test`\n\nor\n\n`npm run lint`\n\nfor just the linter\n\n### Links\n\n- [License](./LICENSE)\n- [Contributing](./CONTRIBUTING.md)\n- [Code Of Conduct](./CODE_OF_CONDUCT.md)\n"
},
{
  "name": "dinobuildr",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "CREDITS",
      "LICENSE",
      "README.md",
      "ambient_display_manifest.json",
      "dino_engine.py",
      "dino_engine.py.bak",
      "dino_engine_legacy.py",
      "dinobuildr.sh",
      "dinobuildr_legacy.sh",
      "production_manifest.json",
      "resources"
    ]
  },
  "makefile": null,
  "readme": "# dinobuildr\n\n## dinobuildr at Mozilla\nThe dinobuildr project is the current production macOS deployment and configuration tool at Mozilla. All Mozilla IT deployed Macs use this repo for initial system configuration via the following procedure:\n\n1. Install and/or update to the latest revision of the current release of macOS using Apple sanctioned installation methods\n2. Follow the macOS Setup Assistant to create a user account for the person that will be receiving the machine, and set some basic configurations\n3. Utilize the `dinobuildr.sh` script to pull down a verified commit of the `dino_engine.py` configuration script and run it on behalf of the user account created in step 2\n4. Enable Filevault and ensure that the password for the user account is set to something sufficiently random and complicated and hand over the machine to the person who requested it\n\ndinobuildr is intended to be a transparent, reliable, and auditable deployment solution. Anyone may inspect the automated components of our build, review what software is being deployed by default, and what configuration changes are made to a machine. Unlike most deployment and configuration management solutions dinobuildr is intended to be easy to understand, contribute to, and to audit. It does not rely on management binaries or other artifacts to work as it operates using Python 2.7 (which is built into macOS) and uses no external Python libraries. All configuration scripts exist in code that has been written and audited by Mozilla IT and all software packages come from trusted sources and are independently hashed by Mozilla IT. \n\n## Background \ndinobuildr is a macOS deployment utility developed by Mozilla IT. It provides a relatively flexible framework for deploying software and shell scripts to macOS clients running relatively new versions of macOS; relying on public-facing infrastructure such as Github and official vendor binary repositories that are exposed over the internet to deliver a consistent configuration. It is intended to be straightforward, simple, and is not feature rich - instead it offers a level of simplicity and transparency that may be useful in certain environments. \n\ndinobuildr relies on a JSON manifest to specify the actions the build will take (and it what order) as well as providing URLs and SHA256 hash values for all the scripts, files, and packages in the build. Updating a package is generally as straightforward as changing the version and hash attributes in the JSON manifest. The current version of dinobuildr supports hosting arbitrary files, scripts, pkg files, and dmg files in the following locations:\n\n* **Arbitrary Files** - Github LFS\n* **.pkg** - Github LFS, HTTP(S)/FTP\n* **.dmg** - HTTP(S)/FTP\n* **Bash Scripts** - Github\n* **.mobileconfig Files** - Github\n"
},
{
  "name": "dmo",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "LICENSE",
      "README.md",
      "jsconfig.json",
      "package-lock.json",
      "package.json",
      "playwright.config.js",
      "src",
      "static",
      "svelte.config.js",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# create-svelte\n\nEverything you need to build a Svelte project, powered by [`create-svelte`](https://github.com/sveltejs/kit/tree/master/packages/create-svelte).\n\n## Creating a project\n\nIf you're seeing this, you've probably already done this step. Congrats!\n\n```bash\n# create a new project in the current directory\nnpm init svelte\n\n# create a new project in my-app\nnpm init svelte my-app\n```\n\n## Developing\n\nOnce you've created a project and installed dependencies with `npm install` (or `pnpm install` or `yarn`), start a development server:\n\n```bash\nnpm run dev\n\n# or start the server and open the app in a new browser tab\nnpm run dev -- --open\n```\n\n## Building\n\nTo create a production version of your app:\n\n```bash\nnpm run build\n```\n\nYou can preview the production build with `npm run preview`.\n\n> To deploy your app, you may need to install an [adapter](https://kit.svelte.dev/docs/adapters) for your target environment.\n"
},
{
  "name": "foundation-security-advisories",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "announce",
      "bug-bounty-hof",
      "foundation_security_advisories",
      "genmd.py",
      "pre-commit-hook.sh",
      "requirements.txt",
      "setup.py"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# MFSA: Mozilla Foundation Security Advisories\n\nCanonical source for Mozilla Foundation Security Advisories. http://www.mozilla.org/security/announce/\n\n[![Build Status](https://circleci.com/gh/mozilla/foundation-security-advisories.svg?style=svg)](https://circleci.com/gh/mozilla/foundation-security-advisories)\n\n## Writing new announcements\n\nAnnouncements are written in [Markdown](http://daringfireball.net/projects/markdown/basics) or [YAML](http://yaml.org/spec/1.1/). They should\nbe named in the pattern `announce/YYYY/mfsaYYYY-XX.EXT` where `YYYY` is the 4 digit year, `XX` is\nthe next in the sequence, and `EXT` is either `md` or `yml`. \n\n### Markdown Format\n\nOnce the file is created some data about the file should be added to the\n[Front Matter](http://jekyllrb.com/docs/frontmatter/). Front Matter is [YAML](http://yaml.org/spec/1.1/)\nencoded data surrounded by lines consisting of 3 dashes. Then the Markdown content can be added below the \nFront Matter. For example:\n\n```markdown\n---\nannounced: April 29, 2014\nfixed_in:\n- Firefox 29\n- Firefox ESR 24.5\n- Thunderbird 24.5\n- Seamonkey 2.26\nimpact: High\nreporter: Abhishek Arya\ntitle: Buffer overflow when using non-XBL object as XBL\n---\n\n### Description\n\nMozilla community member **James Kitchener** reported a crash in\nDirectWrite when rendering MathML content with specific fonts due to an error in\nhow font resources and tables are handled. This leads to use-after-free of a\nDirectWrite font-face object, resulting in a potentially exploitable crash.\n```\n\n> **NOTE:** There is no need to include the MFSA ID in the front matter, it will be extracted from the file name.\n\n> **NOTE:** HTML is valid Markdown. So if you need extra features or classes, just add them.\n\n#### Metadata spec\n\nThere are some required elements in the Front Matter data (metadata). They are:\n\n```yaml\nannounced: Date in Month Day, Year format\nfixed_in: List of product names and versions (see example above)\nimpact: one of (Critical, High, Moderate, Low)\nreporter: Name of bug reporter\ntitle: Title of the advisory (may contain HTML).\n```\n\nOther data will be displayed, but the above will be expected in the template and styled correctly.\n\n> **NOTE:** You should *NOT* add a `products:` section to the data. The list of products is extracted\n> from the `fixed_in:` list when imported into the website.\n\n### YAML Format\n\nThe YAML type is for advisories that are actually a roll-up of multiple advisories. These files are all YAML\nas opposed to the `.md` files which are only partially YAML. The following example should demonstrate the\nfeatures of this file type:\n\n```yaml\nannounced: September 20, 2016\nfixed_in:\n- Thunderbird 45.4\ntitle: Security vulnerabilities fixed in Thunderbird 45.4\ndescription: |\n  Text that will appear at the top of the file. ***Markdown*** allowed.\n  \n  ### An h3 is sometimes good\n  \n  Then you can explain further.\nadvisories:\n  CVE-2016-5270:\n    title: Heap-buffer-overflow in nsCaseTransformTextRunFactory::TransformString\n    impact: high\n    reporter: Atte Kettunen\n    description: |\n      Short description <strong>with HTML</strong> and multiple lines!\n\n      Can also have full breaks and ***markdown***!\n    bugs:\n      - url: 1291016\n        desc: The text for the bug link\n  CVE-2016-5272:\n    title: Bad cast in nsImageGeometryMixin\n    impact: high\n    reporter: Abhishek Arya\n    description: A bad cast when processing layout with <code>input</code> elements can result in a potentially exploitable crash.\n    bugs:\n      - url: 1297934\n  CVE-2016-5276:\n    title: Heap-use-after-free in mozilla::a11y::DocAccessible::ProcessInvalidationList\n    impact: high\n    reporter: Nils\n    description: A use-after-free vulnerability triggered by setting a <code>aria-owns</code> attribute\n    bugs:\n      - url: 1287721\n```\n\nThe main part of the data is the same as the front-matter of the `.md` files. The primary difference is the `advisories`\nkey, which contains a list of CVEs with their individual data. A CVE entry can have a list of bug urls. These can be:\n\n* A bugzilla bug number. These will be converted to a bugzilla link.\n* A comma separated list of bug numbers. These will be converted to a link to a bugzilla list of bugs.\n* A valid URL will be kept as is.\n\nAlong with the `url` field of a bug, a `desc` may optionally be supplied. This will be the link text\nfor the bug link. If it is not supplied the default is `Bug {url}`. For example, the link text for\nthe bug in `CVE-2016-5276` above would be `Bug 1287721`.\n\nThe main `description` field as well as those of the CVE entries can be multi-line and will be processed\nas markdown. The YAML spec provides [different ways of enabling multi-line](http://yaml.org/spec/1.1/#id926836), \nbut the best for this application is to use the `|` character after the `:` \nlike you see in the example for the main description and `CVE-2016-5270` above.\n\n## Bug Bounty Hall of Fame Files\n\nThis repo also contains data that bedrock uses to generate the\n[client](https://www.mozilla.org/en-US/security/bug-bounty/hall-of-fame/) and\n[web](https://www.mozilla.org/en-US/security/bug-bounty/web-hall-of-fame/) hall of fame pages.\nThese are the YAML files in the `bug-bounty-hof` directory. The data format for these YAML files is rather simple.\nThe only required field in the file is `names`: this is a list of data structures about each name in the hall of fame.\nFor each name entry only `name` and `date` are required. The `date` field must be in the format `YYYY-MM-DD`.\nYou can optionally add a `url` field and the entry on the page will link to this url. You are free to add other\ndata to each entry (e.g. `bug`, `organization`), but at present bedrock will not use these items on the site.\n\n## Linter Script\n\nThere is a script in the repo called `check_advisories.py` that will tell you when you've gotten something wrong. It uses\nthe same parsing algorithm as bedrock and so it should catch errors before they cause problems\non the website. By default it will check all modified advisory and bug bounty hall-of-fame files in the repo. If you want\nto check them all you can pass the `--all` switch. And if you only want it to check the changes\nstaged in git's index you can pass the `--staged` switch (this is mostly good for a git pre-commit hook).\n\nYou'll need a couple of dependency libraries. You can get them with the following command:\n\n```shell\n$ pip install ./\n```\n\nIt's best to do that within a [virtualenv](http://virtualenv.readthedocs.org/en/latest/).\nThen you can run the command:\n\n```shell\n$ check_advisories\nChecked 3 files. Found 0 errors.\n```\n\nUse the `--help` switch to see all options.\n\n### Use as a git hook\n\nThe best way to use this linter script is to add a git pre-commit hook. Included in the repo is a\nshell script useful for this purpose. To install it issue the following commands from the root\ndirectory of the repo:\n\n```shell\n$ cd .git/hooks && ln -s ../../pre-commit-hook.sh pre-commit\n```\n\nAfter this if you attempt to commit a change to a file that has a problem being parsed, you'll be\ninformed which file has a problem and the commit will be aborted.\n"
},
{
  "name": "triage-center",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Green_bug_and_broom.svg",
      "LICENSE",
      "README.md",
      "bugzilla.js",
      "components-min.json",
      "index.css",
      "index.html",
      "jquery-ui-1.11.4",
      "process.py"
    ]
  },
  "makefile": null,
  "readme": "# triage-center\n\nTool for triage leads for Firefox-related components in Bugzilla to find bugs\nrequring action.\n\nCan be visited directly at https://mozilla.github.io/triage-center.\n\nTo run locally, clone the repo, run `process.py` and then use\n`python3 -m http.server` or your preferred method to bring up a static server.\n\nUses an [SVG favicon](https://commons.wikimedia.org/wiki/File:Green_bug_and_broom.svg),\nprovided under the GLGPL by Wikimedia User [Poznaniak](https://commons.wikimedia.org/wiki/User:Poznaniak).\n"
},
{
  "name": "hackweek-avatar-maker",
  "files": {
    "/": [
      ".gitignore",
      ".prettierrc",
      "CUSTOMIZING.md",
      "LICENSE",
      "README.md",
      "assets",
      "dist",
      "index.html",
      "package-lock.json",
      "package.json",
      "scripts",
      "src",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# Hackweek Avatar Maker\n\nThis repository is to track work for the 2021 Hubs Team Q1 Hackathon. For discussion and ideation, please use the Issues tab, but note that this project is only lightly maintained by Mozilla. You can use the editor as-is [here](https://mozilla.github.io/hackweek-avatar-maker/).\n\n## Creating an Avatar\n\nTo create an avatar for Hubs:\n\n1. Go to [https://mozilla.github.io/hackweek-avatar-maker/](https://mozilla.github.io/hackweek-avatar-maker/). The first avatar that you see will be a randomly generated avatar using different components that are available.\n2. Have fun! You can mix and match different selections to give your avatar a unique look.\n3. Download your avatar using the 'Export avatar' button.\n4. Sign into a Hubs room\n5. Select the \"More\" icon in the bottom right corner of the screen and click \"Change Name and Avatar\".\n6. Click the \"Change Avatar\" button.\n7. From here, you can see a list of your own avatars, the first spot should say \"Create Avatar\".\n8. Click the link that says 'Custom GLB', found under the three default body shapes\n9. Upload the .glb file that you saved to your computer\n10. Save your avatar and enjoy your new look!\n\n## Creating Accessories and Custom Components\n\nYou can create custom components for the avatar by modeling the object in blender. While you can upload any .glb file as an accessory, using the base template for an avatar built with this editor will ensure that the uploaded custom component sits in the correct place on the avatar. More information can be found in the [customization guide](./CUSTOMIZING.md).\n\n## License\n\nThe 3D models used in this app are \u00a92020-2022 by individual [mozilla.org](https://www.mozilla.org/) contributors, under a [Creative Commons Attribution-ShareAlike 3.0 license](https://www.mozilla.org/en-US/foundation/licensing/website-content/).\n\n## About the editor\n\nWe invite the community to use this editor as a template for creating and hosting new avatar tools. The code is released under the [MPL 2.0 license](./LICENSE) and we'd love to see what you make with it! Here's a bit about how the editor works:\n\n- The avatar that you see is comprised of different pieces that were designed to work on the same skeleton and body format. [Models](https://github.com/mozilla/hackweek-avatar-maker/tree/main/assets/models) for each piece and category are saved with a specific naming convention, which allows the app to correctly put them into a category depending on where the accessory should be placed on the base model.\n\n- A script takes screenshots of the pieces that are contained in the `assets/models` directory and uses these to indicate the different pieces that can be selected from.\n\n- When you've finished customizing your avatar, the meshes are combined and the correct components to have animations in Hubs are added to the avatar .glb file that is saved.\n\nMore information can be found in the [customization guide](./CUSTOMIZING.md).\n\n## FAQ\n\n**Q: I'm an avatar creator - can I make my own avatar editor for Hubs with this?**\n\nA: Yes! We'd love that. Feel free to drop into the #avatars channel in our [Discord chat server](https://discord.gg/dFJncWwHun) if you have any questions about that.\n\n**Q: Why isn't this built-in to Hubs?**\n\nA: We're really keen on keeping a wide range of styles and avatars available for Hubs. We decided to make this a standalone application so that we could encourage others to build avatar tools that will work with Hubs, rather than being prescriptive about what we think avatars should look like.\n\n**Q: Can you add a new hat / accessory / t-shirt / hair style?**\n\nA: Since this was a hack week project, we're not actively building this out as a fully featured editor - so probably not.\n\n**Q: Can _*I*_ add a new hat / accessory / t-shirt / hair style?**\n\nA: Sure! If you build something custom and are interested in contributing it back under a Creative Commons license, feel free to submit a pull request that adds a new accessory with the proper naming convention. If you have questions about this, you can reach us on Discord or email us at hubs@mozilla.com\n\n## Resources\n\n- [Avatar Texture Tool (misslivirose)](https://github.com/misslivirose/avatar-texture-tool)\n- [Avatar Customizer (rhiannanberry)](https://github.com/rhiannanberry/Avatar-Customizer)\n- [Quilt (brianpeiris)](https://github.com/brianpeiris/quilt)\n- [Hubs avatar pipelines](https://github.com/mozillareality/hubs-avatar-pipelines/)\n- [Advanced Avatar Customization](https://hubs.mozilla.com/docs/creators-advanced-avatar-customization.html)\n- [Video: using Hubs components for avatar creation in Blender](https://www.youtube.com/watch?v=qBvZhh6KVcg)\n"
},
{
  "name": "community-data",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "credits",
      "forums"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Community Data\n\nThis repository is the canonical source of the following Mozilla community information:\n\n* `credits/names.csv`: The comprehensive contributor list for [about:credits](https://www.mozilla.org/credits/)\n* `forums/raw-ng-list.txt`: The current forum list for [mozilla.org/about/forums](https://www.mozilla.org/en-US/about/forums/)\n\n"
},
{
  "name": "django-csp",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CHANGES",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "csp",
      "docs",
      "setup.cfg",
      "setup.py",
      "test_settings.py",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "configuration.rst",
      "contributing.rst",
      "decorators.rst",
      "index.rst",
      "installation.rst",
      "nonce.rst",
      "reports.rst",
      "trusted_types.rst"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": ".. image:: https://badge.fury.io/py/django-csp.svg\n   :target: https://pypi.python.org/pypi/django_csp\n\n.. image:: https://circleci.com/gh/mozilla/django-csp/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/mozilla/django-csp/?branch=main\n\n.. image:: https://coveralls.io/repos/github/mozilla/django-csp/badge.svg?branch=main\n   :target: https://coveralls.io/github/mozilla/django-csp?branch=main\n\n==========\nDjango-CSP\n==========\n\nDjango-CSP adds Content-Security-Policy_ headers to Django.\n\nThe code lives on GitHub_, where you can report Issues_. The full\ndocumentation is available on ReadTheDocs_.\n\n\n\n.. _Content-Security-Policy: http://www.w3.org/TR/CSP/\n.. _GitHub: https://github.com/mozilla/django-csp\n.. _Issues: https://github.com/mozilla/django-csp/issues\n.. _ReadTheDocs: http://django-csp.readthedocs.org/\n"
},
{
  "name": "taar",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitignore",
      ".pre-commit-config.yaml",
      "API.md",
      "CODE_OF_CONDUCT.md",
      "DATA_COLLECTION_POLICY.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "analysis",
      "bin",
      "docker-compose.yml",
      "docs",
      "environment.yml",
      "prod-requirements.txt",
      "setup.cfg",
      "setup.py",
      "taar",
      "tests"
    ],
    "/docs": [
      "TAARLITE-README.md",
      "randomized_tails.md",
      "release_instructions.md",
      "taarlite-screenshot.png"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build up tests flake8 ci tests-with-cov\n\nall:\n\t# PySpark only knows eggs, not wheels\n\tpython setup.py sdist\n\nsetup_conda:\n\t# Install all dependencies and setup repo in dev mode\n\tconda env update -n taar-37 -f environment.yml\n\tpython setup.py develop\n\nconda_update:\n    # Actualize env after .yml file was modified\n\tconda env update -n taar-37 -f environment.yml --prune\n\nconda_export:\n\tconda env export > environment.yml\n\nupload:\n\ttwine upload --repository-url https://upload.pypi.org/legacy/ dist/*\n\npytest:\n\tpython setup.py develop\n\tpython setup.py test\n\tflake8 taar tests\n\nbuild:\n\tdocker build . -t taar:latest\n\nup:\n\tdocker-compose up\n\ntest-container:\n\tdocker run -e CODECOV_TOKEN=${CODECOV_TOKEN} -it taar:latest test\n\nrun_local:\n\t. bin/test_env.sh && python taar/flask_app.py -H 0.0.0.0 -P 8001\n\nrun_package_test:\n\tpython setup.py develop\n\tpython bin/run_package_test.py\n\nshell:\n\tdocker run -it taar:latest bash \n",
  "readme": "# Taar\nTelemetry-Aware Addon Recommender\n\n[![CircleCI](https://circleci.com/gh/mozilla/taar.svg?style=svg)](https://circleci.com/gh/mozilla/taar)\n\n\nTable of Contents\n=================\n\n* [Taar](#taar)\n  * [How does it work?](#how-does-it-work)\n    * [Supported models](#supported-models)\n  * [Build and run tests](#build-and-run-tests)\n  * [Pinning dependencies](#pinning-dependencies)\n  * [Instructions for releasing updates to production](#instructions-for-releasing-updates-to-production)\n  * [Collaborative Recommender](#collaborative-recommender)\n  * [Ensemble Recommender](#ensemble-recommender)\n  * [Locale Recommender](#locale-recommender)\n  * [Similarity Recommender](#similarity-recommender)\n  * [Google Cloud Platform resources](#google-cloud-platform-resources)\n    * [Google Cloud BigQuery](#google-cloud-bigquery)\n    * [Google Cloud Storage](#google-cloud-storage)\n    * [Google Cloud BigTable](#google-cloud-bigtable)\n  * [Production Configuration Settings](#production-configuration-settings)\n  * [Deleting individual user data from all TAAR resources](#deleting-individual-user-data-from-all-taar-resources)\n  * [Airflow environment configuration](#airflow-environment-configuration)\n  * [Staging Environment](#staging-environment)\n  * [A note on cdist optimization\\.](#a-note-on-cdist-optimization)\n\n\n## How does it work?\nThe recommendation strategy is implemented through the\n[RecommendationManager](taar/recommenders/recommendation_manager.py).\nOnce a recommendation is requested for a specific [client\nid](https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/telemetry/data/common-ping.html),\nthe recommender iterates through all the registered models (e.g.\n[CollaborativeRecommender](taar/recommenders/collaborative_recommender.py))\nlinearly in their registered order. Results are returned from the\nfirst module that can perform a recommendation.\n\nEach module specifies its own sets of rules and requirements and thus\ncan decide if it can perform a recommendation independently from the\nother modules.\n\n### Supported models\nThis is the ordered list of the currently supported models:\n\n| Order | Model | Description | Conditions | Generator job |\n|-------|-------|-------------|------------|---------------|\n| 1 | [Collaborative](taar/recommenders/collaborative_recommender.py) | recommends add-ons based on add-ons installed by other users (i.e. [collaborative filtering](https://en.wikipedia.org/wiki/Collaborative_filtering))|Telemetry data is available for the user and the user has at least one enabled add-on|[source](https://github.com/mozilla/telemetry-batch-view/blob/master/src/main/scala/com/mozilla/telemetry/ml/AddonRecommender.scala)|\n| 2 | [Similarity](taar/recommenders/similarity_recommender.py) | recommends add-ons based on add-ons installed by similar representative users|Telemetry data is available for the user and a suitable representative donor can be found|[source](https://github.com/mozilla/telemetry-airflow/blob/master/jobs/taar_similarity.py)|\n| 3 | [Locale](taar/recommenders/locale_recommender.py) |recommends add-ons based on the top addons for the user's locale|Telemetry data is available for the user and the locale has enough users|[source](https://github.com/mozilla/telemetry-airflow/blob/master/jobs/taar_locale.py)|\n| 4 | [Ensemble](taar/recommenders/ensemble_recommender.py) &#42;|recommends add-ons based on the combined (by [stacked generalization](https://en.wikipedia.org/wiki/Ensemble_learning#Stacking)) recomendations of other available recommender modules.|More than one of the other Models are available to provide recommendations.|[source](https://github.com/mozilla/telemetry-airflow/blob/master/jobs/taar_ensemble.py)|\n\nAll jobs are scheduled in Mozilla's instance of\n[Airflow](https://github.com/mozilla/telemetry-airflow).  The\nCollaborative, Similarity and Locale jobs are executed on a\n[daily](https://github.com/mozilla/telemetry-airflow/blob/master/dags/taar_daily.py)\nschedule, while the ensemble job is scheduled on a\n[weekly](https://github.com/mozilla/telemetry-airflow/blob/master/dags/taar_weekly.py)\nschedule.\n\n\n## Build and run tests\nYou should be able to build taar using Python 3.5 or 3.7. \nTo run the testsuite, execute ::\n\n```python\n$ python setup.py develop\n$ python setup.py test\n```\n\nAlternately, if you've got GNUMake installed, a Makefile is included\nwith\n[`build`](https://github.com/mozilla/taar/blob/more_docs/Makefile#L20)\nand\n[`test-container`](https://github.com/mozilla/taar/blob/more_docs/Makefile#L55)\ntargets.\n\nYou can just run `make\nbuild; make test-container` which will build a complete Docker\ncontainer and run the test suite inside the container.\n\n## Pinning dependencies\n\nTAAR uses miniconda and a environment.yml file to manage versioning.\n\nTo update versions, edit the `environment.yml` with the new dependency\nyou need then run `make conda_update`.\n\nIf you are unfamiliar with using conda, see the [official\ndocumentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)\nfor reference.\n\n## Instructions for releasing updates to production\n\nBuilding a new release of TAAR is fairly involved.  Documentation to\ncreate a new release has been split out into separate\n[instructions](https://github.com/mozilla/taar/blob/master/docs/release_instructions.md).\n\n\n## Dependencies\n\n### Google Cloud Storage resources\n\nThe final TAAR models are stored in:\n\n```gs://moz-fx-data-taar-pr-prod-e0f7-prod-models```\n\nThe TAAR production model bucket is defined in Airflow under the\nvariable `taar_etl_model_storage_bucket`\n\nTemporary models that the Airflow  ETL jobs require are stored in a\ntemporary bucket defined in the Airflow variable `taar_etl_storage_bucket`\n\nRecommendation engines load models from GCS.\n\nThe following table is a complete list of all resources per\nrecommendation engine.\n\nRecommendation Engine |  GCS Resource \n--- | ---\nRecommendationManager Whitelist | gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/addon_recommender/only_guids_top_200.json.bz2\nSimilarity Recommender | gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/taar/similarity/donors.json.bz2 <br> gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/taar/similarity/lr_curves.json.bz2\nCollaborativeRecommender |  gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/addon_recommender/item_matrix.json.bz2 <br> gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/addon_recommender/addon_mapping.json.bz2\nLocaleRecommender | gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/taar/locale/top10_dict.json.bz2\nEnsembleRecommender | gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/taar/ensemble/ensemble_weight.json.bz2\nTAAR lite | gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/taar/lite/guid_install_ranking.json.bz2 <br/> gs://moz-fx-data-taar-pr-prod-e0f7-prod-models/taar/lite/guid_coinstallation.json.bz2\n\n\n# Production environment variables required for TAAR\n\n## Collaborative Recommender\n\nEnv Variable | Value \n------- | --- \nTAAR_ITEM_MATRIX_BUCKET | \"moz-fx-data-taar-pr-prod-e0f7-prod-models\"\nTAAR_ITEM_MATRIX_KEY  | \"addon_recommender/item_matrix.json.bz2\"\nTAAR_ADDON_MAPPING_BUCKET | \"moz-fx-data-taar-pr-prod-e0f7-prod-models\"\nTAAR_ADDON_MAPPING_KEY | \"addon_recommender/addon_mapping.json.bz2\"\n\n## Ensemble Recommender\n\nEnv Variable | Value\n--- | --- \nTAAR_ENSEMBLE_BUCKET  | \"moz-fx-data-taar-pr-prod-e0f7-prod-models\"\nTAAR_ENSEMBLE_KEY | \"taar/ensemble/ensemble_weight.json.bz2\"\n\n## Locale Recommender\n\nEnv Variable | Value\n--- | --- \nTAAR_LOCALE_BUCKET | \"moz-fx-data-taar-pr-prod-e0f7-prod-models\"\nTAAR_LOCALE_KEY | \"taar/locale/top10_dict.json.bz2\"\n\n## Similarity Recommender\n\nEnv Variable | Value\n--- | --- \nTAAR_SIMILARITY_BUCKET | \"moz-fx-data-taar-pr-prod-e0f7-prod-models\"\nTAAR_SIMILARITY_DONOR_KEY | \"taar/similarity/donors.json.bz2\"\nTAAR_SIMILARITY_LRCURVES_KEY | \"taar/similarity/lr_curves.json.bz2\"\n\n\n## TAAR Lite\n\nEnv Variable | Value\n--- | --- \nTAARLITE_GUID_COINSTALL_BUCKET | \"moz-fx-data-taar-pr-prod-e0f7-prod-models\"\nTAARLITE_GUID_COINSTALL_KEY | \"taar/lite/guid_coinstallation.json.bz2\"\nTAARLITE_GUID_RANKING_KEY | \"taar/lite/guid_install_ranking.json.bz2\"\n\n\n## Google Cloud Platform resources\n### Google Cloud BigQuery\n\nCloud BigQuery uses the GCP project defined in Airflow in the\nvariable `taar_gcp_project_id`.\n\nDataset  \n* `taar_tmp`\n\nTable ID \n* `taar_tmp_profile`\n\nNote that this table only exists for the duration of the taar_weekly\njob, so there should be no need to manually manage this table.\n\n### Google Cloud Storage \n\nThe taar user profile extraction puts Avro format files into \na GCS bucket defined by the following two variables in Airflow:\n\n* `taar_gcp_project_id`\n* `taar_etl_storage_bucket`\n\nThe bucket is automatically cleared at the *start* and *end* of\nthe TAAR weekly ETL job.\n\n### Google Cloud BigTable \n\nThe final TAAR user profile data is stored in a Cloud BigTable\ninstance defined by the following two variables in Airflow:\n\n* `taar_gcp_project_id`\n* `taar_bigtable_instance_id`\n\nThe table ID for user profile information is `taar_profile`.\n\n\n------\n\n## Production Configuration Settings\n\nProduction environment settings are stored in a [private repository](https://github.com/mozilla-services/cloudops-deployment/blob/master/projects/data/puppet/yaml/type/data.api.prod.taar.yaml).\n\n\n## Deleting individual user data from all TAAR resources\n\nDeletion of records in TAAR is fairly straight forward.  Once a user\ndisables telemetry from Firefox, all that is required is to delete\nrecords from TAAR.\n\nDeletion of records from the TAAR BigTable instance will remove the\nclient's list of addons from TAAR.  No further work is required.\n\nRemoval of the records from BigTable will cause JSON model updates to\nno longer take the deleted record into account.  JSON models are\nupdated on a daily basis via the\n[`taar_daily`](https://github.com/mozilla/telemetry-airflow/blob/master/dags/taar_daily.py)\n\nUpdates in the weekly Airflow job in \n[`taar_weekly`](https://github.com/mozilla/telemetry-airflow/blob/master/dags/taar_weekly.py) only update the ensemble weights and the user profile information.\n\nIf the user profile information in `clients_last_seen` continues to\nhave data for the user's telemetry-id, TAAR will repopulate the user\nprofile data.  \n\nUsers who wish to remove their data from TAAR need to: \n1. Disable telemetry in Firefox\n2. Have user telemetry data removed from all telemetry storage systems\n   in GCP. Primarily this means the `clients_last_seen` table in\n   BigQuery.\n3. Have user data removed from BigTable.\n\n\n\n## Airflow environment configuration\n\nTAAR requires some configuration to be stored in Airflow variables for\nthe ETL jobs to run to completion correctly.\n\nAirflow Variable | Value \n--- | ---\ntaar_gcp_project_id | The Google Cloud Platform project where BigQuery temporary tables, Cloud Storage buckets for Avro files and BigTable reside for TAAR.\ntaar_etl_storage_bucket | The Cloud Storage bucket name where temporary Avro files will reside when transferring data from BigQuery to BigTable. \ntaar_etl_model_storage_bucket | The main GCS bucket where the models are stored\ntaar_bigtable_instance_id | The BigTable instance ID for TAAR user profile information\ntaar_dataflow_subnetwork | The subnetwork required to communicate between Cloud Dataflow\n\n\n## Staging Environment\n\nThe staging environment of the TAAR service in GCP can be reached using\ncurl.\n\n```\ncurl https://user@pass:stage.taar.nonprod.dataops.mozgcp.net/v1/api/recommendations/<hashed_telemetry_id>\n```\n\nRequests for a TAAR-lite recommendation can be made using curl as\nwell:\n\n```\ncurl https://stage.taar.nonprod.dataops.mozgcp.net/taarlite/api/v1/addon_recommendations/<addon_guid>/\n```\n\n\n## TAARlite cache tools\n\nThere is a taarlite-redis tool to manage the taarlit redis cache.\n\nThe cache needs to be populated using the `--load` command or TAARlite\nwill return no results.\n\nIt is safe to reload new data while TAARlite is running - no\nperformance degradation is expected.\n\nThe cache contains a 'hot' buffer for reads and a 'cold' buffer to\nwrite updated data to.\n\nSubsequent invocations to `--load` will update the cache in the cold\nbuffer.  After data is successfully loaded, the hot and cold buffers\nare swapped.\n\nRunning the the taarlite-redis tool inside the container:\n\n```\n$ docker run -it taar:latest bin/run python /opt/conda/bin/taarlite-redis.py --help\n\nUsage: taarlite-redis.py [OPTIONS]\n\n  Manage the TAARLite redis cache.\n\n  This expecte that the following environment variables are set:\n\n  REDIS_HOST REDIS_PORT\n\nOptions:\n  --reset  Reset the redis cache to an empty state\n  --load   Load data into redis\n  --info   Display information about the cache state\n  --help   Show this message and exit.\n```\n\n\n## Testing\n\n\nTAARLite will respond with suggestions given an addon GUID.\n\nA sample URL path may look like this:\n\n`/taarlite/api/v1/addon_recommendations/uBlock0%40raymondhill.net/`\n\nTAAR will treat any client ID with only repeating digits (ie: 0000) as\na test client ID and will return a dummy response.\n\nA URL with the path : `/v1/api/recommendations/0000000000/` will\nreturn a valid JSON result\n\n\n## A note on cdist optimization. \ncdist can speed up distance computation by a factor of 10 for the computations we're doing.\nWe can use it without problems on the canberra distance calculation.\n\nUnfortunately there are multiple problems with it accepting a string array. There are different\nproblems in 0.18.1 (which is what is available on EMR), and on later versions. In both cases \ncdist attempts to convert a string to a double, which fails. For versions of scipy later than\n0.18.1 this could be worked around with:\n\n    distance.cdist(v1, v2, lambda x, y: distance.hamming(x, y))\n\nHowever, when you manually provide a callable to cdist, cdist can not do it's baked in \noptimizations (https://github.com/scipy/scipy/blob/v1.0.0/scipy/spatial/distance.py#L2408)\nso we can just apply the function `distance.hamming` to our array manually and get the same\nperformance.\n"
},
{
  "name": "taar_gcp_etl",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "environment.yml",
      "setup.py",
      "taar_etl"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build up tests flake8 ci tests-with-cov\n\nTAG_BASE=gcr.io/${GCP_PROJECT_ID}/taar_gcp_etl\nTAG_REV=$(shell git tag|tail -n 1)\n\nall: build\n\nbuild:\n\tdocker build -t app:build .\n\nsetup_conda:\n\t# Install all dependencies and setup repo in dev mode\n\tconda env create -f environment.yml\n\tpython setup.py develop\n\nshell:\n\tdocker run --rm -it mozilla/taar_amodump:latest /bin/bash\n\ntag_gcr_io:\n\tdocker tag app:build ${TAG_BASE}:${TAG_REV}\n\npush_gcr_io:\n\tdocker push ${TAG_BASE}:${TAG_REV}\n\n\n",
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/taar_gcp_etl.svg?style=svg)](https://circleci.com/gh/mozilla/taar_gcp_etl)\n\nTAARlite and TAAR ETL jobs for GCP\n==================================\n\n\nThis repo contains scripts which are used in ETL jobs for the TAAR and\nTAARlite services.\n\n\n-----\n\nPut all your code into your own repository and package it up as a\ncontainer.  This makes it much easier to deploy your code into both\nGKEPodOperators which run containerized code within a Kubernetes Pod,\nas well as giving you the ability to deploy code into a dataproc\ncluster using a git checkout.\n\n\n## New GCS storage locations\n\nProd buckets: \n\n    moz-fx-data-taar-pr-prod-e0f7-prod-etl\n    moz-fx-data-taar-pr-prod-e0f7-prod-models\n\nTest bucket:\n\n    taar_models\n\n## Jobs\n\ntaar_etl.taar_amodump \n\n    This job extracts the complete AMO listing and emits a JSON blob.\n    Depends On:\n        https://addons.mozilla.org/api/v4/addons/search/\n\n    Output file: \n        Path: gs://taar_models/addon_recommender/extended_addons_database.json\n\ntaar_etl.taar_amowhitelist \n\n    This job filters the AMO whitelist from taar_amodump into 3 filtered lists.\n\n    Depends On:\n        taar_etl.taar_amodump \n\n    Output file:\n        Path: gs://taar_models/addon_recommender/whitelist_addons_database.json\n        Path: gs://taar_models/addon_recommender/featured_addons_database.json\n        Path: gs://taar_models/addon_recommender/featured_whitelist_addons.json\n\ntaar_lite_guid_ranking\n\n    This job loads installation counts by addon from BigQuery telemetry telemetry.addons table\n    and saves it to GCS.\n\n    Output file:\n        Path: gs://taar_models/taar/lite/guid_install_ranking.json\n\n\ntaar_etl.taar_update_whitelist\n\n    This job extracts the editorial approved addons from AMO\n\n    Depends On:\n        https://addons.mozilla.org/api/v4/addons/search/\n\n    Output file:\n        Path: gs://taar_models/addon_recommender/only_guids_top_200.json\n\n\ntaar_etl.taar_profile_bigtable\n\n    This task is responsible for extracting data from BigQuery from\n    the telemetry table: `clients_last_seen`\n    and exports temporary files in Avro format to a bucket in Google\n    to Cloud Storage.\n\n    Avro files are then loaded into Cloud BigTable.\n\n    Each record is keyed on a SHA256 hash of the telemetry client-id.\n\n    While this job runs - several intermediate data files are created.\n    Any intermediate files are destroyed at the end of the job\n    execution.\n\n    The only artifact of this job is records residing in BigTable\n    as defined by the `--bigtable-instance-id` and `--bigtable-table-id`\n    options to the job.\n\n\n## PySpark Jobs\n\ntaar_similarity\n\n    Output file: \n        Path: gs://taar_models/similarity/donors.json\n        Path: gs://taar_models/similarity/lr_curves.json\n\ntaar_locale\n\n    Output file: \n        Path: gs://taar_models/locale/top10_dict.json\n\n\ntaar_lite\n\n    Compute addon coinstallation rates for TAARlite\n    \n    Output file: \n        Path: gs://taar_models/taar/lite/guid_coinstallation.json\n\n\n## Google Cloud Platform jobs\n\ntaar_etl.taar_profile_bigtable\n\n    This job extracts user profile data from `clients_last_seen` to\n    build a user profile table in Bigtable. This job is split into 4\n    parts:\n\n    1. Filling a BigQuery table with all pertinent data so that we\n       can export to Avro on Google Cloud Storage.  The fill is\n       completed using a `CREATE OR REPLACE TABLE` operation in\n       BigQuery.\n\n    2. Exporting the newly populated BigQuery table into Google Cloud\n       Storage in Apache Avro format.\n\n    3. Import of Avro files from Google Cloud Storage into \n       Cloud BigTable.\n\n    4. Delete users that opt-out from telemetry colleciton. \n \n    When this set of tasks is scheduled in Airflow, it is expected\n    that the Google Cloud Storage bucket will be cleared at the start of\n    the DAG, and cleared again at the end of DAG to prevent unnecessary\n    storage.\n\n\n## Uploading images to gcr.io\n\nCircleCI will automatically build a docker image and push the image into\ngcr.io for production using the latest tag.\n\nYou can use images from the gcr.io image repository using a path like:\n\n```\ngcr.io/moz-fx-data-airflow-prod-88e0/taar_gcp_etl:<latest_tag>\n```\n\n\n\n## Running a job from within a container\n\nSample command for the impatient:\n\n```\n\tdocker run \\\n\t\t-v ~/.gcp_creds:/app/creds  \\     # directory where you service_account json file resides \n\t\t-v ~/.config:/app/.config \\\n\t\t-e GOOGLE_APPLICATION_CREDENTIALS=/app/creds/<YOUR_SERVICE_ACCOUNT_JSON_FILE_HERE.json> \\\n\t\t-e GCLOUD_PROJECT=<YOUR_TEST_GCP_PROJECT_HERE> \\\n\t\t-it app:build \\\n\t\t-m taar_etl.taar_profile_bigtable \\\n\t\t--iso-date=<YESTERDAY_ISODATE_NO_DASHES> \\\n\t\t--gcp-project=<YOUR_TEST_GCP_PROJECT_HERE> \\\n\t\t--avro-gcs-bucket=<YOUR_GCS_BUCKET_FOR_AVRO_HERE> \\\n\t\t--bigquery-dataset-id=<BIG_QUERY_DATASET_ID_HERE> \\\n\t\t--bigquery-table-id=<BIG_QUERY_TABLE_ID_HERE> \\\n\t\t--bigtable-instance-id=<BIG_TABLE_INSTANCE_ID> \\\n\t\t--wipe-bigquery-tmp-table\n```\n\nThe container defines an entry point which pre-configures the conda\nenviromet and starts up the python interpreter.  You need to pass in\narguments to run your module as a task.\n\nNote that to test on your local machine - you need to volume mount two\nlocations to get your credentials to load, and you will need to mount\nyour google authentication tokens by mounting `.config` and you will\nalso need to volume mount your GCP service account JSON file.  You\nwill also need to specify your GCP_PROJECT.\n\n### More examples\n\n**amodump**\n```\ndocker run \\\n    -v ~/.config:/app/.config \\\n    -v ~/.gcp_creds:/app/creds \\\n    -e GOOGLE_APPLICATION_CREDENTIALS=/app/creds/<YOUR_SERVICE_ACCOUNT_JSON_FILE_HERE.json> \\\n    -e GCLOUD_PROJECT=<YOUR_TEST_GCP_PROJECT_HERE>   \\\n    -it app:build \\\n    -m taar_etl.taar_amodump \\\n    --date=20220620\n```\n**amowhitelist**\n```\ndocker run \\\n    -v ~/.config:/app/.config \\\n    -v ~/.gcp_creds:/app/creds \\\n    -e GOOGLE_APPLICATION_CREDENTIALS=/app/creds/<YOUR_SERVICE_ACCOUNT_JSON_FILE_HERE.json> \\\n    -e GCLOUD_PROJECT=<YOUR_TEST_GCP_PROJECT_HERE>   \\\n    -it app:build \\\n    -m taar_etl.taar_amowhitelist\n```\n**update_whitelist**\n```\ndocker run \\\n    -v ~/.config:/app/.config \\\n    -v ~/.gcp_creds:/app/creds \\\n    -e GOOGLE_APPLICATION_CREDENTIALS=/app/creds/<YOUR_SERVICE_ACCOUNT_JSON_FILE_HERE.json> \\\n    -e GCLOUD_PROJECT=<YOUR_TEST_GCP_PROJECT_HERE>   \\\n    -it app:build \\\n    -m taar_etl.taar_update_whitelist \\\n    --date=20220620\n```\n**profile_bigtable.delete** \n\nYou might need to replace GCP project specific arguments\n```\ndocker run \\\n    -v ~/.config:/app/.config \\\n    -e GCLOUD_PROJECT=moz-fx-data-taar-nonprod-48b6  \\\n    -it app:build \\\n    -m taar_etl.taar_profile_bigtable \\\n    --iso-date=20210426 \\\n    --gcp-project=moz-fx-data-taar-nonprod-48b6 \\\n    --bigtable-table-id=taar_profile \\\n    --bigtable-instance-id=taar-stage-202006 \\\n    --delete-opt-out-days 28 \\\n    --avro-gcs-bucket moz-fx-data-taar-nonprod-48b6-stage-etl \\\n    --subnetwork regions/us-west1/subnetworks/gke-taar-nonprod-v1 \\\n    --dataflow-workers=2 \\\n    --dataflow-service-account taar-stage-dataflow@moz-fx-data-taar-nonprod-48b6.iam.gserviceaccount.com \\\n    --sample-rate=1.0 \\\n    --bigtable-delete-opt-out\n```"
},
{
  "name": "standards-positions",
  "files": {
    "/": [
      ".github",
      ".nojekyll",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "LICENSE.txt",
      "README.md",
      "activities.json",
      "activities.py",
      "asset",
      "components",
      "index.html",
      "node_modules",
      "package.json",
      "pre-commit",
      "requirements.txt"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla's Positions on Emerging Web Specifications\n\n_See [the dashboard](https://mozilla.github.io/standards-positions/) for Mozilla's current positions._\n\n## What is this Repository?\n\nThis repo is where [Mozilla](https://mozilla.org/) decides and documents what it thinks about\nemerging technical specifications for the Web. Typically they're draft documents in the\n[IETF](https://ietf.org/), [W3C](https://w3.org/) (including the [WICG](https://wicg.github.io/)),\n[WHATWG](https://whatwg.org/), and [Ecma TC39](https://github.com/tc39), but they could come from\nelsewhere too.\n\nHaving a clear Mozilla position on these specifications helps us align our thinking and communicate\nit clearly to these standards bodies, as well as other browsers.\n\n*See our [contribution guidelines](CONTRIBUTING.md) for information about how you can participate.*\n\nImplementation status (or even intention) isn't tracked here; see [dev-platform](https://groups.google.com/a/mozilla.org/g/dev-platform/).\n\n### Possible Specification Positions\n\nThe currently possible positions are:\n\n- `under consideration` - Mozilla's position on this specification is being discussed.\n- `important` - This specification is conceptually good and is important to Mozilla.\n- `worth prototyping` - Mozilla sees this specification as conceptually good, and worth prototyping, getting feedback on its value, and iterating.\n- `non-harmful` - Mozilla does not see this specification as harmful, but is not convinced that it is a good approach or worth working on.\n- `defer` - Mozilla does not intend to look at this specification at all in the near future.\n- `harmful` - Mozilla considers this specification to be harmful in its current state.\n\nNote that these positions do not address whether Mozilla will commit resources to a specification,\nnor does it commit Mozilla to implementing them.\n"
},
{
  "name": "awebpodcast",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintrc.js",
      ".gitignore",
      ".gitlab-ci.yml",
      ".stylelintrc",
      "CNAME",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "archetypes",
      "bin",
      "config-dev.toml",
      "config-prod.toml",
      "config-stage.toml",
      "config.toml",
      "content",
      "docker-compose.yml",
      "docs",
      "gulpfile.js",
      "layouts",
      "package-lock.json",
      "package.json",
      "src",
      "static"
    ],
    "/docs": [
      "CNAME"
    ]
  },
  "makefile": null,
  "readme": "## Running Locally\n\n1. Install dependencies: `npm install`\n2. Start Gulp for asset compilation: `gulp`\n3. In a new terminal window, start the server: `docker-compose up`\n4. Open `http://localhost:1313` in a browser\n\nEdit the `.scss` and `.js` files in the `/src` directory (*NOT* in `/static`).\n\n## Adding new episodes\n\n1. `docker exec awebpodcast_hugo_1 hugo new episodes/201X-XX-XX-SXXEXX-episode-title-here.md`\n\nYou can create this file by hand in the filesystem, but the above command will use the\n`archetypes/episodes.md` template to populate all the necessary front matter, saving\nyou precious, precious time.\n\n## Adding new pages\n\nThis probably won't happen often, but if needed, run:\n\n1. `docker exec awebpodcast_hugo_1 hugo new somepage.md`\n\n## Pushing to dev\n\n1. Push your changes to the `dev` branch: `git push origin my-branch-name:dev`\n2. Test on `https://dev.awebpodcast.org`\n\n## Pushing to stage\n\nAny merge to the `master` branch will automatically update the staging site:\n\n`https://stage.awebpodcast.org/`\n\nA notice will be posted in `#irlpodcast-notify` on Slack when the push has completed.\n\n## Pushing to production\n\nEpisode deployments typically happen at 6AM PT on the publish date. Please\npost in #a-web-podcast-page (private channel) on Slack.\n\nIt is responsibility of the person who codes the episode to find someone to push\nif 6AM PT is too early for them. All members of MozMEAO are happy to do this.\n\n1. Verify all is good on [the staging site](https://stage.awebpodcast.org/)\n  - If the episode was merged before the publish date it will not be visible on\n  staging. You can see it on staging by either re-running the Jenkins job (if\n  you have permission) or by merging a new pull request. If there are no\n  reviewed pull requests that are ready you can submit one that changes the\n  publish date to 1 minute earlier.\n2. Make sure your local `master` branch is up to date\n3. Push the `master` branch to the `prod` branch: `git push origin master:prod`\n\nA notice will be posted in `#irlpodcast-notify` on Slack when the push has completed.\n\n### Kudos\n\n- http://danbahrami.io/articles/building-a-production-website-with-hugo-and-gulp-js/\n- https://regisphilibert.com/blog/2018/02/hugo-the-scope-the-context-and-the-dot/\n"
},
{
  "name": "version-control-tools",
  "files": {
    "/": [
      ".arcconfig",
      ".dockerignore",
      ".editorconfig",
      ".hgignore",
      ".sops.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "ansible",
      "create-deploy-environment",
      "create-environment",
      "d0cker",
      "deploy",
      "deploy-requirements.in",
      "deploy-requirements.txt",
      "docker-compose.yml",
      "docs-requirements.in",
      "docs-requirements.txt",
      "docs",
      "github-webhooks",
      "hgext",
      "hghooks",
      "hgmo",
      "hgserver",
      "hgtemplates",
      "hgwsgi",
      "pulse",
      "pylib",
      "run",
      "run-tests",
      "scripts",
      "terraform",
      "test-requirements-3.in",
      "test-requirements-3.txt",
      "test-requirements.in",
      "test-requirements.txt",
      "testing",
      "third_party",
      "treestatus",
      "upgrade-python-packages"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "devguide",
      "githubwebhooks.rst",
      "headless-repositories.rst",
      "hgmo",
      "hgmods",
      "hgmozilla",
      "hosting-service.png",
      "index.rst",
      "mozautomation",
      "mozhg",
      "settings_local.py",
      "vcttesting"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Version Control Tools\n\nThis repository contains tools, extensions, hooks, etc to support version\ncontrol at Mozilla.\n\nMost documentation exists in the ``docs/`` directory. It can be\n[viewed online](https://mozilla-version-control-tools.readthedocs.io/en/latest/)\non Read the Docs.\n\n## Contributing\n\n- All contributors must abide by the Mozilla Code of Conduct.\n\n- The canonical Mercurial repository is https://hg.mozilla.org/hgcustom/version-control-tools/.\n  The GitHub copy at https://github.com/mozilla/version-control-tools is a mirror and is\n  not used for development.\n\n- Patches are taken through [Mozilla Phabricator](https://phabricator.services.mozilla.com/).\n  For new contributors wanting to quickly submit changes, [`moz-phab`](https://github.com/mozilla-conduit/review) is recommended.\n\n- Bugs are tracked [on Bugzilla](https://bugzilla.mozilla.org), under the components prefixed by `Developer Services :: Mercurial:`.\n\n- If you are interested in getting in touch with the people who maintain\n  this repository, join the ``vcs`` channel on ``chat.mozilla.org``.\n\n## Testing\n\nTo create a test environment and run tests, you should use the `./run` script\nat the root of the repository. This script wraps common `docker-compose`\ncommands to provide better ergonomics and good testing defaults.\n\nThe test runner and all it's dependencies are contained in a Docker image.\nThe state of your version-control-tools checkout is mounted into the container\nbuilt from that image and tests are run inside the contianer using the source\non your host's filesystem. To do this, a `.env` file must be created to tell\nthe container about which user on the host system is running the tests.\n\nYou can run the following command to create a `.env` file:\n\n```shell\n  ./run env > .env\n```\n\nNow you can run the tests via:\n\n```shell\n  ./run tests path/to/test --with-hg=5.3.2\n```\n\n### Ansible-to-Docker Cluster\n\nThe configuration of the production server is managed by the Ansible configs\nin `ansible/`. To test these configs and much of the code that depends on them,\nwe have a custom Docker image build process which applies the Ansible roles to\nDocker images and creates a mock cluster of Docker containers that is mostly\nidentical to the production hosts of `hg.mozilla.org`.\n\nRunning these tests is slow and uses a lot of CPU resources. It is recommended\nto use `-j` to limit the number of concurrently running tests. If you don't\nwant to run the Ansible-to-Docker cluster tests, you can use the `--no-docker`\nflag when running the tests.\n\n"
},
{
  "name": "messaging-system-inflight-assets",
  "files": {
    "/": [
      ".circleci",
      ".yamllint",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "archive",
      "cfr-fxa.json",
      "cfr-fxa.yaml",
      "cfr-heartbeat.json",
      "cfr-heartbeat.yaml",
      "cfr.json",
      "cfr.yaml",
      "message-groups.json",
      "message-groups.yaml",
      "messaging-experiments.json",
      "messaging-experiments.yaml",
      "moments.json",
      "moments.yaml",
      "requirements.txt",
      "schema",
      "scripts",
      "whats-new-panel.json",
      "whats-new-panel.yaml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "PYTHON = python3\nFLAGS = -c\nCMD = 'import sys, yaml, json; json.dump(yaml.load(sys.stdin, Loader=yaml.Loader), sys.stdout, indent=2)'\n\nall: cfr.json cfr-fxa.json cfr-heartbeat.json whats-new-panel.json \\\n\tmessaging-experiments.json message-groups.json moments.json\n\n%.json:%.yaml pre-build\n\t$(PYTHON) $(FLAGS) $(CMD) < $< > $@\n\n.PHONY: clean check\n\npre-build:\n\tyamllint .\n\nclean:\n\trm *.json\n\ncheck:\n\tscripts/validate.py cfr cfr.json\n\tscripts/validate.py moments-page moments.json\n\tscripts/validate.py cfr-fxa cfr-fxa.json\n\tscripts/validate.py whats-new-panel whats-new-panel.json\n\tscripts/validate.py messaging-experiments messaging-experiments.json\n\tscripts/validate.py message-groups message-groups.json\n",
  "readme": "[![Mozilla](https://circleci.com/gh/mozilla/messaging-system-inflight-assets.svg?style=svg)](https://circleci.com/gh/mozilla/messaging-system-inflight-assets)\n\nThis repo hosts various in-flight remote assets of Firefox Messaging System.\n\nCurrently, it consists of CFR, CFR-FXA, and What's New Pannel.\n\n### Usage\n\nTo add/modify/delete assets, please edit the YAML files other than the JSON ones, because the former allows us to use comments in the document. Once you complete the editing, you can sync your changes to the JSON file(s), and copy them over to Remote Settings for publishing.\n\nWhen deleting asset(s), make sure copy the deleted content in the YAML files to the corresponding archive file located in the `archive` directory. For instance, when you're deleting messages in `cfr.yaml`, please copy the deletions to the `archive/cfr-archived.yaml`.\n\nTo sync from YAML to JSON, just run\n\n```sh\n$ make\n```\n\nTo validate the JSON against the schema\n\n```sh\n$ make check\n```\n\nIt requires Python 3 and various libraries for the schema validation and file generation.\n\n```sh\n# if you don't have Python 3 installed\n$ brew install python3\n$ pip3 install -r requirements.txt\n```\n\nNote: make sure you commit all the changes (YAML&JSON) to the repo.\n"
},
{
  "name": "bleach",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGES",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "CONTRIBUTORS",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "SECURITY.md",
      "bleach",
      "docs",
      "scripts",
      "setup.cfg",
      "setup.py",
      "tests",
      "tests_website",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "changes.rst",
      "clean.rst",
      "conf.py",
      "dev.rst",
      "goals.rst",
      "index.rst",
      "linkify.rst",
      "migrating.rst",
      "requirements.txt"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "======\nBleach\n======\n\n.. image:: https://github.com/mozilla/bleach/workflows/Test/badge.svg\n   :target: https://github.com/mozilla/bleach/actions?query=workflow%3ATest\n\n.. image:: https://github.com/mozilla/bleach/workflows/Lint/badge.svg\n   :target: https://github.com/mozilla/bleach/actions?query=workflow%3ALint\n\n.. image:: https://badge.fury.io/py/bleach.svg\n   :target: http://badge.fury.io/py/bleach\n\nBleach is an allowed-list-based HTML sanitizing library that escapes or strips\nmarkup and attributes.\n\nBleach can also linkify text safely, applying filters that Django's ``urlize``\nfilter cannot, and optionally setting ``rel`` attributes, even on links already\nin the text.\n\nBleach is intended for sanitizing text from *untrusted* sources. If you find\nyourself jumping through hoops to allow your site administrators to do lots of\nthings, you're probably outside the use cases. Either trust those users, or\ndon't.\n\nBecause it relies on html5lib_, Bleach is as good as modern browsers at dealing\nwith weird, quirky HTML fragments. And *any* of Bleach's methods will fix\nunbalanced or mis-nested tags.\n\nThe version on GitHub_ is the most up-to-date and contains the latest bug\nfixes. You can find full documentation on `ReadTheDocs`_.\n\n:Code:           https://github.com/mozilla/bleach\n:Documentation:  https://bleach.readthedocs.io/\n:Issue tracker:  https://github.com/mozilla/bleach/issues\n:License:        Apache License v2; see LICENSE file\n\n\nReporting Bugs\n==============\n\nFor regular bugs, please report them `in our issue tracker\n<https://github.com/mozilla/bleach/issues>`_.\n\nIf you believe that you've found a security vulnerability, please `file a secure\nbug report in our bug tracker\n<https://bugzilla.mozilla.org/enter_bug.cgi?assigned_to=nobody%40mozilla.org&product=Webtools&component=Bleach-security&groups=webtools-security>`_\nor send an email to *security AT mozilla DOT org*.\n\nFor more information on security-related bug disclosure and the PGP key to use\nfor sending encrypted mail or to verify responses received from that address,\nplease read our wiki page at\n`<https://www.mozilla.org/en-US/security/#For_Developers>`_.\n\n\nSecurity\n========\n\nBleach is a security-focused library.\n\nWe have a responsible security vulnerability reporting process. Please use\nthat if you're reporting a security issue.\n\nSecurity issues are fixed in private. After we land such a fix, we'll do a\nrelease.\n\nFor every release, we mark security issues we've fixed in the ``CHANGES`` in\nthe **Security issues** section. We include any relevant CVE links.\n\n\nInstalling Bleach\n=================\n\nBleach is available on PyPI_, so you can install it with ``pip``::\n\n    $ pip install bleach\n\n\nUpgrading Bleach\n================\n\n.. warning::\n\n   Before doing any upgrades, read through `Bleach Changes\n   <https://bleach.readthedocs.io/en/latest/changes.html>`_ for backwards\n   incompatible changes, newer versions, etc.\n\n   Bleach follows `semver 2`_ versioning. Vendored libraries will not\n   be changed in patch releases.\n\n\nBasic use\n=========\n\nThe simplest way to use Bleach is:\n\n.. code-block:: python\n\n    >>> import bleach\n\n    >>> bleach.clean('an <script>evil()</script> example')\n    u'an &lt;script&gt;evil()&lt;/script&gt; example'\n\n    >>> bleach.linkify('an http://example.com url')\n    u'an <a href=\"http://example.com\" rel=\"nofollow\">http://example.com</a> url'\n\n\nCode of Conduct\n===============\n\nThis project and repository is governed by Mozilla's code of conduct and\netiquette guidelines. For more details please see the `CODE_OF_CONDUCT.md\n</CODE_OF_CONDUCT.md>`_\n\n\n.. _html5lib: https://github.com/html5lib/html5lib-python\n.. _GitHub: https://github.com/mozilla/bleach\n.. _ReadTheDocs: https://bleach.readthedocs.io/\n.. _PyPI: https://pypi.org/project/bleach/\n.. _semver 2: https://semver.org/\n"
},
{
  "name": "concepts",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      ".nsprc",
      ".prettierrc",
      ".utils",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "docs",
      "gatsby-config.js",
      "gatsby-node.js",
      "package-lock.json",
      "package.json",
      "src",
      "static"
    ],
    "/docs": [
      "deployments.md",
      "metrics.md",
      "system-flow.png"
    ],
    "/.github": [
      "CODEOWNERS"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "\n# Firefox Concepts\n\nFx Concepts is a simple tool for building and testing value propositons for hypothetical future Firefox products and features.\n\n## Working With the Fx Concepts\n\nThis document explains how to create and update concept pages.\n\n### Prerequisites\n\nIn order to add concepts to the Fx Concepts site, you'll first need to make sure you have the correct dependencies seet up on your computer:\n\n1. Make sure you have a GitHub account and have Git running and configured on your computer. [docs](https://help.github.com/articles/set-up-git/).\n1. A text editor like [VSCode](https://code.visualstudio.com/).\n1. Modest familiarity with the Terminal app on your computer.\n1. The latest Version of Node. [Install link](https://nodejs.org/en/download/current/)\n1. The Gatsby command line interface. To install this, open your Terminal and paste `npm install --global gatsby-cli`.  Make sure you've installed Node (step 4) before you do this.\n\n### Set Up Your Environment\n\nSkip this section if you've already cloned the Fx Concepts repository.\n\n1.  Probably a good idea to fork the Fx Concepts repo. You can do this from GitHub.\n1. `cd` to wherever you want your local build of Fx Concepts\n1. Clone your fork: `git clone https://github.com/{YOUR-FORK}/concepts.git`\n1. `cd` into your local Fx Concepts folder\n1. `npm i`\n\n### Developing\n\n`gatsby develop` to run locally at `localhost:8000`. You can test GraphQL queries at `localhost:8000/___graphql`\n\n### Deoployments\n\nCan be found [here](docs/deployments.md).\n\n### Adding Content\n\nProduct concepts can be found in the `src/concepts` directory. Each concept has the following basic structure:\n\n```\nsrc/concepts/{concept-root-slug}\n\u251c\u2500\u2500 images\n\u2502   \u251c\u2500\u2500 image-1.png\n\u2502   \u251c\u2500\u2500 image-2.png\n\u2502   \u2514\u2500\u2500 image-{...n}.png\n\u251c\u2500\u2500 index.md\n\u251c\u2500\u2500 v1.md\n\u2514\u2500\u2500 v{...n}.md\n```\n\nThis directory structure is important to set up your experiment:\n* The root folder `{concept-root-slug}` will be the top level path for your experiment.\n* `index.md` becomes the control branch\n* You can add variants by adding more markdown files. This will automatically create new pages so that, for example `v1.md` will become `localhost:8000/{concept-root-slug}/v1`\n* any images in the image directory will be accessible within the context of your project.\n\n#### copy-me\n\nThe `copy-me` folder in `src/concepts` should be used to bootstrap your concept. Gatsby ignores this directory at compile time so it doesn't produce any paths or content in, but if you copy it and change the name, Gaysby will pick up your new directory and generate a concept for you.\n\n### Overview of concept fields\n\nConcept variants are built on Markdown frontmatter which is really just YAML. Gatsby and YAML are both sticklers for syntactic completeness and accuracy. Below is an annotated version of a concept page:\n\n```\n\n---\nmetaName: \"Copy Me\" // A human-readable name for your project. This should be the same across all variants.\nmetaCleanName: \"copyme\" // A URL-friendly name for your project used as a query parameter in your survey. This should be the same across all variants.\nmetaVariant: \"control\" // A URL-friendly variant name. This should be different for each treatment in your test.\nmetaPrimaryLink: \"valid url\" // A surveyGizmo URL or other\nmetaSecondaryLink: \"valid url\" // Link to a URL explaining this project, probably https://blog.mozilla.org/futurereleases/2019/02/25/exploring-alternative-funding-models-for-the-web/\nmetaDate: \"creation date\" // A human readable datestamp\nconcept: // The concept is what actually gets rendered onto the page\n  -\n    cobrand: \"Partner Name\" // leave this blank if no partner\n    cobrandIcon: \"Partner Icon\" // note there need needs to be a vilid icon pat here...nothing will show up if partner name is left blank\n    hero: // The hero is the big top thing on your concept page.\n      -\n        title: \"Title\"\n        text: \"Description\"\n        cta: \"cta\" // Button copy\n        image: \"./images/default-hero.png\" // make this image a 1168\u200a\u00d7\u200a777 png\n    facets: // facets pieces of the value propisiton. you can have as many of these as you like.\n      -\n        title: \"Facet Title\"\n        text: \"Make as many facets as you like!\"\n        image: \"./images/default-facet.png\" // make this image a 692\u200a\u00d7\u200a692 png\n      -\n        title: \"Facet Title\"\n        text: \"Make as many facets as you like!\"\n        image: \"./images/default-facet.png\"\n    callout:\n      -\n        title: \"Bottom Callout Title\"\n        text: \"This is the thing at the bottom\"\n        cta: \"secondary cta\"\n---\n\n\n"
},
{
  "name": "passwordmgr-remote-settings-updater",
  "files": {
    "/": [
      ".circleci",
      ".env.sample",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "app-constants.js",
      "package-lock.json",
      "package.json",
      "update-script.js",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# passwordmgr-remote-settings-updater\n\nScript that adds new related websites to the \"websites-with-shared-credential-backends\" Remote Setting collection via [Apple's open sourced password manager resources](https://github.com/apple/password-manager-resources/blob/e0d5ba899c57482b06776a18c56b1ad714efd928/quirks/websites-with-shared-credential-backends.json) and adds new password rules to the \"password-rules\" Remote Setting collection via [Apple's open sourced password manager rules](https://github.com/apple/password-manager-resources/blob/main/quirks/password-rules.json).\n\n## Usage\n\nThe script will _not run_ without the following environment variables set in `.env`: \n- `FX_REMOTE_SETTINGS_WRITER_USER`\n- `FX_REMOTE_SETTINGS_WRITER_PASS`\n- `FX_REMOTE_SETTINGS_WRITER_SERVER`\n- `NODE_ENV`\n\nTo run this script:\n\n`$ node update-script.js`\n\n**Note**: Corporate VPN access is required to access the Remote Settings servers\n\nThe script will exit with a `0` for success and a `1` if there were any errors.\n"
},
{
  "name": "app-store-analytics-export",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".eslintrc.js",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.json",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "analytics_export",
      "package.json",
      "test",
      "yarn.lock"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# App Store Analytics Export\n\nScripts for exporting app analytics to BigQuery.\nThe export will export individual metrics grouped by a single dimension and load each\nmetric and dimension combination into a single table.\nMetrics and dimensions that will be exported can be found in the \n[metric metadata file](analytics_export/tableMetadata.js).\n\nA username and password for App Store Connect is required to use the script. \n\n### Adding Metrics To Export\n\nTo add new metrics to export, add an entry to `metricData` in\n[`analytics_export/tableMetadata.js`](analytics_export/tableMetadata.js).\nThe dimensions for the metrics will be automatically detected.\n\nThis job exports a table for each metric and dimension combination.\nThis [script in bigquery-etl](https://github.com/mozilla/bigquery-etl/blob/master/script/marketing/generate_app_store_queries.py)\ngenerates sql that combines all the metrics by dimension, creating one table per dimension.\nThe script checks for existing tables in BigQuery so it must be run after the new metrics have been exported at least once.\n\nOfficial documentation of metrics and dimensions can be found at \nhttps://help.apple.com/app-store-connect/#/itc21781223f\n\n## Usage\n\nThis project uses `yarn` to manage dependencies and was developed with nodejs 12.\n\nTo install dependencies use:\n```sh\nyarn\n```\n\nTo run the export use `yarn export`.  Use `yarn export --help` to view supported options.\n\ne.g.\n```sh\nyarn export \\\n    --username=u \\\n    --password=p \\\n    --app-id=123 \\\n    --app-name=app \\\n    --start-date=2020-01-01 \\\n    --project=test-project \\\n    --dataset=test-dataset\n```\n\nESLint and Prettier are used for code formatting and linting.\nLint check can be run with:\n```sh\nyarn lint\n```\nAnd automatic fixing can be done with:\n```sh\nyarn lint-fix && yarn pretty\n```\n\nTests are run with mocha:\n```sh\nyarn test\n```"
},
{
  "name": "addons-issue-counts",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "README.md",
      "bin",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "screenshots"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# addons-issue-counts\n\nScripts for pushing add-ons specific github issue counts into influxdb. Uses github's GraphQL API and node-influx to push stats into influxdb easy graphing.\n\n## Want to use this for your own project?\n\n* Copy this and create your own repo. You probably don't want to fork this since you won't need to track the changes.\n* Update the query and script in `bin/getGithubIssueCounts` to suit your needs.\n* Be sure to update your event naming - see the prefix in `measurement`.\n* Add the following tokens to a .env file (to test locally) or circle-ci if you are wanting to use Circle-ci's cron to push the stats into datadog (recommended).\n    * Add `GH_TOKEN` you should set this to `public_access` (no scope) since this is most likely public data.\n    * Add `INFLUX_DB_HOST`\n    * Add `INFLUX_DB_PORT`\n    * Add `INFLUX_DB_DATABASE`\n    * Add `INFLUX_DB_USER`\n    * Add `INFLUX_DB_PASSWORD`\n* Check you're receiving data for your events in influx/grafana.\n* When you're happy run this from Circl-ci and find interesting ways to to graph the data.\n* If you make something cool with this please let me know!\n"
},
{
  "name": "telemetry-batch-view",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".jvmopts",
      "CODE_OF_CONDUCT.md",
      "GRAVEYARD.md",
      "README.md",
      "build.sbt",
      "docs",
      "project",
      "run-sbt.sh",
      "scripts",
      "src"
    ],
    "/docs": [
      "Addons.md",
      "CrashSummary.md",
      "Events.md",
      "Longitudinal.md",
      "MainSummary.md",
      "README.md",
      "SyncSummary.md",
      "addons_schema.md",
      "crash_summary_schema.md",
      "events_schema.md",
      "longitudinal_examples.md",
      "main_summary_schema.md",
      "sync_summary_schema.md"
    ],
    "/.circleci": [
      "config.yml",
      "deploy.sh"
    ]
  },
  "makefile": null,
  "readme": "# telemetry-batch-view\n\nThis is a Scala application to build derived datasets, also known as [batch views](http://robertovitillo.com/2016/01/06/batch-views/), of [Telemetry](https://wiki.mozilla.org/Telemetry) data.\n\n[![Build Status](https://travis-ci.org/mozilla/telemetry-batch-view.svg?branch=main)](https://travis-ci.org/mozilla/telemetry-batch-view)\n[![codecov.io](https://codecov.io/github/mozilla/telemetry-batch-view/coverage.svg?branch=main)](https://codecov.io/github/mozilla/telemetry-batch-view?branch=main)\n[![CircleCi Status](https://circleci.com/gh/mozilla/telemetry-batch-view.svg?style=shield&circle-token=ca31167ac42cc39f898e37facb93db70c0af8691)](https://circleci.com/gh/mozilla/telemetry-batch-view)\n\nRaw JSON [pings](https://ci.mozilla.org/job/mozilla-central-docs/Tree_Documentation/toolkit/components/telemetry/telemetry/pings.html) are stored on S3 within files containing [framed Heka records](https://hekad.readthedocs.org/en/latest/message/index.html#stream-framing). Reading the raw data in through e.g. Spark can be slow as for a given analysis only a few fields are typically used; not to mention the cost of parsing the JSON blobs. Furthermore, Heka files might contain only a handful of records under certain circumstances.\n\nDefining a derived [Parquet](https://parquet.apache.org/) dataset, which uses a columnar layout optimized for analytics workloads, can drastically improve the performance of analysis jobs while reducing the space requirements. A derived dataset might, and should, also perform heavy duty operations common to all analysis that are going to read from that dataset (e.g., parsing dates into normalized timestamps).\n\n### Adding a new derived dataset\n\nSee the [views](https://github.com/mozilla/telemetry-batch-view/tree/main/src/main/scala/com/mozilla/telemetry/views) folder for examples of jobs that create derived datasets.\n\nSee the [Firefox Data Documentation](https://mozilla.github.io/firefox-data-docs/datasets/reference.html) for more information about the individual derived datasets.\nFor help finding the right dataset for your analysis, see\n[Choosing a Dataset](https://mozilla.github.io/firefox-data-docs/concepts/choosing_a_dataset.html).\n\n## Development and Deployment\n\nThe general workflow for telemetry-batch-view is:\n1. Make some local changes on your branch\n2. Test locally in [Airflow](https://www.github.com/mozilla/telemetry-airflow), testing just the jobs that your code change touches.\n3. Open PR, tag someone to review. Merge when approved, which will deploy the jar to production.\n\nNote that Airflow deployments depend on cluster bootstrap scripts governed by \n[emr-bootstrap-spark](https://github.com/mozilla/emr-bootstrap-spark/).\nChanges in job behavior that don't seem to correspond to changes in this \nrepository's code could be related to changes in those other projects.\n\n### Local Development\n\nThere are two possible workflows for hacking on telemetry-batch-view: you can either create a docker container for building the package and running tests, or import the project into IntelliJ's IDEA.\n\nTo run sbt tests inside Docker, run:\n\n    # This will take 30+ minutes to run.\n    ./run-sbt.sh test\n\nFor more efficient iteration, just invoke `./run-sbt.sh` without arguments to open up a shell and then test only the class you're working on without invoking sbt startup time on each iteration:\n\n    sbt> testOnly *AddonsViewTest\n\nYou may need to increase the amount of memory allocated to Docker for this to work, as some of the tests are very memory hungry at present. At least 4 gigabytes is recommended.\n\nIf you wish to import the project into IntelliJ IDEA, apply the following changes to `Preferences` -> `Languages & Frameworks` -> `Scala Compile Server`:\n\n- JVM maximum heap size, MB: `2048`\n- JVM parameters: `-server -Xmx2G -Xss4M`\n\nNote that the first time the project is opened it takes some time to download all the dependencies.\n\n### Scala style checker\n[Scalastyle](http://www.scalastyle.org/) is used on the CI for enforcing style rules. In order to run it locally, use:\n```bash\nsbt scalastyle test:scalastyle\n```\n\n### Generating Datasets\n\nSee the [documentation for specific views](https://github.com/mozilla/telemetry-batch-view/tree/main/docs) for details about running/generating them.\n\nFor example, to create a longitudinal view locally:\n```bash\nsbt \"runMain com.mozilla.telemetry.views.LongitudinalView --from 20160101 --to 20160701 --bucket telemetry-test-bucket\"\n```\n\nFor distributed execution we pack all of the classes together into a single JAR and submit it to the cluster:\n```bash\nsbt assembly\nspark-submit --master yarn --deploy-mode client --class com.mozilla.telemetry.views.LongitudinalView target/scala-2.11/telemetry-batch-view-*.jar --from 20160101 --to 20160701 --bucket telemetry-test-bucket\n```\n\n### Caveats\nIf you run into memory issues during compilation time or running the test suite, issue the following command before running sbt:\n```bash\nexport _JAVA_OPTIONS=\"-Xms4G -Xmx4G -Xss4M -XX:MaxMetaspaceSize=256M\"\n```\n\n**Running on Windows**\n\nExecuting scala/Spark jobs could be particularly problematic on this platform. Here's a list of common issues and the relative solutions:\n\n**Issue:** *I see a weird reflection error or an odd exception when trying to run my code.*\n\nThis is probably due to *winutils* being missing or not found. Winutils are needed by HADOOP and can be downloaded from [here](https://github.com/steveloughran/winutils).\n\n**Issue:** *java.net.URISyntaxException: Relative path in absolute URI: ...*\n\nThis means that *winutils* cannot be found or that Spark cannot find a valid warehouse directory. Add the following line at the beginning of your entry function to make it work:\n\n```scala\nSystem.setProperty(\"hadoop.home.dir\", \"C:\\\\path\\\\to\\\\winutils\")\nSystem.setProperty(\"spark.sql.warehouse.dir\", \"file:///C:/somereal-dir/spark-warehouse\")\n```\n\n**Issue:** *The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: ---------*\n\nSee [SPARK-10528](https://issues.apache.org/jira/browse/SPARK-10528). Run \"winutils chmod 777 /tmp/hive\" from a privileged prompt to make it work.\n\nAny commits to main should also trigger a circleci build that will do the sbt publishing for you to our local maven repo in s3.\n"
},
{
  "name": "data-docs",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".linkcheck.json",
      ".spelling",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "book.toml",
      "dtmo.css",
      "mermaid-init.js",
      "mermaid.css",
      "mermaid.min.js",
      "package-lock.json",
      "package.json",
      "scripts",
      "src"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Data Documentation\n\n[![Build Status](https://github.com/mozilla/data-docs/workflows/Build/badge.svg)](https://github.com/mozilla/data-docs/actions?query=workflow%3ABuild)\n\nThis documentation was written to help Mozillians analyze and interpret data collected by our products, such as [Firefox](https://www.mozilla.org/firefox) and [Mozilla VPN](https://www.mozilla.org/products/vpn/).\n\nAt [Mozilla](https://www.mozilla.org), our data-gathering and data-handling\npractices are anchored in our\n[Data Privacy Principles](https://www.mozilla.org/en-US/privacy/principles/)\nand elaborated in the [Mozilla Privacy Policy](https://www.mozilla.org/en-US/privacy/).\n\nTo learn more about what data Firefox collects and the choices you can make\nas a user, please see the [Firefox Privacy Notice](https://www.mozilla.org/en-US/privacy/firefox/).\n\nThe rendered documentation is hosted at [https://docs.telemetry.mozilla.org/](https://docs.telemetry.mozilla.org/).\n\nIssues for this documentation are [tracked in Bugzilla][docszilla] ([file a bug]).\n\n[docszilla]: https://bugzilla.mozilla.org/buglist.cgi?product=Data%20Platform%20and%20Tools&component=Documentation%20and%20Knowledge%20Repo%20%28RTMO%29&resolution=---\n[file a bug]: https://bugzilla.mozilla.org/enter_bug.cgi?component=Documentation%20and%20Knowledge%20Repo%20(RTMO)&product=Data%20Platform%20and%20Tools\n\n## Building the Documentation\n\nThe documentation is rendered with [mdBook](https://github.com/rust-lang/mdBook).\nWe use a fork named [mdbook-dtmo](https://github.com/badboy/mdbook-dtmo) that includes a number of custom additions to mdbook for our environment\n(for example, a plugin to automatically generate a table-of-contents).\n\nYou can download `mdbook-dtmo` on the [GitHub releases page](https://github.com/badboy/mdbook-dtmo/releases).\nPlease use the latest version.\nUnpack it and place the binary in a directory of your `$PATH`.\n\nIf you have [rustc](https://www.rust-lang.org/) already installed, you can install a pre-compiled binary directly:\n\n```bash\ncurl -LSfs https://japaric.github.io/trust/install.sh | sh -s -- --git badboy/mdbook-dtmo\n```\n\nMake sure this directory is in your `$PATH` or copy it to a directory of your `$PATH`.\n\nYou can also build and install the preprocessors:\n\n```bash\ncargo install mdbook-dtmo\n```\n\nYou can then serve the documentation locally with:\n\n```\nmdbook-dtmo serve\n```\n\nThe complete documentation for the mdBook toolchain is available online at <https://rust-lang.github.io/mdBook/>.\nIf you run into any problems, please [let us know](https://docs.telemetry.mozilla.org/concepts/getting_help.html). We are happy to change the tooling to make it as much fun as possible to write.\n\n### Spell checking\n\nArticles should use proper spelling, and pull requests will be automatically checked for spelling\nerrors.\n\nTechnical articles often contain words that are not recognized by common dictionaries, if this\nhappens you may either put specialized terms in `code blocks`, or you may add an exception to\nthe `.spelling` file in the code repository.\n\nFor things like dataset names or field names, `code blocks` should be preferred. Things like\nproject names or common technical terms should be added to the `.spelling` file.\n\nThe [markdown-spell-check](https://www.npmjs.com/package/markdown-spellcheck) package checks spelling as part of the build process. To run it locally, install [node.js](https://nodejs.org/en/) (if not already installed) and run `npm install` at the root of the repository. Then run the `scripts/spell_check.sh` script.\n\nYou may also remove the `--report` parameter to begin an interactive fixing session. In this\ncase, it is highly recommended to also add the `--no-suggestions` parameter, which greatly\nspeeds things up.\n\n### Link checking\n\nAny web links should be valid. A dead link might not be your fault, but you will earn a lot of good karma by fixing a dead link!\n\nThe [markdown-link-check](https://www.npmjs.com/package/markdown-link-check) package checks links as part of the build process. Note that dead links do not fail the build: links often go dead for all sorts of reasons, and making it a required check constantly caused otherwise-fine pull requests to appear broken. Still, you should check the status of this check yourself when submitting a pull request: you can do this by looking at the Travis CI status after submitting it.\n\nTo run link checking locally, run the installation steps [described for spell checking](#spell-checking) if you haven't already, then run the `scripts/link_check.sh` script.\n\n### Markdown formatting\n\nWe use [prettier](https://prettier.io) to ensure a consistent formatting style in our markdown.\nTo reduce friction, this is not a required check but running it on the files\nyou're modifying before submission is highly appreciated!\nMost editors can be configured to run prettier automatically,\nsee for example the\n[Prettier Plugin for VSCode](https://marketplace.visualstudio.com/items?itemName=esbenp.prettier-vscode).\n\nTo run prettier locally on the entire repository, run the installation steps\n[described for spell checking](#spell-checking) if you haven't already, then\nrun the `scripts/prettier_fix.sh` script.\n\n## Contributing\n\nSee [contributing](https://docs.telemetry.mozilla.org/contributing/index.html) for detailed information on making changes to the documentation.\n"
},
{
  "name": "mozjpeg",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      "BUILDING.md",
      "BUILDING.txt",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "ChangeLog.md",
      "LICENSE.md",
      "README-mozilla.txt",
      "README-turbo.txt",
      "README.ijg",
      "README.md",
      "appveyor.yml",
      "cderror.h",
      "cdjpeg.c",
      "cdjpeg.h",
      "change.log",
      "cjpeg.1",
      "cjpeg.c",
      "cmakescripts",
      "cmyk.h",
      "coderules.txt",
      "croptest.in",
      "djpeg.1",
      "djpeg.c",
      "doc",
      "doxygen-extra.css",
      "doxygen.config",
      "example.txt",
      "fuzz",
      "jaricom.c",
      "java",
      "jcapimin.c",
      "jcapistd.c",
      "jcarith.c",
      "jccoefct.c",
      "jccolext.c",
      "jccolor.c",
      "jcdctmgr.c",
      "jcext.c",
      "jchuff.c",
      "jchuff.h",
      "jcicc.c",
      "jcinit.c",
      "jcmainct.c",
      "jcmarker.c",
      "jcmaster.c",
      "jcmaster.h",
      "jcomapi.c",
      "jconfig.h.in",
      "jconfig.txt",
      "jconfigint.h.in",
      "jcparam.c",
      "jcphuff.c",
      "jcprepct.c",
      "jcsample.c",
      "jcstest.c",
      "jctrans.c",
      "jdapimin.c",
      "jdapistd.c",
      "jdarith.c",
      "jdatadst-tj.c",
      "jdatadst.c",
      "jdatasrc-tj.c",
      "jdatasrc.c",
      "jdcoefct.c",
      "jdcoefct.h",
      "jdcol565.c",
      "jdcolext.c",
      "jdcolor.c",
      "jdct.h",
      "jddctmgr.c",
      "jdhuff.c",
      "jdhuff.h",
      "jdicc.c",
      "jdinput.c",
      "jdmainct.c",
      "jdmainct.h",
      "jdmarker.c",
      "jdmaster.c",
      "jdmaster.h",
      "jdmerge.c",
      "jdmerge.h",
      "jdmrg565.c",
      "jdmrgext.c",
      "jdphuff.c",
      "jdpostct.c",
      "jdsample.c",
      "jdsample.h",
      "jdtrans.c",
      "jerror.c",
      "jerror.h",
      "jfdctflt.c",
      "jfdctfst.c",
      "jfdctint.c",
      "jidctflt.c",
      "jidctfst.c",
      "jidctint.c",
      "jidctred.c",
      "jinclude.h",
      "jmemmgr.c",
      "jmemnobs.c",
      "jmemsys.h",
      "jmorecfg.h",
      "jpeg_nbits_table.h",
      "jpegcomp.h",
      "jpegint.h",
      "jpeglib.h",
      "jpegtran.1",
      "jpegtran.c",
      "jpegyuv.c",
      "jquant1.c",
      "jquant2.c",
      "jsimd.h",
      "jsimd_none.c",
      "jsimddct.h",
      "jstdhuff.c",
      "jutils.c",
      "jversion.h.in",
      "libjpeg.map.in",
      "libjpeg.txt",
      "md5",
      "mozjpeg.podspec",
      "rd_average.sh",
      "rd_collect.sh",
      "rd_collect_sub.sh",
      "rd_plot.sh",
      "rdbmp.c",
      "rdcolmap.c",
      "rdgif.c",
      "rdjpeg.c",
      "rdjpgcom.1",
      "rdjpgcom.c",
      "rdpng.c",
      "rdppm.c",
      "rdswitch.c",
      "rdtarga.c",
      "release",
      "sharedlib",
      "simd",
      "strtest.c",
      "structure.txt",
      "testimages",
      "tjbench.c",
      "tjbenchtest.in",
      "tjbenchtest.java.in",
      "tjexample.c",
      "tjexampletest.in",
      "tjexampletest.java.in",
      "tjunittest.c",
      "tjutil.c",
      "tjutil.h",
      "transupp.c",
      "transupp.h",
      "turbojpeg-jni.c",
      "turbojpeg-mapfile",
      "turbojpeg-mapfile.jni",
      "turbojpeg.c",
      "turbojpeg.h",
      "usage.txt",
      "win",
      "wizard.txt",
      "wrbmp.c",
      "wrgif.c",
      "wrjpgcom.1",
      "wrjpgcom.c",
      "wrppm.c",
      "wrtarga.c",
      "yuvjpeg.c"
    ]
  },
  "makefile": null,
  "readme": "Mozilla JPEG Encoder Project [![Build Status](https://ci.appveyor.com/api/projects/status/github/mozilla/mozjpeg?branch=master&svg=true)](https://ci.appveyor.com/project/kornel/mozjpeg-4ekrx)\n============================\n\nMozJPEG improves JPEG compression efficiency achieving higher visual quality and smaller file sizes at the same time. It is compatible with the JPEG standard, and the vast majority of the world's deployed JPEG decoders.\n\nMozJPEG is a patch for [libjpeg-turbo](https://github.com/libjpeg-turbo/libjpeg-turbo). **Please send pull requests to libjpeg-turbo** if the changes aren't specific to newly-added MozJPEG-only compression code. This project aims to keep differences with libjpeg-turbo minimal, so whenever possible, improvements and bug fixes should go there first.\n\nMozJPEG is compatible with the libjpeg API and ABI. It is intended to be a drop-in replacement for libjpeg. MozJPEG is a strict superset of libjpeg-turbo's functionality. All MozJPEG's improvements can be disabled at run time, and in that case it behaves exactly like libjpeg-turbo.\n\nMozJPEG is meant to be used as a library in graphics programs and image processing tools. We include a demo `cjpeg` command-line tool, but it's not intended for serious use. We encourage authors of graphics programs to use libjpeg's [C API](libjpeg.txt) and link with MozJPEG library instead.\n\n## Features\n\n* Progressive encoding with \"jpegrescan\" optimization. It can be applied to any JPEG file (with `jpegtran`) to losslessly reduce file size.\n* Trellis quantization. When converting other formats to JPEG it maximizes quality/filesize ratio.\n* Comes with new quantization table presets, e.g. tuned for high-resolution displays.\n* Fully compatible with all web browsers.\n* Can be seamlessly integrated into any program that uses the industry-standard libjpeg API. There's no need to write any MozJPEG-specific integration code.\n\n## Releases\n\n* [Latest release](https://github.com/mozilla/mozjpeg/releases/latest)\n* [Overview of 3.0 features](https://calendar.perfplanet.com/2014/mozjpeg-3-0/)\n* [Version 2.0 Announcement](https://blog.mozilla.org/research/2014/07/15/mozilla-advances-jpeg-encoding-with-mozjpeg-2-0/)\n* [Version 1.0 Announcement](https://blog.mozilla.org/research/2014/03/05/introducing-the-mozjpeg-project/)\n\n## Compiling\n\nSee [BUILDING](BUILDING.md). MozJPEG is built exactly the same way as libjpeg-turbo, so if you need additional help please consult [libjpeg-turbo documentation](https://libjpeg-turbo.org/).\n"
},
{
  "name": "angle",
  "files": {
    "/": [
      ".clang-format",
      ".gitattributes",
      ".gitignore",
      ".gn",
      ".style.yapf",
      ".vpython",
      ".vpython3",
      ".yapfignore",
      "AUTHORS",
      "BUILD.gn",
      "CONTRIBUTORS",
      "DEPS",
      "DIR_METADATA",
      "LICENSE",
      "OWNERS",
      "PRESUBMIT.py",
      "README.chromium",
      "README.md",
      "WATCHLISTS",
      "additional_readme_paths.json",
      "android",
      "build_overrides",
      "codereview.settings",
      "doc",
      "dotfile_settings.gni",
      "extensions",
      "gni",
      "include",
      "infra",
      "samples",
      "scripts",
      "src",
      "third_party",
      "tools",
      "util"
    ]
  },
  "makefile": null,
  "readme": "# ANGLE - Almost Native Graphics Layer Engine\n\nThe goal of ANGLE is to allow users of multiple operating systems to seamlessly run WebGL and other\nOpenGL ES content by translating OpenGL ES API calls to one of the hardware-supported APIs available\nfor that platform. ANGLE currently provides translation from OpenGL ES 2.0, 3.0 and 3.1 to Vulkan,\ndesktop OpenGL, OpenGL ES, Direct3D 9, and Direct3D 11. Future plans include ES 3.2, translation to\nMetal and MacOS, Chrome OS, and Fuchsia support.\n\n### Level of OpenGL ES support via backing renderers\n\n|                |  Direct3D 9   |  Direct3D 11     |   Desktop GL   |    GL ES      |    Vulkan     |    Metal      |\n|----------------|:-------------:|:----------------:|:--------------:|:-------------:|:-------------:|:-------------:|\n| OpenGL ES 2.0  |    complete   |    complete      |    complete    |    complete   |    complete   |    complete   |\n| OpenGL ES 3.0  |               |    complete      |    complete    |    complete   |    complete   |  in progress  |\n| OpenGL ES 3.1  |               | [incomplete](doc/ES31StatusOnD3D11.md) |    complete    |    complete   |    complete   |               |\n| OpenGL ES 3.2  |               |                  |  in progress   |  in progress  |  in progress  |               |\n\n### Platform support via backing renderers\n\n|              |    Direct3D 9  |   Direct3D 11  |   Desktop GL  |    GL ES    |   Vulkan    |    Metal    |\n|-------------:|:--------------:|:--------------:|:-------------:|:-----------:|:-----------:|:-----------:|\n| Windows      |    complete    |    complete    |   complete    |   complete  |   complete  |             |\n| Linux        |                |                |   complete    |             |   complete  |             |\n| Mac OS X     |                |                |   complete    |             |             | in progress |\n| iOS          |                |                |               |             |             |   planned   |\n| Chrome OS    |                |                |               |   complete  |   planned   |             |\n| Android      |                |                |               |   complete  |   complete  |             |\n| GGP (Stadia) |                |                |               |             |   complete  |             |\n| Fuchsia      |                |                |               |             |   complete  |             |\n\nANGLE v1.0.772 was certified compliant by passing the OpenGL ES 2.0.3 conformance tests in October 2011.\n\nANGLE has received the following certifications with the Vulkan backend:\n* OpenGL ES 2.0: ANGLE 2.1.0.d46e2fb1e341 (Nov, 2019)\n* OpenGL ES 3.0: ANGLE 2.1.0.f18ff947360d (Feb, 2020)\n* OpenGL ES 3.1: ANGLE 2.1.0.f5dace0f1e57 (Jul, 2020)\n\nANGLE also provides an implementation of the EGL 1.4 specification.\n\nANGLE is used as the default WebGL backend for both Google Chrome and Mozilla Firefox on Windows\nplatforms. Chrome uses ANGLE for all graphics rendering on Windows, including the accelerated\nCanvas2D implementation and the Native Client sandbox environment.\n\nPortions of the ANGLE shader compiler are used as a shader validator and translator by WebGL\nimplementations across multiple platforms. It is used on Mac OS X, Linux, and in mobile variants of\nthe browsers. Having one shader validator helps to ensure that a consistent set of GLSL ES shaders\nare accepted across browsers and platforms. The shader translator can be used to translate shaders\nto other shading languages, and to optionally apply shader modifications to work around bugs or\nquirks in the native graphics drivers. The translator targets Desktop GLSL, Vulkan GLSL, Direct3D\nHLSL, and even ESSL for native GLES2 platforms.\n\n## Sources\n\nANGLE repository is hosted by Chromium project and can be\n[browsed online](https://chromium.googlesource.com/angle/angle) or cloned with\n\n    git clone https://chromium.googlesource.com/angle/angle\n\n\n## Building\n\nView the [Dev setup instructions](doc/DevSetup.md).\n\n## Contributing\n\n* Join our [Google group](https://groups.google.com/group/angleproject) to keep up to date.\n* Join us on IRC in the #ANGLEproject channel on FreeNode.\n* Join us on [Slack](https://chromium.slack.com) in the #angle channel.\n* [File bugs](http://anglebug.com/new) in the [issue tracker](https://bugs.chromium.org/p/angleproject/issues/list) (preferably with an isolated test-case).\n* [Choose an ANGLE branch](doc/ChoosingANGLEBranch.md) to track in your own project.\n\n\n* Read ANGLE development [documentation](doc).\n* Look at [pending](https://chromium-review.googlesource.com/q/project:angle/angle+status:open)\n  and [merged](https://chromium-review.googlesource.com/q/project:angle/angle+status:merged) changes.\n* Become a [code contributor](doc/ContributingCode.md).\n* Use ANGLE's [coding standard](doc/CodingStandard.md).\n* Learn how to [build ANGLE for Chromium development](doc/BuildingAngleForChromiumDevelopment.md).\n* Get help on [debugging ANGLE](doc/DebuggingTips.md).\n* Go through [ANGLE's orientation](doc/Orientation.md) and sift through [starter projects](doc/Starter-Projects.md).\n\n\n* Read about WebGL on the [Khronos WebGL Wiki](http://khronos.org/webgl/wiki/Main_Page).\n* Learn about implementation details in the [OpenGL Insights chapter on ANGLE](http://www.seas.upenn.edu/~pcozzi/OpenGLInsights/OpenGLInsights-ANGLE.pdf) and this [ANGLE presentation](https://drive.google.com/file/d/0Bw29oYeC09QbbHoxNE5EUFh0RGs/view?usp=sharing).\n* Learn about the past, present, and future of the ANGLE implementation in [this presentation](https://docs.google.com/presentation/d/1CucIsdGVDmdTWRUbg68IxLE5jXwCb2y1E9YVhQo0thg/pub?start=false&loop=false).\n* Watch a [short presentation](https://youtu.be/QrIKdjmpmaA) on the Vulkan back-end.\n* Track the [dEQP test conformance](doc/dEQP-Charts.md)\n* Read design docs on the [Vulkan back-end](src/libANGLE/renderer/vulkan/README.md)\n* Read about ANGLE's [testing infrastructure](infra/README.md)\n* If you use ANGLE in your own project, we'd love to hear about it!\n"
},
{
  "name": "reps-archive",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc",
      ".github",
      ".gitignore",
      ".snyk",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "data",
      "dist",
      "gather.js",
      "index.js",
      "lib",
      "package-lock.json",
      "package.json",
      "templates",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "Mozilla Reps Archive\n=====\n\nThis repository holds all old reps.mozilla.org data before we migrated to the Community Portal. It serves an index.html with a list of all Reps, and generates an HTML page per Rep, including their public profile data, events and activity reports.\n\nSetup\n-----\n\nFirst install [Node](http://nodejs.org/) and clone this repository.\n\n```\nnpm ci\n```\n\nNow you can run the script:\n\n```\nnpm start\n```\n\nYou can add `FETCH=\"true\"` if you want to update the data, but note that this takes several hours to complete. We will run this fetch again before we shut down the Reps Portal. Until then the data is from around April 4th.\n"
},
{
  "name": "nimbus-shared",
  "files": {
    "/": [
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.json",
      ".therapist.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "data",
      "docs",
      "index.ts",
      "package-lock.json",
      "package.json",
      "python",
      "src",
      "test",
      "tsconfig.json",
      "types"
    ],
    "/docs": [
      ".gitignore",
      "README.md",
      "adr",
      "components",
      "global_style.scss",
      "next-env.d.ts",
      "next.config.js",
      "package-lock.json",
      "package.json",
      "pages",
      "public",
      "tsconfig.json"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "NPM_PKGNAME := $(shell cat package.json | jq -r .name)\nPYTHON_PKGNAME := mozilla_nimbus_shared\nVERSION := $(shell cat package.json | jq -r .version)\nPY_VERSION := $(shell cat package.json | jq -r .version | sed -e 's/-dev/.dev0/')\n\nMOCHA := ./node_modules/.bin/mocha\nESLINT := ./node_modules/.bin/eslint\nTSC := ./node_modules/.bin/tsc\nTS_NODE := ./node_modules/.bin/ts-node-script\nPRETTIER := ./node_modules/.bin/prettier\nPYTEST := poetry run pytest\n\nTYPES := $(shell find ./types -name '*.ts')\nDATA_SOURCES := $(shell find ./data)\nTS_SRC := index.ts $(shell find ./src -name '*.ts')\nPY_SRC := $(shell find ./python/mozilla_nimbus_shared -name '*.py')\nNPM_PACK_FILE := $(shell echo $(NPM_PKGNAME) | sed -e 's/@//' -e 's_/_-_')-$(VERSION).tgz\n\nPYTHON_SDIST := python/dist/$(PYTHON_PKGNAME)-$(PY_VERSION).tar.gz\nPYTHON_WHEEL := python/dist/$(PYTHON_PKGNAME)-$(PY_VERSION)-py3-none-any.whl\nPYTHON_PACK_FILE := $(PYTHON_SDIST) $(PYTHON_WHEEL)\n\nGENERATED_TS := ./src/_generated/typeGuardHelpers.ts ./src/_generated/schemas.ts ./src/_generated/data.ts\nGENERATED_PYTHON := ./python/pyproject.toml\nGENERATED_DATA := ./dist/data.json\nGENERATED_DATA_PYTHON := ./python/$(PYTHON_PKGNAME)/data.json\nGENERATED := $(GENERATED_TS) $(GENERATED_PYTHON) $(GENERATED_DATA)\nJS_TEST_FILES := $(shell find ./test -name 'test-*')\nPY_TEST_FILES = python/mozilla_nimbus_shared/test.py\n\nTIMESTAMP_DIR := ./.timestamps\nTSC_STAMP := $(TIMESTAMP_DIR)/tsc-last-run\nNPM_INSTALL_STAMP := $(TIMESTAMP_DIR)/npm-install\nSCHEMA_STAMP := $(TIMESTAMP_DIR)/schemas\nPYTHON_SCHEMAS := ./python/$(PYTHON_PKGNAME)/schemas\nPYTHON_INSTALL_STAMP := $(TIMESTAMP_DIR)/poetry-install\nDATA_STAMP := $(TIMESTAMP_DIR)/data\nDOCS_NPM_INSTALL_STAMP := $(TIMESTAMP_DIR)/docs-npm-install\nDOCS_BUILT_STAMP := $(TIMESTAMP_DIR)/docs-built\n\nDOC_SOURCES := $(shell find docs/pages -type f) $(shell find docs/public -type f) docs/global_style.scss\n\n# Phony targets - commands that don't make files\n.PHONY: default install build clean test artifact lint pack docs\n\ndefault: build\n\ninstall: $(NPM_INSTALL_STAMP) $(PYTHON_INSTALL_STAMP) $(DOCS_NPM_INSTALL_STAMP)\n\nbuild: $(TSC_STAMP) $(SCHEMA_STAMP) $(GENERATED_CODE) $(GENERATED_DATA) $(PYTHON_SCHEMAS) $(GENERATED_DATA_PYTHON)\n\nclean:\n\trm -rf dist schemas $(TSC_STAMP) $(NPM_INSTALL_STAMP) node_modules $(TIMESTAMP_DIR) $(GENERATED) \\\n\t\tartifacts python/dist python/mozilla_nimbus_shared.egg-info python/poetry.lock src/_generated \\\n\t\tdocs/out docs/node_modules docs/.next $(shell cd python; poetry env info -p) python/README.md \\\n\t\t$(GENERATED_DATA_PYTHON) $(PYTHON_SCHEMAS)\n\ntest: build $(NPM_INSTALL_STAMP) $(PYTHON_INSTALL_STAMP) $(JS_TEST_FILES) $(PY_TEST_FILES)\n\t$(MOCHA) -r ts-node/register $(JS_TEST_FILES)\n\tcd python; $(PYTEST) mozilla_nimbus_shared/test.py\n\nartifact: build pack docs\n\t./bin/pack-artifact.sh\n\tmkdir -p artifacts/python\n\tcp $(PYTHON_SDIST) $(PYTHON_WHEEL) artifacts/python/\n\tmkdir -p artifacts/npm\n\tcp mozilla-nimbus-shared-$(VERSION).tgz artifacts/npm/\n\tcp -r docs/out artifacts/docs\n\nlint: $(NPM_INSTALL_STAMP) $(PYTHON_INSTALL_STAMP) build\n\t$(ESLINT) .\n\t$(TSC) --noEmit --project tsconfig.json\n\t$(PRETTIER) --check .\n\tcd python; poetry run black --check .\n\nlint-fix:\n\t$(ESLINT) --fix .\n\t$(TSC) --noEmit --project tsconfig.json\n\t$(PRETTIER) --write .\n\tcd python; poetry run black .\n\npack: $(NPM_PACK_FILE) $(PYTHON_PACK_FILE)\n\ndocs: $(DOCS_BUILT_STAMP)\n\n# Commands that make files. None of these should depend on phony targets\n\n$(NPM_INSTALL_STAMP): package.json package-lock.json\n\tnpm ci\n\t@mkdir -p $(@D)\n\t@touch $@\n\n$(PYTHON_INSTALL_STAMP): python/pyproject.toml\n\tcd python; poetry install\n\t@mkdir -p $(TIMESTAMP_DIR)\n\t@touch $(PYTHON_INSTALL_STAMP)\n\n$(TSC_STAMP): $(GENERATED_TS) $(TS_SRC) $(TYPES) $(NPM_INSTALL_STAMP) tsconfig.json\n\t$(TSC)\n\t@mkdir -p $(@D)\n\t@touch $@\n\n$(SCHEMA_STAMP): $(TYPES) $(NPM_INSTALL_STAMP) bin/build-schemas.ts\n\t$(TS_NODE) ./bin/build-schemas.ts\n\t@mkdir -p $(@D)\n\t@touch $@\n\n$(GENERATED_DATA): $(DATA_SOURCES) $(NPM_INSTALL_STAMP) ./src/_generated/schemas.ts ./src/_generated/typeGuardHelpers.ts bin/translate-data.ts\n\t$(TS_NODE) ./bin/translate-data.ts\n\t@touch $@\n\nsrc/_generated/typeGuardHelpers.ts: $(NPM_INSTALL_STAMP) $(SCHEMA_STAMP) bin/generate-type-guards.ts\n\t$(TS_NODE) ./bin/generate-type-guards.ts\n\nsrc/_generated/schemas.ts: $(NPM_INSTALL_STAMP) $(SCHEMA_STAMP) bin/generate-schema-code.ts\n\t$(TS_NODE) ./bin/generate-schema-code.ts\n\nsrc/_generated/data.ts: $(NPM_INSTALL_STAMP) $(GENERATED_DATA) bin/generate-data-code.ts\n\t$(TS_NODE) ./bin/generate-data-code.ts\n\n$(GENERATED_DATA_PYTHON): $(GENERATED_DATA)\n\tcp $(GENERATED_DATA) $(GENERATED_DATA_PYTHON)\n\n$(PYTHON_SCHEMAS): $(SCHEMA_STAMP)\n\tcp -R ./schemas $(PYTHON_SCHEMAS)\n\n$(NPM_PACK_FILE): package.json $(TSC_STAMP) $(SCHEMA_STAMP) $(GENERATED_DATA) $(GENERATED_TS)\n\tnpm pack\n\npython/pyproject.toml: python/pyproject.toml.template bin/generate-pyproject-toml.ts $(NPM_INSTALL_STAMP) python/README.md\n\t$(TS_NODE) ./bin/generate-pyproject-toml.ts\n\n$(PYTHON_SDIST): python/pyproject.toml $(PY_SRC) $(SCHEMA_STAMP) $(GENERATED_DATA) $(GENERATED_DATA_PYTHON) $(GENERATED_PYTHON) $(PYTHON_SCHEMAS) $(PYTHON_INSTALL_STAMP)\n\tcd python; poetry build --format sdist\n\n$(PYTHON_WHEEL): $(PYTHON_SDIST)\n\tcd python; poetry build --format wheel\n\n$(DOCS_NPM_INSTALL_STAMP): docs/package.json docs/package-lock.json\n\tcd docs; npm ci\n\t@mkdir -p $(@D)\n\t@touch $@\n\n$(DOCS_BUILT_STAMP): $(DOCS_NPM_INSTALL_STAMP) docs/next.config.js $(DOC_SOURCES) $(TSC_STAMP)\n\tcd docs; npm run build\n\t@mkdir -p $(@D)\n\t@touch $@\n\npython/README.md: README.md\n\tcp README.md $@\n",
  "readme": "# Nimbus Shared ![CircleCI](https://img.shields.io/circleci/build/github/mozilla/nimbus-shared) ![npm (scoped)](https://img.shields.io/npm/v/@mozilla/nimbus-shared)\n\nThis is a place to define data and schemas used across Project Nimbus.\n\nAny data that moves between systems should have TypeScript types defined here, which will be\nautomatically converted to JSON Schema. Any data that needs to be re-used by multiple systems should\nbe stored here to be shared.\n\nFor more information on the data and schemas included here, how to use them, and how to add to them,\nsee the documentation at https://mozilla.github.io/nimbus-shared\n"
},
{
  "name": "webextension-polyfill",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc",
      ".gitattributes",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gruntfile.js",
      "LICENSE",
      "README.md",
      "api-metadata.json",
      "package.json",
      "renovate.json",
      "scripts",
      "src",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# WebExtension `browser` API Polyfill\n\nThis library allows extensions that use the Promise-based WebExtension/BrowserExt API being standardized by the\n[W3 Browser Extensions][w3-browserext] group to run on Google Chrome with minimal or no changes.\n\n[![CircleCI](https://circleci.com/gh/mozilla/webextension-polyfill.svg?style=svg)](https://circleci.com/gh/mozilla/webextension-polyfill)\n[![codecov](https://codecov.io/gh/mozilla/webextension-polyfill/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/webextension-polyfill)\n[![devDependency Status](https://david-dm.org/mozilla/webextension-polyfill/dev-status.svg)](https://david-dm.org/mozilla/webextension-polyfill#info=devDependencies)\n[![npm version](https://badge.fury.io/js/webextension-polyfill.svg)](https://badge.fury.io/js/webextension-polyfill)\n\n> This library doesn't (and it is not going to) polyfill API methods or options that are missing on Chrome but natively provided\n> on Firefox, and so the extension has to do its own \"runtime feature detection\" in those cases (and then eventually polyfill the\n> missing feature on its own or enable/disable some of the features accordingly).\n\n[w3-browserext]: https://www.w3.org/community/browserext/\n\nTable of contents\n=================\n\n* [Supported Browsers](#supported-browsers)\n* [Installation](#installation)\n* [Basic Setup](#basic-setup)\n  * [Basic Setup with ES6 module loader](#basic-setup-with-es6-module-loader)\n  * [Basic Setup with module bundlers](#basic-setup-with-module-bundlers)\n  * [Usage with webpack without bundling](#usage-with-webpack-without-bundling)\n* [Using the Promise-based APIs](#using-the-promise-based-apis)\n* [Examples](#examples)\n* [Usage with TypeScript](#usage-with-typescript)\n* [Known Limitations and Incompatibilities](#known-limitations-and-incompatibilities)\n* [Contributing to this project](#contributing-to-this-project)\n\nSupported Browsers\n==================\n\n| Browser                   | Support Level                                                                                      |\n| ------------------------- | -------------------------------------------------------------------------------------------------- |\n| Chrome                    | *Officially Supported* (with automated tests)                                                        |\n| Firefox                   | *Officially Supported as a NO-OP* (with automated tests for comparison with the behaviors on Chrome) |\n| Opera / Edge (>=79.0.309) | *Unofficially Supported* as a Chrome-compatible target (but not explicitly tested in automation)     |\n\nThe polyfill is being tested explicitly (with automated tests that run on every pull request) on **officially supported** \nbrowsers (that are currently the last stable versions of Chrome and Firefox).\n\nOn Firefox, this library is actually acting as a NO-OP: it detects that the `browser` API object is already defined \nand it does not create any custom wrappers.\nFirefox is still included in the automated tests, to ensure that no wrappers are being created when running on Firefox,\nand for comparison with the behaviors implemented by the library on Chrome.\n\n## Installation\n\nA new version of the library is built from this repository and released as an npm package.\n\nThe npm package is named after this repo: [webextension-polyfill](https://www.npmjs.com/package/webextension-polyfill).\n\nFor the extension that already include a package.json file, the last released version of this library can be quickly installed using:\n\n```\nnpm install --save-dev webextension-polyfill\n```\n\nInside the `dist/` directory of the npm package, there are both the minified and non-minified builds (and their related source map files):\n\n- node_modules/webextension-polyfill/dist/browser-polyfill.js\n- node_modules/webextension-polyfill/dist/browser-polyfill.min.js\n\nFor extensions that do not include a package.json file and/or prefer to download and add the library directly into their own code repository, all the versions released on npm are also available for direct download from unpkg.com:\n\n- https://unpkg.com/webextension-polyfill/dist/\n\nand linked to the Github releases:\n\n- https://github.com/mozilla/webextension-polyfill/releases\n\n## Basic Setup\n\nIn order to use the polyfill, it must be loaded into any context where `browser` APIs are accessed. The most common cases\nare background and content scripts, which can be specified in `manifest.json` (make sure to include the `browser-polyfill.js` script before any other scripts that use it):\n\n```javascript\n{\n  // ...\n\n  \"background\": {\n    \"scripts\": [\n      \"browser-polyfill.js\",\n      \"background.js\"\n    ]\n  },\n\n  \"content_scripts\": [{\n    // ...\n    \"js\": [\n      \"browser-polyfill.js\",\n      \"content.js\"\n    ]\n  }]\n}\n```\n\nFor HTML documents, such as `browserAction` popups, or tab pages, it must be\nincluded more explicitly:\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <script type=\"application/javascript\" src=\"browser-polyfill.js\"></script>\n    <script type=\"application/javascript\" src=\"popup.js\"></script>\n  </head>\n  <!-- ... -->\n</html>\n```\n\nAnd for dynamically-injected content scripts loaded by `tabs.executeScript`,\nit must be injected by a separate `executeScript` call, unless it has\nalready been loaded via a `content_scripts` declaration in\n`manifest.json`:\n\n```javascript\nbrowser.tabs.executeScript({file: \"browser-polyfill.js\"});\nbrowser.tabs.executeScript({file: \"content.js\"}).then(result => {\n  // ...\n});\n```\n\n### Basic Setup with ES6 module loader\n\nThe polyfill can also be loaded using the native ES6 module loader available in\nthe recent browsers versions.\n\nBe aware that the polyfill module does not export the `browser` API object,\nbut defines the `browser` object in the global namespace (i.e. `window`).\n\n```html\n<!DOCTYPE html>\n<html>\n  <head>\n    <script type=\"module\" src=\"browser-polyfill.js\"></script>\n    <script type=\"module\" src=\"background.js\"></script>\n  </head>\n  <!-- ... -->\n</html>\n```\n\n```javascript\n// In background.js (loaded after browser-polyfill.js) the `browser`\n// API object is already defined and provides the promise-based APIs.\nbrowser.runtime.onMessage.addListener(...);\n```\n\n### Basic Setup with module bundlers\n\nThis library is built as a **UMD module** (Universal Module Definition), and so it can also be used with module bundlers (and explicitly tested on both **webpack** and **browserify**) or AMD module loaders.\n\n**src/background.js**:\n```javascript\nvar browser = require(\"webextension-polyfill\");\n\nbrowser.runtime.onMessage.addListener(async (msg, sender) => {\n  console.log(\"BG page received message\", msg, \"from\", sender);\n  console.log(\"Stored data\", await browser.storage.local.get());\n});\n\nbrowser.browserAction.onClicked.addListener(() => {\n  browser.tabs.executeScript({file: \"content.js\"});\n});\n```\n\n**src/content.js**:\n```javascript\nvar browser = require(\"webextension-polyfill\");\n\nbrowser.storage.local.set({\n  [window.location.hostname]: document.title,\n}).then(() => {\n  browser.runtime.sendMessage(`Saved document title for ${window.location.hostname}`);\n});\n```\n\nBy using `require(\"webextension-polyfill\")`, the module bundler will use the non-minified version of this library, and the extension is supposed to minify the entire generated bundles as part of its own build steps.\n\nIf the extension doesn't minify its own sources, it is still possible to explicitly ask the module bundler to use the minified version of this library, e.g.:\n\n```javascript\nvar browser = require(\"webextension-polyfill/dist/browser-polyfill.min\");\n\n...\n```\n\n### Usage with webpack without bundling\n\nThe previous section explains how to bundle `webextension-polyfill` in each script. An alternative method is to include a single copy of the library in your extension, and load the library as shown in [Basic Setup](#basic-setup). You will need to install [copy-webpack-plugin](https://www.npmjs.com/package/copy-webpack-plugin):\n\n```sh\nnpm install --save-dev copy-webpack-plugin\n```\n\n**In `webpack.config.js`,** import the plugin and configure it this way. It will copy the minified file into your _output_ folder, wherever your other webpack files are generated.\n\n```js\nconst CopyWebpackPlugin = require('copy-webpack-plugin');\n\nmodule.exports = {\n  /* Your regular webpack config, probably including something like this:\n  output: {\n    path: path.join(__dirname, 'distribution'),\n    filename: '[name].js'\n  },\n  */\n  plugins: [\n    new CopyWebpackPlugin({\n      patterns: [{\n        from: 'node_modules/webextension-polyfill/dist/browser-polyfill.js',\n      }],\n    })\n  ]\n}\n```\n\nAnd then include the file in each context, using the `manifest.json` just like in [Basic Setup](#basic-setup).\n\n## Using the Promise-based APIs\n\nThe Promise-based APIs in the `browser` namespace work, for the most part,\nvery similarly to the callback-based APIs in Chrome's `chrome` namespace.\nThe major differences are:\n\n* Rather than receiving a callback argument, every async function returns a\n  `Promise` object, which resolves or rejects when the operation completes.\n\n* Rather than checking the `chrome.runtime.lastError` property from every\n  callback, code which needs to explicitly deal with errors registers a\n  separate Promise rejection handler.\n\n* Rather than receiving a `sendResponse` callback to send a response,\n  `onMessage` listeners simply return a Promise whose resolution value is\n  used as a reply.\n\n* Rather than nesting callbacks when a sequence of operations depend on each\n  other, Promise chaining is generally used instead.\n\n* The resulting Promises can be also used with `async` and `await`, rather\n  than dealt with directly.\n\n## Examples\n\nThe following code will retrieve a list of URLs patterns from the `storage`\nAPI, retrieve a list of tabs which match any of them, reload each of those\ntabs, and notify the user that is has been done:\n\n```javascript\nbrowser.storage.local.get(\"urls\").then(({urls}) => {\n  return browser.tabs.query({url: urls});\n}).then(tabs => {\n  return Promise.all(\n    Array.from(tabs, tab => browser.tabs.reload(tab.id))\n  );\n}).then(() => {\n  return browser.notifications.create({\n    type: \"basic\",\n    iconUrl: \"icon.png\",\n    title: \"Tabs reloaded\",\n    message: \"Your tabs have been reloaded\",\n  });\n}).catch(error => {\n  console.error(`An error occurred while reloading tabs: ${error.message}`);\n});\n```\n\nOr, using an async function:\n\n```javascript\nasync function reloadTabs() {\n  try {\n    let {urls} = await browser.storage.local.get(\"urls\");\n\n    let tabs = await browser.tabs.query({url: urls});\n\n    await Promise.all(\n      Array.from(tabs, tab => browser.tabs.reload(tab.id))\n    );\n\n    await browser.notifications.create({\n      type: \"basic\",\n      iconUrl: \"icon.png\",\n      title: \"Tabs reloaded\",\n      message: \"Your tabs have been reloaded\",\n    });\n  } catch (error) {\n    console.error(`An error occurred while reloading tabs: ${error.message}`);\n  }\n}\n```\n\nIt's also possible to use Promises effectively using two-way messaging.\nCommunication between a background page and a tab content script, for example,\nlooks something like this from the background page side:\n\n```javascript\nbrowser.tabs.sendMessage(tabId, \"get-ids\").then(results => {\n  processResults(results);\n});\n```\n\nAnd like this from the content script:\n\n```javascript\nbrowser.runtime.onMessage.addListener(msg => {\n  if (msg == \"get-ids\") {\n    return browser.storage.local.get(\"idPattern\").then(({idPattern}) => {\n      return Array.from(document.querySelectorAll(idPattern),\n                        elem => elem.textContent);\n    });\n  }\n});\n```\n\nor:\n\n```javascript\nbrowser.runtime.onMessage.addListener(async function(msg) {\n  if (msg == \"get-ids\") {\n    let {idPattern} = await browser.storage.local.get(\"idPattern\");\n\n    return Array.from(document.querySelectorAll(idPattern),\n                      elem => elem.textContent);\n  }\n});\n```\n\nOr vice versa.\n\n## Usage with TypeScript\n\nThere are multiple projects that add TypeScript support to your web-extension project:\n\n| Project | Description |\n| ------------- | ------------- |\n| [@types/webextension-polyfill](https://www.npmjs.com/package/@types/webextension-polyfill) | Types and JS-Doc are automatically generated from the mozilla schema files, so it is always up-to-date with the latest APIs. Formerly known as [webextension-polyfill-ts](https://github.com/Lusito/webextension-polyfill-ts). |\n| [web-ext-types](https://github.com/kelseasy/web-ext-types) | Manually maintained types based on MDN's documentation. No JS-Doc included. |\n| [@types/chrome](https://www.npmjs.com/package/@types/chrome) | Manually maintained types and JS-Doc. Only contains types for chrome extensions though! |\n\n## Known Limitations and Incompatibilities\n\nThis library tries to minimize the amount of \"special handling\" that a cross-browser extension has to do to be able to run on the supported browsers from a single codebase, but there are still cases when polyfillling the missing or incompatible behaviors or features is not possible or out of the scope of this polyfill.\n\nThis section aims to keep track of the most common issues that an extension may have.\n\n### No callback supported by the Promise-based APIs on Chrome\n\nWhile some of the asynchronous API methods in Firefox (the ones that return a promise) also support the callback parameter (mostly as a side effect of the backward compatibility with the callback-based APIs available on Chrome), the Promise-based APIs provided by this library do not support the callback parameter (See [\"#102 Cannot call browser.storage.local.get with callback\"][I-102]).\n\n### No promise returned on Chrome for some API methods\n\nThis library takes its knowledge of the APIs to wrap and their signatures from a metadata JSON file:\n[api-metadata.json](api-metadata.json).\n\nIf an API method is not yet included in this \"API metadata\" file, it will not be recognized.\nPromises are not supported for unrecognized APIs, and callbacks have to be used for them.\n\nChrome-only APIs have no promise version, because extensions that use such APIs\nwould not be compatible with Firefox.\n\nFile an issue in this repository for API methods that support callbacks in Chrome *and*\nFirefox but are currently missing from the \"API metadata\" file.\n\n### Issues that happen only when running on Firefox\n\nWhen an extension that uses this library doesn't behave as expected on Firefox, it is almost never an issue in this polyfill, but an issue with the native implementation in Firefox.\n\n\"Firefox only\" issues should be reported upstream on Bugzilla:\n- https://bugzilla.mozilla.org/enter_bug.cgi?product=WebExtensions&component=Untriaged\n\n### API methods or options that are only available when running in Firefox\n\nThis library does not provide any polyfill for API methods and options that are only available on Firefox, and they are actually considered out of the scope of this library.\n\n### tabs.executeScript\n\nOn Firefox `browser.tabs.executeScript` returns a promise which resolves to the result of the content script code that has been executed, which can be an immediate value or a Promise.\n\nOn Chrome, the `browser.tabs.executeScript` API method as polyfilled by this library also returns a promise which resolves to the result of the content script code, but only immediate values are supported.\nIf the content script code result is a Promise, the promise returned by `browser.tabs.executeScript` will be resolved to `undefined`.\n\n### MSEdge support\n\nMSEdge versions >= 79.0.309 are unofficially supported as a Chrome-compatible target (as for Opera or other Chrome-based browsers that also support extensions).\n\nMSEdge versions older than 79.0.309 are **unsupported**, for extension developers that still have to work on extensions for older MSEdge versions, the MSEdge `--ms-preload` manifest key and the [Microsoft Edge Extension Toolkit](https://docs.microsoft.com/en-us/microsoft-edge/extensions/guides/porting-chrome-extensions)'s Chrome API bridge can be used to be able to load the webextension-polyfill without any MSEdge specific changes.\n\nThe following Github repository provides some additional detail about this strategy and a minimal test extension that shows how to put it together:\n\n- https://github.com/rpl/example-msedge-extension-with-webextension-polyfill\n\n## Contributing to this project\n\nRead the [contributing section](CONTRIBUTING.md) for additional information about how to build the library from this repository and how to contribute and test changes.\n\n[PR-114]: https://github.com/mozilla/webextension-polyfill/pull/114\n[I-102]: https://github.com/mozilla/webextension-polyfill/issues/102#issuecomment-379365343\n"
},
{
  "name": "docker-sbt",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# docker-sbt\nDockerfile for sbt (Scala build tool)\n\nThis is built on top of the\n[openjdk](https://hub.docker.com/_/openjdk/) image\nand takes inspiration from\n[hseeberger/scala-sbt](https://github.com/hseeberger/scala-sbt).\n\n## Usage\n\nInstall [Docker](https://www.docker.com/) and pull the image\n([mozilla/sbt](https://hub.docker.com/r/mozilla/sbt/) on DockerHub):\n\n    docker pull mozilla/sbt\n\nYou can then run `sbt` inside docker to compile code like:\n\n    docker run -it --rm mozilla/sbt sbt shell\n\nIf you want to execute sbt commands on a project on your local\nfilesystem, you may want to mount the current directory and various\nlocal caches as volumes and set the working directory as well:\n\n    docker run -it --rm -v ~/.ivy2:/root/.ivy2 -v ~/.sbt:/root/.sbt -v $PWD:/app -w /app mozilla/sbt sbt shell\n\n## Building\n\nTo build, you need to specify the desired openjdk and sbt versions via\n`--build-arg` parameters:\n\n    docker build --build-arg OPENJDK_TAG=11.0.13 --build-arg SBT_VERSION=1.6.2 .\n\n## Pushing a new tag to DockerHub\n\nTo push and tag a new image with updated versions of openjdk and sbt,\nyou'll need to have a DockerHub account that's a member of the appropriate\nMozilla organization and be logged in to DockerHub:\n\n    docker login --username=mydockerhubusername\n\nThen use the following recipe to build and push:\n\n```bash\nOPENJDK_TAG=11.0.13\nSBT_VERSION=1.6.2\n\ndocker build \\\n    --build-arg OPENJDK_TAG=$OPENJDK_TAG \\\n    --build-arg SBT_VERSION=$SBT_VERSION \\\n    --tag mozilla/sbt:${OPENJDK_TAG}_${SBT_VERSION} \\\n    --tag mozilla/sbt:latest \\\n    .\n\ndocker push mozilla/sbt:${OPENJDK_TAG}_${SBT_VERSION}\ndocker push mozilla/sbt:latest\n```\n"
},
{
  "name": "translate",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "bergamot-httpserver.js",
      "css",
      "index.html",
      "js",
      "package-lock.json",
      "package.json",
      "start_dev_server.sh"
    ]
  },
  "makefile": null,
  "readme": "## Translations Website\n\nThis repo contains a static website utilizing the proceedings of project [Bergamot](https://browser.mt/).\n\nThe concept is the same as [Firefox Translation](https://github.com/mozilla-extensions/firefox-translations), where the inference occurs entirely in the webpage by utilizing the WebAssembly port of the neural machine translation engine  [(marian)](github.com/mozilla/bergamot-translator), while the models are dynamically downloaded and loaded as the user switches between the languages.\n\n## Live Demo\nThe live demo is hosted on Github Pages and published on https://mozilla.github.io/translate.\n\nCompatible and tested on:\n- Firefox desktop\n- Chrome desktop\n- Edge desktop\n- Brave desktop\n- Firefox Nightly for Android\n- Chrome for Android\n\n## Testing locally\n\n```\n$npm install\n$bash start_dev_server.sh\n$firefox https://mozilla.github.io/translate\n```\n"
},
{
  "name": "extension-activity-monitor",
  "files": {
    "/": [
      ".babelrc.json",
      ".circleci",
      ".editorconfig",
      ".eslintrc.json",
      ".gitignore",
      ".prettierrc",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Extension Activity Monitor\n\nThis is a privileged Firefox extension that uses the `activityLog` API to monitor the activities of the other installed extensions.\n\n[![CircleCI](https://circleci.com/gh/mozilla/extension-activity-monitor.svg?style=svg)](https://circleci.com/gh/mozilla/extension-activity-monitor)\n[![codecov](https://codecov.io/gh/mozilla/extension-activity-monitor/branch/master/graph/badge.svg)](https://codecov.io/gh/mozilla/extension-activity-monitor)\n[![devDependency Status](https://david-dm.org/mozilla/extension-activity-monitor/dev-status.svg)](https://david-dm.org/mozilla/extension-activity-monitor#info=devDependencies)\n\n## Installation\n\n### Prerequisite\n\n- Clone / download the respository in your local machine.\n- Install [Firefox Nightly](https://www.mozilla.org/en-US/firefox/all/#product-desktop-nightly) to run this privileged extension.\n- [optional] run `$ npm ci` to install dev dependency to develop a patch or to run the extension using the web-ext dependency.\n\n### Using [web-ext](https://github.com/mozilla/web-ext)\n\nGo to project directory and and run the following command:\n\n```\n$ npm start\n```\n\n_NOTE:_ You may need to run Firefox Nightly once to let the above command to auto-discovery where the Firefox Nightly binary is located in your system.\nAlternatively the Firefox binary location can be provided manually on the command line:\n\n```\n$ web-ext run -f /path/to/firefox-nightly/firefox\n```\n\n### Manual Installation\n\n- Open Firefox Nightly and go to the following URL: `about:config`.\n  - Set `extensions.experiments.enabled` to `true`.\n- Go to the following URL: `about:debugging#/runtime/this-firefox`.\n  - Click on \"Load Temporary Add-on\" button, then go to `src` directory of your cloned/ downloaded repository and choose `manifest.json` file.\n\nThe extension will be loaded temporarily in the browser.\n\n## Get Involved\n\nTo get involved with Extension Activity Monitor, you can file bugs or issues, request features and even fix bugs or issues. For more information, check out our [contributing section](https://github.com/mozilla/extension-activity-monitor/blob/master/CONTRIBUTING.md).\n"
},
{
  "name": "rust-components-swift",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "FocusRustComponentsWrapper",
      "LICENSE",
      "MozillaRustComponentsWrapper",
      "Package.swift",
      "README.md",
      "appservices_local_xcframework.sh",
      "automation",
      "generate.sh",
      "make_tag.sh",
      "swift-source"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Swift Package for Mozilla's Rust Components\n\nThis repository is a Swift Package for distributing releases of Mozilla's various\nRust-based application components. It provides the Swift source code packaged in\na format understood by the Swift package manager, and depends on a pre-compiled\nbinary release of the underlying Rust code published from [mozilla/application-services](\nhttps://github.com/mozilla/application-service).\n\nFor more information, please consult:\n\n* [application-services ADR-0003](https://github.com/mozilla/application-services/blob/main/docs/adr/0003-swift-packaging.md),\n  which describes the overall plan for distributing Rust-based components as Swift packages.\n* The [Swift Package Manager docs](https://swift.org/package-manager/) and [GitHub repo](https://github.com/apple/swift-package-manager),\n  which explain the details of how Swift Packages work (and hence why this repo is set up the way it is).\n* The [`ios-rust` crate](https://github.com/mozilla/application-services/tree/main/megazords/ios-rust) which is currently\n  responsible for publishing the pre-built `MozillaRustComponents.xcframework.zip` and `FocusRustComponents.xcframework.zip` bundles on which this repository depends.\n\n## Overview\n\nHere's a diagram of how this repository relates to the application-services repository\nand its release artifacts:\n\n<!--\n  N.B. you can edit this image in Google Docs and changes will be reflected automatically:\n\n    https://docs.google.com/drawings/d/1tX05I-e6hNBQxch7PescDH7k4G7ddAJwXDPoIqp1RYk/edit\n-->\n<img src=\"https://docs.google.com/drawings/d/e/2PACX-1vRnyxy7VjdD3bYTso8V3AL5FpIQ4_S54dOCDI6fxfZEbG3_CVBwZZP1uLYbUVE9M54GSXUkNgewzOQm/pub?w=720&h=540\" width=\"720\" height=\"540\" alt=\"A box diagram describing how the rust-components-swift repo, applicaiton-services repo, and MozillaRustComponents XCFramework interact\">\n\nKey points:\n\n* The `application-services` repo publishes two binary artifacts `MozillaRustComponents.xcframework.zip` and `FocusRustComponents.xcframework.zip` containing\n  the Rust code and FFI definitions for all components, compiled together into a single library.\n* The `Package.swift` file refrences the xcframeworks as Swift binary targets.\n* The `Package.swift` file defines a library per target (one for all the components used by `firefox-ios` and one for `focus-ios`)\n    * Each library references its Swift source code directly as files in the repo. All components used by a target are copied into the same directory. For example, all the `firefox-ios` files are in the `swift-source/all` directory.\n    * Each library depends on wrapper which wraps the binary to provide the pre-compiled Rust code. For example, [`FocusRustComponentWrapper`](./FocusRustComponentsWrapper/) wraps the Focus xcframework.\n\n## Cutting a new release\n\nWhenever a new release of the underlying components is availble, we need to tag a new release\nin this repo to make them available to Swift components. To do so:\n\n* Edit `Package.swift` to update the URL and checksum of `MozillaRustComponents.xcframework.zip`.\n* Run `./make_tag.sh --as-version {APP_SERVICES_VERSION} X.Y.Z` to create the new tag.\n* Run `git push origin X.Y.Z` to publish it to GitHub.\n\n## Adding a new component\n\nCheck out the instructions in the [docs in `application-services` for adding a new component and publishing it for iOS](https://github.com/mozilla/application-services/blob/main/docs/howtos/adding-a-new-component.md#distribute-your-component-with-rust-components-swift). The docs are also published for convenience in <https://mozilla.github.io/application-services/book/index.html>.\n\n\n## Testing\nFor testing instructions, you can checkout the [docs in the `application-services`](https://github.com/mozilla/application-services/tree/main/docs/howtos) which are published for convenience in <https://mozilla.github.io/application-services/book/index.html>\n"
},
{
  "name": "redash",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".editorconfig",
      ".github",
      ".gitignore",
      ".restyled.yaml",
      "CHANGELOG.md",
      "CONTRIBUTING.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "SECURITY.md",
      "bin",
      "client",
      "cypress.json",
      "docker-compose.yml",
      "manage.py",
      "migrations",
      "netlify.toml",
      "package-lock.json",
      "package.json",
      "pytest.ini",
      "redash",
      "requirements.txt",
      "requirements_all_ds.txt",
      "requirements_bundles.txt",
      "requirements_dev.txt",
      "requirements_oracle_ds.txt",
      "setup.cfg",
      "setup",
      "tests",
      "viz-lib",
      "webpack.config.js",
      "worker.conf"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "PULL_REQUEST_TEMPLATE.md",
      "config.yml",
      "support.yml",
      "weekly-digest.yml"
    ],
    "/.circleci": [
      "Dockerfile.cypress",
      "config.yml",
      "docker-compose.circle.yml",
      "docker-compose.cypress.yml",
      "docker_build",
      "pack",
      "update_version"
    ]
  },
  "makefile": ".PHONY: compose_build up test_db create_database clean down bundle tests lint backend-unit-tests frontend-unit-tests test build watch start redis-cli bash\n\ncompose_build:\n\tdocker-compose build\n\nup:\n\tdocker-compose up -d --build\n\ntest_db:\n\t@for i in `seq 1 5`; do \\\n\t\tif (docker-compose exec postgres sh -c 'psql -U postgres -c \"select 1;\"' 2>&1 > /dev/null) then break; \\\n\t\telse echo \"postgres initializing...\"; sleep 5; fi \\\n\tdone\n\tdocker-compose exec postgres sh -c 'psql -U postgres -c \"drop database if exists tests;\" && psql -U postgres -c \"create database tests;\"'\n\ncreate_database:\n\tdocker-compose run server create_db\n\nclean:\n\tdocker-compose down && docker-compose rm\n\ndown:\n\tdocker-compose down\n\nbundle:\n\tdocker-compose run server bin/bundle-extensions\n\ntests:\n\tdocker-compose run server tests\n\nlint:\n\t./bin/flake8_tests.sh\n\nbackend-unit-tests: up test_db\n\tdocker-compose run --rm --name tests server tests\n\nfrontend-unit-tests: bundle\n\tnpm ci\n\tnpm run bundle\n\tnpm test\n\ntest: lint backend-unit-tests frontend-unit-tests\n\nbuild: bundle\n\tnpm run build\n\nwatch: bundle\n\tnpm run watch\n\nstart: bundle\n\tnpm run start\n\nredis-cli:\n\tdocker-compose run --rm redis redis-cli -h redis\n\nbash:\n\tdocker-compose run --rm server bash\n",
  "readme": "<p align=\"center\">\n  <img title=\"Redash\" src='https://redash.io/assets/images/logo.png' width=\"200px\"/>\n</p>\n\n[![Documentation](https://img.shields.io/badge/docs-redash.io/help-brightgreen.svg)](https://redash.io/help/)\n[![Datree](https://s3.amazonaws.com/catalog.static.datree.io/datree-badge-20px.svg)](https://datree.io/?src=badge)\n[![Build Status](https://circleci.com/gh/getredash/redash.png?style=shield&circle-token=8a695aa5ec2cbfa89b48c275aea298318016f040)](https://circleci.com/gh/getredash/redash/tree/master)\n\nRedash is designed to enable anyone, regardless of the level of technical sophistication, to harness the power of data big and small. SQL users leverage Redash to explore, query, visualize, and share data from any data sources. Their work in turn enables anybody in their organization to use the data. Every day, millions of users at thousands of organizations around the world use Redash to develop insights and make data-driven decisions.\n\nRedash features:\n\n1. **Browser-based**: Everything in your browser, with a shareable URL.\n2. **Ease-of-use**: Become immediately productive with data without the need to master complex software.\n3. **Query editor**: Quickly compose SQL and NoSQL queries with a schema browser and auto-complete.\n4. **Visualization and dashboards**: Create [beautiful visualizations](https://redash.io/help/user-guide/visualizations/visualization-types) with drag and drop, and combine them into a single dashboard.\n5. **Sharing**: Collaborate easily by sharing visualizations and their associated queries, enabling peer review of reports and queries.\n6. **Schedule refreshes**: Automatically update your charts and dashboards at regular intervals you define.\n7. **Alerts**: Define conditions and be alerted instantly when your data changes.\n8. **REST API**: Everything that can be done in the UI is also available through REST API.\n9. **Broad support for data sources**: Extensible data source API with native support for a long list of common databases and platforms.\n\n<img src=\"https://raw.githubusercontent.com/getredash/website/8e820cd02c73a8ddf4f946a9d293c54fd3fb08b9/website/_assets/images/redash-anim.gif\" width=\"80%\"/>\n\n## Getting Started\n\n* [Setting up Redash instance](https://redash.io/help/open-source/setup) (includes links to ready-made AWS/GCE images).\n* [Documentation](https://redash.io/help/).\n\n## Supported Data Sources\n\nRedash supports more than 35 SQL and NoSQL [data sources](https://redash.io/help/data-sources/supported-data-sources). It can also be extended to support more. Below is a list of built-in sources:\n\n- Amazon Athena\n- Amazon DynamoDB\n- Amazon Redshift\n- Axibase Time Series Database\n- Cassandra\n- ClickHouse\n- CockroachDB\n- CSV\n- Databricks (Apache Spark)\n- DB2 by IBM\n- Druid\n- Elasticsearch\n- Google Analytics\n- Google BigQuery\n- Google Spreadsheets\n- Graphite\n- Greenplum\n- Hive\n- Impala\n- InfluxDB\n- JIRA\n- JSON\n- Apache Kylin\n- OmniSciDB (Formerly MapD)\n- MemSQL\n- Microsoft Azure Data Warehouse / Synapse\n- Microsoft Azure SQL Database\n- Microsoft SQL Server\n- MongoDB\n- MySQL\n- Oracle\n- PostgreSQL\n- Presto\n- Prometheus\n- Python\n- Qubole\n- Rockset\n- Salesforce\n- ScyllaDB\n- Shell Scripts\n- Snowflake\n- SQLite\n- TreasureData\n- Vertica\n- Yandex AppMetrrica\n- Yandex Metrica\n\n## Getting Help\n\n* Issues: https://github.com/getredash/redash/issues\n* Discussion Forum: https://discuss.redash.io/\n\n## Reporting Bugs and Contributing Code\n\n* Want to report a bug or request a feature? Please open [an issue](https://github.com/getredash/redash/issues/new).\n* Want to help us build **_Redash_**? Fork the project, edit in a [dev environment](https://redash.io/help-onpremise/dev/guide.html) and make a pull request. We need all the help we can get!\n\n## Security\n\nPlease email security@redash.io to report any security vulnerabilities. We will acknowledge receipt of your vulnerability and strive to send you regular updates about our progress. If you're curious about the status of your disclosure please feel free to email us again. If you want to encrypt your disclosure email, you can use [this PGP key](https://keybase.io/arikfr/key.asc).\n\n## License\n\nBSD-2-Clause.\n"
},
{
  "name": "network-pulse-api",
  "files": {
    "/": [
      ".envrc",
      ".github",
      ".gitignore",
      ".mergify.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "app.json",
      "appveyor.yml",
      "dev-requirements.in",
      "dev-requirements.txt",
      "generate_client_secrets.py",
      "invoke.yml",
      "manage.py",
      "media",
      "migrate.py",
      "migrationData.json",
      "public",
      "pulseapi",
      "requirements.in",
      "requirements.txt",
      "reset_database.py",
      "runtime.txt",
      "sample.env",
      "shell.nix",
      "staticfiles",
      "tasks.py",
      "templates",
      "tox.ini"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Travis Build Status](https://travis-ci.org/mozilla/network-pulse-api.svg?branch=master)](https://travis-ci.org/mozilla/network-pulse-api) [![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/mozilla/network-pulse-api?svg=true)](https://ci.appveyor.com/project/mozillafoundation/network-pulse-api) [![Current API Version](https://img.shields.io/badge/dynamic/json.svg?label=Current%20API%20Version&colorB=blue&query=$.latestApiVersion&uri=https%3A%2F%2Fpulse-api.mofostaging.net%2Fapi%2Fpulse%2Fstatus%2F)](#supported-api-versions)\n\n# The Mozilla Foundation Network Pulse API Server\n\nThis is the REST API server for the Mozilla Network Pulse project.\n\nAll API routes are prefixed with `/api/pulse/`. The \"pulse\" might seem redundant, but this lets us move the API to different domains and fold it into other API servers without namespace conflicts in the future.\n\n---\n\n# API documentation\n\n- [Versioning](#versioning)\n- [General Routes](#general-routes)\n- [Content-specific routes](#content-specific-routes)\n\t- [Creators](#creators)\n\t- [Entries](#entries)\n\t- [Help Types](#help-types)\n\t- [Issues](#issues)\n\t- [Profiles](#profiles)\n\t- [Tags](#tags)\n- [Syndication](#syndication)\n\n---\n\n# Developer information\n\n- [Getting up and running for local development](#getting-up-and-running-for-local-development)\n- [Local development documentation](#local-development)\n- [Environment variables](#environment-variables)\n- [Deploying to Heroku](#deploying-to-heroku)\n- [Debugging](#debugging-all-the-things)\n- [Resetting your database](#resetting-your-database-because-of-incompatible-model-changes)\n- [Migrating data from Google sheets](#migrating-data-from-google-sheets)\n\n---\n\n# Versioning\n\n## How is the API versioned?\n\nAll Pulse API routes are versioned via their url path so that any future changes made to data responses will not break API clients that rely on specific versions of the data. To get a specific version of the API, add the version after the API prefix (`/api/pulse/`) in the url. For example, if you want version 2 (v2) of the API for the `entries` route, you would query the `/api/pulse/v2/entries/` url.\n\nAPI routes that do not include the `/api/pulse/` prefix are also versioned in the same way. For example, `/v1/login/` is a valid versioned route without the prefix. Note however that the `/rss` and `/atom` routes are unversioned, as these are syndication endpoints.\n\n## What happens if you don't specify an API version in the URL?\n\nTo maintain legacy support, if the version is not specified in the url, we default to version 1 (v1) of the API. For example, querying `/api/pulse/entries/` will have the same result as querying `/api/pulse/v1/entries/`. However, we strongly recommend specifying a version in the URL as this feature may be removed in the future.\n\n## Supported API Versions\n\n- Version 3 - `/api/pulse/v3/` and `/v3/`\n- Version 2 - `/api/pulse/v2/` and `/v2/`\n- Version 1 - `/api/pulse/v1/` and `/v1/`\n\n---\n\n# General Routes\n\nAll routes, including those with the `/api/pulse/` prefix should include a version in the URL (see the section on [Versioning](#versioning)) and although a URLs without the version are currently supported, versions will be made mandatory in the future.\n\n## Login routes\n\n### `GET /login?original_url=<url>`\n\nThis will kick off a Google OAuth2 login process. This process is entirely based on browser redirects, and once this process completes the user will be redirect to `original_url` with an additional url query argument `loggedin=True` or `loggedin=False` depending on whether the login attempt succeeded or not.\n\n### `GET /logout`\n\nThis will log out a user if they have an authenticated session going. Note that this route does not have a redirect path associated with it: calling `/logout` with an XHR or Fetch operation is enough to immediately log the user out and invalidate their session at the API server. The only acknowledgement that callers will receive around this operation succeeding is that if an HTTP 200 status code is received, logout succeeded.\n\n### `GET /oauth2callback`\n\nThis is the route that oauth2 login systems must point to in order to complete an oauth2 login process with in-browser callback URL redirection.\n\n### `GET /api/pulse/userstatus/`\n\nThis gets the current user's session information in the form of their full name and email address.\n\nThe call response is a JSON object of the following form:\n\n```\n{\n  username: <string: the user's full name according to Google>\n  profileid: <string: the user's profile id>\n  customname: <string: the user's custom name as set in their profile>\n  email: <string: the user's google-login-associated email address>\n  loggedin: <boolean: whether this user is logged in or not>\n  moderator: <boolean: whether this logged-in user has moderation rights>\n}\n```\n\nIf a user is authenticated, all three fields will be present. If a user is not authenticated, the response object will only contain the `loggedin` key, with value `false`.\n\n**This data should never be cached persistently**. Do not store this in localStorage, cookies, or any other persistent data store. When the user terminates their client, or logs out, this information should immediately be lost. Also do not store this in a global namespace like `window` or `document`, or in anything that isn't protected by a closure.\n\n## POST protection\n\n### `GET /api/pulse/nonce/`\n\nThis gets a current user's session information in the form of their CSRF token, as well as a \"nonce\" value for performing a one-time post operation. Every time a user POSTs data to the /entry route, this nonce gets invalidated (whether it matches or not) to prevent repeat-posting. As such, is a user is to post several entries, they will need to call `/nonce` as many times.\n\nThe call response is a 403 for not authenticated users, or a JSON object when authenticated, of the form:\n\n```\n{\n  csrf_token: <string: the user's session CSRF value>,\n  nonce: <string: one-time-use value>\n}\n```\n\n**This data should never be cached persistently**. Do not store this in localStorage, cookies, or any other persistent data store. When the user terminates their client, or logs out, this information should immediately be lost. Also do not store this in a global namespace like `window` or `document`, or in anything that isn't protected by a closure.\n\nAlso note that \"the page itself\" counts as global scope, so you generally don't want to put these values on the page as `<form>` elements. Instead, a form submission should be intercepted, and an in-memory form should be created with all the information of the original form, but with the nonce and csrf values copied. The submission can then use this in-memory form as basis for its POST payload instead.\n\n## Healthcheck\n\n### `GET /status/`\n\nThis is a healthcheck route that can be used to check the status of the pulse API server. The response contains the following information:\n\n```\n{\n  latestApiVersion: <version string>\n}\n```\n\n# Content-specific routes\n\n## Creators\n\n### `GET /api/pulse/creators/?name=...`\n\n#### DEPRECATION NOTICE\n\nThis route has been deprecated in favor of [`GET /api/pulse/profiles/?name=`](). Version 1 (v1) of this API route is supported for now.\n\n#### Version 1 - `GET /api/pulse/v1/creators/?name=...`\n\nGets the list of all creators whose name starts with the string passed as `name` argument. This yields a response that uses the following schema:\n\n```\n{\n    \"count\": 123,\n    \"next\": \"http://.../api/pulse/creators/?page=2&search=...\",\n    \"previous\": null,\n    \"results\": [\n        {\n            \"name\": \"...\",\n            \"creator_id\": number,\n            \"profile_id\": number or false\n        },\n        ...\n    ]\n}\n```\n\n\nIn this response:\n\n- `count` property represents how many hits the system knows about,\n- `next` is a URL if there are more results than fit in a single result set (set to `null` if there are no additional pages of results).\n- `results` points to an array of creator records, where each creator has a `name` (string data), a `creator_id` (integer which is the same as `profile_id`) as well as a `profile_id` (integer). By default, this array will contain 6 objects, but this number can be increased (to a maximum of 20) by adding `&page_size=...` to the query with the desired results-per-page number.\n\n## Entries\n\n### Entry object schema\n\n```\n  {\n    id: <integer: id of the entry>,\n    is_bookmarked: <boolean: whether this entry is bookmarked for the currently authenticated user>,\n    tags: <array: list of tags as strings>,\n    issues: <array: list of issues as strings>,\n    help_types: <array: list of help categories as strings>,\n    published_by: <string: name of the user who published this entry>,\n    submitter_profile_id: <integer: id of the profile of the user who published this entry>,\n    bookmark_count: <integer: number of users who have bookmarked this entry>,\n    related_creators: <array: list of related creator objects (see below)>,\n    title: <string: title of this entry>,\n    content_url: <string: external url for the contents of the entry>,\n    description: <string: description of the entry>,\n    get_involved: <string: CTA text for this entry>,\n    get_involved_url: <string: CTA url for this entry>,\n    interest: <string: description of why this entry might be interesting>,\n    featured: <boolean: whether this entry is featured on the pulse homepage or not>,\n    published_by_creator: <boolean: does this entry mention the user who submitted it as a creator>,\n    thumbnail: <string: url to a thumbnail image for the entry>,\n    created: <timestamp: ISO 8601 timestamp of when this entry was created>,\n    moderation_state: <integer: id of the moderation state of this entry>\n  }\n```\n\n### Related creator object schema\n\nThe related creator object will differ based on the API version that is used.\n\n#### Version 2 - `GET /api/pulse/v2/`\n\nEach `related_creator` object will have the following schema:\n\n```\n{\n  name: <string: name of the creator profile>\n  profile_id: <integer: id of the creator profile>\n  is_active: <boolean: flag indicating whether this creator profile is attached to a user>\n}\n```\n\n#### Version 1 - `GET /api/pulse/v1/`\n\nEach `related_creator` object should have the following schema:\n\n```\n{\n  name: <string: name of the creator profile>\n  profile_id: <integer: id of the creator profile>\n  creator_id: <integer: same as the profile_id>\n}\n```\n\n### `GET /api/pulse/entries/` with optional `?format=json`\n\nThis retrieves a paginated list of all [entries](#entry-object-schema) as stored in the database. As a base URL call this returns an HTML page with formatted results, as url with `?format=json` suffix this results a JSON object for use as data input to applications, webpages, etc.\n\nThe schema for the payload returned is:\n```\n{\n    \"count\": <integer: number of entries>,\n    \"next\": <string: url to the next page of entries> or null,\n    \"previous\": <string: url to the previous page of entries> or null,\n    \"results\": <array: list of entry objects (see above)>\n}\n```\n\n#### Filters\n\n- `?search=<string>` - Search for entries by their title, description, CTA, interest, creators, or tags\n- `?ids=<comma-separated integers>` - Filter entries with specific ids\n- `?tag=<string>` - Filter entries by a specific tag\n- `?issue=<string>` - Filter entries by an issue area\n- `?help_type=<string>` - Filter entries by a specific help category\n- `?has_help_types=<True or False>` - Filter entries by whether they have help types or not. Note that `True` or `False` is case-sensitive.\n- `?featured=<true or false>` - Filter featured or non-featured entries\n- `?ordering=<string>` - Order entries by a certain property e.g. `?ordering=title`. Prepend the property with a hyphen to get entries in descending order, e.g. `?ordering=-title`\n- `?moderationstate=<string>` - Filter entries by its moderation state. This filter will only be applied if the API call was made by an authenticated user with moderation permissions\n\n### `GET /api/pulse/entries/<id=number>/` with optional `?format=json`\n\nThis retrieves a single [entry](#entry-object-schema) with the indicated `id` as stored in the database. As a base URL call this returns an HTML page with formatted results, as url with `?format=json` suffix this results a JSON object for use as data input to applications, webpages, etc.\n\n\n### `POST /api/pulse/entries/`\n\nPOSTing of entries requires sending the following payload object:\n\n```\n{\n  csrfmiddlewaretoken: required csrf token string obtained from [GET /nonce]\n  nonce: required nonce string obtained from [GET /nonce]\n\n  title: required string (max length 140 characters)\n  content_url: required url string\n\n  description: optional string (max length 600 characters)\n  get_involved: optional 'how to get involved' string (max length 300 characters)\n  get_involved_url: optional URL that people can visit to get involved\n  interest: optional subject string (Free form, max length 600 characters)\n  featured: optional boolean to set the \"shown on featured page\" flag\n\n  thumbnail: optional object {\n    name: name of the file,\n    base64: the base64 encoded binary representation of the file's bytes\n  }\n\n  tags: optional array of strings\n  issue: optional string, must match value from [GET /issues?format=json]\n  help_type: optional string, must match value from [GET /helptypes?format=json]\n  related_creators: optional array of related creator objects (see below)where each object either has a creator_id or a name. The creator_id should be the id of an existing creator.\n  published_by_creator: optional boolean to indicate that this user is (one of) the content creator(s)\n}\n```\n\n---\n\n__Related creator object schema__\n\nThe related creator object will differ based on the API version that is used.\n\n#### Version 2 - `POST /api/pulse/v2/entries/`\n\nEach `related_creator` object should have the following schema:\n\n```\n{\n  name: optional string that represents a profile that does not exist yet\n  profile_id: optional id of an existing profile\n}\n```\n\nEither the `name` or the `profile_id` must be specified.\n\n#### Version 1 - `POST /api/pulse/v1/entries/`\n\nEach `related_creator` object should have the following schema:\n\n```\n{\n  name: optional string that represents a creator that does not exist yet\n  creator_id: optional id of an existing creator/profile\n}\n```\n\nEither the `name` or the `creator_id` must be specified.\n\n---\n\nAlso note that this POST **must** be accompanied by the following header:\n\n```\nX-CSRFToken: csrfmiddlewaretoken as used above\n```\n\nA successful post will yield a JSON object:\n\n```\n{\n  status: \"submitted\"\n}\n```\n\nA failed post will yield an HTTP 400 response.\n\n## Entry Moderation\n\n### Moderation state\n\n#### `GET /api/pulse/entries/moderation-states/` with optional `?format=json`\n\nThis retrieves the list of moderation states that are used for entry moderation. As a base URL call this returns an HTML page with formatted results, as url with `?format=json` suffix this results a JSON object for use as data input to applications, webpages, etc.\n\nThe result is of the format:\n```\n[\n{\n  id: \"id number as string\",\n  name: \"human-readable name for this moderation state\"\n},\n{...},\n...\n]\n```\n\n#### `PUT /api/pulse/entries/<id=number>/moderate/<id=number>` with optional `?format=json`\n\nThis changes the moderation state for an entry to the passed moderations state. Note that the moderation state is indicated by `id` number, **not** by moderation state name.\n\n### Featured Entries\n\n#### `PUT /api/pulse/entries/<id=number>/feature` with optional `?format=json`\n\nThis *toggles* the featured state for an entry if called by a user with moderation rights. An entry that was not featured will become featured, and already featured entries will become unfeatured when this route is called.\n\n\n## Entry Bookmarking\n\n\n### `POST /api/pulse/entries/bookmarks/ids=<a comma-separated list of integer ids>`\n\nPOSTing to bookmark a list of entries requires sending the following payload object:\n\n```\n{\n  csrfmiddlewaretoken: required csrf token string obtained from [GET /nonce]\n  nonce: required nonce string obtained from [GET /nonce]\n}\n```\nAlso note that this POST **must** be accompanied by the following header:\n\n```\nX-CSRFToken: csrfmiddlewaretoken as used above\n```\n\nA successful post will yield a JSON object:\n\n```\n{\n  status: \"Entries bookmarked.\"\n}\n```\n\nA failed post will yield\n - an HTTP 400 response if any entry id passed is invalid\n - an HTTP 403 response if the current user is not authenticated\n\n\n### `PUT /api/pulse/entries/<id=number>/bookmark`\n\nThis toggles the \"bookmarked\" status for an entry for an authenticated user. No payload is expected, and no response is sent other than an HTTP 204 on success, HTTP 403 for not authenticated users, and HTTP 500 if something went terribly wrong on the server side.\n\nThis operation requires a payload of the following form:\n```\n{\n  csrfmiddlewaretoken: required csrf token string obtained from [GET /nonce]\n  nonce: required nonce string obtained from [GET /nonce]\n}\n```\n\n### `GET /api/pulse/entries/bookmarks` with optional `?format=json`\n\nGet the list of all [entries](#entry-object-schema) that have been bookmarked by the currently authenticated user. Calling this as anonymous user yields an object with property `count` equals to `0`.  As a base URL call this returns an HTML page with formatted result, as url with `?format=json` suffix this results a JSON object for use as data input to applications, webpages, etc.\n\n## Help Types\n\n### `GET /api/pulse/helptypes/`\n\nGets the list of help types that are used by entries to indicate how people can get involved. This route yields a documentation page unless the request mimetype is set to `application/json`, or the `?format=json` query argument is passed. When requesting JSON, this route yields an object of the form:\n\n```\n[{\n  name: \"help type name\",\n  description: \"help type description\"\n},{\n  name: ...\n  description: ...\n},\n...]\n```\n\n\n## Issues\n\n### `GET /api/pulse/issues/`\n\nGets the list of internet health issues that entries can be related to. This route yields a documentation page unless the request mimetype is set to `application/json`, or the `?format=json` query argument is passed. When requesting JSON, this route yields an object of the form:\n\n```\n[{\n  name: \"issue name\",\n  description: \"issue description\"\n},{\n  name: ...\n  description: ...\n},\n...]\n```\n\n### `GET /api/pulse/issues/<Issue Name>`\n\nFetches the same data as above, but restricted to an individual issue queried for. Note that this is a URL query, not a URL argument query, so to see the data for an issue named \"Security and Privacy\" for example, the corresponding URL will be `/api/pulse/issues/Security and Privacy`.\n\n\n## Profiles\n\n### Profile object schema\n\nThis represents a full profile object schema. Changes to the schema based on the API version are mentioned below the schema.\n\n```\n  {\n    profile_id: <integer: id of the profile>,\n    custom_name: <string: a custom name for this profile or empty string if not set>,\n    name: <string: the custom name if set, otherwise the name of the user associated with this profile>,\n    location: <string: location of the person this profile is associated to>,\n    thumbnail: <string: url of the thumbnail for this profile>,\n    issues: <array: list of issue areas related to this profile as strings>,\n    twitter: <string: url to Twitter profile or empty string if not set>,\n    linkedin: <string: url to LinkedIn profile or empty string if not set>,\n    github: <string: url to Github profile or empty string if not set>,\n    website: <string: url to personal website or empty string if not set>,\n    user_bio: <string: biography of this profile>,\n    profile_type: <string: type of profile>,\n    my_profile: <boolean: whether this profile belongs to the currently authenticated user or not>,\n    entry_count: <object: specifies the counts of entries related to this profile> -\n                 {\n                    created: <integer: count of entries created by this profile>,\n                    published: <integer: count of entries published by this profile>,\n                    favorited: <integer: count of entries bookmarked by this profile>\n                 }\n}\n```\n\n#### Version 2 - `/api/pulse/v2/profiles/*`\n\nReturns a user profile object as specified by the schema exactly as shown above.\n\n#### Version 1 - `/api/pulse/v1/profiles/*`\n\nReturns a user profile object as specified by the schema above with two additional properties in the schema:\n\n- `published_entries` - <array: list of [entries](#entry-object-schema) that were published by the user associated with this profile>\n- `created_entries` - <array: list of [entries](#entry-object-schema) in which this profile was mentioned as a creator>\n\n### `GET /api/pulse/profiles/<id=number>/` with optional `?format=json`\n\nThis retrieves a single [user profile object](#profile-object-schema) with the indicated `id` as stored in the database. Any profile can be retrieved using this route even without being authenticated. As a base URL call this returns an HTML page with formatted results, as url with `?format=json` suffix this results a JSON object for use as data input to applications, webpages, etc.\n\n### `GET /api/pulse/profiles/<id=number>/entries/?...` with filter arguments, and optional `?format=json`\n\nThis retrieves a list of entries associated with a profile specified by `id`. The entries returned can be filtered based on any combination of the following query arguments:\n\n- `?created=true`: Include a list of entries (with their `related_creators`) created by this profile.\n- `?published=true`: Include a list of entries (with their `related_creators`) published by this profile.\n- `?favorited=true`: Include a list of entries (with their `related_creators`) favorited/bookmarked by this profile.\n\n__NOTE__: If none of the filters are specified, only the number of entries directly associated with the profile will be returned.\n\nBased on the filter specified, the response payload will accordingly contain a `created`, `published`, and/or `favorited` property, each of whose value is a list of their corresponding entry objects.\n\nThe order of the entries returned for each filter can be specified using the corresponding ordering query argument. The value of the query argument should be the entry field (prefixed with `-` for descending order) to use for ordering. The supported ordering arguments are:\n- `?created_ordering`: Specify the order of entries created by this profile.\n- `?published_ordering`: Specify the order of entries published by this profile.\n- `?favorited_ordering`: Specify the order of entries bookmarked/favorited by this profile.\n\nFor example, `?created&created_ordering=-id` will return the entries created by the profile reverse ordered by the entry `id`.\n\nThe schema of the entry objects is specified below:\n\n```\n{\n  id: <integer: id of the entry>,\n  title: <string: title of the entry>,\n  content_url: <string: external url for the contents of the entry>,\n  thumbnail: <string: url to a thumbnail image for the entry>,\n  is_bookmarked: <boolean: whether this entry is bookmarked for the currently authenticated user>,\n  related_creators: <array: list of related creator objects (see below)>\n}\n```\n\nDepending on the API version specified, the `related_creator` object schema will vary as mentioned [here](#related-creator-object-schema).\n\n### `GET /api/pulse/profiles/?...` with filter arguments, and optional `format=json`\n\nReturns a list (paginated for `v3` and above) of user profile objects each with the following schema:\n```\n{\n  id: <integer: id of the profile>,\n  name: <string: the custom name if set, otherwise the name of the user associated with this profile>,\n  location: <string: location of the person this profile is associated to>,\n  thumbnail: <string: url of the thumbnail for this profile>,\n  user_bio: <string: biography of this profile>,\n  profile_type: <integer: the id of the profile type associated with this profile>,\n  is_active: <boolean: true if the profile has a user attached to it, false otherwise>,\n  is_group: <boolean: whether this profile belongs to a group>,\n\n  // The following properties will only be included if they are enabled\n  // for the profile\n  affiliation: <string: affiliations associated with this profile>,\n  user_bio_long: <string: a longer biography for this profile>,\n  program_type: <integer: the id of the program type this profile is associated with>,\n  program_year: <integer: the id of the program year this profile belongs to>\n}\n```\n\nThe schema for the paginated payload returned for `v3` and above is:\n```\n{\n    \"count\": <integer: total number of profiles found for this query>,\n    \"next\": <string: url to the next page of profiles> or null,\n    \"previous\": <string: url to the previous page of profiles> or null,\n    \"results\": <array: list of profile objects (see above)>\n}\n```\n\n__NOTE__: Versions below `v3` will not use the above schema and will follow the [general profile object schema](#profile-object-schema) instead.\n\nThis route supports filtering based on properties of profiles.\n\n__NOTE__: At least one filter or search query from below must be specified, otherwise an empty array is returned in the payload.\n\n#### Filters and Search Queries\n\n- `?search=...`: search for profiles by their name, user bio, affiliation, or their location\n- `?profile_type=...`: filter the list by profile types `board member`, `fellow`, `grantee`, `plain`, or `staff`.\n- `?program_type=...`: filter the list by program types `media fellow`, `open web fellow`, `science fellow`, `senior fellow`, or `tech policy fellow`.\n- `?program_year=...`: filter the list by program year in the range 2015-2019 (inclusive).\n- `?is_active=<true or false>`: filter profiles by their active state.\n- `?name=...`: filter profiles by their name. Supports partial and full matches.\n- `?limit=...`: limit the number of results to some maximum.\n\n#### Other Supported Queries\n\n- `?ordering=...` - You can sort these results using the `ordering` query param, passing it either `id`, `custom_name`, or `program_year` (reversed by prefixing a `-`, e.g. `-custom_name` for descending alphabetical order based on the custom profile name).\n- `?basic=<true or false>` - This provides a way to only get basic information about profiles. Each profile object in the list will only contain the `id` of the profile and the `name` of the profile. This query can be useful for providing autocomplete options for profiles. __NOTE__ - This query is not compatible with version 1 of the API.\n- `?page_size=<number>` - Specify how many profiles will be in each page of the API call (only for `v3` and above)\n- `?page=<number>` - Page number of the API call (`v3` and above)\n\n### `GET /api/pulse/myprofile/` with optional `?format=json`\n\nThis retrieves the **editable** [user profile](#profile-object-schema) for the currently authenticated user without the `name` and `my_profile` properties in the payload. An unauthenticated user will receive an HTTP 403 Forbidden response if they try to access this route. As a base URL call this returns an HTML page with formatted results, as url with `?format=json` suffix this results a JSON object for use as data input to applications, webpages, etc.\n\n### `PUT /api/pulse/myprofile/`\n\nAllows an authenticated user to update their profile data. The payload that needs to be passed into this PUT request is:\n\n```\n{\n\tcustom_name: <optional string: a custom name for this profile>,\n\tlocation: <optional string: location of the person this profile is associated to>,\n\tthumbnail: <optional object: a thumbnail object (see below) with the profile's image>,\n\tissues: <optional array: list of issue areas related to this profile as strings>,\n\ttwitter: <optional string: url to Twitter profile>,\n\tlinkedin: <optional string: url to LinkedIn profile>,\n\tgithub: <optional string: url to Github profile>,\n\twebsite: <optional string: url to personal website>,\n\tuser_bio: <optional string: biography of this profile>\n}\n```\n\nThe thumbnail object should have the following schema:\n```\n{\n  name: <string: name of the image file>,\n  base64: <string: the base64 encoded binary representation of the image file's bytes>\n}\n```\nThe thumbnail object can also be `null` to set the thumbnail to `null` in the database (i.e. delete the image).\n\nAlso note that this PUT **must** be accompanied by the following header:\n\n```\nX-CSRFToken: required csrf token string obtained from [GET /nonce]\n```\n\n#### Updating extended profile information\n\nIf a user's profile has the `enable_extended_information` flag set to `True`, then there are additional fields that can be updated by this user:\n\n```\n{\n  ...\n  program_type: <string: the name of the program type this profile is associated with (must already exist)>,\n  program_year: <string: the program year this profile belongs to (must already exist)>,\n  affiliation: <string: affiliations associated with this profile>,\n  user_bio_long: <string: a longer biography for this profile>\n}\n```\n\n### `GET /api/pulse/profiles/categories`\n\nThis retrieves an object containing exhaustive lists of the profile types, program types, and program years available to use when filtering profiles\n\n```\n{\n    profile_types: [\"plain\", \"staff\", \"fellow\", \"board member\", \"grantee\"],\n    program_types: [\"tech policy fellowship\", \"mozfest speaker\"],\n    program_years: [\"2014\", \"2015\", \"2016\", \"2017\", \"2018\"]\n}\n```\n\n## Tags\n\n### `GET /api/pulse/tags/` with optional `?format=json`\n\nThis retrieves the list of all tags known to the system. When requesting JSON, this route yields an object of the form:\n\n```\n[\n  \"tag 1\",\n  \"tag 2\",\n  \"tag 3\",\n  ...\n]\n```\n\n### Filtering\n\nThe above route can also be passed a `?search=...` query argument, which will filter the tag list based on `starts-with` logic, such that searching on `?search=abc` will find all tags that start with the partial string `abc`.\n\n# Syndication\n\nThere are several syndication routes for RSS/Atom feeds available - these do not use the `/api/pulse` prefix and they are not versioned:\n\n## RSS\n\n### `GET /rss/latest`\n\nReplies with an RSS feed consisting of (a subset of) the latest entries that were published to Mozilla Pulse.\n\n### `GET /rss/featured`\n\nReplies with an RSS feed consisting of (a subset of) only those entries that are currently considered featured content.\n\n## Atom\n\n### `GET /atom/latest`\n\nReplies with an Atom feed consisting of (a subset of) the latest entries that were published to Mozilla Pulse.\n\n### `GET /atom/featured`\n\nReplies with an Atom feed consisting of (a subset of) only those entries that are currently considered featured content.\n\n\n\n---\n\n# Getting up and running for local development\n\n## Setup\n\n**Requirements**:\n- python 3.7 (https://www.python.org/)\n- invoke (with python installed, `pip install invoke` - if you are on legacy hardward or a legacy OS that still has python 2.7 installed, `pip3 install invoke`).\n- PostgreSQL, consult the internet for how to install this for your Operating System.\n\n- Run `inv setup`.\n- Start your server with `inv runserver`.\n- Your admin login is `admin@mozillafoundation.org` with password `admin`.\n- (Optional) To enable Google and/or GitHub login, follow the [instructions below](#setting-up-social-authentication).\n\nUpdate your branch by doing a git pull and `inv catchup`.\nWhen switching branches, get a new virtualenv and database by running `inv new-env`.\n\nYou can get a full list of inv commands by running `inv -l`.\n\nInstructions on how to setup this project using `nix-shell` (Linux and MacOS) are [available here](#nix-shell).\n\n## Setting up Social Authentication\n\n### Google\n\n1. Set up a [Google OAuth client](https://console.developers.google.com/apis/credentials).\n  - If you need to set up an oauth consent screen, click through and:\n    - make sure it's set to \"testing\" (not \"production\") and\n    - make sure it's set to \"external\" (not \"internal\")\n  - Use `http://localhost:8000` as the \"Authorized Javascript URL\"\n  - Use `http://localhost:8000/accounts/google/login/callback/` as the \"Authorized Redirect URL\"\n  - Keep the client ID and client secret handy\n2. Run the server with `inv runserver`\n3. Go to http://localhost:8000/admin and log in using the superuser credentials [noted above](#requirements)\n4. Add and save a new \"Social Application\" instance:\n  - The \"Provider\" should be `Google`\n  - The \"Name\" can be anything that will identify your Google social application\n  - Fill in the client ID and client secret (leave \"key\" empty)\n  - move `example.com` from the \"Available sites\" list to the \"Chosen sites\" list\n5. Log out of the admin interface.\n\n### GitHub\n\n1. Log in to GitHub and setup a [GitHub OAuth client](https://github.com/settings/applications/new).\n  - Use `http://localhost:8000` as the \"Homepage URL\"\n  - Use `http://localhost:8000/accounts/github/login/callback/` as the \"Authorized Callback URL\"\n  - Keep the client ID and client secret handy\n2. Run the server with `inv runserver`.\n3. Go to http://localhost:8000/admin and log in using the superuser credentials [noted above](#requirements)\n4. Add and save a new \"Social Application\" instance:\n  - The \"Provider\" should be `GitHub`\n  - The \"Name\" can be anything that will identify your GitHub social application\n  - Fill in the client ID and client secret\n  - move `example.com` from the \"Available sites\" list to the \"Chosen sites\" list\n5. Logout of the admin interface.\n\n## Setting up Email\n\nThis app allows Django to send email to users for various purposes (e.g. email verification during social login). An email backend is required to make this possible and the email backend is determined based on the value of the `USE_CONSOLE_EMAIL` environment variable.\n\nIf it is set to `True` (or no value is provided), all emails will be sent to the terminal window in which Pulse API is running.\n\nIf it is set to `False`, you are required to use [Mailgun](https://www.mailgun.com/) as the email SMTP server and configure the `MAILGUN_SMTP_SERVER`, `MAILGUN_SMTP_PORT`, `MAILGUN_SMTP_LOGIN`, and `MAILGUN_SMTP_PASSWORD` environment variables.\n\n## Generating fake data\n\nFake model data can be loaded into your dev site with the following command:\n\n- `inv manage load_fake_data`\n\n`inv manage \"load_fake_data -e 30\"` will run the full `load_fake_data` script, but changes the number of entries per variations to 30 instead of 20.\n\nOptions available:\n\n- `--delete`: Delete bookmarks, creators, entries, profiles, tags and users from the database,\n- `--seed`: A seed value to pass to Faker before generating data.\n- `-f`, `--fellows-count`: The number of fellows to generate per program type, per year. Defaults to 3\n- `-u`, `--users-count`: The number of users to generate per possible variations. Default: 1, variations: 80\n- `-e`, `--entries-count`: The number of entries to generate per possible variations. Default: 20, variations: 16\n- `-t`, `--tags-count`: The number of tags to generate. Default: 6\n\n## Local development\n\n### How to use\n\n- Install [Invoke](http://www.pyinvoke.org/installing.html) using [pipx](https://pypi.org/project/pipx/).\n- Run `inv setup`.\n- Start your server with `inv runserver`.\n\nUpdate your branch by doing a git pull and `inv catchup`.\nWhen switching branches, get a new virtualenv and database by running `inv new-env`.\n\n### Using invoke\n\nInvoke is a task execution tool. Instead of running `./pulsevenv/bin/python manage.py runserver`, you can run `inv\nrunserver`.\n\nAvailable tasks:\n```\n  catch-up (catchup)                           Install dependencies and apply migrations\n  makemigrations                               Creates new migration(s) for apps\n  manage                                       Shorthand to manage.py. inv docker-manage \"[COMMAND] [ARG]\"\n  migrate                                      Updates database schema\n  new-db                                       Create a new database with fake data\n  pip-compile (docker-pip-compile)             Shorthand to pip-tools. inv pip-compile \"[COMMAND] [ARG]\"\n  pip-compile-lock (docker-pip-compile-lock)   Lock prod and dev dependencies\n  pip-sync (docker-pip-sync)                   Sync your python virtualenv\n  runserver                                    Start a web server\n  setup (new-env)                              Automate project's configuration and dependencies installation\n  test                                         Run tests\n```\n\n### How to install or update dependencies?\n\n**Note on [pip-tools](https://github.com/jazzband/pip-tools)**:\n- Only edit the `.in` files and use `invoke pip-compile-lock` to generate `.txt` files.\n- Both `(dev-)requirements.txt` and `(dev-)requirements.in` files need to be pushed to Github.\n- `.txt` files act as lockfiles, where dependencies are pinned to a precise version.\n\nDependencies live on your filesystem: you don't need to rebuild the `backend` image when installing or updating dependencies.\n\n**Install packages:**\n\n- Modify the `requirements.in` or `dev-requirements.in` to add the dependency you want to install.\n- Run `invoke pip-compile-lock`.\n- Run `invoke pip-sync`.\n\n**Update packages:**\n\n- `invoke pip-compile \"-upgrade (dev-)requirements.in\"`: update all (the dev) dependencies.\n- `invoke pip-compile \"--upgrade-package [PACKAGE](==x.x.x)\"`: update the specified dependency. To update multiple dependencies, you always need to add the `-P` flag.\n\nWhen it's done, run `inv pip-sync`.\n\n### Nix-shell\n\nIf you want to use nix-shell to isolate your dev environment:\n\n- Install [Nix](https://nixos.org/nix/): `curl https://nixos.org/nix/install | sh`,\n- In `network-pulse-api` directory, enter `nix-shell`. It will install Python 3.7 and invoke.\n- Enter `inv setup` to setup the project.\n- When it's done, use invoke commands as usual.\n\nIf you want to use another shell instead of bash, use `nix-shell --command SHELL` (`nix-shell --command zsh` for example).\n\n#### Direnv\n[Direnv](https://direnv.net/) will load your `nix-shell` automatically when you enter the `network-pulse-api` directory.\n\nTo use it:\n- Follow the instruction to [install direnv](https://direnv.net/) on your system.\n- Allow direnv to auto-load your nix-shell by entering `direnv allow .` in the `network-pulse-api` directory.\n\n\n### Testing the API using the \"3rd party library\" test file\n\nFire up a localhost server with port 8080 pointing at the `public` directory (some localhost servers like [http-server](https://npmjs.com/package/http-server) do this automatically for you) and point your browser to [http://localhost:8080](http://localhost:8080). If all went well (but read this README.md to the end, first) you should be able to post to the API server running \"on\" http://localhost:8000\n\n\n## Environment variables\n\nHeroku provisions some environments on its own, like a `PORT` and `DATABASE_URL` variable, which this codebase will make use of if it sees them, but these values are only really relevant to Heroku deployments and not something you need to mess with for local development purposes.\n\nConfigure the following environment variables as needed in your `.env` file. All variables are optional, but some are highly recommended to be set explicitly.\n\n### Authentication variables\n\n- `ALLOW_SIGNUP` &mdash; Determines whether signing up for a new account is permitted. **Defaults to `True`.**\n- `LOGIN_ALLOWED_REDIRECT_DOMAINS` &mdash; A comma-separated list of domains that are allowed to be redirected to after logging in a user. **Defaults to  `localhost:3000`.**\n- `AUTH_STAFF_EMAIL_DOMAINS` &mdash; A comma-separated list of email domains that should be considered \"safe\" to make as \"staff\" in Django. **Defaults to `mozillafoundation.org`.**\n- `AUTH_REQUIRE_EMAIL_VERIFICATION` &mdash; A boolean indicating whether a user needs to verify their email attached to a social account (e.g. Github) before being able to login. **Defaults to `False`.**\n- `AUTH_EMAIL_REDIRECT_URL` &mdash; The url to redirect to after a user verifies their email to login successfully. **Defaults to `/`.**\n- `PULSE_CONTACT_URL` &mdash; A contact url for users to file questions or problems when authenticating. **Defaults to an empty string.**\n- `USE_RECAPTCHA` &mdash; Determines whether the login route is locked behind Google reCAPTCHA or not. **Defaults to `True`**.\n- `RECAPTCHA_SECRET` &mdash; The Google reCAPTCHA secret value. **Defaults to an empty string.**\n\n### Email variables\n\n- `USE_CONSOLE_EMAIL` &mdash; A boolean to indicate whether the terminal should be used to display emails sent from Django, instead of an email backend. Useful for local development. **Defaults to `True`.**\n- `EMAIL_VERIFICATION_FROM` &mdash; The email address used in the \"From:\" section of emails sent by Django. **Defaults to `webmaster@localhost`.**\n\nThese variables are only used (and are required) if `USE_CONSOLE_EMAIL` is set to `False`.\n\n- `MAILGUN_SMTP_SERVER` &mdash; The url for the Mailgun SMTP server that will be used to send emails.\n- `MAILGUN_SMTP_PORT` &mdash; The port (as a number) to connect to the Mailgun server.\n- `MAILGUN_SMTP_LOGIN` &mdash; The login user credential to use to authenticate with the Mailgun server.\n- `MAILGUN_SMTP_PASSWORD` &mdash; The password to authenticate with the Mailgun server.\n\n### Data storage variables\n\n- `DATABASE_URL` &mdash; The url to connect to the database. **Defaults to `False` which forces Django to create a local SQLite database file.**\n- `USE_S3` &mdash; A boolean to indicate whether to store user generated assets (like images) on Amazon S3. **Defaults to `False`.**\n\nThese variables are only used (and are required) if `USE_S3` is set to `True`.\n\n- `AWS_ACCESS_KEY_ID` &mdash; Amazon user Access Key for authentication.\n- `AWS_SECRET_ACCESS_KEY` &mdash; Amazon user Secret Key for authentication.\n- `AWS_STORAGE_BUCKET_NAME` &mdash; S3 bucket name where the assets will be stored.\n- `AWS_STORAGE_ROOT` &mdash; S3 root path in the bucket where assets will be stored.\n- `AWS_S3_CUSTOM_DOMAIN` &mdash; A custom domain (for e.g. Amazon CloudFront) used to access assets in the S3 bucket.\n\n### Security variables\n\n- `DEBUG` *(recommended)* &mdash; A boolean that indicates whether Django should run in debug mode. DO NOT SET THIS TO `True` IN PRODUCTION ENVIRONMENTS SINCE YOU RISK EXPOSING PRIVATE DATA SUCH AS CREDENTIALS. **Defaults to `True`.**\n- `SECRET_KEY` *(recommended)* &mdash; A unique, unpredictable string that will be used for cryptographic signing. PLEASE GENERATE A NEW SECRET STRING FOR PRODUCTION ENVIRONMENTS. **Defaults to a set string of characters.**\n- `SSL_PROTECTION` *(recommended)* &mdash; A catch-all boolean that indicates whether SSL encryption, XSS filtering, content-type sniff protection, HSTS, and cookie security should be enabled. THIS SHOULD LIKELY BE SET TO `True` IN A PRODUCTION ENVIRONMENT. **Defaults to `False`.**\n- `ALLOWED_HOSTS` &mdash; A comma-separated list of host domains that this app can serve. This is meant to prevent HTTP Host header attacks. **Defaults to a list of `localhost`, `localhost`, `network-pulse-api-staging.herokuapp.com`, and `network-pulse-api-production.herokuapp.com`.**\n- `CORS_ORIGIN_REGEX_WHITELIST` *(recommended)* &mdash; A comma-separated list of python regular expressions matching domains that should be enabled for CORS. **Defaults to anything running on `localhost` or on `localhost`.**\n- `CORS_ORIGIN_WHITELIST` &mdash; A comma-separated list of domains that should be allowed to make CORS requests. **Defaults to an empty list.**\n- `CSRF_TRUSTED_ORIGINS` &mdash; A comma-separated list of trusted domains that can send POST, PUT, and DELETE requests to this API. **Defaults to a list of `localhost:3000`, `localhost:8000`, `localhost:8080` , `localhost:8000`, and `localhost:3000`.**\n\n### Front-end variables\n\n- `PULSE_FRONTEND_HOSTNAME` &mdash; The hostname for the front-end used for Pulse. This is used for the RSS and Atom feed data. **Defaults to `localhost:3000`.**\n\n### Review Apps bot variables\n\n- `GITHUB_TOKEN` &mdash; Used to get the PR title.\n- `SLACK_WEBHOOK_RA` &mdash; Incoming webhook of the `HerokuReviewAppBot` Slack app.\n\n### Miscellaneous variables\n\n- `HEROKU_APP_NAME` &mdash; A domain used to indicate if this app is running as a review app on Heroku. This is used to determine if social authentication is available or not (since it isn't for review apps). **Defaults to an empty string.**\n\n## Deploying to Heroku\n\nWhile for local development we provide a `sample.env` that you can use as default environment variables, for Heroku deployment all the above-stated variables need to be real things. **Make sure to add these to the Heroku config!**\n\n### Review App\n\n#### Review App for PRs\n\nOpening a PR will automatically create a Review App in the `network-pulse-api` pipeline. A slack bot posts credentials and links to Review Apps in to the `mofo-ra-pulse-api` Slack channel.\n\n*Note:* This only work for Mo-Fo staff: you will need to manually open a Review App on Heroku for PRs opened by external contributors.\n\n#### Review App for branches\n\nYou can manually create a review app for any branch pushed to this repo. It's useful if you want to test your code on Heroku without opening a PR yet.\n\nTo create one:\n- log into Heroku.\n- Go to the `network-pulse-api` pipeline.\n- Click on `+ New app` and select the branch you want to use.\n\nThe review app slack bot will post a message in the `mofo-ra-pulse-api` channel with links and credentials as soon as the review app is ready.\n\n#### Environment variables:\n\n- `GITHUB_TOKEN`: GITHUB API authentication,\n- `SLACK_WEBHOOK_RA`: Webhook to `mofo-ra-pulse-api`\n\nNon-secret envs can be added to the `app.json` file. Secrets must be set on Heroku in the `Review Apps` (pipelines' `settings` tab).\n\n## Debugging all the things\n\nYou may have noticed that when running with `DEBUG=TRUE`, there is a debugger toolbar to the right of any page you try to access. This is the [Django Debug Toolbar](https://django-debug-toolbar.readthedocs.io/en/stable/), and that link will take you straight to the documentation for it on how to get the most out of it while trying to figure out what's going on when problems arise.\n\n\n## Resetting your database because of incompatible model changes\n\nWhen working across multiple branches with multiple model changes, it sometimes becomes necessary to reset migrations and build a new database from scratch. You can either do this manually by deleting your `db.sqlite3` as well as all model migration files that start with a number (**except** for the 0001 migration for `issues`, which instantiates various records in addition to boostrapping the issues table, and should never be deleted), but because this is inconvenient, there is a helper script to do this for you.\n\nrun `pulsevenv/bin/python reset_database.py` and the steps mentioned above will be run automatically.\n\n**Note:** This does wipe *everything* so you will still need to call `pulsevenv/bin/python manage.py createsuperuser` afterwards to make sure you have a super user set up again.\n\n## Migrating data from Google sheets\n\nTo migrate data, export JSON from the Google Sheets db, and save it in the root directory as `migrationData.json`. Then run `pulsevenv/bin/python migrate.py`. This generates `massagedData.json`.\nIn `public/migrate.html`, update the endpoint to be the address of the one you're trying to migrate data into. If it's a local db, leave as is.\nSpin up a server from the `public` folder on port 8080. Log in to your API using Oauth (either the hosted site or `localhost:8000` if doing this locally)\nVisit `http://localhost:8080/migrate.html`, paste the contents of `massagedData.json`, and submit. It will process the entire array of entries one at a time, POSTing them to the server. Check your developer console and network requests if it doesn't complete after a minute or two.\n"
},
{
  "name": "gud",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".dockerignore",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierignore",
      "CHANGELOG.md",
      "Dockerfile",
      "README.md",
      "docs",
      "package-lock.json",
      "package.json",
      "public",
      "rollup.config.js",
      "scripts",
      "server.js",
      "src",
      "tests"
    ],
    "/docs": [
      "Gemfile",
      "Gemfile.lock",
      "README.md",
      "_config.yml",
      "_includes",
      "assets",
      "index.md",
      "js"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Growth and Usage Dashboard\n\nThis is a light, server-powered dashboard showing the smoot growth metrics. The frontend talks to a tiny node server by passing it the segments / usage criteria / etc. necessary for the query, and the tiny web server sends the query to be run by BigQuery.\n\n## Community\n\nPost in `#gud` on Slack for any other questions.\n\n## Reporting Issues and Feature Requests\n\nFeel free to file an issue in this repository w/ questions / concerns.\n\n## Development\n\nDependencies:\n\n\u2013 Node 11.5.0 / current NPM version\n\nTo install:\n\n- Make sure you nave Node / npm.\n- run `npm install` in the directory where you cloned this repository.\n\nTo run locally:\n\nThe GCP commands in these instructions will not work unless you work under Katie Parlante. If you want to run this project and you don't work under Katie Parlante, please contact Jason Thomas or Blake Imsland.\n\n- Run `gcloud auth application-default login`\n- Run `gcloud config set project moz-fx-data-shared-prod`\n- To run the server, run `node server` which starts a tiny web server on port 3000 (go to `localhost:3000` in your browser).\n- To build / update the frontend, type `npm run dev`, which spins up another web server (that we're not going to use, sorry for the redundancy here) and builds the little dev version of the frontend.\n\u2013 I'll make it so you don't have to run two servers like this at some point, but this works for now!\n"
},
{
  "name": "pdfjs-dist",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bower.json",
      "build",
      "cmaps",
      "image_decoders",
      "legacy",
      "lib",
      "package.json",
      "standard_fonts",
      "types",
      "web",
      "webpack.js"
    ]
  },
  "makefile": null,
  "readme": "# PDF.js\n\nPDF.js is a Portable Document Format (PDF) library that is built with HTML5.\nOur goal is to create a general-purpose, web standards-based platform for\nparsing and rendering PDFs.\n\nThis is a pre-built version of the PDF.js source code. It is automatically\ngenerated by the build scripts.\n\nFor usage with older browsers or environments, without support for modern\nfeatures such as `async`/`await`, optional chaining, nullish coalescing,\nand private `class` fields/methods; please see the `legacy/` folder.\n\nSee https://github.com/mozilla/pdf.js for learning and contributing.\n"
},
{
  "name": "security-standards-research-2020",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "app.js",
      "package-lock.json",
      "package.json",
      "public",
      "scripts",
      "template-helpers.js",
      "views"
    ]
  },
  "makefile": null,
  "readme": "### Security Standardisation Research Conference 2020\n\nhttps://ssr2020.mozilla.org\n"
},
{
  "name": "dogear",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "book.toml",
      "docs",
      "src"
    ],
    "/docs": [
      "README.md",
      "SUMMARY.md",
      "application.md",
      "divergences.md",
      "integration.md",
      "matching.md",
      "merging.md",
      "trees.md"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Dogear\n\n**Dogear** is a library that implements bookmark tree merging for Firefox Sync. It takes two trees\u2014a valid, consistent local tree, and a possibly inconsistent remote tree\u2014and produces a complete merged tree, with all conflicts and inconsistencies resolved.\n\nDogear implements the merge algorithm only; it doesn't handle syncing, storage, or application. It's up to the crate that embeds Dogear to store local and incoming bookmarks, describe how to build a tree from a storage backend, persist the merged tree back to storage, and upload records for changed bookmarks.\n\n## Requirements\n\n* Rust 1.31.0 or higher\n\n\n## Updating this package\nOnce a new version of Dogear is ready to release. The new version will need to be published to [crates.io](https://crates.io/crates/dogear). Dogear follows the documentation detailed in the [Cargo book](https://doc.rust-lang.org/cargo/reference/publishing.html#publishing-a-new-version-of-an-existing-crate).\n### Steps to publish a new verison\n1. Bump the version in the `Cargo.toml` file\n2. Run `cargo publish --dry-run`\n    - Validate it does what you want it to do\n3. Run `cargo publish` and follow the steps cargo provides\n"
},
{
  "name": "service-map",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Pipfile",
      "README.md",
      "api.py",
      "bucket.py",
      "config.dev.yml",
      "config.prod.yml",
      "cron.py",
      "models",
      "package-lock.json",
      "package.json",
      "serverless.yml",
      "tests",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "service-map\n===========\n\nService map is a set of micro serverless services that marry\n- Services\n- Their risk levels\n- Indicators about the security posture of the service\n- Various control metrics\n\ninto an ongoing risk score for each service based on what we know about it.\n\n## Architecture\nThe goal is to use as little servers, containers, etc as possible and rely on serverless tech such as lambda and dynamodb to focus on the business logic rather than the infra.\n\n\n## Deployment\nFirst setup creds in credstash for the functions to use:\nThe cron function needs oauth creds as per the gspread docs:\nhttps://gspread.readthedocs.io/en/latest/oauth2.html\n\nStore these in credstash as a json blob:\n```\ncredstash  --profile devadmin put serviceapi.gdrive @yourfilename.json app=serviceapi\n```\nwhere devadmin is the name of your aws profile in ~/.aws/config specifying where you will be deploying.\n\nThen deploy:\n\n    ```\n    pipenv shell\n    sls deploy\n\n    ```\n\n## Testing\nGet credstash creds for oidc auth:\n```\ncredstash --profile devadmin get serviceapi.oidc_client_secret app=serviceapi\ncredstash --profile devadmin get serviceapi.oidc_client_id app=serviceapi\n```\n\nSet env variables:\n```\nexport API_URL=\"https://serviceapi.security.allizom.org\"\nexport CLIENT_ID = value from above\nexport CLIENT_SECRET =  value from above\n```\nand run pytest"
},
{
  "name": "campaign-form",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "app.js",
      "field_config.json",
      "index.js",
      "lib",
      "package-lock.json",
      "package.json",
      "public",
      "routes",
      "views"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Campaign Form\n\nThis is a basic campaign form to submit issues to a GitHub repository. This has previously been used in campaigns such as [Firefox Foxfooding](https://community.mozilla.org/en/campaigns/firefox-foxfooding-campaign/) and [FxA testing](https://community.mozilla.org/en/campaigns/fxa-settings-test-day/). These campaigns previously lived in separate repos, upcoming campaigns will be handled in this repository as they come up.\n\nNote that this form is basic and might not support everything you need. Several parts could be abstracted, however these campaigns are mostly short-lived and code gets adjusted as needed. PRs are welcome though!\n\n## Setting up the server\n\n### Requirements\n\n* Make sure you have Node.js installed\n* Create a repository to hold the submitted issues\n* Create a \"triage\" label for issues in that repository\n* Create the necessary labels in the GitHub repo according to `lib/githubBackend.js`\n* Create a personal access token for your GitHub user\n\n### Starting\n\nThen you can start the server with the following command. Make sure to replace the placeholders with your data.\n\n```\n$ git clone <URL>\n$ cd campaign-form\n$ npm ci\n$ GITHUB_TOKEN=<yourGitHubToken> OWNER=<yourGitHubUsername> REPO=<yourGitHubRepoForIssues> SESSION_SECRET=someSECRET BASE_URL=http://localhost:4000/ AWS_ACCESS_KEY_ID=\"<yourAWSAccessKeyID\" AWS_SECRET_ACCESS_KEY=\"<yourAWSAccessKey>\" AWS_S3_BUCKET_NAME=\"<yourAWSS3BucketName>\" npm start\n```\n\nNote: you can leave off the AWS configuration, however attachments won't be uploaded and won't be shown in the resulting GitHub issue.\n\nNow you can access the website for it at ```localhost:4000```.\n\n## Environment variables\n\nThe following environment variables are needed. Note that you will need to set these up in Heroku as well!\n\n| Variable | Description | Example\n|---|---|---|\n| AWS_ACCESS_KEY_ID | AWS Access Key ID to upload attachments | ... |\n| AWS_SECRET_ACCESS_KEY | AWS Access Key Secret to upload attachments | ... |\n| AWS_S3_BUCKET_NAME | AWS S3 Bucket name to upload attachments to | campaign-form-uploads |\n| BASE_URL | URL of the form | https://foxfooding-form.mozilla.community/ |\n| GITHUB_TOKEN | Token for the GitHub user to post issues | ... |\n| OWNER | GitHub username / organization hosting the reporting repo | mozilla |\n| REPO | GitHub repository name to host the issues | campaign-form |\n| SESSION_SECRET | Random string used as session secret | foobarbaz.... |\n\n## Changes needed for new campaigns\n\n### Coding\n\n* Check the `fields_config.json` file and adjust if needed\n* Adjust `lib/formHandling.js` to work together with the defined fields\n* Adjust `views/index.pug` to reflect correct strings and fields (if a previous campaign got stopped, you might need to revert previous changes to get back the form fields and then adjust them)\n* Adjust logo in `public/` and its alt text in `views/index.pug`\n* Adjust title in `views/layout.pug`\n* Check `lib/githubBackend.js` to adjust possible version / OS GitHub labels and create them in the GitHub issues repo\n* Adjust `public/client.js` if any fields need pre-filling (such as UA)\n* Adjust header background color in `public/style.css` if needed\n\n### Operation / Admin\n\n* Add `mozilla-campaign-form-bot` GitHub user as collaborator to issues repository\n"
},
{
  "name": "translation-service",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".gitmodules",
      "3rd_party",
      "CMakeLists.txt",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "scripts",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "#!make\n\n.ONESHELL:\nSHELL=/bin/bash\n\nbuild-docker:\n\tdocker build -t translation-service .\n\ncompile:\n\tbash scripts/compile.sh\n\ndownload-models:\n\tbash scripts/download-test-models.sh\n\nrun:\n\tdocker run --name translation-service -it --rm -v $$(pwd)/models:/models -p 8080:8080 -e PORT=8080 translation-service\n\ndebug:\n\tdocker run --name translation-service -it --rm -v $$(pwd):/app -v $$(pwd)/models:/models -e PORT=8080 -p 8080:8080 translation-service bash\n\ncall:\n\tcurl --header \"Content-Type: application/json\" \\\n      --request POST \\\n      --data '{\"from\":\"es\", \"to\":\"en\", \"text\": \"Hola Mundo\"}' \\\n      http://0.0.0.0:8080/v1/translate\n\npython-env:\n\tpip3 install pytest locust\n\ntest:\n\tpytest tests/integration\n\nload-test-ui:\n\tlocust -f tests/load/stress.py --host http://0.0.0.0:8080 --tags mixed\n\nload-test:\n\tlocust -f tests/load/stress.py --host http://0.0.0.0:8080 --headless --tags mixed --spawn-rate 1 --users 500\n\nsetup-models:\n\tgit clone https://github.com/mozilla/firefox-translations-models\n\trm -rf models\n\tmkdir models\n\tmv firefox-translations-models/models/dev/* models/\n\tmv firefox-translations-models/models/prod/* models/\n\tgunzip -r models/\n\trm -rf firefox-translations-models\n",
  "readme": "# Translation service\n\n[<img src=\"https://img.shields.io/badge/dockerhub-images-important.svg?logo=Docker\">](https://hub.docker.com/r/mozilla/translation-service)\n\n\nHTTP service that uses [bergamot-translator](https://github.com/mozilla/bergamot-translator) and compressed neural machine translation [models](https://github.com/mozilla/firefox-translations-models) for fast inference on CPU.\n\n## Running locally\n1. Install Git LFS `https://git-lfs.github.com/`\n2. git clone this repo\n3. `make setup-models`\n4. `make build-docker`\n5. `make run`\n6. `make call`\n\n## Calling the service\n````\n$ curl --header \"Content-Type: application/json\" \\\n      --request POST \\\n      --data '{\"from\":\"es\", \"to\":\"en\", \"text\": \"Hola Mundo\"}' \\\n      http://0.0.0.0:8080/v1/translate\n> {\"result\": \"Hello World\"}\n`````\n\n## Service configuration\n\nDirectory that contains models ('esen', 'ende' etc.) should be mounted to `/models` in Docker container.\n\nEnvironment variables to set in container:\n\n`PORT` - service port (default is 8000)\n\n`LOGGING_LEVEL` - ERROR, WARNING, INFO or DEBUG (default is INFO)\n\n`WORKERS` - number of bergamot-translator workers (default is 1). 0 - automatically set as number of available CPUs.\nIt is recommended to minimize workers and scale horizontaly with k8s means.\n\n\n## Testing\n\n`make python-env` - install pip packages\n\n`make test` - to run integration API tests\n\n`make load-test` - to run a stress test (requires more models to download that unit tests)\n"
},
{
  "name": "community-portal",
  "files": {
    "/": [
      ".babelrc",
      ".github",
      ".gitignore",
      "404.php",
      "README.md",
      "SECURITY.md",
      "acf",
      "buddypress",
      "campaign-status.php",
      "class-privacysettings.php",
      "composer.json",
      "countries.php",
      "css",
      "fonts",
      "footer.php",
      "front-page.php",
      "functions.php",
      "header.php",
      "images",
      "inc",
      "index.php",
      "js",
      "languages.php",
      "languages",
      "lib",
      "package-lock.json",
      "package.json",
      "page-activities.php",
      "page-campaigns.php",
      "page-edit-event.php",
      "page-events.php",
      "page-groups.php",
      "page-newsletter.php",
      "phpcs.xml",
      "plugins",
      "pronouns.php",
      "remove-page.php",
      "scss",
      "search.php",
      "single-activity.php",
      "single-campaign.php",
      "single-static-page.php",
      "templates"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Community Portal \n\nThis repo will contain all the theme files for the new Mozilla community portal.  \n\n## Setup Wordpress and Install the Theme\n\n- Keep in mind that when you\u2019re serving the site, your server needs to be running at the root of the Wordpress install. The URL in the address bar when you view the site should not contain any sub-directories.\n- During development it\u2019s recommended that you use a local URL.\n\t- NOTE: If you have Docker running you may need to quit it, as it may cause port conflicts with Local or your virtual host. \n- In a new directory, set up a fresh Wordpress install (wordpress.org)\n- Go to the Mozilla community portal repo, and clone or download the theme files. They need to go into the `wp-content/themes` directory of the Wordpress install you just created.\n- Navigate to this theme folder in your terminal, and run `npm install` to install dependencies.\n- Run `npm run compile` to compile the .scss files and generate a `style.css` file for the theme, to live update the styles run `npm run watch`.\n- Depending on how you are running Wordpress, you may need to create a new database for the site (or you can use the DB dump at the bottom of this readme). \n- Run the site in the browser. Depending on your set up, you may be lead through the typical Wordpress site setup.\n- Login to the site as an administrator, and go to `appearance > themes` and activate the Mozilla theme. It has no preview screenshot. ## Add the Theme Settings\n\n### Theme settings\n\nGo to `Mozilla Settings` in the left sidebar to add the following settings.\n\n| Setting | Value |\n| ------- | ----- |\n| Report Group / Event Email | `<Your email>` |\n| Github Link |\thttps://github.com/mozilla/community-portal |\n| Community Portal Discourse |  |\n| Mailchimp API Key\t| `<a test API key>` |\n| Company |\tMozilla |\n| Address | |\n| City | Toronto |\n| State / Province |  |\n| Postal / Zip |  |\n| Country |  |\n| Phone |  |\n| Google Analytics ID |  | (only on production)\n| Discourse API Key |\t | (only on production)\n| Discourse API URL |\t | (only on production)\n| Discourse URL |  (only on production)\n| Mapbox Access Token |  | (only on production)\n| Default Open Graph Title | Community Portal - Mozilla |\n| Default Open Graph Description | Community Portal - Mozilla |\n| Max Image Filesize Upload (KB) | 250 |\n| 404 Error Title | Page not Found |\n| 404 Error Copy | Oops, we got lost! |\n\n### Set up Theme Menus\n- Next set up the menus `Appearance > Menus`:\n    - Create 4 new Menus with the followign items:\n        - Main\n          - Sign Up / Log Out (#)\n          - Search (#)\n        - Mozilla\n          - About (#)\n          - Mission (#)\n          - Contact (#)\n          - Donate (#)\n        - Mozilla Main Menu\n          - Campaigns (/campaigns)\n          - Activities (/activities)\n          - Events (/events)\n          - Groups (/groups)\n          - People (/people)\n          - * Assign this menu to the Display locations of \u2018Mozilla Custom Theme Menu\u2019\n        - Resources\n          - Code of Conduct (#)\n          - Privacy Policy (#)\n          - Creative License (#)\n          - Legal Notices (#)\n\n## Sample data\n\nThis is [MySQL dump](https://github.com/mozilla/community-portal/files/8116830/community-2022-02-22-c19c553.tar.gz) with credentials as `admin/password` with admin rights and plugins configured (as this readme) with some example events as host `community.test`.\n\n## Install Plugins\n- Once the site is created, the following plugins need to be installed:\n    - [Auth0](https://auth0.com/wordpress) (only on production)\n    - [BuddyPress](https://buddypress.org/)\n    - [Events Manager (for BuddyPress)](http://wp-events-plugin.com/)\n    - [ACF Pro](https://www.advancedcustomfields.com/pro/) (free version works)\n    - [WPML](https://wpml.org/) (works not always without but in case of error is enough to remove `/en/` from the URL)\n- Only for development\n    - [Query Monitor](https://wordpress.org/plugins/query-monitor/)\n    - [Classic Editor](https://wordpress.org/plugins/classic-editor/)\n    \n### Create the required Pages\n- Before creating pages, activate BuddyPress. This will automatically create some pages, and reduce those you need to manually create\n\n| Page Name | URL Slug | Page Parent |\n| --------- | -------- | ----------- |  \n| Activate | `/activate` | n/a |\n| Community Portal | set to front page in `Settings > Reading` | n/a |\n| Events | `/events` | n/a |\n| Categories | `/categories` | Events |\n| Edit | `/edit` | Events | \n| Locations | `/locations` | Events |\n| My Bookings | `/my-bookings` | Events |\n| Tags |\t`/tags` |\tEvents |\n| Groups |\t`/groups` |\tn/a |\n| Members |\t`/people` |\tn/a |\n| Register |\t`/register` |\tn/a |\n\n### Settings\n\n## Auth0 Setup (only production)\n- Go to the Plugins page and activate the Auth0 plugin\n- In 1password there is a `domain`, `client ID` and `client Secret` in the secure note - configure the plugin with this information.\n\n## Setup BuddyPress\n- Activate BuddyPress if you have not already\n- Go to Plugins > BuddyPress > Settings\n- Components Tab\n  - everything except \u2018Site Tracking\u2019 should be enabled\n- Options Tab\n  - Set the template to \u2018BuddyPress Legacy\u2019\n  - everything else here should be checked except for \u2018Post Comments\u2019\n- Pages Tab\n  - Set Members -> Members\n  - Set Activity Streams -> Activity\n  - Set User Groups -> Groups\n\n## Setup ACF Pro (free version works)\n- Activate the plugin\n- Import the ACF fields\n  - Go to `Custom Fields > Tools`\n  - We\u2019re going to import the field settings via a JSON import\n  - In the theme files, there is a top-level directory called ACF, this contains JSON files with the info ACF needs to set up our fields\n   - Under `Import Field Groups` choose the oldest file in this directory and import it.\n   - Repeat this step for the remaining files, in chronological order.\n\nRequire various plugin settings and also to check the code about what is doing. The first step is required that you user accept the T&C or you cannot create events.\n\n## Dev instructions\n\n### Style rules enforcement\n\n```\ncomposer update # To download PHPCS\n./vendor/bin/phpcs --standard=WordPress .\n./vendor/bin/phpcbf --standard=WordPress .\n```\n\n### Create a Dev Environment locally\n\nUsing [VVV](https://github.com/Varying-Vagrant-Vagrants/vvv) copy the file `default-config.yml` as `config.yml` in the same folder.  \nNext in the `sites` section in the YAML file add:\n```\n    community:\n        repo: https://github.com/Varying-Vagrant-Vagrants/custom-site-template\n        custom:\n            delete_default_plugins: true\n            install_plugins:\n                - query-monitor\n\t\t- classic-editor\n\t\t- buddypress\n\t\t- events-manager\n\t\t- advanced-custom-fields\n            wpconfig_constants:\n                WP_DEBUG: true\n                WP_DEBUG_LOG: wp-content/debug.log\n                WP_DISABLE_FATAL_ERROR_HANDLER: true\n        nginx_upstream: php73\n        hosts:\n            - community.test\n```\n\nDownload the sample data above in this readme, put in the `VVV/database/sql/backups/` folder as `community.sql.gz`.  \nNow you can launch in the terminal `vagrant up` (check the [documentation](https://varyingvagrantvagrants.org/docs/en-US/installation/) to install VVV).  \nAfter running you will have a working dev environment with database and plugins configured.\n"
},
{
  "name": "python_mozetl",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "bin",
      "docs",
      "mozetl",
      "scheduling",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "conf.py",
      "index.rst",
      "readme.rst",
      "source"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Telemetry Python ETL\n\n[![CircleCI](https://circleci.com/gh/mozilla/python_mozetl.svg?style=svg)](https://circleci.com/gh/mozilla/python_mozetl)\n[![codecov](https://codecov.io/gh/mozilla/python_mozetl/branch/main/graph/badge.svg)](https://codecov.io/gh/mozilla/python_mozetl)\n\nThis repository is a collection of ETL jobs for Firefox Telemetry.\n\n# Benefits\n\nJobs committed to python_mozetl can be **scheduled via\n[airflow](https://github.com/mozilla/telemetry-airflow)\nor\n[ATMO](https://analysis.telemetry.mozilla.org/)**.\nWe provide a **testing suite** and **code review**, which makes your job more maintainable.\nCentralizing our jobs in one repository allows for\n**code reuse** and **easier collaboration**.\n\nThere are a host of benefits to moving your analysis out of a Jupyter notebook\nand into a python package.\nFor more on this see the writeup at\n[cookiecutter-python-etl](https://github.com/harterrt/cookiecutter-python-etl/blob/master/README.md#benefits).\n\n# Tests\n## Dependencies\nFirst install the necessary runtime dependencies -- snappy and the java runtime\nenvironment. These are used for the `pyspark` package. In ubuntu:\n```bash\n$ sudo apt-get install libsnappy-dev openjdk-8-jre-headless\n```\n\n## Calling the test runner\nRun tests by calling `tox` in the root directory.\n\nArguments to `pytest` can be passed through tox using `--`.\n```\ntox -- -k test_main.py # runs tests only in the test_main module\n```\n\nTests are configured in [tox.ini](tox.ini)\n\n# Manual Execution\n## ATMO\n\nThe first method of manual execution is the `mozetl-submit.sh` script located in `bin`.\nThis script is used with the `EMRSparkOperator` in `telemetry-airflow` to schedule execution of `mozetl` jobs.\nIt may be used with [ATMO](https://analysis.telemetry.mozilla.org/) to manually test jobs.\n\nIn an SSH session with an ATMO cluster, grab a copy of the script:\n```\n$ wget https://raw.githubusercontent.com/mozilla/python_mozetl/main/bin/mozetl-submit.sh\n```\nPush your code to your own fork, where the job has been added to `mozetl.cli`. Then run it.\n\n```bash\n$ ./mozetl-submit.sh \\\n    -p https://github.com/<USERNAME>/python_mozetl.git \\\n    -b <BRANCHNAME> \\\n    <COMMAND> \\\n        --first-argument foo \\\n        --second-argument bar\n```\n\nSee comments in `bin/mozetl-submit.sh` for more details.\n\n## Databricks\n\nJobs may also be executed on [Databricks](https://dbc-caf9527b-e073.cloud.databricks.com/).\nThey are scheduled via the `MozDatabricksSubmitRunOperator` in `telemetry-airflow`.\n\nThis script runs on your local machine and submits the job to a remote spark executor.\nFirst, generate an API token in the User Settings page in Databricks.\nThen run the script.\n\n```bash\npython bin/mozetl-databricks.py \\\n    --git-path https://github.com/<USERNAME>/python_mozetl.git \\\n    --git-branch <BRANCHNAME> \\\n    --token <TOKEN>  \\\n    <COMMAND> \\\n        --first-argument foo \\\n        --second-argument bar\n```\n\nRun `python bin/mozetl-databricks.py --help` for more options, including increasing the number of workers and using python 3.\nRefer to this [pull request](https://github.com/mozilla/python_mozetl/pull/296) for more examples.\n\nIt is also possible to use this script for external mozetl-compatible modules by setting the `--git-path` and `--module-name` options appropriately.\nSee this [pull request](https://github.com/mozilla/python_mozetl/pull/316) for more information about building a mozetl-compatible repository that can be scheduled on Databricks.\n\n\n# Scheduling\n\nYou can schedule your job on either\n[ATMO](https://analysis.telemetry.mozilla.org/)\nor\n[airflow](https://github.com/mozilla/telemetry-airflow).\n\nScheduling a job on ATMO is easy and does not require review,\nbut is less maintainable.\nUse ATMO to schedule jobs you are still prototyping\nor jobs that have a limited lifespan.\n\nJobs scheduled on Airflow will be more robust.\n\n* Airflow will automatically retry your job in the event of a failure.\n* You can also alert other members of your team when jobs fail,\n  while ATMO will only send an email to the job owner.\n* If your job depends on other datasets,\n  you can identify these dependencies in Airflow.\n  This is useful if an upstream job fails.\n\n## ATMO\n\nTo schedule a job on ATMO, take a look at the\n[load_and_run notebook](scheduling/load_and_run.ipynb).\nThis notebook clones and installs the python_mozetl package.\nYou can then run your job from the notebook.\n\n## Airflow\n\nTo schedule a job on Airflow,\nyou'll need to add a new Operator to the DAGs and provide a shell script for running your job.\nTake a look at \n[this example shell script](https://github.com/mozilla/telemetry-airflow/blob/master/jobs/topline_dashboard.sh).\nand\n[this example Operator](https://github.com/mozilla/telemetry-airflow/blob/master/dags/topline.py#L31)\nfor templates.\n\n# Early Stage ETL Jobs\n\nWe usually require tests before accepting new ETL jobs.\nIf you're still prototyping your job,\nbut you'd like to move your code out of a Jupyter notebook\ntake a look at\n[cookiecutter-python-etl](https://github.com/harterrt/cookiecutter-python-etl).\n\nThis tool will initialize a new repository\nwith all of the necessary boilerplate for testing and packaging.\nIn fact, this project was created with\n[cookiecutter-python-etl](https://github.com/harterrt/cookiecutter-python-etl).\n"
},
{
  "name": "dennis",
  "files": {
    "/": [
      ".editorconfig",
      ".github",
      ".gitignore",
      "CHANGELOG",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTORS",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "dennis",
      "docs",
      "requirements-dev.txt",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "api.rst",
      "changelog.rst",
      "conf.py",
      "contributors.rst",
      "hacking.rst",
      "index.rst",
      "license.rst",
      "linting.rst",
      "recipes.rst",
      "statusing.rst",
      "translating.rst"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "DEFAULT_GOAL := help\n\n.PHONY: help\nhelp:\n\t@echo \"Available rules:\"\n\t@fgrep -h \"##\" Makefile | fgrep -v fgrep | sed 's/\\(.*\\):.*##/\\1:/'\n\n.PHONY: clean\nclean:  ## Clean build artifacts\n\trm -rf build dist dennis.egg-info\n\trm -rf docs/_build/*\n\trm -rf _pytest_cache/ .tox\n\tfind dennis/ tests/ -name __pycache__ | xargs rm -rf\n\tfind dennis/ tests/ -name '*.pyc' | xargs rm -rf\n\n.PHONY: lint\nlint:  ## Lint and black reformat files\n\tblack --target-version=py37 setup.py dennis tests\n\tflake8 dennis tests\n\n.PHONY: test\ntest:  ## Run tests\n\ttox\n",
  "readme": "=======\nRead me\n=======\n\nDennis is a set of utilities for working with PO files to ease development and\nimprove quality. Translate POT files to find problems with localization in your\ncode. Lint PO files for common problems like variable formatting, mismatched\nHTML, missing variables, etc.\n\n``dennis-cmd`` has the following subcommands:\n\n* **lint**: Lints PO and POT files for problems including errors that can cause\n  your production system to crash and problems in strings that can lead to poor\n  translations.\n\n  The system allows for defining other variable formats.\n\n* **status**: Get a high-level status of a PO file including a list of\n  unstranslated strings.\n\n* **translate**: Translates strings in PO files into something else! Comes with\n  an HTML extractor (tokenizes strings so that only the text is translated) and\n  a bunch of translations like Pirate!.\n\n  This is helpful for l10n testing, development, finding unicode/layout\n  problems, amazing your friends, hilarious April 1st shenanigans, etc.\n\n  Specify the tokenizer/transform pipeline you want to use that combines\n  things. Zombie? Sure! Shouty Zombie? Ok! Manic shouty Dubstep? Bring it on!\n\n  This also works on strings passed in as command line arguments and as\n  stdin---it doesn't have to be a PO file or in a PO format format. For\n  example, Dennis uses Dennis to translate all Dennis commit messages into\n  Pirate!. That's how cool Dennis is!\n\n\nQuick start\n===========\n\nInstall::\n\n    $ pip install dennis\n\nLint a PO file for problems::\n\n    $ dennis-cmd lint locale/fr/LC_MESSAGES/messages.po\n\nLint all your PO files for errors::\n\n    $ dennis-cmd lint --errorsonly locale/\n\nLint a POT file for problems::\n\n    $ dennis-cmd lint locale/templates/LC_MESSAGES/messages.pot\n\nTranslate a PO file in place into Pirate!::\n\n    $ dennis-cmd translate --pipeline=html,pirate \\\n        locale/xx/LC_MESSAGES/messages.po\n\nGet help::\n\n    $ dennis-cmd\n\n\nProject details\n===============\n\n:Code:          https://github.com/mozilla/dennis\n:Documentation: https://dennis.readthedocs.io/\n:Issue tracker: https://github.com/mozilla/dennis/issues\n:License:       BSD 3-clause; see LICENSE file\n\n\nWhy is it called Dennis?\n========================\n\nThis is how @willkg names his software projects.\n"
},
{
  "name": "webext-experiment-utils",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bin",
      "example",
      "package-lock.json",
      "package.json",
      "wee-schema-schema.json"
    ]
  },
  "makefile": null,
  "readme": "# WebExtension Experiment Utils\n\nTools for rapid development of WebExtension Experiments based on a `schema.json` API definition.\n\n- generateStubApi - generates a stub api.js based on `schema.json`\n- documentSchema - generates documentation in Markdown based on `schema.json`\n- verifyWeeSchema - verifies that `schema.json` is a valid WebExtension Experiments schema\n\nFor a usage example, see the `npm run generate` command in [./package.json](./package.json) and the example files in [./example/](./example/).\n\n## Installation\n\n```shell\nnpm install --save-dev mozilla/webext-experiment-utils#develop\n```\n\n## Hints\n\n1.  Put all the privileged code of your add-on in `src/privileged` as a best practice\n2.  The 'Firefox privileged' modules cannot use WebExtension apis (`browserAction`, `management`, etc.). Use a `background.js` script (using messages and events) to co-ordinate multiple privileged modules.\n\n## A note on automatic testing of WebExtension Experiments\n\nWebExtension Experiments can not be automatically tested in the same manner as the official (in-tree) WebExtension APIs. Partially, such testing could work by copying over source files into the Firefox source tree and then building from source, but doing so would not let us catch the specific issues associated with bundling WebExtension Experiments (which have different life-cycle behavior than in-tree WebExtension APIs).\n\nInstead, we can leverage Selenium for functional testing in a way that mimics unit testing of our experiment APIs.\n\nThe [SHIELD utils test add-on](https://github.com/mozilla/shield-studies-addon-utils/tree/master/test-addon) currently accomplishes this by opening an [extension page](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/user_interface/Extension_pages), switching to that tab/window and executing javascript using driver.executeAsyncScript().\n\nCheck this helper method for more information and an initial implementation: https://github.com/mozilla/shield-studies-addon-utils/blob/develop/testUtils/executeJs.js\n\nTests can then be written as such:\n\n```\n  it(\"should be able to access window.browser from the extension page for tests\", async () => {\n    const hasAccessToWebExtensionApi = await utils.executeJs.executeAsyncScriptInExtensionPageForTests(\n      driver,\n      async callback => {\n        callback(typeof browser === \"object\");\n      },\n    );\n    assert(hasAccessToWebExtensionApi);\n  });\n```\n\nThis way, WebExtension APIs that are only exposed in background scripts can be accessed directly in automated tests.\n\nFor an example of such a testing set-up, see https://github.com/mozilla/shield-studies-addon-utils/blob/master/test/functional/browser.study.api.js\n"
},
{
  "name": "fxrecord",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "contrib",
      "docs",
      "fakefox",
      "fxrecord.example.toml",
      "fxrecorder",
      "fxrunner",
      "integration-tests",
      "libfxrecord",
      "libfxrecord_macros",
      "requirements.in",
      "requirements.txt",
      "test",
      "vendor"
    ],
    "/docs": [
      "Makefile",
      "make.bat",
      "requirements.txt",
      "source"
    ]
  },
  "makefile": null,
  "readme": "# fxrecord\n\n`fxrecord` is a tool for capturing and analyzing recordings of Firefox for\ndesktop startup. It consists of two parts: `fxrecorder`, which records the\noutput of the reference hardware and does analysis, and `fxrunner`, which\ninstruments Firefox on the reference hardware.\n\n## License\n\n`fxrecord` is released under the terms of the [Mozilla Public License 2.0](LICENSE).\n\n## Code of Conduct\n\nThis repository follows a [code of conduct](CODE_OF_CONDUCT.md).\n\n## Documentation\n\nDocumentation for `fxrecord` is available [here](https://mozilla.github.io/fxrecord).\n"
},
{
  "name": "data-review-stats",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "render.py",
      "requirements.in",
      "requirements.txt",
      "template.html"
    ]
  },
  "makefile": null,
  "readme": "# Data review statistics\n\nPowers https://protosaur.dev/data-steward-stats/.\n\nExpects to find a file named `bugzilla_api_key` containing, well, a Bugzilla API key, in its current working directory.\n\nRunning this looks something like:\n\n```sh\npython render.py\ngsutil -q cp result.html gs://data-steward-stats/index.html\n```\n"
},
{
  "name": "prio-processor",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "NOTICE",
      "README.md",
      "bin",
      "config",
      "deployment",
      "docker-compose.yml",
      "docs",
      "examples",
      "google-cloud-sdk.repo",
      "mkdocs.yml",
      "notebooks",
      "prio_processor",
      "requirements-dev.in",
      "requirements-dev.txt",
      "requirements.txt",
      "scripts",
      "setup.py",
      "tests"
    ],
    "/docs": [
      "README.md",
      "airflow.md",
      "cli-help.md",
      "guide.md",
      "images",
      "link"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build clean test\n\nbuild:\n\tdocker-compose build\n\nclean:\n\tdocker-compose down\n",
  "readme": "# prio-processor\n\n[![CircleCI](https://circleci.com/gh/mozilla/prio-processor.svg?style=svg)](https://circleci.com/gh/mozilla/prio-processor)\n\nPrio is a system for aggregating data in a privacy-preserving way. This\nrepository includes a command-line tool for batch processing in Prio's\nmulti-server architecture.\n\nFor more information about Prio, see [this blog\npost](https://hacks.mozilla.org/2018/10/testing-privacy-preserving-telemetry-with-prio/).\n\n## Docker\n\nThis project contains a pre-configured build and test environment via docker.\n\n```bash\nmake\n\n# or run directly though docker-compose\ndocker-compose build\n```\n\nYou can mount your working directory and shell into the container for\ndevelopment work.\n\n```bash\ndocker-compose run -v $PWD:/app prio_processor bash\n```\n\n## Adding new dependencies\n\nTo add new Python dependencies to the container, use `pip-tools` to manage the\n`requirements.txt`.\n\n```bash\npip install pip-tools\n\n# generate the installation requirements from setup.py\npip-compile\n\n# generate dev requirements\npip-compile requirements-dev.in\n```\n\nAny new system dependencies should be added to the `Dockerfile` at the root of\nthe repository. These will be available during runtime.\n\n## Deployment Configuration\n\nSee the `deployment` directory for examples of configuration that can be used to\naid deployment. These may also be run as integration tests to determine whether\nresources are configured properly. These will typically assume Google Cloud\nPlatform (GCP) as a resource provider.\n\nSee the [guide](docs/guide.md) for more details.\n"
},
{
  "name": "ensemble-transposer",
  "files": {
    "/": [
      ".editorconfig",
      ".env-dist",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "config",
      "docs",
      "package-lock.json",
      "package.json",
      "src"
    ],
    "/docs": [
      "example-redash-config.json"
    ]
  },
  "makefile": ".PHONY: build start lint test compare shell\n\nhelp:\n\t@echo \"Makefile commands for local development:\"\n\t@echo\n\t@echo \"  build         Build the Docker image\"\n\t@echo \"  start         Run ensemble-transposer\"\n\t@echo \"  lint          Lint source code\"\n\t@echo \"  test          Run tests\"\n\t@echo \"  compare       Compare development output to production output\"\n\t@echo \"  shell         Start a Bash shell\"\n\nbuild:\n\tdocker image build --tag ensemble-transposer .\n\nstart: build\n\tdocker container run --rm --tty --env-file=.env ensemble-transposer npm start\n\nlint: build\n\tdocker container run --rm --tty ensemble-transposer npm run lint\n\ntest: build\n\tdocker container run --rm --tty --env-file=.env ensemble-transposer npm test\n\ncompare: build\n\tdocker container run --rm --tty --env-file=.env \\\n\tensemble-transposer npm run compare\n\nshell:\n\tdocker container run --rm --tty --interactive --env-file=.env \\\n\tensemble-transposer /bin/bash\n",
  "readme": "ensemble-transposer re-formats existing data so that it can be used by the\n[Firefox Public Data Report](https://data.firefox.com).\n\nMozilla already publishes raw data: numbers and identifiers. That's great, but\nit can be difficult to work with. ensemble-transposer takes that raw data,\norganizes it, adds useful information like explanations, and generates a series\nof files that are much easier for developers to work with.\n[Ensemble](https://github.com/mozilla/ensemble), the platform that powers the\nFirefox Public Data Report, uses this improved and re-formatted data to build\ndashboards.\n\nOther applications are also welcome to use the data that ensemble-transposer\noutputs. See the [API documentation](#API) for more information.\n\nensemble-transposer can easily enhance any data that adheres to [this\nformat](https://public-data.telemetry.mozilla.org/prod/usage_report_data/v1/master/fxhealth.json).\nIt can also process Redash dashboards (see this [example configuration\nfile](docs/example-redash-config.json)). Let us know if you have any questions\nor if you have a dataset that you would like us to spruce up.\n\n## API\n\nRe-formatted data is currently hosted under the data.firefox.com domain, but you\nare also welcome to run ensemble-transposer yourself and host the re-formatted\ndata elsewhere.\n\n* **Valid `platform` values:** *desktop*\n* **Valid `datasetName` values:** *hardware*, *user-activity*, *usage-behavior*\n* **Valid `categoryName` values:** Listed in the output of the\n  */datasets/[platform]/[datasetName]* endpoint\n* **Valid `metricName` values:** Listed in the output of the\n  */datasets/[platform]/[datasetName]* endpoint\n\n### /datasets/[platform]/[datasetName]/index.json\n\nFor example: https://data.firefox.com/datasets/desktop/user-activity/index.json\n\nA summary of the given dataset. For example, this includes a description of the\ndataset and a list of all metrics within it.\n\n### /datasets/[platform]/[datasetName]/[categoryName]/[metricName]/index.json\n\nFor example: https://data.firefox.com/datasets/desktop/user-activity/Italy/YAU/index.json\n\nEverything you need to know about a given metric in a given category. For\nexample, this includes a title, a description, and a set of suggested axis\nlabels.\n\n## Development\n\n### Setup\n\n1. Install [Docker](https://docs.docker.com/install/)\n2. Create a new [Amazon S3](https://aws.amazon.com/s3/) bucket\n3. Copy *.env-dist* to *.env* and provide values for all environment variables\n\n### Inspecting output\n\nRun `make start` and inspect that data that is uploaded to S3.\n\n### Testing\n\nRun `make test` to lint code and run standard tests.\n\nRun `make compare` to compare the data in your S3 bucket to the data in the\nproduction S3 bucket. This can be useful when upgrading packages or refactoring\ncode, for example.\n\n## Deployment\n\nThis project is meant to be run as a cloud task, like a Lambda function or\nGoogle Cloud Function. The main function is specified as the value of `main` in\n*package.json*. Most services read this value and do the right thing. If not,\nyou may need to manually point your service to that function.\n\nBefore triggering the function, be sure to create an [Amazon\nS3](https://aws.amazon.com/s3/) bucket and set the following environment\nvariables:\n\n* `AWS_BUCKET_NAME`\n* `AWS_REGION`\n* `AWS_ACCESS_KEY_ID`\n* `AWS_SECRET_ACCESS_KEY`\n\n## Notes\n\n### Versioning\n\nWe maintain a version number for this project in *package.json*. It should be\nincremented whenever new code is pushed.\n\nThe number looks like a semantic version number, but [semver isn't meant for\napplications](https://softwareengineering.stackexchange.com/a/255201). We\ninstead follow these basic guidelines: the first number is incremented for major\nchanges, the second number is incremented for medium-sized changes, and the\nthird number is incremented for small changes.\n"
},
{
  "name": "DeepSpeech-examples",
  "files": {
    "/": [
      "README.rst",
      "android_mic_streaming",
      "autosub",
      "batch_processing",
      "electron",
      "ffmpeg_vad_streaming",
      "hotword_adjusting",
      "mic_vad_streaming",
      "net_framework",
      "nim_mic_vad_streaming",
      "nodejs_mic_vad_streaming",
      "nodejs_wav",
      "tests.sh",
      "uwp",
      "vad_transcriber",
      "web_microphone_websocket"
    ]
  },
  "makefile": null,
  "readme": "DeepSpeech 0.9.x Examples\n==========================\n\nThese are various examples on how to use or integrate DeepSpeech using our packages.\n\nIt is a good way to just try out DeepSpeech before learning how it works in detail, as well as a source of inspiration for ways you can integrate it into your application or solve common tasks like voice activity detection (VAD) or microphone streaming.\n\nContributions are welcome!\n\n**Note:** These examples target DeepSpeech **0.9.x** only. If you're using a different release, you need to go to the corresponding branch for the release:\n\n* `v0.9.x <https://github.com/mozilla/DeepSpeech-examples/tree/r0.9>`_\n* `v0.8.x <https://github.com/mozilla/DeepSpeech-examples/tree/r0.8>`_\n* `v0.7.x <https://github.com/mozilla/DeepSpeech-examples/tree/r0.7>`_\n* `v0.6.x <https://github.com/mozilla/DeepSpeech-examples/tree/r0.6>`_\n* `master branch <https://github.com/mozilla/DeepSpeech-examples/tree/master>`_\n\n**List of examples**\n\nPython:\n-------\n\n* `Microphone VAD streaming <mic_vad_streaming/README.rst>`_\n* `VAD transcriber <vad_transcriber/>`_\n* `AutoSub <autosub/>`_\n\nJavaScript:\n-----------\n\n* `FFMPEG VAD streaming <ffmpeg_vad_streaming/README.MD>`_\n* `Node.JS microphone VAD streaming <nodejs_mic_vad_streaming/Readme.md>`_\n* `Node.JS wav <nodejs_wav/Readme.md>`_\n* `Web Microphone Websocket streaming <web_microphone_websocket/Readme.md>`_\n* `Electron wav transcriber <electron/Readme.md>`_\n\nWindows/C#:\n-----------\n\n* `.NET framework <net_framework/>`_\n* `Universal Windows Platform (UWP) <uwp/>`_.\n\nJava/Android:\n-------------\n\n* `mozilla/androidspeech library <https://github.com/mozilla/androidspeech/>`_\n\nNim:\n----\n\n* `nim_mic_vad_streaming <nim_mic_vad_streaming/README.md>`_.\n"
},
{
  "name": "marian-regression-tests",
  "files": {
    "/": [
      ".build.yml",
      ".gitignore",
      ".shared.yml",
      "LICENSE.md",
      "Makefile",
      "README.md",
      "data",
      "github-events.cyml",
      "linux-opt-build.sh",
      "models",
      "requirements.txt",
      "run_mrt.sh",
      "show_tags.sh",
      "tc-schedule.sh",
      "test-linux-opt-base.tyml",
      "test-win-opt-base.tyml",
      "tests",
      "tools",
      "true.sh",
      "win-opt-build.sh",
      "worker.cyml"
    ]
  },
  "makefile": "THREADS=16\n\nGIT_MOSES_SCRIPTS=http://github.com/mozilla/moses-scripts.git\nGIT_SUBWORD_NMT=http://github.com/mozilla/subword-nmt.git\nGIT_SACREBLEU=http://github.com/mozilla/sacreBLEU.git\n\n.PHONY: install pip tools models data run\n.SECONDARY:\n\n\n#####################################################################\n\nrun: install\n\tbash ./run_mrt.sh\n\ninstall: tools models data\n\ntools: pip\n\tmkdir -p $@\n\tgit -C $@/moses-scripts pull || git clone $(GIT_MOSES_SCRIPTS) $@/moses-scripts\n\tgit -C $@/subword-nmt pull   || git clone $(GIT_SUBWORD_NMT) $@/subword-nmt\n\tgit -C $@/sacrebleu pull     || git clone $(GIT_SACREBLEU) $@/sacrebleu\n\npip: requirements.txt\n\tpip3 install -r $<\n\nmodels:\n\tmkdir -p $@\n\tcd $@ && bash ./download-wmt16.sh\n\tcd $@ && bash ./download-wmt17.sh\n\tcd $@ && bash ./download-char-s2s.sh\n\tcd $@ && bash ./download-wnmt18.sh\n\tcd $@ && bash ./download-transformer.sh\n\tcd $@ && bash ./download-lm.sh\n\tcd $@ && bash ./download-rnn-spm.sh\n\tcd $@ && bash ./download-wngt19.sh\n\tcd $@ && bash ./download-ape.sh\n\ndata:\n\tmkdir -p $@\n\tcd $@ && bash ./download-data.sh\n\nclean:\n\tgit clean -x -d -f tests\n\nclean-all:\n\tgit clean -x -d -f\n",
  "readme": "Marian regression tests\n=======================\n\n<b>Marian</b> is an efficient Neural Machine Translation framework written in\npure C++ with minimal dependencies.\n\nThis repository contains the regression test framework for the main development\nrepository: https://github.com/marian-nmt/marian-dev.\n\n\n## Structure\n\nDirectories:\n\n* `tests` - regression tests\n* `tools` - scripts and repositories\n* `models` - models used in regression tests\n* `data` - data used in training or decoding tests\n\nEach test consists of:\n\n* `test_*.sh` file\n* `setup.sh` (optional)\n* `teardown.sh` (optional)\n\n\n## Usage\n\nDownloading required data and tools:\n\n    make install\n\nRunning regression tests:\n\n    MARIAN=/path/to/marian-dev/build ./run_mrt.sh\n\nEnabling multi-GPU tests:\n\n    CUDA_VISIBLE_DEVICES=0,1 ./run_mrt.sh\n\nMore invocation examples:\n\n    ./run_mrt.sh tests/training/basics\n    ./run_mrt.sh tests/training/basics/test_valid_script.sh\n    ./run_mrt.sh previous.log\n    ./run_mrt.sh '#tag'\n\nwhere `previous.log` contains a list of test files, one test per line.  This\nfile is automatically generated each time `./run_mrt.sh` finishes running.\nThe last example starts all regression tests labeled with '#tag'.  The list of\ntests annotated with each available tag can be displayed by running\n`./show_tags.sh`.\n\nCleaning test artifacts:\n\n    make clean\n\n\n## Debugging failed tests\n\nFailed tests are displayed at the end of testing or in `previous.log`, e.g.:\n\n    Failed:\n    - tests/training/restoring/multi-gpu/test_async.sh\n    - tests/training/embeddings/test_custom_embeddings.sh\n    ---------------------\n    Ran 145 tests in 00:48:48.210s, 143 passed, 0 skipped, 2 failed\n\nLogging messages are in files ending with _.sh.log_ suffix:\n\n    less tests/training/restoring/multi-gpu/test_async.sh.log\n\nThe last command in most tests is an execution of a custom `diff` tool, which\nprints the exact invocation commands with absolute paths. It can be used to\ndisplay the differences that cause the test fails.\n\n\n## Adding new tests\n\nUse templates provided in `tests/_template`.\n\nPlease follow these recommendations:\n\n* Test one thing at a time\n* For comparing outputs with numbers, please use float-friendly\n  `tools/diff-nums.py` instead of GNU `diff`\n* Make your tests deterministic using `--no-shuffle --seed 1111` or similar\n* Make training execution time as short as possible, for instance, by reducing\n  the size of the network and the number of iterations\n* Do not run decoding or scoring on files longer than ca. 10-100 lines\n* If your tests require downloading and running a custom model, please keep it\n  as small as possible, and contact me (Roman) to upload it into our storage\n\n\n## Jenkins\n\nThe regression tests are run automatically on Jenkins after each push to the\nmaster branch and a successful compilation with g++ 5.4.0 20160609 and CUDA\n10.1.243: http://vali.inf.ed.ac.uk/jenkins/view/marian/\n\nOn Jenkins, Marian is compiled using the following commands:\n\n    cmake -DUSE_SENTENCEPIECE=ON -DCOMPILE_TESTS=ON -DCOMPILE_EXAMPLES=ON \\\n        -DCUDA_TOOLKIT_ROOT_DIR=/var/lib/jenkins/cuda-10.1 ..\n    make -j\n    make test\n\nIf this succeeds, created executables are used to run regression tests.\n\n\n## Acknowledgements\n\nThe development of Marian received funding from the European Union's\n_Horizon 2020 Research and Innovation Programme_ under grant agreements\n688139 ([SUMMA](http://www.summa-project.eu); 2016-2019),\n645487 ([Modern MT](http://www.modernmt.eu); 2015-2017),\n644333 ([TraMOOC](http://tramooc.eu/); 2015-2017),\n644402 ([HiML](http://www.himl.eu/); 2015-2017),\n825303 ([Bergamot](https://browser.mt/); 2019-2021),\nthe Amazon Academic Research Awards program,\nthe World Intellectual Property Organization,\nand is based upon work supported in part by the Office of the Director of\nNational Intelligence (ODNI), Intelligence Advanced Research Projects Activity\n(IARPA), via contract #FA8650-17-C-9117.\n\nThis software contains source code provided by NVIDIA Corporation.\n\n"
},
{
  "name": "mach-perftest-notebook-dev",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "__init__.py",
      "perftestnotebook",
      "requirements.txt",
      "setup.py",
      "template_upload_file.html",
      "testing"
    ]
  },
  "makefile": null,
  "readme": "# mach-perftest-notebook-dev\n[![Build Status](https://travis-ci.com/mozilla/mach-perftest-notebook-dev.svg?branch=starter)](https://travis-ci.com/mozilla/mach-perftest-notebook-dev)\n\nDevelopment Repository for Mach Perftest Notebook Tooling. This tool will be used to standardize analysis/visualization techniques across Mozilla.\n\n## Running the tool\nInstall the tool with `python3 setup.py install`. Then call `perftestnotebook --config=<CONFIG>` with a config of your choice. See `testing/configs` for examples.\nThis will standardize the data based on the Transformer you are using and output that. In the near future, it will also open an Iodide webpage to visualize and analyze the data with a set of pre-built analysis/visualization techniques.\n\n## Development Steps\nThese are the issues we will need to solve in this development repository (1 and 2 will likely need to be developed in parallel). The audience for this tool are software developers so we can take advantage of this for data transformation/standardization and other aspects of this tool.\n\n1. Develop data standardization techniques.\n    1. Ideas:\n        1. Build a decorator to decoarte custom transform/standardization functions in a provided script.\n        1. Use a named function from a supplied script as the transform function.\n        1. In addition to these two, build simple transformers for simple data formats so they don't need to be rewritten by developpers.\n1. Build some basic units of analysis/visualization in Python.\n    1. Plotting data in a graph with custom x and y labels. Graph can be a line plot, scatter plot, or bar graph for now.\n1. Dynamically integrate basic units into an Iodide web page.\n    1. Ensure that each basic unit can have it's standardized data modified easily.\n\nEach of these steps should have proper testing for the functionality as well, pytest will work fine in this case.\n\nOnce something basic is built, we will look into builing the mach tool for this in mozilla-central.\n\n## Contributing\nFeel free to ping us in #perftest on [Riot](https://wiki.mozilla.org/Matrix) for information on how to contribute.\n\n \n"
},
{
  "name": "taar_monitor",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "requirements.txt",
      "setup.py",
      "taar_monitor",
      "tests"
    ]
  },
  "makefile": "all:\n\tpython setup.py sdist\n\ntest:\n\tpython setup.py test\n\npypi:\n\ttwine upload --repository-url https://upload.pypi.org/legacy/ dist/*\n",
  "readme": "Code for extracting TAAR metrics across:\n\n* Production logs filed into sql.telemetry.mozilla.org\n* Statistics from workflow.telemetry.mozilla.org\n* Operational metrics from Datadog\n"
},
{
  "name": "mozilla-schema-generator",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      ".gitmodules",
      ".isort.cfg",
      "AUTHORS.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "Dockerfile",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "aliases.json",
      "bin",
      "common_pings.json",
      "disallowlist",
      "docker-compose.yml",
      "incompatibility-allowlist",
      "mozilla_schema_generator",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tests",
      "validation-schemas"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: help clean clean-pyc clean-build list test coverage release\n\nhelp:\n\t@echo \"  clean-build - remove build artifacts\"\n\t@echo \"  clean-pyc - remove Python file artifacts\"\n\t@echo \"  lint - check style with flake8, isort and black\"\n\t@echo \"  format - autoformat with autoflake, isort and black\"\n\t@echo \"  test - run tests quickly with the default Python\"\n\t@echo \"  coverage - check code coverage quickly\"\n\t@echo \"  coverage-report - open the coverage report in your browser\"\n\t@echo \"  release - package and upload a release\"\n\t@echo \"  install-requirements - install the requirements for development\"\n\t@echo \"  build       Builds the docker images for the docker-compose setup\"\n\t@echo \"  docker-rm       Stops and removes all docker containers\"\n\t@echo \"  run         Run a command. Can run scripts, e.g. make run COMMAND=\\\"./scripts/schema_generator.sh\\\"\"\n\t@echo \"  test-script Run a local script. e.g. make test-script SCRIPT=\\\"a-local-script.sh\\\"\"\n\t@echo \"  shell       Opens a Bash shell\"\n\nclean: clean-build clean-pyc docker-rm\n\nclean-build:\n\trm -fr build/\n\trm -fr dist/\n\trm -fr *.egg-info\n\nclean-pyc:\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\nformat:\n\tautoflake -r -i --remove-all-unused-imports mozilla_schema_generator tests\n\tisort mozilla_schema_generator tests ./*.py\n\tblack mozilla_schema_generator tests ./*.py\n\nlint:\n\tflake8 mozilla_schema_generator tests --max-line-length 100\n\tisort --check mozilla_schema_generator tests ./*.py\n\tblack --check mozilla_schema_generator tests ./*.py\n\ntest:\n\tpy.test -v\n\tjsonschema -i aliases.json validation-schemas/aliases.json\n\ncoverage:\n\tpytest tests/ --cov=mozilla_schema_generator\n\tcoverage report -m\n\ncoverage-report: coverage\n\tcoverage html\n\topen htmlcov/index.html\n\nrelease: dist\n\ttwine upload dist/*\n\ndist: clean ## builds source and wheel package\n\tpython setup.py sdist\n\tpython setup.py bdist_wheel\n\tls -l dist\n\ninstall-requirements:\n\tpip install -r requirements/requirements.txt\n\tpip install -r requirements/test_requirements.txt\n\nbuild:\n\tdocker-compose build\n\ndocker-rm: stop\n\tdocker-compose rm -f\n\nshell:\n\tdocker-compose run --entrypoint /bin/bash app\n\nrun:\n\tdocker-compose run app $(COMMAND)\n\nrun-script:\n\tdocker-compose run app bash < $(SCRIPT)\n\nstop:\n\tdocker-compose down\n\tdocker-compose stop\n\nup:\n\tdocker-compose up\n",
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/mozilla-schema-generator/tree/main.svg?style=svg)](https://circleci.com/gh/mozilla/mozilla-schema-generator/tree/main)\n[![Latest Version](https://img.shields.io/pypi/v/mozilla-schema-generator.svg)](https://pypi.python.org/pypi/mozilla-schema-generator/)\n\n# Mozilla Schema Generator\n\nA library for generating full representations of Mozilla telemetry pings.\n\nSee [Mozilla Pipeline Schemas](https://www.github.com/mozilla-services/mozilla-pipeline-schemas)\nfor the more generic structure of pings. This library takes those generic structures and fills in\nall of the probes we expect to see in the appropriate places.\n\n## Telemetry Integration\n\nThere are two generic ping types we're targeting for this library:\n\n1. [The Common Ping Format](http://gecko-docs.mozilla.org.s3.amazonaws.com/toolkit/components/telemetry/telemetry/data/main-ping.html)\n   is used for many legacy pings from Firefox Desktop ping, including the \"main\" ping\n2. [The Glean Ping Format](https://github.com/mozilla/glean_parser) is the common structure being used for\n   all newly instrumented products at Mozilla, including mobile browsers.\n\nThis library takes the information for what should be in those pings from the [Probe Info Service](https://www.github.com/mozilla/probe-scraper).\n\n## Data Store Integration\n\nThe primary use of the schemas is for integration with the\n[Schema Transpiler](https://www.github.com/mozilla/jsonschema-transpiler). \nThe schemas that this repository generates can be transpiled into Avro and Bigquery. They define\nthe schema of the Avro and BigQuery tables that the [BQ Sink](https://www.github.com/mozilla/gcp-ingestion)\nwrites to.\n\n### BigQuery Limitations and Splitting\n\n[BigQuery has a hard limit of ten thousand columns on any single table](https://cloud.google.com/bigquery/quotas). \nThis library can take that limitation into account by splitting schemas into multiple tables,\nalthough so far we have been able to avoid this complication. We retain schema\nsplitting support here as an option to use in the future. The option is currently disabled.\n\nWhen a schema is split, each\ntable has some common information duplicated in every table, and then a set\nof fields that are unique to that table. The join of these tables gives the full\nset of fields available from the ping.\n\nTo decide on a table split, we include the `table_group` configuration in the configuration\nfile. For example, `payload/histograms` has `table_group: histograms`; this indicates that\nthere will be a table outputted with just histograms.\n\nIf a single table expands beyond the configured column limit, we move the new fields to the next table.\nFor example, we could have main_histograms_1 and main_histograms_2.\n\n_Note_: Tables are only split if the `--split` parameter is provided, and this\nis option is not currently used for our production configuration.\n\n## Validation\n\nWhen we validate pings against a schema in the data pipeline, we use the generic versions\nrather than the versions generated by this repository's machinery. While the schemas produced\nhere are guaranteed to be more correct since they include explicit definitions of every metric and probe,\nwe find in practice there are too many edge cases where a probe is sent with the incorrect type\nand we need to coerce it to the correct type when loading to BigQuery.\nWe also purposely represent some complex types as JSON strings in schemas, relying on the BQ loader\nto coerce objects to string.\nWe could still consider using the generated schemas for validation in the future, but\nadditional work would be required to ensure it does not lead to mass rejection of pings.\n\n## Usage\n\n### Main Ping\n\nGenerate the Full Main Ping schema:\n\n```\nmozilla-schema-generator generate-main-ping\n```\n\nGenerate the Main Ping schema divided among tables (for BigQuery):\n```\nmozilla-schema-generator generate-main-ping --split --out-dir main-ping\n```\n\nThe `out-dir` parameter will be the namespace for the pings.\n\nTo see a full list of options, run `mozilla-schema-generator generate-main-ping --help`.\n\n\n### Glean\n\nGenerate all Glean ping schemas - one for each application, for each ping\nthat application sends:\n\n```\nmozilla-schema-generator generate-glean-pings\n```\n\nWrite schemas to a directory:\n```\nmozilla-schema-generator generate-glean-pings --out-dir glean-ping\n```\n\nTo see a full list of options, run `mozilla-schema-generator generate-glean-pings --help`.\n\n\n## Configuration Files\n\nConfiguration files are by default found in `/config`. You can also specify your own when running the generator.\n\nConfiguration files match certain parts of a ping to certain types of probes or metrics. The nesting\nof the config file matches the ping it is filling in. For example, Glean stores probe types under\nthe `metrics` key, so the nesting looks like this:\n```\n{\n    \"metrics\": {\n        \"string\": {\n            <METRIC_ID>: {...}\n        }\n    }\n}\n```\n\nWhile the generic schema doesn't include information about the specific `<METRIC_ID>`s being included,\nthe schema-generator does. To include the correct metrics that we would find in that section of the ping,\nwe would organize the `config.yaml` file like this:\n\n```\nmetrics:\n    string:\n        match:\n            type: string\n```\n\nThe `match` key indicates that we should fill-in this section of the ping schema with metrics,\nand the `type: string` makes sure we only put string metrics in there. You can do an exact\nmatch on any field available in the ping info from the [probe-info-service](https://probeinfo.telemetry.mozilla.org/glean/glean/metrics),\nwhich also contains the [Desktop probes](https://probeinfo.telemetry.mozilla.org/firefox/all/main/all_probes).\n\nThere are a few additional keywords allowable under any field:\n* `contains` - e.g. `process: contains: main`, indicates that the `process` field is an array\n  and it should only match those that include the entry `main`.\n* `not` - e.g. `send_in_pings: not: glean_ping_info`, indicates that we should match\n  any field for `send_in_pings` _except_ `glean_ping_info`.\n\n### `table_group` Key\n\nThis specific field is for indicating which table group that section of the ping should be included in when\nsplitting the schema. Currently we do not split any pings. See the section on [BigQuery\nLimitations and Splitting](#bigquery-limitations-and-splitting) for more info.\n\n## Allowing schema incompatible changes\n\nOn every run of the schema generator, there is a check for incompatible changes\nbetween the previous revision and current generated revision. A schema\nincompatible change includes a removal of a schema or a column, or a change in\nthe type definition of a column.\n\nThere are two methods to get around these restrictions. If you are actively\ndeveloping the schema generator and need to introduce a schema incompatible\nchange, set `MPS_VALIDATE_BQ=false`.\n\nIf a schema incompatible change needs to be introduced in production (i.e.\n`generated-schemas`), then modify the `incompatibility-allowlist` at the root of\nthe repository. Add documents in the form of\n`{namespace}.{doctype}.{docversion}`. Globs are allowed. For example, add the\nfollowing line to allow remove schemas under the `my_glean_app` namespace:\n\n```bash\nmy_glean_app.*\n```\n\nOnce the commit has gone through successfully, this line should be removed from\nthe document.\n\n## Development and Testing\n\nInstall requirements:\n\n```bash\nmake install-requirements\n```\n\nEnsure that the mozilla-pipeline-schemas submodule has been checked out:\n\n```bash\ngit submodule init\ngit submodule update --remote\n```\n\nRun tests:\n\n```bash\nmake test\n```\n\nPublish generated schemas to [mozilla-generated-schemas/test-generated-schemas](https://github.com/mozilla-services/mozilla-pipeline-schemas/tree/test-generated-schemas)\nrun:\n\n```bash\ngit fetch origin\n\ngit checkout <branch-to-test>\n\nexport MPS_SSH_KEY_BASE64=$(cat ~/.ssh/id_rsa | base64)\n\n# generate all schemas for current main\ngit checkout main && git pull make build && make run\n\n# generate all schemas with changes and compare with main\ngit checkout <branch-to-test> make build && make run\n```\n"
},
{
  "name": "guardian-e2e",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".prettierc.js",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "playwright.config.js",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "## To Run\n\n1. npm install\n2. npm run test\n"
},
{
  "name": "layout-triage",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "config.template.json",
      "index.js",
      "package-lock.json",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Platform Layout Triage\n\nThe script in this repository updates and generates triage duty cycle rotations for the Firefox Platform Layout team. Data is published as JSON for consumption by [release management auto-nag bots](https://github.com/mozilla/relman-auto-nag/blob/master/auto_nag/scripts/configs/layout_round_robin.json), as well as [ICAL](https://mozilla.github.io/layout-triage/layout-triage.ics) for consumption by team members.\n\n## Installation\n\n1. Clone the repository.\n2. Run `npm install`.\n3. Copy `config.template.json` to `config.json` and edit as needed.\n4. If desired, run `npm run init` to fetch the current state of the triage history from GitHub.\n\n## Adding a Triage Cycle\n\nRun `npm run update`. Output (JSON and ICAL files) will be placed in the `dist` directory.\n\n## Publishing Updated Data\n\nRun `npm run publish`. This will publish everything in the `dist` directory to the `gh-pages` branch and push it to GitHub.\n\n## Clearing Future Data\n\nRun `npm run clear`. This will remove triage duty cycles from next week onwards. Useful if the set of triagers changes.\n\n## Resetting All Data\n\n**Note:** This will erase all triage history in `dist/history.json` and start triage rotation over using the first team member listed on `config.json`.\n\nRun `npm run reset`.\n"
},
{
  "name": "Foxfooding_Firefox_translation_community",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Translations Foxfooding Issues\n\nThis is a repo that was used for the [Firefox Translations add-on](https://addons.mozilla.org/firefox/addon/firefox-translations/) Foxfooding campaign. This campaign is now over.  \nIf you have any bugs or feature requests, please sumbit them here: https://github.com/mozilla/translate.\n\nIf you're looking to help test more Mozilla products/features, please join us at https://community.mozilla.org/.\n"
},
{
  "name": "gzipServer",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "gzipServer.py"
    ]
  },
  "makefile": null,
  "readme": "# gzipServer\nWeb server that receives gzip'd POST requests and saves them uncompressed locally\n\nTo force Firefox to send a Telemetry payload to this server:\n\n1. Start the server: `./gzipServer.py` (you may need to install simplejson Python module)\n1. In `about:config`, change the preference \"toolkit.telemetry.server\" to `\"http://localhost\"`\n1. Restart Firefox to have Telemetry pick up the above pref change\n1. Open `about:telemetry` (it has some Telemetry namespaces nicely set up) and open the DevTools console\n1. Paste the following into the console:  \n   `Cu.import(\"resource://gre/modules/TelemetrySession.jsm\");`  \n   `TelemetrySession.testPing();`\n1. The script will save the request it receives to *report1.json* in the script's working directory\n\nNote: The procedure above will create a \"test ping\", which is equivalent to a regular Telemetry \"saved-session\" ping.\n\n***Alternatives:***\n\n1. If you just need to ***see*** the Telemetry measurements from the current session, you can see them directly on the `about:telemetry` page in Firefox.\n1. If you need to see what the full ping looks like, but you don't need to send it to a server, simply exit Firefox and restart. In `about:telemetry` select the last \"main\" ping from the *archived ping data*. You can switch to the raw ping data to see the full raw contents.\n1. You can also get the full ping from the DevTools console by opening the about:telemetry page and running the commands:  \n   `Cu.import(\"resource://gre/modules/TelemetrySession.jsm\");`  \n  `ping = TelemetrySession.getPayload()`\n"
},
{
  "name": "crash-clouseau",
  "files": {
    "/": [
      ".coveragerc",
      ".flake8",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "HOWTO.md",
      "Procfile",
      "README.md",
      "bin",
      "config",
      "crashclouseau",
      "docker-compose.yml",
      "images",
      "mozdata.ini",
      "requirements.txt",
      "runtime.txt",
      "static",
      "templates",
      "test-requirements.txt",
      "tests",
      "webextension"
    ]
  },
  "makefile": null,
  "readme": "# crash-clouseau\n>  Tool to help to find patches which are potentially responsible of a crash\n\n[![Build Status](https://api.travis-ci.org/mozilla/crash-clouseau.svg?branch=master)](https://travis-ci.org/mozilla/crash-clouseau)\n[![codecov.io](https://img.shields.io/codecov/c/github/mozilla/crash-clouseau/master.svg)](https://codecov.io/github/mozilla/crash-clouseau?branch=master)\n\n## See it in action\n\nhttps://crash-clouseau.herokuapp.com/reports.html\n\nResults on Firefox code are tracked in a meta bug: https://bugzilla.mozilla.org/show_bug.cgi?id=1396527\n\n## Setup\n\nInstall the prerequisites via `pip`:\n```sh\nsudo pip install -r requirements.txt\n```\n\n## Running tests\n\nInstall test prerequisites via `pip`:\n```sh\nsudo pip install -r test-requirements.txt\n```\n\nRun tests:\n```sh\ncoverage run --source=crashclouseau -m unittest discover tests/\n```\n\n## UI Documentation\n\nSee [HOWTO](/HOWTO.md).\n\n## Bugs\n\nhttps://github.com/mozilla/crash-clouseau/issues/new\n\n## Number of bugs reported with the tool\n\nhttps://bugzilla.mozilla.org/rest/bug?count_only=1&blocks=1396527\n\n## Contact\n\nEmail: release-mgmt@mozilla.com or calixte@mozilla.com\n"
},
{
  "name": "firefox-translations-training",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".gitmodules",
      "3rd_party",
      "CODE_OF_CONDUCT.md",
      "DAG.pdf",
      "LICENSE",
      "Makefile",
      "README.md",
      "Singularity.def",
      "Snakefile",
      "configs",
      "envs",
      "pipeline",
      "profiles",
      "reports",
      "utils"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "#!make\n\n.ONESHELL:\nSHELL=/bin/bash\n\n### 1. change these settings or override with env variables\nCONFIG=configs/config.prod.yml\nCONDA_PATH=../mambaforge\nSNAKEMAKE_OUTPUT_CACHE=../cache\nPROFILE=local\n# execution rule or path to rule output, default is all\nTARGET=\nREPORTS=../reports\n# for tensorboard\nMODELS=../models\n\n###\n\nCONDA_ACTIVATE=source $(CONDA_PATH)/etc/profile.d/conda.sh ; conda activate ; conda activate\nSNAKEMAKE=export SNAKEMAKE_OUTPUT_CACHE=$(SNAKEMAKE_OUTPUT_CACHE); snakemake\n\n### 2. setup\n\ngit-modules:\n\tgit submodule update --init --recursive\n\nconda:\n\twget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$$(uname)-$$(uname -m).sh\n\tbash Mambaforge-$$(uname)-$$(uname -m).sh -b -p $(CONDA_PATH)\n\nsnakemake:\n\t$(CONDA_ACTIVATE) base\n\tmamba create -c conda-forge -c bioconda -n snakemake snakemake==6.12.2 --yes\n\tmkdir -p \"$(SNAKEMAKE_OUTPUT_CACHE)\"\n\n# build container image for cluster and run-local modes (preferred)\nbuild:\n\tsudo singularity build Singularity.sif Singularity.def\n\n# or pull container image from a registry if there is no sudo\npull:\n\tsingularity pull Singularity.sif library://evgenypavlov/default/bergamot2:latest\n\n### 3. dry run\n\n# if you need to activate conda environment for direct snakemake commands, use\n# . $(CONDA_PATH)/etc/profile.d/conda.sh && conda activate snakemake\n\ndry-run:\n\techo \"Dry run with config $(CONFIG) and profile $(PROFILE)\"\n\t$(CONDA_ACTIVATE) snakemake\n\t$(SNAKEMAKE) \\\n\t  --profile=profiles/$(PROFILE) \\\n\t  --configfile $(CONFIG) \\\n\t  -n \\\n\t  $(TARGET)\n\ntest-dry-run: CONFIG=configs/config.test.yml\ntest-dry-run: dry-run\n\n### 4. run\n\nrun:\n\techo \"Running with config $(CONFIG) and profile $(PROFILE)\"\n\t$(CONDA_ACTIVATE) snakemake\n\tchmod +x profiles/$(PROFILE)/*\n\t$(SNAKEMAKE) \\\n\t  --profile=profiles/$(PROFILE) \\\n\t  --configfile $(CONFIG) \\\n\t  $(TARGET)\n\ntest: CONFIG=configs/config.test.yml\ntest: run\n\n\n### 5. create a report\n\nreport:\n\t$(CONDA_ACTIVATE) snakemake\n    DT=$$(date '+%Y-%m-%d_%H-%M'); \\\n\tmkdir -p $(REPORTS) && \\\n\tsnakemake \\\n\t\t--profile=profiles/$(PROFILE) \\\n\t\t--configfile $(CONFIG) \\\n\t\t--report $(REPORTS)/$${DT}_report.html\n\nrun-file-server:\n\t$(CONDA_ACTIVATE) snakemake\n\tpython -m  http.server --directory $(REPORTS) 8000\n\n### extra\n\nclean-meta:\n\t$(CONDA_ACTIVATE) snakemake\n\t$(SNAKEMAKE) \\\n\t  --profile=profiles/$(PROFILE) \\\n\t  --configfile $(CONFIG) \\\n\t  --cleanup-metadata $(TARGET)\n\ndag: CONFIG=configs/config.test.yml\ndag:\n\t$(CONDA_ACTIVATE) snakemake\n\t$(SNAKEMAKE) \\\n\t  --profile=profiles/$(PROFILE) \\\n\t  --configfile $(CONFIG) \\\n\t  --dag \\\n\t  | dot -Tpdf > DAG.pdf\n\ninstall-tensorboard:\n\t$(CONDA_ACTIVATE) base\n\tconda env create -f envs/tensorboard.yml\n\ntensorboard:\n\t$(CONDA_ACTIVATE) tensorboard\n\tls -d $(MODELS)/*/*/* > tb-monitored-jobs\n\ttensorboard --logdir=$(MODELS) --host=0.0.0.0 &\n\tpython utils/tb_log_parser.py --prefix=",
  "readme": "# Firefox Translations training\nTraining pipelines for Firefox Translations machine translation models.\nThe trained models are hosted in [firefox-translations-models](https://github.com/mozilla/firefox-translations-models/),\ncompatible with [bergamot-translator](https://github.com/mozilla/bergamot-translator) and can be used by\n[firefox-translations](https://github.com/mozilla/firefox-translations) web extension. This work is a part of [Bergamot](https://browser.mt/) project  that focuses on improving client-side machine translation in a web browser.\n\nThe pipeline is capable of training a translation model for a language pair end to end. \nTranslation quality depends on chosen datasets, data cleaning procedures and hyperparameters. \nSome settings, especially low resource languages might require extra tuning.\n\nIt uses fast translation engine [Marian](https://marian-nmt.github.io) \nand [Snakemake](https://snakemake.github.io/) framework for workflow management and parallelization.\n\n## System requirements\n\n### Local mode\n\n- Ubuntu 18.04 (it can work on other Linux distributions, but might require `setup` scripts fixes; see more details in [marian installation instructions](https://marian-nmt.github.io/quickstart/)).\n- One or several Nvidia GPUs with CUDA drivers installed and at least 8 GB of memory.\n- CUDNN installed\n- At least 16 CPU cores ( some steps of the pipeline utilize multiple cores pretty well, so the more the better).\n- 64 GB RAM (128 GB+ might be required for bigger datasets)\n- 200+ GB of disk space ( mostly for datasets and transformations ). \n  It depends on chosen datasets and can be significantly higher.\n  \nIt was tested on: \n- Ubuntu 18.04\n- 56 core Xeon server\n- 128 GB of RAM\n- x8 NVIDIA RTX 2080 GPUs with 12 GB of memory\n- CUDA 11.2\n- 100 GB of local disk space\n- Many terabytes of NFS mounted storage\n\n### Cluster mode\n\n- Slurm cluster with CPU and Nvidia GPU nodes\n- CUDA 11.2 ( it was also tested on 11.5)\n- CUDNN library installed\n- Singularity module if running with containerization (recommended)\n- If running without containerization, there is no procedure to configure the environment automatically.\n  All the required modules (for example `parallel`) should be preinstalled and loaded in ~/.bashrc\n\nIt was tested on Mozilla Slurm cluster using Singularity containers.\nThe pipeline can also be launched on [CSD3 HPC](https://docs.hpc.cam.ac.uk/hpc/index.html) but it was not fully tested.\n\n### Cloud mode\n\nSnakemake workflows can work on Kubernetes, Google Cloud Life Sciences and other cloud platforms. \nThe pipeline was not tested in this mode and might require modification.\n\nPlease refer to [Cloud execution](https://snakemake.readthedocs.io/en/stable/executing/cloud.html) section of Snakemake documentation.\n\nIt is also possible to deploy Slurm cluster in the cloud. For example, using [Slurm on Google Cloud Platform](https://github.com/SchedMD/slurm-gcp).\n\n## Configuration\n\n0. Clone the repo:\n``` \ngit clone https://github.com/mozilla/firefox-translations-training.git\ncd firefox-translations-training\n```\n1. Choose a [Snakemake profile](https://github.com/Snakemake-Profiles) from `profiles/` or create a new one \n2. Adjust paths in the `Makefile` if needed and set `PROFILE` variable to the name of your profile\n3. Adjust Snakemake and workflow settings in the `profiles/<profile>/config.yaml`, see [Snakemake CLI reference](https://snakemake.readthedocs.io/en/stable/executing/cli.html) for details\n4. Configure experiment and datasets in `configs/config.prod.yml` (or `configs/config.test.yml` for test run)\n5. Change source code if needed for the experiment\n6. **(Cluster mode)** Adjust cluster settings in the cluster profile.\n   For `slurm-moz`: `profiles/slurm-moz/config.cluster.yml`\n   You can also modify `profiles/slurm-moz/submit.sh` or create a new Snakemake [profile](https://github.com/Snakemake-Profiles).\n7. **(Cluster mode)** It might require further tuning of requested resources in `Snakemake` file:\n    - Use `threads` for a rule to adjust parallelism\n    - Use `resources: mem_mb=<memory>` to adjust total memory requirements per task \n      (default is set in `profile/slurm-moz/config.yaml`)\n\n## Installation\n\nSee also [Snakemake installation](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html)\n\n1. Install Mamba - fast Conda package manager\n\n```\nmake conda\n```\n\n2. Install Snakemake\n\n```\nmake snakemake\n```\n\n3. Update git submodules\n\n```\nmake git-modules\n```\n\n4. (Optional) Install Singularity if running with containerization \n\nLocal mode: See [Singularity installation](https://sylabs.io/guides/3.8/user-guide/quick_start.html), requries root\n\nCluster mode: \n\nFor example,\n```\nmodule load singularity\n```\nbut the way to load Singularity depends on cluster installation\n\n5. (Optional) Prepare a container image if using Singularity\n\n    \nEither pull the prebuilt image:\n\n```\nmake pull\n```\n\nOr build it (requires root):\n\n```\nmake build\n```\n\n## Running\n\nDry run first to check that everything was installed correctly:\n\n```\nmake dry-run\n```\n\nTo run the pipeline:\n```\nmake run\n```\n\nTo test the whole pipeline end to end (it is supposed to run relatively quickly and does not train anything useful):\n\n```\nmake test\n```\nYou can also run a speicific profile or config by overriding variables from Makefile\n```\nmake run PROFILE=slurm-moz CONFIG=configs/config.test.yml\n```\n\n### Specific target\n\nBy default, all Snakemake rules are executed. To run the pipeline up to a specific rule use:\n```\nmake run TARGET=<non-wildcard-rule-or-path>\n```\nFor example, collect corpus first:\n```\nmake run TARGET=merge_corpus\n```\n\nYou can also use the full file path, for example:\n```\nmake run TARGET=/models/ru-en/bicleaner/teacher-base0/model.npz.best-ce-mean-words.npz\n```\n### Rerunning\n\nIf you want to rerun a specific step or steps, you can delete the result files that are expected in the Snakemake rule output.\nSnakemake might complain about a missing file and suggest to run it with `--clean-metadata` flag. In this case run:\n```\nmake clean-meta TARGET=<missing-file-name>\n```\nand then as usual:\n```\nmake run\n```\n\n### Reporting\n\nTo create a Snakemake [html report](https://snakemake.readthedocs.io/en/stable/snakefiles/reporting.html), run:\n```\nmake report\n```\n\n### Results\n\nSee [Directory Structure](#directory-structure) section.\n\nThe main directories inside `SHARED_ROOT` are:\n- `data/<lang_pair>/<experiment>` - data produced by the pipeline jobs\n- `logs/<lang_pair>/<experiment>` - logs of the jobs for troubleshooting\n- `experiments/<lang_pair>/<experiment>` - saved experiment settings for future reference\n- `models/<lang_pair>/<experiment>` - all models produced by the pipeline. The final compressed models are in `exported` folder.\n\n#### Exported models example\n\n```\n/models/ru-en/test/exported/model.ruen.intgemm.alphas.bin.gz\n/models/ru-en/test/exported/lex.50.50.ruen.s2t.bin.gz\n/models/ru-en/test/exported/vocab.ruen.spm.gz\n```\n\n## Pipeline steps\n\nThe steps are based on [train-student](https://github.com/browsermt/students/tree/master/train-student) recipe.\n\nStep | Description | Bottleneck | Comments\n--- | --- | --- | ---\nInstallation | Installing dependencies and compiling | CPU | Takes ~1 hour\nData downloading | Downloads datasets, samples sentences | Network, Disk | Time depends on dataset size, sampling of huge mono datasets (100M+ sentences) is the most intensive operation.\nData cleaning | Basic preprocessing, dataset specific, language specific, rule based and other attempts to clean noisy data in parallel and mono datasets | CPU | Good parallelization across CPU cores. To make cleaning of a new language more efficient add it to [clean_parallel.py](/pipeline/clean/tools/clean_parallel.py).\nBicleaner | Filters noisy sentence pairs in a parallel corpus using [bicleaner](https://github.com/bitextor/bicleaner) or [bicleaner-ai](https://github.com/bitextor/bicleaner-ai) depending on available language packs. | CPU, GPU | If there are no pretrained language packs for bicleaner-ai, it uses bicleaner. If there are no ones for bicleaner either, this step is skipped. Cleaning thresholds are configurable per dataset, see [Dataset cleaning](##Dataset cleaning).\nMerge and dedupe | Merges clean dataset and applies deduplicaiton | CPU, Disk | \nTraining vocabulary | Trains [SentencePiece](https://github.com/google/sentencepiece) vocabulary/tokenizer model on parallel corpus. | CPU |\nTraining s2s | Trains a backward shallow s2s model, which is useful for back-translations and ce-filtering | GPU | Inspired by a [marian example](https://github.com/marian-nmt/marian-examples/tree/master/training-basics-sentencepiece).\nAugmentation with back-translations | Translates mono corpus combined from monolingual datasets in target language using shallow s2s model. | GPU | It is more useful for low-resource languages and can be skipped for others.\nTraining teacher | Trains an ensemble of big transformer models on augmented dataset | GPU | You might want to adjust [early stopping](pipeline/train/configs/training/teacher.transformer.train.yml) or `after-epochs` parameters depending on datasets size.\nFine-tuning teacher | Continue training an ensemble of teachers on parallel data only | GPU | You might want to adjust [early stopping](pipeline/train/configs/training/teacher.transformer.train.yml) parameters depending on datasets size.\nTranslation by teacher | Translates a corpus and monolingual data combined from configurable `dataset.mono-src` using the ensemble of teacher models | GPU | The slowest part of the pipeline. Can take days. It is possible to speed it up by using multiple nodes in cluster mode.\nCross-entropy filtering | Scores translated corpus with backward s2s model and removes a part of the corpus with the lowest scores to reduce noise | GPU, CPU, Disk | At this point we work with huge datasets. Very disk intensive.\nTraining alignments and shortlist | Trains alignments using [fast_align](https://github.com/clab/fast_align) and extracts lexical shortlist using [extract_lex](https://github.com/marian-nmt/extract-lex) tool | CPU, Disk | Some tools require uncompressed datasets on disk and they are huge at this point. Good CPU parallelization.\nTraining student | Trains a small transformer student model on filtered data and using alignments. Shuffling in RAM might fail if dataset is huge and there's not enough RAM on the machine, so it's recommended to remove it and use `shuffle: batches` marian settings (see [issue](https://github.com/mozilla/firefox-translations-training/issues/21)).  | GPU |\nFine-tuning student | Finetunes the student model by emulating 8bit GEMM during training | GPU | Converges very quickly and then degrades. It's quick but you might want to reduce early stopping threshold.\nQuantizaiton |  Applies 8 bit quantization to the fined-tuned student model and runs evaluation on CPU | CPU | CPU threads must be set to 1 for this step.\nEvaluation |  Calculates metrics for all models (BLEU, chrf) using [SacreBLEU](https://github.com/mjpost/sacrebleu) | GPU | Uses `datasets.test` configuration section.\nExport | Exports trained model and shortlist to (bergamot-translator)(https://github.com/mozilla/bergamot-translator) format | |\n\n## Dataset importers\n\nDataset importers can be used in `datasets` sections of the config.\n\nExample:\n```\n  train:\n    - opus_ada83/v1\n    - mtdata_newstest2014_ruen\n```\n\nData source | Prefix | Name examples | Type | Comments\n--- | --- | --- | ---| ---\n[MTData](https://github.com/thammegowda/mtdata) | mtdata | newstest2017_ruen | corpus | Supports many datasets. Run `mtdata list -l ru-en` to see datasets for a specific language pair.\n[OPUS](opus.nlpl.eu/) | opus | ParaCrawl/v7.1 | corpus | Many open source datasets. Go to the website, choose a language pair, check links under Moses column to see what names and version is used in a link.\n[SacreBLEU](https://github.com/mjpost/sacrebleu) | sacrebleu | wmt20 | corpus | Official evaluation datasets available in SacreBLEU tool. Recommended to use in `datasets:test` config section. Look up supported datasets and language pairs in `sacrebleu.dataset` python module.\n[Flores](https://github.com/facebookresearch/flores) | flores | dev, devtest | corpus | Evaluation dataset from Facebook that supports 100 languages.\nCustom parallel | custom-corpus | /tmp/test-corpus | corpus | Custom parallel dataset that is already downloaded to a local disk. The dataset name is an absolute path prefix without \".lang.gz\"\n[Paracrawl](https://paracrawl.eu/) | paracrawl-mono | paracrawl8 | mono | Datasets that are crawled from the web. Only [mono datasets](https://paracrawl.eu/index.php/moredata) are used in this importer. Parallel corpus is available using opus importer.\n[News crawl](http://data.statmt.org/news-crawl) | news-crawl | news.2019 | mono | Some news monolingual datasets from [WMT21](https://www.statmt.org/wmt21/translation-task.html)\n[Common crawl](https://commoncrawl.org/) | commoncrawl | wmt16 | mono | Huge web crawl datasets. The links are posted on [WMT21](https://www.statmt.org/wmt21/translation-task.html)\nCustom mono | custom-mono | /tmp/test-mono | mono | Custom monolingual dataset that is already downloaded to a local disk. The dataset name is an absolute path prefix without \".lang.gz\"\n\nYou can also use [find-corpus](pipeline/utils/find-corpus.py) tool to find all datasets for an importer and get them formatted to use in config.\n\n```\nconda env create -f envs/corpus.yml \nconda activate corpus\npython utils/find-corpus.py en ru opus\npython utils/find-corpus.py en ru mtdata\npython utils/find-corpus.py en ru sacrebleu\n```\nMake sure to check licenses of the datasets before using them.\n\n### Adding a new importer\n\nJust add a shell script to [corpus](pipeline/data/importers/corpus) or [mono](pipeline/data/importers/mono) which is named as `<prefix>.sh` \nand accepts the same parameters as the other scripts from the same folder.\n\n## Dataset fixing\n\nSome datasets require fixes like detokenization. Dataset and language specific fixes are implemented in [pipeline/clean/fixes](pipeline/clean/fixes).\nNaming convention: \n- `<dataset_name>.sh` for parallel dataset cleaning\n- `<dataset_name>.<lang>.sh` for language specific cleaning of parallel or monolingual dataset\n- `/` in dataset name should be replaced with `_`\n\n## Dataset cleaning\nSome parallel datasets require more aggressive filtering.\nDataset specific Bicleaner thresholds can be set in config. \n`0` means skipping filtering entirely (useful for Paracrawl).\n\nExample:\n\n```\nexperiment:\n...\n  bicleaner:\n    default-threshold: 0.5\n    dataset-thresholds:\n      opus_ParaCrawl/v8: 0\n      mtdata_neulab_tedtalksv1_train: 0.6\n```\n\n## Utilities\n\n### Tensorboard\n\nTo see training graphs run tensorboard:\n\n```\nmake install-tensorboard\nmake tensorboard\n```\n\nThen port forward 6006.\n\n## Directory structure\n    \n    \u251c data\n    \u2502   \u2514 ru-en\n    \u2502      \u2514 test\n    \u2502        \u251c original\n    \u2502        \u2502   \u251c corpus\n    \u2502        \u2502   \u2502   \u251c mtdata_JW300.en.gz\n    \u2502        \u2502   \u2502   \u2514 mtdata_JW300.ru.gz\n    \u2502        \u2502   \u251c devset\n    \u2502        \u2502   \u2502   \u251c flores_dev.en.gz\n    \u2502        \u2502   \u2502   \u2514 flores_dev.ru.gz\n    \u2502        \u2502   \u251c eval\n    \u2502        \u2502   \u2502   \u251c sacrebleu_wmt20.en.gz\n    \u2502        \u2502   \u2502   \u2514 sacrebleu_wmt20.ru.gz\n    \u2502        \u2502   \u251c mono\n    \u2502        \u2502   \u2502   \u251c news-crawl_news.2020.ru.gz\n    \u2502        \u2502   \u2502   \u2514 news-crawl_news.2020.en.gz\n    \u2502        \u2502   \u251c devset.ru.gz\n    \u2502        \u2502   \u2514 devset.en.gz\n    \u2502        \u251c clean\n    \u2502        \u2502   \u251c corpus\n    \u2502        \u2502   \u2502   \u251c mtdata_JW300.en.gz\n    \u2502        \u2502   \u2502   \u2514 mtdata_JW300.ru.gz\n    \u2502        \u2502   \u251c mono\n    \u2502        \u2502   \u2502   \u251c news-crawl_news.2020.ru.gz\n    \u2502        \u2502   \u2502   \u2514 news-crawl_news.2020.en.gz\n    \u2502        \u2502   \u251c mono.ru.gz\n    \u2502        \u2502   \u2514 mono.en.gz\n    \u2502        \u251c biclean\n    \u2502        \u2502   \u251c corpus\n    \u2502        \u2502   \u2502   \u251c mtdata_JW300.en.gz\n    \u2502        \u2502   \u2502   \u2514 mtdata_JW300.ru.gz\n    \u2502        \u2502   \u251c corpus.ru.gz\n    \u2502        \u2502   \u251c corpus.en.gz\n    \u2502        \u251c translated\n    \u2502        \u2502   \u251c mono.ru.gz\n    \u2502        \u2502   \u2514 mono.en.gz\n    \u2502        \u251c augmented\n    \u2502        \u2502   \u251c corpus.ru.gz\n    \u2502        \u2502   \u2514 corpus.en.gz\n    \u2502        \u251c alignment\n    \u2502        \u2502   \u251c corpus.aln.gz\n    \u2502        \u2502   \u2514 lex.s2t.pruned.gz\n    \u2502        \u251c merged\n    \u2502        \u2502   \u251c corpus.ru.gz\n    \u2502        \u2502   \u2514 corpus.en.gz\n    \u2502        \u2514 filtered\n    \u2502            \u251c corpus.ru.gz\n    \u2502            \u2514 corpus.en.gz\n    \u251c models\n    \u2502   \u2514 ru-en\n    \u2502       \u2514 test\n    \u2502          \u251c backward\n    \u2502          \u251c teacher-base0\n    \u2502          \u251c teacher-base1\n    \u2502          \u251c teacher-finetuned0\n    \u2502          \u251c teacher-finetuned1\n    \u2502          \u251c student\n    \u2502          \u251c student-finetuned\n    \u2502          \u251c speed\n    \u2502          \u251c evaluation\n    \u2502          \u2502  \u251c backward\n    \u2502          \u2502  \u251c teacher-base0\n    \u2502          \u2502  \u251c teacher-base1\n    \u2502          \u2502  \u251c teacher-finetuned0\n    \u2502          \u2502  \u251c teacher-finetuned1\n    \u2502          \u2502  \u251c teacher-ensemble\n    \u2502          \u2502  \u251c student\n    \u2502          \u2502  \u251c student-finetuned\n    \u2502          \u2502  \u2514 speed\n    \u2502          \u2514 exported\n    \u2502\n    \u251c experiments\n    \u2502   \u2514 ru-en\n    \u2502      \u2514 test\n    \u2502         \u2514 config.sh\n    \u251c logs\n    \u2502   \u2514 ru-en\n    \u2502      \u2514 test\n    \u2502         \u2514 clean_corpus.log\n\n## Development\n\n### Architecture\n\nAll steps are independent and contain scripts that accept arguments, read input files from disk and output the results to disk.\nIt allows writing the steps in any language (currently it's historically mostly bash and Python) and \nrepresent the pipeline as a directed acyclic graph (DAG).\n\nSnakemake workflow manager infers the DAG implicitly from the specified inputs and outputs of the steps. The workflow manager checks which files are missing and runs the corresponding jobs either locally or on a cluster depending on the configuration. \n\nSnakemake parallelizes steps that can be executed simultaneously. It is especially useful for teacher ensemble training and translation.\n\nThe main Snakemake process (scheduler) should be launched interactively. It runs job processes on the worker nodes in cluster mode or on a local machine in local mode.\n\n### Conventions\n  \n- Scripts inside the `pipeline` directory are independent and operate only using input arguments, input files \n  and global envs.\n  \n- All scripts test expected environment variables early.\n\n- If a script step fails, it can be safely retried.\n\n- Ideally, every script should start from the last unfinished step, \n  checking presence of intermediate results of previous steps.\n\n- A script fails as early as possible.\n\n- Maximum bash verbosity is set for easy debugging.\n\n- Input data is always read only.\n\n- Output data is placed in a new folder for script results.\n  \n- It is expected that the specified output folder might not exist and should be created by the script.\n\n- A script creates a folder for intermediate files and cleans it in the end \n  unless intermediate files are useful for retries.\n    \n- Global variables are upper case, local variables are lower case.\n\n- Scripts should utilize resources provided by Snakemake (number of threads, memory).\n  \n\n## References\n\nHere is a list of selected publications on which the training pipeline is based. You can find more relevant publications on [Bergamot project web-site](https://browser.mt/publications).\n\n1. V. M. S\u00e1nchez-Cartagena, M. Ba\u00f1\u00f3n, S. Ortiz-Rojas and G. Ram\u00edrez-S\u00e1nchez, \n\"[Prompsit's submission to WMT 2018 Parallel Corpus Filtering shared task](http://www.statmt.org/wmt18/pdf/WMT116.pdf)\",\nin *Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers*.\nBrussels, Belgium: Association for Computational Linguistics, October 2018\n\n2. Gema Ram\u00edrez-S\u00e1nchez, Jaume Zaragoza-Bernabeu, Marta Ba\u00f1\u00f3n and Sergio Ortiz Rojas \n\"[Bifixer and Bicleaner: two open-source tools to clean your parallel data.](https://eamt2020.inesc-id.pt/proceedings-eamt2020.pdf#page=311)\",\nin *Proceedings of the 22nd Annual Conference of the European Association for Machine Translation*.\nLisboa, Portugal: European Association for Machine Translation, November 2020\n   \n3. M\u00f6lder F, Jablonski KP, Letcher B, et al. [Sustainable data analysis with Snakemake](https://pubmed.ncbi.nlm.nih.gov/34035898/). F1000Res. 2021;10:33. Published 2021 Jan 18. doi:10.12688/f1000research.29032.2\n\n\n4. [Edinburgh\u2019s Submissions to the 2020 Machine Translation Efficiency Task](https://aclanthology.org/2020.ngt-1.26) (Bogoychev et al., NGT 2020)\n\n5. [From Research to Production and Back: Ludicrously Fast Neural Machine Translation](https://aclanthology.org/D19-5632) (Kim et al., EMNLP 2019)\n\n6. [The University of Edinburgh\u2019s Submissions to the WMT19 News Translation Task](https://aclanthology.org/W19-5304) (Bawden et al., 2019)\n\n7. J\u00f6rg Tiedemann, 2012, [Parallel Data, Tools and Interfaces in OPUS](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf). In Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC'2012)\n8. [The University of Edinburgh\u2019s Neural MT Systems for WMT17](https://arxiv.org/abs/1708.00726), Rico Sennrich, Alexandra Birch, Anna Currey, Ulrich Germann, Barry Haddow, Kenneth Heafield, Antonio Valerio Miceli Barone, and Philip Williams. In Proceedings of the EMNLP 2017 Second Conference on Machine Translation (WMT17), 2017.\n9. [Marian: Fast Neural Machine Translation in C++](https://arxiv.org/abs/1804.00344), Marcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth Heafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay Bogoychev, Andre \u0301 F. T. Martins, and Alexandra Birch.\n10. [Improving Neural Machine Translation Models with Monolingual Data](https://arxiv.org/abs/1511.06709), Rico Sennrich,Barry Haddow,Alexandra Birch, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016.\n11. [A Call for Clarity in Reporting BLEU Scores](https://aclanthology.org/W18-6319) (Post, 2018)\n12. [The FLORES-101 Evaluation Benchmark for Low-Resource and Multilingual Machine Translation](https://ai.facebook.com/research/publications/the-flores-101-evaluation-benchmark-for-low-resource-and-multilingual-machine-translation), Facebook\n13. [Many-to-English Machine Translation Tools, Data, and Pretrained Models](https://aclanthology.org/2021.acl-demo.37) (Gowda et al., ACL 2021)\n14. Chris Dyer, Victor Chahuneau, and Noah A. Smith. (2013). [A Simple, Fast, and Effective Reparameterization of IBM Model 2](http://www.ark.cs.cmu.edu/cdyer/fast_valign.pdf). In Proc. of NAACL.\n15. [Neural Machine Translation of Rare Words with Subword Units](https://aclanthology.org/P16-1162) (Sennrich et al., ACL 2016)\n16. [Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://arxiv.org/abs/1804.10959) (Taku Kudo, 2018)\n"
},
{
  "name": "webext-compat-tool",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "gulpfile.js",
      "lib",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "server.js",
      "static",
      "styles",
      "views"
    ]
  },
  "makefile": null,
  "readme": "# Web Extension Compatibility Test for Firefox\n\n## Installation\n\n```\n$ npm install\n```\n\n## Develop\nRuns gulp. Builds and monitors SCSS files. SCSS sourcemaps. Hot reloading for CSS.\n```\n$ npm run dev\n```\n\n## Build\nCompiles & minifies css.\n```\n$ npm run build\n```\n"
},
{
  "name": "addons-blog",
  "files": {
    "/": [
      ".circleci",
      ".eleventyignore",
      ".eslintignore",
      ".eslintrc",
      ".gitignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      ".stylelintrc",
      "LICENSE.txt",
      "README.md",
      "bin",
      "codecov.yml",
      "composer.json",
      "composer.lock",
      "eleventy.config.js",
      "package.json",
      "phpunit.xml.dist",
      "renovate.json",
      "src",
      "tests",
      "yarn.lock"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# addons-blog\n\n[![CircleCI](https://circleci.com/gh/mozilla/addons-blog.svg?style=svg)](https://circleci.com/gh/mozilla/addons-blog)\n\nThis is the AMO Blog, which uses WP as a headless CMS with 11ty as a static generator.\n\n## Getting started\n\n1. clone the project\n2. run `yarn` to install the dependencies\n3. run `yarn start` to build the blog and serve it locally\n\nFor other tasks, take a look at the available commands in the section below.\n\n## Available commands\n\nIn the project directory, you can run the following commands. There are a few commands not mentioned here (see `package.json`) but those are only used by internal processes.\n\n### `yarn prettier`\n\nThis runs [Prettier][] to automatically format the entire codebase.\n\n### `yarn prettier-dev`\n\nThis runs [Prettier][] on only your changed files. This is intended for development.\n\n### `yarn build`\n\nThis is the _base_ command for building the content of the blog in the `build/` directory.\n\n### `yarn build:debug`\n\nThis is similar to `yarn build:serve` but with Eleventy debug logs turned on.\n\n### `yarn build:production`\n\nThis builds the blog in production mode, which essentially optimizes the different assets.\n\n### `yarn build:serve`\n\nThis starts a web server to serve the blog on local port `8081`.\n\n### `yarn build:wptheme`\n\nThis command builds a WordPress theme that is used on our WP instance (mainly for previews).\n\n### `yarn start`\n\nThis starts a web server to serve the blog on local port `8081` as well as watchers for the assets (JS/CSS files). Any change made will rebuild the blog and update it automatically. This command is for development purposes.\n\n### `yarn start:debug`\n\nThis is similar to `yarn start` but with Eleventy debug logs turned on.\n\n### `yarn start:nocache`\n\nThis is similar to `yarn start` but the WordPress API results won't be cached locally.\n\n### `yarn start:https`\n\nThis is similar to `yarn start` but it configures [browsersync][] to serve the blog using the `example.com` domain and with HTTPS enabled. This is useful when one wants to interact with the `mozAddonManager` locally (note: the `extensions.webapi.testing` pref should be set to `true`).\n\n**Important:** you need to generate development certificates with [mkcert][]:\n\n```\nmkcert example.com\n```\n\nNote: if you never used `mkcert` before, you also need to install the local CA with `mkcert -install`\n\nThe site is available at: https://example.com:8081/\n\n### `yarn sass:build`\n\nThis compiles the Sass files and generates a `styles.css` file.\n\n### `yarn sass:watch`\n\nThis starts a watcher to rebuild the CSS files automatically.\n\n### `yarn script:build`\n\nThis compiles the JavaScript files and generates a `bundle.js` file.\n\n### `yarn script:watch`\n\nThis starts a watcher to rebuild the JS files automatically.\n\n## Environment variables\n\nThis project relies on the following environment variables for configuration:\n\n- `AMO_BASE_URL`: the base URL of AMO (default: `https://addons.mozilla.org`)\n- `BUILD_WORDPRESS_THEME`: build the WordPress theme instead of the blog when set to `'1'` (default: unset)\n- `DONT_FIX_INTERNAL_URLS`: do not rewrite internal URLs when set to `'1'` (default: unset)\n- `ELEVENTY_CWD`: the current working directory for Eleventy (default: the project's root directory)\n- `ELEVENTY_ENV`: the Eleventy environment, used by the build scripts\n- `NO_CACHE`: skip cache when set to `'1'` (default: unset)\n- `USE_HTTPS`: serves the blog using HTTPS locally when set to `'1'` (default: unset)\n- `WORDPRESS_BASE_URL`: the base URL of the WordPress instance (default: `https://mozamo.wpengine.com`)\n\n## Production builds\n\nThe Eleventy process and the JS and CSS builds happen in series. Then a 3rd `asset-pipeline` process initiates and takes the built content from `./build` directory and runs it through various optimizations.\n\nDuring these optimizations, the following takes place:\n\n- Binary files are versioned with hashes in the file names.\n- References to these file in CSS and JS are updated.\n- CSS and JS are minified.\n- The HTML is processed to update the references to the assets new hash-based filenames.\n\nAll of this means that we can serve the site with far-future `Expires` headers. If the resource is in the browser's cache, the browser won't even make a request for it. To break the cache, the resource's URL needs to change. When something is updated and the script is re-run, the hash in the filename will change, so the new filename won't be cached and the browser will know to fetch it. This helps the site be fast.\n\nWhilst the `asset-pipline` script is custom, it leverages a lot of existing libs where possible, these include Terser, postHTML, postCSS, and various plugins.\n\n### Asset paths\n\nFor the `asset-pipeline` script to do its thing, all you need to do is refer to all assets with a path beginning with `/blog/assets/`. If you do that, everything else is handled for you \u2728\n\n## Deployment strategy\n\nCurrently, we use a \"simple\" deployment strategy based on git tags and Circle CI to deploy this blog. We have three different tag patterns:\n\n- `<year>.<month>.<date>-stage` (e.g., `2021.07.22-stage`): these tags will be deployed to our [-stage instance][stage]\n- `<year>.<month>.<date>` (e.g., `2021.07.22`): these tags will be deployed to our [-prod instance][prod]\n- `x.y.z` (e.g., `1.3.0`): this is the `version` in the `package.json` and it is used to version the WordPress theme that can be constructed in this project\n\nEach deployment will fetch the content from the (headless) WordPress instance, which ensures that the most recent content will be deployed. The Editorial team uses the WordPress instance to prepare blog posts and decides when to mark them as \"visible\" (or public). When we publish the blog through a tag, we pull the content available to it at that point in time.\n\n### -dev\n\nAll commits on the `main` branch are automatically deployed to our [-dev instance][dev]. The content of the blog might not always be up-to-date.\n\n#### How to redeploy -dev?\n\nEither push a new commit to the `main` branch or go to [the Circle CI page][circle-addons-blog], select the `main` branch and re-run the most recent `default-workflow` workflow.\n\n### -stage\n\nAs mentioned previously, git tags matching `<year>.<month>.<date>-stage` will be automatically deployed to our [-stage instance][stage].\n\nIn addition, we automatically update -stage every 3 hours using a CRON task configured in Circle CI (see `autodeploy-stage` workflow in the [Circle CI configuration](.circleci/config.yml)). This task looks for the most recent git tag matching the pattern above and deploy it. As mentioned previously, the content is fetched from the WordPress API every time.\n\n#### How to redeploy -stage?\n\nEither wait up to 3 hours or go to [the Circle CI page][circle-addons-blog] and re-run the `autodeploy-stage` workflow (or the most recent \"stage\" tag).\n\n### -prod\n\n**Important:** when deploying to production, please deploy to -stage first.\n\nGit tags matching `<year>.<month>.<date>` will be automatically deployed to our [-prod instance][prod].\n\nA git tag for production should point to a commit already tagged for stage so that we can deploy the exact same commit in both environments:\n\n```\n# an example from `git log`\n\ncommit 68da8f1d4ea536f7890012ab2b4c39299a853cc5 (tag: 2021.07.22-stage, tag: 2021.07.22)\nAuthor: William Durand <will+git@drnd.me>\nDate:   Mon Jul 19 11:39:56 2021 +0200\n\n    Use Node 14 (#265)\n```\n\n#### How to redeploy -prod?\n\nGo to [the Circle CI page][circle-addons-blog] and re-run the `default-workflow` workflow for the most recent \"prod\" tag.\n\n### About the WordPress theme\n\nThis project is also able to build a WordPress theme for the WordPress instance. Use `npm version` to create new releases and run `yarn build:wptheme` to build a ZIP file containing the theme. Finally, push the commit and tag (created with `npm version`) and make a [GitHub Release][gh-release], including the ZIP file as an asset.\n\n**Important:** it is generally a good idea to update the WordPress theme when a new version of `addons-frontend-blog-utils` has been merged. Once the ZIP file containing the theme has been generated and published as described above, a user with elevated privileges should _manually_ update the theme on the WordPress instance. This is usually a safe operation that consists in uploading the ZIP file in `Dashboard > Appearance > Themes`.\n\n[prettier]: https://prettier.io/\n[browsersync]: https://browsersync.io/\n[mkcert]: https://github.com/FiloSottile/mkcert\n[dev]: https://addons-dev.allizom.org/blog/\n[stage]: https://addons.allizom.org/blog/\n[prod]: https://addons.mozilla.org/blog/\n[circle-addons-blog]: https://app.circleci.com/pipelines/github/mozilla/addons-blog\n[gh-release]: https://github.com/mozilla/addons-blog/releases\n"
},
{
  "name": "nunjucks",
  "files": {
    "/": [
      ".babelrc",
      ".eslintignore",
      ".eslintrc.js",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".npmignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "MAINTENANCE.md",
      "README.md",
      "appveyor.yml",
      "bench",
      "bin",
      "bower.json",
      "codecov.yml",
      "contribute.json",
      "docs",
      "nunjucks",
      "package.json",
      "samples",
      "scripts",
      "tests"
    ],
    "/docs": [
      ".gitignore",
      "Makefile",
      "README.md",
      "_config-prod.yml",
      "_config.yml",
      "_layouts",
      "_plugins",
      "api.md",
      "bower_components",
      "cn",
      "css",
      "faq.md",
      "files",
      "fr",
      "getting-started.md",
      "img",
      "index.html",
      "js",
      "templating.md"
    ],
    "/.github": [
      "PULL_REQUEST_TEMPLATE.md",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Nunjucks\n\n[![NPM Version][npm-image]][npm-url]\n[![NPM Downloads][downloads-image]][downloads-url]\n[![Linux Build][github-actions-image]][github-actions-url]\n[![Windows Build][appveyor-image]][appveyor-url]\n[![Test Codecov][codecov-image]][codecov-url]\n\n[Nunjucks](https://mozilla.github.io/nunjucks/) is a full featured\ntemplating engine for javascript. It is heavily inspired by\n[jinja2](http://jinja.pocoo.org/). View the docs\n[here](https://mozilla.github.io/nunjucks/).\n\n## Installation\n\n`npm install nunjucks`\n\nTo use the file watcher built-in to Nunjucks, Chokidar must be installed separately.\n\n`npm install nunjucks chokidar`\n\n(View the [CHANGELOG](https://github.com/mozilla/nunjucks/releases))\n\n## Documentation\n\nSee [here](https://mozilla.github.io/nunjucks/).\n\n## Browser Support\n\nSupported in all modern browsers. For IE8 support, use [es5-shim](https://github.com/es-shims/es5-shim).\n\n## Tests\n\nRun the tests with `npm test`.\n\nWatch `master` branch's [tests running in the browser](https://mozilla.github.io/nunjucks/files/tests/browser/).\n\n## Mailing List\n\nJoin our mailing list and get help with and issues you have:\nhttps://groups.google.com/forum/?fromgroups#!forum/nunjucks\n\n## Want to help?\n\nContributions are always welcome! Before you submit an issue or pull request, please read our [contribution guidelines](CONTRIBUTING.md).\n\n[Contributors](https://github.com/mozilla/nunjucks/graphs/contributors)\n\n[npm-image]: https://img.shields.io/npm/v/nunjucks.svg\n[npm-url]: https://npmjs.org/package/nunjucks\n[downloads-image]: https://img.shields.io/npm/dm/nunjucks.svg\n[downloads-url]: https://npmjs.org/package/nunjucks\n[github-actions-image]: https://img.shields.io/github/workflow/status/mozilla/nunjucks/Tests/master.svg?label=linux\n[github-actions-url]: https://github.com/mozilla/nunjucks/actions\n[appveyor-image]: https://img.shields.io/appveyor/ci/fdintino/nunjucks/master.svg?label=windows\n[appveyor-url]: https://ci.appveyor.com/project/fdintino/nunjucks\n[codecov-image]: https://img.shields.io/codecov/c/gh/mozilla/nunjucks.svg\n[codecov-url]: https://codecov.io/gh/mozilla/nunjucks/branch/master\n"
},
{
  "name": "eslint-plugin-no-unsanitized",
  "files": {
    "/": [
      ".babelrc",
      ".eslintrc.json",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "NOTES",
      "README.md",
      "SCHEMA.md",
      "SECURITY.md",
      "docs",
      "index.js",
      "lib",
      "package-lock.json",
      "package.json",
      "tests",
      "yarn.lock"
    ],
    "/docs": [
      "rules"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/mozilla/eslint-plugin-no-unsanitized.svg?branch=master)](https://travis-ci.org/mozilla/eslint-plugin-no-unsanitized)\n# Disallow unsanitized code (no-unsanitized)\n\nThese rules disallow unsafe coding practices that may result into security\nvulnerabilities. We will disallow assignments (e.g., to innerHTML) as well as\ncalls (e.g., to insertAdjacentHTML) without the use of a pre-defined escaping\nfunction. The escaping functions must be called with a template string.\nThe function names are hardcoded as `Sanitizer.escapeHTML` and `escapeHTML`.\nThe plugin also supports the\n[Sanitizer API](https://developer.mozilla.org/en-US/docs/Web/API/HTML_Sanitizer_API)\nand calls to `.setHTML()` are also allowed by default.\n\nThis plugin is built for and used within Mozilla to maintain and improve the security\nof our products and services.\n\n# Rule Details\n\n## method\nThe *method* rule disallows certain function calls.\nE.g., `document.write()` or `insertAdjacentHTML()`.\nSee [docs/rules/method.md](docs/rules/method.md) for more.\n\n## property\nThe *property* rule disallows certain assignment expressions, e.g., to `innerHTML`.\n\nSee [docs/rules/property.md](docs/rules/property.md) for more.\n\n\n## Examples\n\nHere are a few examples of code that we do not want to allow:\n\n```js\nfoo.innerHTML = input.value;\nbar.innerHTML = \"<a href='\"+url+\"'>About</a>\";\n```\n\nA few examples of allowed practices:\n\n```js\nfoo.innerHTML = 5;\nbar.innerHTML = \"<a href='/about.html'>About</a>\";\nbar.innerHTML = escapeHTML`<a href='${url}'>About</a>`;\n```\n\n\n\n\n# Install\n\nWith **yarn** or **npm**:\n```\n$ yarn add -D eslint-plugin-no-unsanitized\n$ npm install --save-dev eslint-plugin-no-unsanitized\n```\n\n## Usage\n\nIn your `.eslintrc.json` file enable this rule with the following:\n\n```\n{\n    \"extends\": [\"plugin:no-unsanitized/DOM\"]\n}\n```\n\nOr:\n```\n{\n    \"plugins\": [\"no-unsanitized\"],\n    \"rules\": {\n        \"no-unsanitized/method\": \"error\",\n        \"no-unsanitized/property\": \"error\"\n    }\n}\n```\n\n# Documentation\nSee [docs/](docs/).\n"
},
{
  "name": "node-fx-runner",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bin",
      "index.js",
      "lib",
      "package-lock.json",
      "package.json",
      "renovate.json",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Runner\n\n[![CircleCI](https://circleci.com/gh/mozilla/node-fx-runner.svg?style=svg)](https://circleci.com/gh/mozilla/node-fx-runner)\n[![npm version](https://badge.fury.io/js/fx-runner.svg)](https://badge.fury.io/js/fx-runner)\n\n## API\n\n```\nUsage: fx-runner [options] [command]\n\nCommands:\n\nstart Start Firefox\n\nOptions:\n\n-h, --help               output usage information\n-V, --version            output the version number\n-b, --binary <path>      Path of Firefox binary to use.\n--binary-args <CMDARGS>  Pass additional arguments into Firefox.\n-p, --profile <path>     Path or name of Firefox profile to use.\n-v, --verbose            More verbose logging to stdout.\n--new-instance           Use a new instance\n--no-remote              Do not allow remote calls\n--foreground             Bring Firefox to the foreground\n-l, --listen <port>      Start the debugger server on a specific port.\n```\n\n### Releasing\n\nTo create a new release, do the following:\n\n* Pull from master to make sure you're up to date.\n* Bump the version in `package.json`.\n* Commit and push the version change\n  (or create and merge a pull request for it).\n* Create a [new release](https://github.com/mozilla/fx-runner/releases/new)\n  and paste in a changelog in Markdown format.\n  Title the github release after the new version you just\n  added in the previous commit to `package.json` (example: `1.0.4`).\n* When you publish the release, github creates a tag.\n  When TravisCI builds the tag,\n  it will automatically publish the package to\n  [npm](https://www.npmjs.com/package/fx-runner).\n"
},
{
  "name": "legal-docs",
  "files": {
    "/": [
      ".github",
      "CODE_OF_CONDUCT.md",
      "LICENSE.md",
      "README.md",
      "ab",
      "ar",
      "as",
      "bn",
      "br",
      "ca",
      "cnh",
      "cs",
      "cv",
      "cy",
      "de",
      "dv",
      "el",
      "en",
      "eo",
      "es-ES",
      "es-MX",
      "es",
      "et",
      "eu",
      "fa",
      "fr",
      "fy",
      "ga",
      "he",
      "hi",
      "hr",
      "hu",
      "id",
      "it",
      "ja",
      "kab",
      "kk",
      "ko",
      "ky",
      "lv",
      "mk",
      "mn",
      "ms",
      "mt",
      "nl",
      "pa",
      "package.json",
      "pl",
      "pt-BR",
      "pt",
      "rm",
      "ro",
      "ru",
      "rw",
      "sah",
      "sk",
      "sl",
      "sq",
      "sr",
      "sv",
      "sw",
      "ta",
      "tl-PH",
      "tr",
      "tt",
      "uk",
      "vi",
      "zh-CN",
      "zh-TW"
    ],
    "/.github": [
      "requirements.txt",
      "scripts",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Legal Documents\n\n[![Docs Linter](https://github.com/mozilla/legal-docs/actions/workflows/doc_linter.yml/badge.svg)](https://github.com/mozilla/legal-docs/actions/workflows/doc_linter.yml)\n\nThis repository contains legal documents and their applicable translations. Each subdirectory represents a single legal document. Within a document directory is a series of markdown files, each named for the language the document is written in. For instance, `firefox_privacy_notice/de.md` is the German version of the Firefox privacy policy.\n\n## Editing\n\n### Legal Team\n\nIf you're a member of the legal team, it's easy to get started. First, make sure you're logged in with your GitHub account. When you have a change which you wish to make, simply find the `en-US.md` version of the document that you wish to change. Open it and press the `Edit` button. If you have never gone through this process before, you will be asked to \"fork\" the repository (you should).\n\nOn the edit page, you'll be presented with a \"raw\" version of the legal document. Edit the text you wish to change. When you are done editing, there will be a section at the bottom of the page to save your work. Under \"Commit summary\", enter a brief (tweet-length) description of the change(s) you're making. In the \"Extended description\" field, explain what the changes are and why they're needed. When you're done, commit your changes.\n\nAt this point, you'll be presented with a form to submit a pull request. The bottom section of the page will show all of the change you've made to the document, and all you should need to do is press the \"Send Pull Request\" button.\n\nOnce you've submitted a pull request, your colleagues can review the work under the Pull Requests tab on the right-hand side, and a contributor to the repo can merge your changes.\n\n### Markdown\n\nThese files are written in the [Markdown syntax](https://daringfireball.net/projects/markdown/syntax). In some languages, the date format contains dots like `15. travnja 2014.` but this may end up as an ordered list according to the syntax. The workaround is using a backslash to escape a dot after the leading digit(s) like `15\\. travnja 2014.`\n\nSome Markdown files use the [Attribute Lists extension](https://pythonhosted.org/Markdown/extensions/attr_list.html) of the [Python Markdown library](https://pypi.python.org/pypi/Markdown). This is useful when you'd like to set the ID or class of an element, but the extended syntax may cause an error if your parser doesn't support this extension.\n\n### Localization\n\nLocalization of these documents is managed through an external vendor. As a localizer, you're encouraged to find and fix translation issues. All document translations should match the precise meaning in the corresponding `en` version of the document. Additionally, we're always interested in finding errors in markup, broken links, etc.\n\nFile a pull request that updates your locale's markdown file to match the `en` version as closely as possible. When you're done, submit a pull request. Someone at Mozilla will review your changes. If there are no problems, your changes will be merged.\n\nImportant: we won't be accepting brand new translations for existing documents, only fixes to existing ones.\n\nUseful links:\n* A summary of the localization status is [available here](https://github.com/mozilla/legal-docs/tree/l10n_reference#readme), with a list of localized and non localized documents.\n* A full breakdown, with the list of locales for each document, is available in JSON format [here](https://github.com/mozilla/legal-docs/blob/l10n_reference/.github/stats.json).\n\n### Package.json\n\n[Firefox Accounts](https://github.com/mozilla/fxa/tree/main/packages/fxa-content-server) relies on `package.json` to import this repository through `npm` and convert specific documents to HTML.\n\n## Getting changes to production\n\nFor www.mozilla.org, the process is as follows:\n\n1. Check the documents to be published on the staging site and make sure they look right:\n    * https://www-dev.allizom.org/privacy/\n    * https://www-dev.allizom.org/about/legal/\n2. [Open this link](https://github.com/mozilla/legal-docs/actions/workflows/prod_push.yml) to access the _Actions_ tab in the repo on the `Publish Documents to Production` workflow. ![Screen Shot 2022-03-25 at 09 35 53](https://user-images.githubusercontent.com/19176817/160134801-1cacc2fa-ba16-4c11-a0fd-1dc9ac6b7240.png)\n3. There will be a banner that says \"This workflow has a workflow_dispatch event trigger.\" and, next to it on the right, a button that says `Run Workflow`. Click this button. ![Screen Shot 2022-03-25 at 09 36 14](https://user-images.githubusercontent.com/19176817/160134925-05b56aec-4c0a-4104-bc1e-e57c9cb812d0.png)\n4. A form should now open with a field for a list of \"files to publish\". The default value is `ALL`. If you change nothing and click the `Run workflow` button, all changes to all files will be included in the pull request that is opened. If that is not what you want, you can enter file names in this field. It can be a comma separated list of filenames (e.g. `mdn_plus_terms.md, mdn_plus_privacy.md`), or you can use the `*` character to include all files with a prefix (e.g. `mdn_plus_*`).\n5. Click `Run workflow`.\n6. It may take a few seconds, but the running workflow will show up on the screen with an animated progress indicator. ![Screen Shot 2022-03-25 at 09 38 40](https://user-images.githubusercontent.com/19176817/160135112-29db7e7c-7589-4055-a56a-577ad3527344.png)\n7. Once this is finished, and everything was successful, a new [Pull Request](https://github.com/mozilla/legal-docs/pulls?q=is%3Aopen+is%3Apr+label%3Aprod) should have been opened with all of the requested changes.\n8. Review this Pull Request. It will have the title `[prod] Publish document updates`.\n9. Once you are satisfied that the correct changes have been included, merge the Pull Request and the changes will be published to the website within a few minutes.\n\n\n## URL Inventory\n\nFollowing is a list of directories in this repository and either their URL counterparts on the web or directions to see the document in case it isn't easily linkable.  The URLs below will link to the *English (US)* versions of the document but replacing *en-US* in the URL with an available language code will show alternative languages.\n\nGenerally speaking Privacy Notice documents will be at https://www.mozilla.org/privacy/ and Terms documents at\nhttps://www.mozilla.org/about/legal/.\n\n* /firefox_privacy_notice\n    * https://www.mozilla.org/en-US/privacy/firefox/\n* /mozilla_privacy_policy\n    * https://www.mozilla.org/en-US/privacy/\n* /websites_privacy_notice\n    * https://www.mozilla.org/en-US/privacy/websites/\n"
},
{
  "name": "hglib-rust",
  "files": {
    "/": [
      ".clippy.toml",
      ".gitignore",
      "Cargo.lock",
      "Cargo.toml",
      "README.md",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# hglib-rust\n\n[![Crates.io](https://img.shields.io/crates/v/hglib-rs.svg)](https://crates.io/crates/hglib-rs)\n\n**hlgib-rust** is a Rust library to interact with a mercurial repository.\nIt's pretty similar to python-hglib but in Rust.\n\n## License\n\nPublished under the MPL 2.0 license.\n"
},
{
  "name": "mozaggregator2bq",
  "files": {
    "/": [
      ".env.template",
      ".gitignore",
      "Dockerfile",
      "README.md",
      "bin",
      "docker-compose.yml",
      "notebooks",
      "requirements-dev.in",
      "requirements-dev.txt",
      "requirements.in",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# mozaggregator2bq\n\nA set of scripts for loading Firefox Telemetry aggregates into BigQuery. These\naggregates power the Telemetry Dashboard and Evolution Viewer.\n\n## Overview\n\nBuild the container and launch it:\n\n```bash\ndocker-compose build\ndocker-compose run --rm app bash\n```\n\n### Interacting with the database\n\nTo start a psql instance with the read-only replica of the production Postgres\ninstance, run the following commands. Ensure that you have the appropriate AWS\ncredentials.\n\n```bash\nsource bin/export_postgres_credentials_s3\n\nPGPASSWORD=$POSTGRES_PASS psql \\\n    --host=\"$POSTGRES_HOST\" \\\n    --username=\"$POSTGRES_USER\" \\\n    --dbname=\"$POSTGRES_DB\"\n```\n\nAn example query:\n\n```sql\n-- list all aggregates by build_id\nselect tablename\nfrom pg_catalog.pg_tables\nwhere schemaname='public' and tablename like 'build_id%';\n\n--  build_id_aurora_0_20130414\n--  build_id_aurora_0_20150128\n--  build_id_aurora_0_20150329\n--  build_id_aurora_1_20130203\n--  build_id_aurora_1_20150604\n-- ...\n\n-- list all aggregates by submission_date\nselect tablename\nfrom pg_catalog.pg_tables\nwhere schemaname='public' and tablename like 'submission_date%';\n\n--  submission_date_beta_1_20151027\n--  submission_date_nightly_40_20151029\n--  submission_date_beta_39_20151027\n--  submission_date_nightly_1_20151025\n--  submission_date_nightly_39_20151031\n-- ...\n```\n\n### Database dumps by aggregate type and date\n\nTo start dumping data, run the following commands.\n\n```bash\nsource bin/export_postgres_credentials_s3\n\ntime DATA_DIR=data AGGREGATE_TYPE=submission DS_NODASH=20191201 bin/pg_dump_by_day\n# 23.92s user 1.97s system 39% cpu 1:05.48 total\n\ntime DATA_DIR=data AGGREGATE_TYPE=build_id DS_NODASH=20191201 bin/pg_dump_by_day\n# 3.47s user 0.49s system 24% cpu 16.188 total\n```\n\nThis should result in gzipped files in the following hierarchy.\n\n```bash\ndata\n\u251c\u2500\u2500 [  96]  build_id\n\u2502   \u2514\u2500\u2500 [ 128]  20191201\n\u2502       \u251c\u2500\u2500 [8.4M]  474306.dat.gz\n\u2502       \u2514\u2500\u2500 [1.6K]  toc.dat\n\u2514\u2500\u2500 [  96]  submission\n    \u2514\u2500\u2500 [3.2K]  20191201\n        \u251c\u2500\u2500 [ 74K]  474405.dat.gz\n        \u251c\u2500\u2500 [ 48K]  474406.dat.gz\n        ....\n        \u251c\u2500\u2500 [1.8M]  474504.dat.gz\n        \u2514\u2500\u2500 [ 93K]  toc.dat\n\n4 directories, 103 files\n```\n\nSee the [`pg_dump` documentation](https://www.postgresql.org/docs/9.1/app-pgdump.html) for details on the file format.\n\n```bash\n$ gzip -cd data/submission/20191201/474405.dat.gz | head -n3\n{\"os\": \"Windows_NT\", \"child\": \"false\", \"label\": \"\", \"metric\": \"A11Y_INSTANTIATED_FLAG\", \"osVersion\": \"6.3\", \"application\": \"Firefox\", \"architecture\": \"x86\"}    {0,2,0,2,2}\n{\"os\": \"Windows_NT\", \"child\": \"false\", \"label\": \"\", \"metric\": \"A11Y_CONSUMERS\", \"osVersion\": \"6.3\", \"application\": \"Firefox\", \"architecture\": \"x86\"}    {0,0,0,0,0,0,0,0,0,0,2,0,20,2}\n{\"os\": \"Windows_NT\", \"child\": \"false\", \"label\": \"\", \"metric\": \"A11Y_ISIMPLEDOM_USAGE_FLAG\", \"osVersion\": \"6.3\", \"application\": \"Firefox\", \"architecture\": \"x86\"}        {2,0,0,0,2}\n```\n\n### Running a notebook\n\nEnsure your data directory in the top-level directory matches the one in the\nnotebook. Run the following script.\n\n```bash\nbin/start-jupyter\n```\n\nThis script can be modified to include various configuration parameters for\nspark, including the default parallelism and the amount of executor memory.\n\n### Processing `pg_dump` into parquet\n\nRun the following scripts to transform the data dumps into parquet, where the\njson fields have been transformed into appropriate columns and arrays.\n\n```bash\nbin/submit-local bin/pg_dump_to_parquet.py \\\n    --input-dir data/submission_date/20191201 \\\n    --output-dir data/parquet/submission_date/20191201\n```\n\n### Running backfill\n\nThe `bin/backfill` script will dump data from the Postgres database, transform\nthe data into Parquet, and load the data into a BigQuery table. The current\nschema for the table is as follows:\n\nField name | Type | Mode\n-|-|-\ningest_date | DATE | REQUIRED\naggregate_type | STRING | NULLABLE\nds_nodash | STRING | NULLABLE\nchannel | STRING | NULLABLE\nversion | STRING | NULLABLE\nos | STRING | NULLABLE\nchild | STRING | NULLABLE\nlabel | STRING | NULLABLE\nmetric | STRING | NULLABLE\nosVersion | STRING | NULLABLE\napplication | STRING | NULLABLE\narchitecture | STRING | NULLABLE\naggregate | STRING | NULLABLE\n\nThere is a table for the build id aggregates and the submission date aggregates.\nThe build ids are truncated to the nearest date.\n\nIt may be useful to use a small entry-point script.\n\n```bash\n#!/bin/bash\nset -x\nstart=2015-06-01\nend=2020-04-01\nwhile ! [[ $start > $end ]]; do\n    rm -r data\n    up_to=$(date -d \"$start + 1 month\" +%F)\n    START_DS=$start END_DS=$up_to bash -x bin/backfill\n    start=$up_to\ndone\n```"
},
{
  "name": "rust-cascade",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Cargo.toml",
      "README.md",
      "license.txt",
      "src",
      "test_data"
    ]
  },
  "makefile": null,
  "readme": "# rust-cascade\nA Bloom filter cascade implementation in rust. This can utilize one of two hash\nfunctions:\n\n* MurmurHash32, or\n* SHA256, with an optional salt\n\nThis implementation is designed to match up with the Python [filter-cascade\nproject](https://pypi.org/project/filtercascade/)\n[[github](https://github.com/mozilla/filter-cascade)]\n\nSee tests in src/lib.rs to get an idea of usage.\n"
},
{
  "name": "crlite",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitignore",
      ".pre-commit-config.yaml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Pipfile",
      "README.md",
      "benchmarking",
      "containers",
      "docs",
      "go.mod",
      "go",
      "hooks",
      "moz_kinto_publisher",
      "pytest.ini",
      "rust-create-cascade",
      "rust-query-crlite",
      "setup.py",
      "setup",
      "test-via-docker.sh",
      "version.json",
      "workflow"
    ],
    "/docs": [
      ".DS_Store",
      "figure1-information_flow.png",
      "figure2-filter_process.png",
      "figure3-filter_structure.png",
      "figure4-certificate_identifier.png"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://circleci.com/gh/mozilla/crlite.svg?style=shield)](https://circleci.com/gh/mozilla/crlite)\n![Maturity Level: Beta](https://img.shields.io/badge/maturity-beta-blue.svg)\n![Docker Version](https://img.shields.io/docker/v/mozilla/crlite)\n\nCRLite uses a Bloom filter cascade [and whole-ecosystem analysis of the Web PKI](https://www.certificate-transparency.org/) to push the entire web\u2019s TLS revocation information to Firefox clients, replacing [OCSP](https://en.wikipedia.org/wiki/Online_Certificate_Status_Protocol) for most browser TLS connections, speeding up connection time while continuing to support PKI revocations. The system was [originally proposed at IEEE S&P 2017](http://www.ccs.neu.edu/home/cbw/static/pdf/larisch-oakland17.pdf).\n\nFor details about CRLite, [Mozilla Security Engineering has a blog post series](https://blog.mozilla.org/security/tag/crlite/), and [this repository has a FAQ](https://github.com/mozilla/crlite/wiki#faq).\n\nThere are also useful end-user tools for querying CRLite: [moz_crlite_query](https://github.com/mozilla/moz_crlite_query), to query the current CRLite filter for revocations, and a diagnostic tool [crlite_status](https://github.com/jcjones/crlite_status) to monitor filter generation metrics.\n\n\n## General Structure\n\nCRLite is designed to run in Kubernetes, with the following services:\n\n1. [`containers/crlite-fetch`](https://github.com/mozilla/crlite/tree/main/containers/crlite-fetch), a constantly-running task that downloads from Certificate Transparency logs into Redis and Google Firestore\n1. [`containers/crlite-generate`](https://github.com/mozilla/crlite/tree/main/containers/crlite-generate), a periodic (cron) job that produces a CRLite filter from the data in Redis and uploads the artifacts into Google Cloud Storage\n1. [`containers/crlite-publish`](https://github.com/mozilla/crlite/tree/main/containers/crlite-publish), a periodic (cron) job that publishes the results of a `crlite-generate` run to a Kinto instance.\n1. [`containers/crlite-signoff`](https://github.com/mozilla/crlite/tree/main/containers/crlite-signoff), a periodic (cron) job that verifies and approves data `crlite-publish` placed in a Kinto instance.\n\nThere are scripts in [`containers/`](https://github.com/mozilla/crlite/tree/main/containers) to build Docker images both using Docker, see`build-local.sh`. There are also builds at Docker Hub in the [`mozilla/crlite`](https://hub.docker.com/r/mozilla/crlite) project.\n\n\n### Storage\nStorage consists of these parts:\n\n1. Redis, e.g. Google Cloud Memorystore, for certificate metadata (CRL DPs, serial numbers, expirations, issuers), used in filter generation.\n1. Google Cloud Storage, for storage of the artifacts when a job is completed.\n1. A local persistent disk, for persistent storage of downloaded CRLs. This is defined in [`containers/crl-storage-claim.yaml`](https://github.com/mozilla/crlite/blob/main/containers/crl-storage-claim.yaml).\n\n\n### Information Flow\n\nThis tooling monitors Certificate Transparency logs and, upon secheduled execution, `crlite-generate` produces a new filter and uploads it to Cloud Storage.\n\n![Information flow](docs/figure1-information_flow.png)\n\nThe process for producing a CRLite filter, is run by [`system/crlite-fullrun`](https://github.com/mozilla/crlite/blob/main/system/crlite-fullrun), which is described in block form in this diagram:\n\n![Process for building a CRLite Bloom filter](docs/figure2-filter_process.png)\n\nThe output Bloom filter cascade is built by the Python [`mozilla/filter-cascade`](https://github.com/mozilla/filter-cascade) tool and then read in Firefox by the Rust [`mozilla/rust-cascade`](https://github.com/mozilla/rust-cascade) package.\n\nFor complete details of the filter construction see Section III.B of the [CRLite paper](http://www.ccs.neu.edu/home/cbw/static/pdf/larisch-oakland17.pdf).\n\n![Structure of the CRLite Bloom filter cascade](docs/figure3-filter_structure.png)\n\nThe keys used into the CRLite data structure consist of the SHA256 digest of the issuer's `Subject Public Key Information` field in DER-encoded form, followed by the the certificate's serial number, unmodified, in DER-encoded form.\n\n![Structure of Certificate Identifiers](docs/figure4-certificate_identifier.png)\n\n\n## Local Installation\n\nIt's possible to run the tools locally, though you will need local instances of Redis and Firestore. First, install the tools and their dependencies\n\n```sh\ngo install -u github.com/mozilla/crlite/go/cmd/ct-fetch\ngo install -u github.com/mozilla/crlite/go/cmd/aggregate-crls\ngo install -u github.com/mozilla/crlite/go/cmd/aggregate-known\n\npipenv install\n```\n\n\n### Configuration\n\nYou can configure via environment variables, or via a config file. Environment variables are specified in the [`/containers/*.properties.example`](https://github.com/mozilla/crlite/tree/main/containers) files. To use a configuration file,  `~/.ct-fetch.ini` (or any file selected on the CLI using `-config`), construct it as so:\n\n```\ncertPath = /ct\nnumThreads = 16\ncacheSize = 128\n```\n\n\n#### Parameters\n\nYou'll want to set a collection of configuration parameters:\n\n* `runForever [true/false]`\n* `logExpiredEntries [true/false]`\n* `numThreads 16`\n* `cacheSize [number of cache entries. An individual entry contains an issuer-day's worth of serial numbers, which could be as much as 64 MB of RAM, but is generally closer to 1 MB.]`\n* `outputRefreshMs [milliseconds]`\n\nThe log list is all the logs you want to sync, comma separated, as URLs:\n* `logList = https://ct.googleapis.com/icarus, https://oak.ct.letsencrypt.org/2021/`\n\nTo get all current ones from\n[certificate-transparency.org](https://certificate-transparency.org/):\n```\necho \"logList = $(setup/list_all_active_ct_logs)\" >> ~/.ct-fetch.ini\n```\n\nIf running forever, set the delay on polling for new updates, per log. This will have some jitter added:\n* `pollingDelay` [minutes]\n\nIf not running forever, you can give limits or slice up CT log data:\n* `limit` [uint]\n* `offset` [uint]\n\nYou'll also need to configure credentials used for Google Cloud Storage:\n* `GOOGLE_APPLICATION_CREDENTIALS` [base64-encoded string of the service credentials JSON]\n\nIf you need to proxy the connection, perhaps via SSH, set the `HTTPS_PROXY` to something like `socks5://localhost:32547/\"` as well.\n\n\n### General Operation\n\n[`containers/build-local.sh`](https://github.com/mozilla/crlite/tree/main/containers/build-local.sh) produces the Docker containers locally.\n\n[`test-via-docker.sh`](https://github.com/mozilla/crlite/tree/main/test-via-docker.sh) executes a complete \"run\", syncing with CT and producing a filter. It's configured using a series of environment variables.\n\nNote that since all data is stored in Redis, a robust backup for the Redis information is warranted to avoid expensive resynchronization.\n\n### Starting the Local Dependencies\n\nRedis can be provided in a variety of ways, easiest is probably the Redis docker distribution. For whatever reason, I have the\nbest luck remapping ports to make it run on 6379:\n```sh\ndocker run -p 6379:7000 redis:4 --port 7000\n```\n\n\n## Running from a Docker Container\n\nTo construct a container, see [`containers/README.md`](https://github.com/mozilla/crlite/tree/main/containers/README.md).\n\nThe crlite-fetch container runs forever, fetching CT updates:\n\n```sh\ndocker run --rm -it \\\n  -e \"FIRESTORE_EMULATOR_HOST=my_ip_address:8403\" \\\n  -e \"outputRefreshMs=1000\" \\\n  crlite:staging-fetch\n```\n\nThe crlite-generate container constructs a new filter. To use local disk, set the `certPath` to `/ctdata` and mount that volume in Docker. You should also mount the volume `/processing` to get the output files:\n\n```sh\ndocker run --rm -it \\\n  -e \"certPath=/ctdata\" \\\n  -e \"outputRefreshMs=1000\" \\\n  --mount type=bind,src=/tmp/ctlite_data,dst=/ctdata \\\n  --mount type=bind,src=/tmp/crlite_results,dst=/processing \\\n  crlite:staging-generate\n```\n\nSee the [`test-via-docker.sh`](https://github.com/mozilla/crlite/blob/main/test-via-docker.sh) for an example.\n\nTo run in a remote container, such as a Kubernetes pod, you'll need to make sure to set all the environment variables properly, and the container should otherwise work. See [`containers/crlite-config.properties.example`](https://github.com/mozilla/crlite/blob/main/containers/crlite-config.properties.example) for an example of the Kubernetes environment that can be imported using `kubectl create configmap`, see the `containers` README.md for details.\n\n\n## Tools\n\n*`ct-fetch`*\nDownloads all CT entries' certificates to a Firestore instance and collects their metadata.\n\n*`aggregate-crls`*\nObtains all CRLs defined in all CT entries' certificates, verifies them, and collates their results\ninto `*issuer SKI base64*.revoked` files.\n\n*`aggregate-known`*\nCollates all CT entries' unexpired certificates into `*issuer SKI base64*.known` files.\n\n\n\n## Credits\n\n* The CRLite research team: James Larsich, David Choffnes, Dave Levin, Bruce M. Maggs, Alan Mislove, and Christo Wilson.\n* Benton Case for [certificate-revocation-analysis](https://github.com/casebenton/certificate-revocation-analysis), which kicked off this effort.\n* Mark Goodwin for the original Python [`filter_cascade`](https://gist.githubusercontent.com/mozmark/c48275e9c07ccca3f8b530b88de6ecde/raw/19152f7f10925379420aa7721319a483273d867d/sample.py) and the [`filter-cascade`](https://github.com/mozilla/filter-cascade) project.\n* Dana Keeler and Mark Goodwin together for the Rust [`rust-cascade`](https://github.com/mozilla/rust-cascade).\n"
},
{
  "name": "firefox-translations-evaluation",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "data",
      "eval",
      "install",
      "requirements.txt",
      "start_docker.sh",
      "translators"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Translations Evaluation\nCalculates BLEU scores for Firefox Translations [models](https://github.com/mozilla/firefox-translations-models) \nusing [bergamot-translator](https://github.com/mozilla/bergamot-translator) and compares them to other translation systems.\n\n## Running\n\n### Clone repo\n```\ngit clone https://github.com/mozilla/firefox-translations-evaluation.git\ncd firefox-translations-evaluation\n```\n\n### Download models\n\nUse `install/download-model.sh` to get Firefox Translations [models](https://github.com/mozilla/firefox-translations-models) or use your own ones.\n\n### Start docker\nRecommended memory size for Docker is **8gb**.\n\n\n```\nexport MODELS=<absolute path to a local directory with models>\n\n# Specify Azure key and location if you want to add Azure Translator API for comparison\nexport AZURE_TRANSLATOR_KEY=<Azure translator resource API key>\n# optional, specify if it's different than default 'global'\nexport AZURE_LOCATION=<location>\n\n# Specify GCP credentials json path if you want to add Google Translator API for comparison\nexport GCP_CREDS_PATH=<absolute path to .json>\n\n# Build and run docker container\nbash start_docker.sh\n```\n\nOn completion, your terminal should be attached to the launched container.\n\n### Run evaluation\nFrom inside docker container run:\n```\npython3 eval/evaluate.py --translators=bergamot,microsoft,google --pairs=all --skip-existing --models-dir=/models/models/prod --results-dir=/models/evaluation/prod\n```\nMore options:\n```\npython3 eval/evaluate.py --help\n```\n\n## Details\n### Installation scripts\n`install/install-bergamot-translator.sh` - clones and compiles [bergamot-translator](https://github.com/mozilla/bergamot-translator) and [marian](https://github.com/marian-nmt/marian-dev) (launched in docker image).\n\n`install/download-models.sh` - downloads current Mozilla production [models](https://github.com/mozilla/firefox-translations-models).\n\n### Translators\n1. **bergamot** - uses compiled [bergamot-translator](https://github.com/mozilla/bergamot-translator) in wasm mode\n2. **marian** - uses compiled [marian](https://github.com/marian-nmt/marian-dev)\n3. **google** - users Google Translation [API](https://cloud.google.com/translate)\n4. **microsoft** - users Azure Cognitive Services Translator [API](https://azure.microsoft.com/en-us/services/cognitive-services/translator/)\n\n### Reuse already calculated scores\nUse `--skip-existing` option to reuse already calculated scores saved as `results/xx-xx/*.bleu` files.\nIt is useful to continue evaluation if it was interrupted \nor to rebuild a full report reevaluating only selected translators.\n\n### Datasets\n[SacreBLEU](https://github.com/mjpost/sacrebleu) - all available datasets for a language pair are used for evaluation.\n\n[Flores](https://github.com/facebookresearch/flores) - parallel evaluation dataset for 101 languages.\n\n### Language pairs\nWith option `--pairs=all`, language pairs will be discovered \nin the specified models folder (option `--models-dir`) \nand evaluation will run for all of them.\n\n### Results\nResults will be written to the specified directory (option `--results-dir`).\n\nEvaluation results for models that are used in Firefox Translation can be found in [firefox-translations-models/evaluation](https://github.com/mozilla/firefox-translations-models/tree/main/evaluation)\n"
},
{
  "name": "great_expectations_demo",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "great_expectations",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# Data Monitoring MVP\n\nUses [Great Expectations](https://docs.greatexpectations.io/docs)\n\nThe idea was to demonstrate that introducing a data monitoring solution would provide value to Mozilla and help improve our data health status visibility.\n\n"
},
{
  "name": "mozilla-django-oidc",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".github",
      ".gitignore",
      "AUTHORS.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "docker-compose.yml",
      "docs",
      "integration_tests",
      "mozilla_django_oidc",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "authors.rst",
      "conf.py",
      "contributing.rst",
      "drf.rst",
      "history.rst",
      "index.rst",
      "installation.rst",
      "make.bat",
      "settings.rst",
      "xhr.rst"
    ],
    "/.github": [
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: clean-pyc clean-build docs help\n.DEFAULT_GOAL := help\n\nhelp:\n\t@perl -nle'print $& if m{^[a-zA-Z_-]+:.*?## .*$$}' $(MAKEFILE_LIST) | sort | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-25s\\033[0m %s\\n\", $$1, $$2}'\n\nclean: clean-build clean-pyc\n\nclean-build: ## remove build artifacts\n\trm -fr build/\n\trm -fr dist/\n\trm -fr *.egg-info\n\nclean-pyc: ## remove Python file artifacts\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\nlint: ## check style with flake8\n\tflake8 mozilla_django_oidc tests\n\ntest: ## run tests quickly with the default Python\n\tDJANGO_SETTINGS_MODULE=tests.settings django-admin.py test\n\ntest-all: ## run tests on every Python version with tox\n\ttox\n\ncoverage: ## check code coverage quickly with the default Python\n\tDJANGO_SETTINGS_MODULE=tests.settings coverage run --source mozilla_django_oidc  `which django-admin.py` test\n\tcoverage report -m\n\tcoverage html\n\topen htmlcov/index.html\n\ndocs: ## generate Sphinx HTML documentation, including API docs\n\trm -rf docs/source\n\tsphinx-apidoc -o docs/source/ mozilla_django_oidc\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C docs html\n\nrelease: clean ## package and upload a release\n\tpython setup.py sdist upload\n\tpython setup.py bdist_wheel upload\n\nsdist: clean ## package\n\tpython setup.py sdist\n\tls -l dist\n",
  "readme": "===================\nmozilla-django-oidc\n===================\n\n.. image:: https://badge.fury.io/py/mozilla-django-oidc.svg\n   :target: https://badge.fury.io/py/mozilla-django-oidc\n\n.. image:: https://travis-ci.org/mozilla/mozilla-django-oidc.svg?branch=master\n   :target: https://travis-ci.org/mozilla/mozilla-django-oidc\n\n.. image:: https://codecov.io/gh/mozilla/mozilla-django-oidc/branch/master/graph/badge.svg\n   :target: https://codecov.io/gh/mozilla/mozilla-django-oidc\n\n.. image:: https://circleci.com/gh/mozilla/mozilla-django-oidc/tree/master.svg?style=svg\n   :target: https://circleci.com/gh/mozilla/mozilla-django-oidc/tree/master\n\nA lightweight authentication and access management library for integration with OpenID Connect enabled authentication services.\n\n\nDocumentation\n-------------\n\nThe full documentation is at `<https://mozilla-django-oidc.readthedocs.io>`_.\n\n\nDesign principles\n-----------------\n\n* Keep it as minimal/lightweight as possible\n* Store as few authn/authz artifacts as possible\n* Allow custom functionality by overriding the authentication backend\n* Mainly support OIDC authorization code flow\n* Allow shipping Mozilla-centric authn/authz features\n* Test against all supported Python/Django version\n* E2E tested and audited by `Mozilla InfoSec <https://infosec.mozilla.org/>`_\n\n\nRunning Unit Tests\n-------------------\n\nUse ``tox`` to run as many different versions of Python you have. If you\ndon't have ``tox`` installed (and executable) already you can either\ninstall it in your system Python or `<https://pypi.python.org/pypi/pipsi>`_.\nOnce installed, simply execute in the project root directory.\n\n.. code-block:: shell\n\n    $ tox\n\n``tox`` will do the equivalent of installing virtual environments for every\ncombination mentioned in the ``tox.ini`` file. If your system, for example,\ndoesn't have ``python3.4`` those ``tox`` tests will be skipped.\n\nFor a faster test-rinse-repeat cycle you can run tests in a specific\nenvironment with a specific version of Python and specific version of\nDjango of your choice. Here is such an example:\n\n\n.. code-block:: shell\n\n    $ virtualenv -p /path/to/bin/python3.5 venv\n    $ source venv\n    (venv) $ pip install -r requirements/requirements_dev.txt\n    (venv) $ DJANGO_SETTINGS_MODULE=tests.settings django-admin.py test\n\nMeasuring code coverage, continuing the steps above:\n\n.. code-block:: shell\n\n    (venv) $ pip install coverage\n    (venv) $ DJANGO_SETTINGS_MODULE=tests.settings coverage run --source mozilla_django_oidc `which django-admin.py` test\n    (venv) $ coverage report\n    (venv) $ coverage html\n    (venv) $ open htmlcov/index.html\n\nLocal development\n-----------------\n\nThe local development setup is based on Docker so you need the following installed in your system:\n\n* `docker`\n* `docker-compose`\n\nYou will also need to edit your ``hosts`` file to resolve ``testrp`` and ``testprovider`` hostnames to ``127.0.0.1``.\n\nRunning test services\n=====================\n\nTo run the `testrp` and `testprovider` instances run the following:\n\n.. code-block:: shell\n\n   (venv) $ docker-compose up -d testprovider testrp\n\nThen visit the testing django app on: ``http://testrp:8081``.\n\nThe library source code is mounted as a docker volume and source code changes are reflected directly in.\nIn order to test a change you need to restart the ``testrp`` service.\n\n.. code-block:: shell\n\n   (venv) $ docker-compose stop testrp\n   (venv) $ docker-compose up -d testrp\n\nRunning integration tests\n=========================\n\nIntegration tests are mounted as a volume to the docker containers. Tests can be run using the following command:\n\n.. code-block:: shell\n\n   (venv) $ docker-compose run --service-ports testrunner\n\nLinting\n-------\n\nAll code is checked with `<https://pypi.python.org/pypi/flake8>`_ in\ncontinuous integration. To make sure your code still passes all style guides\ninstall ``flake8`` and check:\n\n.. code-block:: shell\n\n    $ flake8 mozilla_django_oidc tests\n\n.. note::\n\n    When you run ``tox`` it also does a ``flake8`` run on the main package\n    files and the tests.\n\nYou can also run linting with ``tox``:\n\n.. code-block:: shell\n\n    $ tox -e lint\n\n\nReleasing a new version\n------------------------\n\n``mozilla-django-oidc`` releases are hosted in `PyPI <https://pypi.python.org/pypi/mozilla-django-oidc>`_.\nHere are the steps you need to follow in order to push a new release:\n\n* Make sure that ``HISTORY.rst`` is up-to-date focusing mostly on backwards incompatible changes.\n\n  Security vulnerabilities should be clearly marked in a \"Security issues\" section along with\n  a level indicator of:\n\n  * High: vulnerability facilitates data loss, data access, impersonation of admin, or allows access\n    to other sites or components\n\n    Users should upgrade immediately.\n\n  * Medium: vulnerability endangers users by sending them to malicious sites or stealing browser\n    data.\n\n    Users should upgrade immediately.\n\n  * Low: vulnerability is a nuissance to site staff and/or users\n\n    Users should upgrade.\n\n* Bump the project version and create a commit for the new version.\n\n  * You can use ``bumpversion`` for that. It is a tool to automate this procedure following the `semantic versioning scheme <http://semver.org/>`_.\n\n    * For a patch version update (eg 0.1.1 to 0.1.2) you can run ``bumpversion patch``.\n    * For a minor version update (eg 0.1.0 to 0.2.0) you can run ``bumpversion minor``.\n    * For a major version update (eg 0.1.0 to 1.0.0) you can run ``bumpversion major``.\n\n* Create a `signed tag <https://git-scm.com/book/tr/v2/Git-Tools-Signing-Your-Work>`_ for that version\n\n  Example::\n\n      git tag -s 0.1.1 -m \"Bump version: 0.1.0 to 0.1.1\"\n\n* Push the signed tag to Github\n\n  Example::\n\n      git push origin 0.1.1\n\nThe release is pushed automatically to PyPI using a travis deployment hook on every new tag.\n\n\nLicense\n-------\n\nThis software is licensed under the MPL 2.0 license. For more info check the LICENSE file.\n\n\nCredits\n-------\n\nTools used in rendering this package:\n\n*  Cookiecutter_\n*  `cookiecutter-djangopackage`_\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`cookiecutter-djangopackage`: https://github.com/pydanny/cookiecutter-djangopackage\n"
},
{
  "name": "oldpto",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "FirePHPCore",
      "README.md",
      "auth.php",
      "class.Debug.php",
      "class.Filtering.php",
      "config-dist.php",
      "contribute.json",
      "css",
      "edit.php",
      "export.php",
      "favicon.ico",
      "filtering.inc",
      "img",
      "index.php",
      "js",
      "mypto.php",
      "nubis",
      "output.inc",
      "perms.inc",
      "prefetch.inc",
      "pto.inc",
      "report.inc",
      "report.php",
      "schema.sql",
      "submit.php",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "# PTO Nubis deployment repository\n\nThis is the deployment repository for\n[pto.mozilla.org](https://pto.mozilla.org)\n\n## Components\n\nDefined in [nubis/terraform/main.tf](nubis/terraform)\n\n### Webservers\n\nDefined in [nubis/puppet/apache.pp](nubis/puppet)\n\nThe produced image is that of a simple Ubuntu Apache webserver running PHP\n\n### Load Balancer\n\nSimple ELB\n\n### Email\n\nThis application sends outbound e-mails using SES\n\n### SSO\n\nThis entire application is protected behind [mod_auth_openidc](https://github.com/zmartzone/mod_auth_openidc)\n\n### Database\n\nMain application state is persisted in an RDS/MySQL database\n\nAdministrative access to it can be gained thru the db-admin service.\n\n### Cache\n\nElasticache/Memcache is used to provide persistency for\n[mod_auth_openidc](https://github.com/zmartzone/mod_auth_openidc)'s session cache\n\n## Configuration\n\nThe application's configuration file is\n[config.php](nubis/puppet/files/config.php)\nand is not managed, it simply sources nubis_configuration\nfrom */etc/nubis-config/${project_name}.php*\n\n### Consul Keys\n\nThis application's Consul keys, living under\n*${project_name}-${environment}/${environment}/config/*\nand defined in Defined in [nubis/terraform/consul.tf](nubis/terraform)\n\n#### Debug\n\n*Operator Supplied* Controls an application-specific debugging mode\n\n#### export_users\n\n*Operator Supplied* List of email addresses of users allowed to export reports\n\n#### hr_managers\n\n*Operator Supplied* List of email addresses of HR managers\n\n#### mail_blacklist\n\n*Operator Supplied* List of email addresses where mail may **NOT** be sent\n\n#### mail_submitter\n\n*Operator Supplied* Full e-mail address of the sender of PTO emails\n\n#### notified_people\n\n*Operator Supplied* Full e-amil address that will always recieve PTO emails\n\n#### ldap_host\n\n*Operator Supplied* LDAP Url to connect to the server, for example\n\n```\nldaps://ldap.company.com:636\n```\n\n#### ldal_bind_user\n\n*Operator Supplied* Bind DN to use to authenticate to the LDAP server\n\n#### ldap_bind_pass\n\n*Operator Supplied* Password to use to authenticate to the LDAP server\n\n#### Cache/Endpoint\n\nDNS endpoint of Elasticache/memcache\n\n#### Cache/Port\n\nTCP port of Elasticache/memcache\n\nThe hostname of the RDS/MySQL Database\n\n#### OpenID/Server/Memcached\n\nHostname:Port of Elasticache/memcache\n\n#### OpenID/Server/Passphrase\n\n*Generated* OpenID passphrase for session encryption\n\n#### OpenID/Client/Domain\n\n*Operator Supplied* Auth0 Domain for this application, typically 'mozilla'\n\n#### OpenID/Client/ID\n\n*Operator Supplied* Auth0 Client ID for this application\n\n#### OpenID/Client/Secret\n\n*Operator Supplied* Auth0 Client Secret for this application 'mozilla'\n\n#### OpenID/Client/Site\n\n*Operator Supplied* Auth0 Site URL for this application\n\n#### SMTP/Server\n\nSES SMTP server hostname\n\n#### SMTP/User\n\nSES SMTP username\n\n#### SMTP/Password\n\nSES SMTP password\n\n## Cron Jobs\n\nDaily backup job copies data from [Storage](#storage) to [Buckets](#buckets)\n\n## Logs\n\nNo application specific logs\n"
},
{
  "name": "normandy",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".dockerignore",
      ".flake8",
      ".gitignore",
      ".nsprc",
      ".pyup.yml",
      ".therapist.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Dockerfile.development",
      "LICENSE",
      "Makefile",
      "README.md",
      "assets",
      "bin",
      "bors.toml",
      "ci",
      "client",
      "contract-tests",
      "docker-compose.yml",
      "docs",
      "etc",
      "generate_from_openapi.py",
      "karma.conf.js",
      "manage.py",
      "normandy",
      "package.json",
      "parquet_schemas",
      "poetry.lock",
      "pyproject.toml",
      "pytest.ini",
      "requirements",
      "test_v3_api.py",
      "yarn.lock"
    ],
    "/docs": [
      ".gitignore",
      "Makefile",
      "adrs",
      "conf.py",
      "dev",
      "index.rst",
      "make.bat",
      "ops",
      "qa",
      "user"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "geo_data: GeoLite2-Country.mmdb\n\nGeoLite2-Country.mmdb:\n\t./bin/download_geolite2.sh\n\nbuild:\n\tdocker-compose build\n\ncreateuser: build\n\tdocker-compose run app python manage.py createsuperuser\n\ncheck_migrations: build\n\tdocker-compose run app sh -c \"/app/bin/wait-for-it.sh db:5432 -- python manage.py makemigrations --check --dry-run --noinput\"\n\nmakemigrations: build\n\tdocker-compose run app sh -c \"/app/bin/wait-for-it.sh db:5432 --  python manage.py makemigrations\"\n\nmigrate: build\n\tdocker-compose run app sh -c \"/app/bin/wait-for-it.sh db:5432 -- python manage.py migrate\"\n\nload_geo_location: build\n\tdocker-compose run app ./bin/download_geolite2.sh\n\nupdate_actions: build\n\tdocker-compose run app python manage.py update_actions\n\nupdate_product_details: build\n\tdocker-compose run app python manage.py update_product_details\n\t\nload_initial_data: build\n\tdocker-compose run app python manage.py initial_data\n\nload_data: migrate update_actions load_initial_data\n\nlint: SHELL:=/bin/bash -O extglob\nlint:\n\tdocker-compose run app therapist run --disable-git ./!(node_modules|assets|docs|venv)\n\ncode_format: SHELL:=/bin/bash -O extglob\ncode_format:\n\tdocker-compose run app therapist run --fix --disable-git ./!(node_modules|assets|docs|venv)\ncheck: check_migrations lint test\n\ncompose_stop:\n\tdocker-compose kill\n\ncompose_rm:\n\tdocker-compose rm -f -v\n\t\nvolumes_rm:\n\tdocker volume ls -q | xargs docker volume rm -f | echo\n\nkill: compose_stop compose_rm volumes_rm\n\techo \"All containers removed!\"\n\ntest: build\n\tdocker-compose run app sh -c \"/app/bin/wait-for-it.sh db:5432 -- pytest\"\n\nshell: build\n\tdocker-compose run app python manage.py shell\n\nbash: build\n\tdocker-compose run app bash\n\nup: build\n\tdocker-compose up\n\nrefresh: kill migrate load_data\n\n# Usage: make generate_deploy_bug FROM=fromtag TO=totag\ngenerate_deploy_bug: build\n\tdocker-compose run app python bin/generate_deploy_bug.py $(FROM) $(TO)\n\n# Used for running the contract tests in two containers instead of running locally\ncreate_superuser:\n\tdocker-compose run app sh -c \"/app/bin/wait-for-it.sh db:5432 -- python manage.py createsuperuser --noinput --email=test-user@example.com --user=testuser\"\n\ncontract_tests: refresh create_superuser\n\tdocker-compose run test sh -c \"/app/bin/wait-for-it.sh app:8000 -- pytest contract-tests/ --server https://app:8000\"\n",
  "readme": "# Normandy Recipe Server\n\nNormandy manages recipes of changes to make to Firefox, including temporary\nstudies, user surveys and controlled rollout of new features.\n\n[![CircleCI](https://img.shields.io/circleci/project/mozilla/normandy/master.svg?maxAge=2592000&label=CI)](https://circleci.com/gh/mozilla/normandy/tree/master)\n[![Updates](https://pyup.io/repos/github/mozilla/normandy/shield.svg)](https://pyup.io/repos/github/mozilla/normandy/)\n[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\n# License\n\nNormandy is licensed under the MPLv2. See the `LICENSE` file for details.\n\nThe Docker images published for Normandy include GeoLite2 data created by\nMaxMind, available from https://www.maxmind.com.\n\n# Hacking\n\nPlease see the [Developer Setup] documentation to get started.\n\n[Developer Setup]: https://mozilla.github.io/normandy/dev/install.html\n"
},
{
  "name": "security",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "client",
      "operations"
    ]
  },
  "makefile": null,
  "readme": "Mozilla Security Repository\n========\n\nThis repository will contain various tools around security at Mozilla. Work in progress :)\n"
},
{
  "name": "mozillavpn-product-details",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE.txt",
      "README.md",
      "docs",
      "package-lock.json",
      "package.json",
      "schema.jtd.json",
      "schema.test.js",
      "schema_release.jtd.json"
    ],
    "/docs": [
      "index.html",
      "mozillavpn.json",
      "mozillavpn_stage.json"
    ],
    "/.github": [
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "![ci](https://github.com/mozilla/mozillavpn-product-details/actions/workflows/test.yml/badge.svg)\n\n### https://mozilla.github.io/mozillavpn-product-details\n\nHosts a [product-details](https://github.com/mozilla-releng/product-details) style json files for the vpn.\n\nAutomatically deploys to gh pages.\n\n#### How it works, and why.\n\n* `schema.jtd.json` contains the main schema that all files must validate.\n* `schema_release.jtd.json` contains a mostly lax schema, that exists only to force the build number to be null on the\n  prod mozillavpn.json\n\nThe product details files are currently (Oct-28-21) used to:\n* Build links to http://archive.mozilla.org/pub/vpn/ to packages for download on vpn.mozilla.org\n\nThe constraints in the schema capture constraints we've coded elsewhere e.g. in code, or in the archives\nlink structure.\n\n#### Enforced restrictions\n* No pushing to main\n* Codeowners must approve production file changes\n* Commits must be signed\n* No exceptions\n\n#### Running locally\n\n```sh\nnpm i\nnpm test\n```\n"
},
{
  "name": "infosec.mozilla.org",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "aws",
      "docs",
      "misc"
    ],
    "/docs": [
      "Gemfile",
      "_config.yml",
      "_layouts",
      "_sass",
      "assets",
      "contribute.json",
      "favicon.ico",
      "fundamentals",
      "guidelines",
      "index.md",
      "tools"
    ]
  },
  "makefile": null,
  "readme": "# infosec.mozilla.org\nGuidelines, principles published on https://infosec.mozilla.org\n\n## How to contribute\n\n### Propose your changes directly\n\n- Either using the GitHub integrated editor or your own, make your changes in Markdown.\n- Request merging by creating a pull-request.\n- That's it - thanks for helping making our content better!\n\n### Open issues or discussion topics\n\nJust create new issues as you see fit, really.\n\n### Converting Mediawiki to Markdown\n\n- Install [Pandoc](https://pandoc.org/).\n- `pandoc -f mediawiki -t gfm --atx yourfile.mediawiki`\n- Fix it up (tip: use existing documents and copy their formatting!)\n- Profit.\n\n### How to locally test\n\nEnsure Ruby, Gem and Bundle are installed.\n\n- Checkout a copy of this repository (feel free to fork it first, specially if you're going to propose changes).\n- Go into the `docs` sub-directory.\n- Run `bundle install` to ensure all dependencies are installed.\n- Run `bundle exec jekyll serve` to locally serve contents for testing.\n\n## Site setup\n\n- The theme is [Frontierline](https://github.com/craigcook/frontierline-theme) and is based on jekyll-theme-slate for the purpose of Jekyll integration.\n- The site is rendered by [Jekyll](https://jekyllrb.com/).\n- The font (ZillaLab) and logos are from the [Mozilla Design Language](https://mozilla.ninja/).\n- https://infosec.mozilla.org is fronted by AWS CloudFront and utilizes a Lambda@Edge function, that are described in\n  the `aws` directory of this repository.\n\n## Licensing\n\nAll content is licensed under the [Mozilla Public License (MPL)](https://www.mozilla.org/en-US/MPL/) unless indicated otherwise.\n"
},
{
  "name": "delete_backup_vaults",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "cleanupAWS.py",
      "data",
      "emptyVault.py",
      "example_filter_vaults.py",
      "example_list_stack_owned_vaults.py",
      "example_list_stacks.py",
      "example_list_vaults.py",
      "json_utils.py",
      "library",
      "stack_utils.py",
      "test.py",
      "vault_utils.py"
    ]
  },
  "makefile": null,
  "readme": "# Delete Backups\n\nThese python scripts delete unnecessary backup vaults from AWS.\n\n# Setup\n\n- Install `python`\n- Install `pip`\n- Install `maws-aws-cli` (via `pip`)\n- Install `boto3` (via `pip`)\n- Install `termcolor` (via `pip`)\n- Run `$(maws)` before executing these scripts\n- Set your AWS region (Probably `us-east-1`).\n\n## The `maws` command\n\nThe `$(maws)` command sets these ENVIRONMENT VARIABLES that will be used when you run the scripts:\n\n```\nAWS_ACCESS_KEY_ID\nAWS_SECRET_ACCESS_KEY\nAWS_SESSION_TOKEN\nMAWS_PROMPT\nAWS_SESSION_EXPIRATION\n```\n\nYou can inspect their values by running `env | grep AWS` in your shell. If you're curious about what other environment variables are set, you can run `env` to see all of them.\n\n## Configuring your default region\n\nThe `maws` command configures your environment variables with most everything you need. The last thing you need to do for these scripts is to specify a region. You can either run `export AWS_DEFAULT_REGION=us-east-1` for each terminal session, or save an AWS config file at `$HOME/.aws/config`:\n\n```\n[default]\nregion = us-east-1\n```\n\n# Files\n\n|                                 File | What is it                                                                                                                                                                                 |\n| -----------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n|                          `README.md` | Documentation                                                                                                                                                                              |\n|             `example_list_vaults.py` | Request a list of backup vaults from AWS. Save the output to a `json` file.                                                                                                                |\n|             `example_list_stacks.py` | Request a list of stacks from AWS. Filter those without parents (non-nested stacks), and save those to a `json` file.                                                                      |\n| `example_list_stack_owned_vaults.py` | Load a json file of stacks. Find the BackupVaultName from its resource list.                                                                                                               |\n|           `example_filter_vaults.py` | Filter the list of backup vaults to delete. Do not delete vaults associated with active stacks.                                                                                            |\n|                      `json_utils.py` | Provides `read_json_file` and `write_json_file`, which do what they sound like. Also resolves a problem with serializing and deserializing ISO timestamps, which we receive from AWS APIs. |\n\n# Running the examples\n\n- Follow the setup steps above.\n- Run `example_list_vaults.py`. If successful, you should see \"Found N vaults.\" This verifies that your setup is working.\n- Read `example_list_stacks.py`. Inspect the output file.\n- Read `example_list_stack_owned_vaults.py`. Inspect the output file.\n- Read `example_filter_vaults.py`. Inspect the output.\n\n# What's next?\n\nThese scripts should get you started towards a solution of deleting unnecessary stacks. Before AWS lets you delete stacks, they require you to empty them. You can read the boto3 documentation to find the functions you will need to invoke.\n\nI also encourage you to familiarize yourself with the following python topics / functions, as they are useful in a wide variety of situations, including this one:\n\n- Python [`Dictionaries`](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)\n- Higher-order functions like [`map`](https://docs.python.org/3/library/functions.html#map), [`filter`](https://docs.python.org/3/library/functions.html#filter), and [`reduce`](https://docs.python.org/3/library/functools.html#functools.reduce). Each take a function and an iterable (e.g. a list) and will perform an action for each item in the list. (Note that sometimes I have to wrap these in a call to `list` so that it actually performs the action, instead of just \"getting ready to perform the action\".)\n- The `lambda` keyword, which lets you write inline-functions.\n"
},
{
  "name": "dialog",
  "files": {
    "/": [
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierrc.json",
      "CODEOWNERS",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "config.js",
      "habitat",
      "index.js",
      "lib",
      "package-lock.json",
      "package.json",
      "scripts"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# dialog\nMediasoup based WebRTC SFU for Mozilla Hubs.\n\n## Development\n1. Clone repo\n2. In root project folder, `npm ci` (this may take a while).\n3. Create a folder in the root project folder called `certs` if needed (see steps 4 & 5).\n4. Add the ssl cert and key to the `certs` folder as `fullchain.pem` and `privkey.pem`, or set the path to these in your shell via `HTTPS_CERT_FULLCHAIN` and `HTTPS_CERT_PRIVKEY` respectively. You can provide these certs yourself or use the ones available in https://github.com/mozilla/reticulum/tree/master/priv (`dev-ssl.cert` and `dev-ssl.key`).\n\n5. Add the reticulum permissions public key to the `certs` folder as `perms.pub.pem`, or set the path to the file in your shell via `AUTH_KEY`.\n\n  * If using one of the public keys from hubs-ops (located in https://github.com/mozilla/hubs-ops/tree/master/ansible/roles/janus/files), you will need to convert it to standard pem format.    \n    * e.g. for use with dev.reticulum.io:  `openssl rsa -in perms.pub.der.dev -inform DER -RSAPublicKey_in -out perms.pub.pem`\n\n6. Start dialog with `MEDIASOUP_LISTEN_IP=XXX.XXX.XXX.XXX MEDIASOUP_ANNOUNCED_IP=XXX.XXX.XXX.XXX npm start` where `XXX.XXX.XXX.XXX` is the local IP address of the machine running the server. (In the case of a VM, this should be the internal IP address of the VM).\n  * If you choose to set the paths for `HTTPS_CERT_FULLCHAIN`, `HTTPS_CERT_PRIVKEY` and/or `AUTH_KEY` you may also define them inline here as well. e.g. \n  ```\n  HTTPS_CERT_FULLCHAIN=/path/to/cert.file HTTPS_CERT_PRIVKEY=/path/to/key.file AUTH_KEY=/path/to/auth.key MEDIASOUP_LISTEN_IP=XXX.XXX.XXX.XXX MEDIASOUP_ANNOUNCED_IP=XXX.XXX.XXX.XXX npm start\n  ```\n     \n7. Navigate to https://localhost:4443/ in your browser, and accept the self-signed cert.\n\n8. You may now point Hubs/Reticulum to use `localhost:4443` as the WebRTC host/port.`\n\nSee `config.js` for all available configuration options.\n"
},
{
  "name": "ssl-config-generator",
  "files": {
    "/": [
      ".editorconfig",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "config",
      "docs",
      "package-lock.json",
      "package.json",
      "src",
      "tools"
    ],
    "/docs": [
      "4d1704f082fb39a7eefb.index.css",
      "4d1704f082fb39a7eefb.index.js",
      "CNAME",
      "analytics.js",
      "ffdhe2048.txt",
      "ffdhe4096.txt",
      "fonts",
      "guidelines",
      "images",
      "index.html"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla SSL Configuration Generator\n\nThe Mozilla SSL Configuration Generator is a tool which builds configuration files to help you follow the Mozilla [Server Side TLS](https://wiki.mozilla.org/Security/Server_Side_TLS) configuration guidelines.\n\n## Installation\n\n```bash\n$ npm install\n```\n\n## Development\n\nOnce you've installed, you can simply run:\n\n```bash\n$ npm run watch\n```\n\nThis starts a local webserver that will automatically reload your changes.\n\n## Adding new software\n\nThere are two places that need to be updated in order to add support for a new piece of software:\n\n* `src/js/configs.js`, which sets the supported features for your software, and\n* `src/templates/partials/your-software.hbs`, a Handlebars.js template that mirrors your software's configuration\n\n### Creating templates\n\nAll of the templates are written in [Handlebars.js](https://handlebarsjs.com/), and so therefore support all of its standard features. This includes `if`/`else`/`unless` conditionals and `each` loops, for example. In addition, the configuration generator supports the following helpers:\n\n- `eq(item, value)` - `true` if `item` equals `value`\n- `includes(item, stringOrArray)` - `true` if `stringOrArray` contains `item`\n- `join(array, joiner)` - split a array into a string based on `joiner`\n  - `{{{join output.ciphers \":\"}}}`\n- `last(array)` - returns the last item in the array\n- `minpatchver(minimumver, curver)` - `true` if `curver` is greater than or equal to `minimumver`, and both versions are the same patch version, e.g. `2.2`\n  - `{{#if (minpatchver \"2.4.3\" form.serverVersion)}}`\n- `minver(minimumver, curver)` - `true` if `curver` is greater than or equal to `minver`\n  - `{{#if (minver \"1.9.5\" form.serverVersion)}}`\n- `replace(string, whattoreplace, replacement)` - replaces whatToReplace with replacement\n  - `replace(protocol, \"TLSv\", \"TLS \")`\n- `reverse(array)` - reverses the order of an array\n  - `{{#each (reverse output.protocols)}`\n- `sameminorver(version, otherVersion)` - returns `true` if `version` and `otherVersion` are of the same minor version, e.g. `2.2`\n  - `{{#if (sameminorver \"2.4.0\" form.serverVersion)}}`\n- `split(string, splitter)` - split a string into an array based on `splitter`\n  - `{{#each (split somearray \":\")}}`\n\n## Building\n\nTo publish to GitHub Pages, simply run:\n\n```bash\n$ npm run build\n```\n\n## History\n\nThe SSL Config Generator was kept in [the `mozilla/server-side-tls` repository](https://github.com/mozilla/server-side-tls/tree/last-revision-before-move)\nprior to mid 2019 at which point it was moved to this dedicated repository. It\nwas initially created [at the end of 2014](https://github.com/mozilla/server-side-tls/commit/b201a1191ba38e6f933cd02a4f425f683ffa9be4)\nand started out supporting Apache HTTP, Nginx and HAProxy.\n\n## Authors\n\n* [April King](https://github.com/april)\n* [Gene Wood](https://github.com/gene1wood)\n* [Julien Vehent](https://github.com/jvehent)\n\n## License\n\n* Mozilla Public License Version 2.0\n"
},
{
  "name": "source-map",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitattributes",
      ".gitignore",
      ".travis.yml",
      ".waiting.html",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "bench",
      "lib",
      "package.json",
      "source-map.d.ts",
      "source-map.js",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# Source Map\n\n[![Build Status](https://travis-ci.org/mozilla/source-map.svg?branch=master)](https://travis-ci.org/mozilla/source-map)\n\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/source-map/badge.svg)](https://coveralls.io/github/mozilla/source-map)\n\n[![NPM](https://nodei.co/npm/source-map.png?downloads=true&downloadRank=true)](https://www.npmjs.com/package/source-map)\n\nThis is a library to generate and consume the source map format\n[described here][format].\n\n[format]: https://docs.google.com/document/d/1U1RGAehQwRypUTovF1KRlpiOFze0b-_2gc6fAH0KY0k/edit\n\n## Use with Node\n\n    $ npm install source-map\n\n## Use on the Web\n\n```html\n<script src=\"https://unpkg.com/source-map@0.7.3/dist/source-map.js\"></script>\n<script>\n    sourceMap.SourceMapConsumer.initialize({\n        \"lib/mappings.wasm\": \"https://unpkg.com/source-map@0.7.3/lib/mappings.wasm\"\n    });\n</script>\n```\n---\n\n<!-- `npm run toc` to regenerate the Table of Contents -->\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n## Table of Contents\n\n- [Examples](#examples)\n  - [Consuming a source map](#consuming-a-source-map)\n  - [Generating a source map](#generating-a-source-map)\n    - [With SourceNode (high level API)](#with-sourcenode-high-level-api)\n    - [With SourceMapGenerator (low level API)](#with-sourcemapgenerator-low-level-api)\n- [API](#api)\n  - [SourceMapConsumer](#sourcemapconsumer)\n    - [SourceMapConsumer.initialize(options)](#sourcemapconsumerinitializeoptions)\n    - [new SourceMapConsumer(rawSourceMap)](#new-sourcemapconsumerrawsourcemap)\n    - [SourceMapConsumer.with](#sourcemapconsumerwith)\n    - [SourceMapConsumer.prototype.destroy()](#sourcemapconsumerprototypedestroy)\n    - [SourceMapConsumer.prototype.computeColumnSpans()](#sourcemapconsumerprototypecomputecolumnspans)\n    - [SourceMapConsumer.prototype.originalPositionFor(generatedPosition)](#sourcemapconsumerprototypeoriginalpositionforgeneratedposition)\n    - [SourceMapConsumer.prototype.generatedPositionFor(originalPosition)](#sourcemapconsumerprototypegeneratedpositionfororiginalposition)\n    - [SourceMapConsumer.prototype.allGeneratedPositionsFor(originalPosition)](#sourcemapconsumerprototypeallgeneratedpositionsfororiginalposition)\n    - [SourceMapConsumer.prototype.hasContentsOfAllSources()](#sourcemapconsumerprototypehascontentsofallsources)\n    - [SourceMapConsumer.prototype.sourceContentFor(source[, returnNullOnMissing])](#sourcemapconsumerprototypesourcecontentforsource-returnnullonmissing)\n    - [SourceMapConsumer.prototype.eachMapping(callback, context, order)](#sourcemapconsumerprototypeeachmappingcallback-context-order)\n  - [SourceMapGenerator](#sourcemapgenerator)\n    - [new SourceMapGenerator([startOfSourceMap])](#new-sourcemapgeneratorstartofsourcemap)\n    - [SourceMapGenerator.fromSourceMap(sourceMapConsumer)](#sourcemapgeneratorfromsourcemapsourcemapconsumer)\n    - [SourceMapGenerator.prototype.addMapping(mapping)](#sourcemapgeneratorprototypeaddmappingmapping)\n    - [SourceMapGenerator.prototype.setSourceContent(sourceFile, sourceContent)](#sourcemapgeneratorprototypesetsourcecontentsourcefile-sourcecontent)\n    - [SourceMapGenerator.prototype.applySourceMap(sourceMapConsumer[, sourceFile[, sourceMapPath]])](#sourcemapgeneratorprototypeapplysourcemapsourcemapconsumer-sourcefile-sourcemappath)\n    - [SourceMapGenerator.prototype.toString()](#sourcemapgeneratorprototypetostring)\n  - [SourceNode](#sourcenode)\n    - [new SourceNode([line, column, source[, chunk[, name]]])](#new-sourcenodeline-column-source-chunk-name)\n    - [SourceNode.fromStringWithSourceMap(code, sourceMapConsumer[, relativePath])](#sourcenodefromstringwithsourcemapcode-sourcemapconsumer-relativepath)\n    - [SourceNode.prototype.add(chunk)](#sourcenodeprototypeaddchunk)\n    - [SourceNode.prototype.prepend(chunk)](#sourcenodeprototypeprependchunk)\n    - [SourceNode.prototype.setSourceContent(sourceFile, sourceContent)](#sourcenodeprototypesetsourcecontentsourcefile-sourcecontent)\n    - [SourceNode.prototype.walk(fn)](#sourcenodeprototypewalkfn)\n    - [SourceNode.prototype.walkSourceContents(fn)](#sourcenodeprototypewalksourcecontentsfn)\n    - [SourceNode.prototype.join(sep)](#sourcenodeprototypejoinsep)\n    - [SourceNode.prototype.replaceRight(pattern, replacement)](#sourcenodeprototypereplacerightpattern-replacement)\n    - [SourceNode.prototype.toString()](#sourcenodeprototypetostring)\n    - [SourceNode.prototype.toStringWithSourceMap([startOfSourceMap])](#sourcenodeprototypetostringwithsourcemapstartofsourcemap)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Examples\n\n### Consuming a source map\n\n```js\nconst rawSourceMap = {\n  version: 3,\n  file: \"min.js\",\n  names: [\"bar\", \"baz\", \"n\"],\n  sources: [\"one.js\", \"two.js\"],\n  sourceRoot: \"http://example.com/www/js/\",\n  mappings: \"CAAC,IAAI,IAAM,SAAUA,GAClB,OAAOC,IAAID;CCDb,IAAI,IAAM,SAAUE,GAClB,OAAOA\"\n};\n\nconst whatever = await SourceMapConsumer.with(rawSourceMap, null, consumer => {\n  console.log(consumer.sources);\n  // [ 'http://example.com/www/js/one.js',\n  //   'http://example.com/www/js/two.js' ]\n\n  console.log(\n    consumer.originalPositionFor({\n      line: 2,\n      column: 28\n    })\n  );\n  // { source: 'http://example.com/www/js/two.js',\n  //   line: 2,\n  //   column: 10,\n  //   name: 'n' }\n\n  console.log(\n    consumer.generatedPositionFor({\n      source: \"http://example.com/www/js/two.js\",\n      line: 2,\n      column: 10\n    })\n  );\n  // { line: 2, column: 28 }\n\n  consumer.eachMapping(function(m) {\n    // ...\n  });\n\n  return computeWhatever();\n});\n```\n\n### Generating a source map\n\nIn depth guide:\n[**Compiling to JavaScript, and Debugging with Source Maps**](https://hacks.mozilla.org/2013/05/compiling-to-javascript-and-debugging-with-source-maps/)\n\n#### With SourceNode (high level API)\n\n```js\nfunction compile(ast) {\n  switch (ast.type) {\n    case \"BinaryExpression\":\n      return new SourceNode(ast.location.line, ast.location.column, ast.location.source, [\n        compile(ast.left),\n        \" + \",\n        compile(ast.right)\n      ]);\n    case \"Literal\":\n      return new SourceNode(ast.location.line, ast.location.column, ast.location.source, String(ast.value));\n    // ...\n    default:\n      throw new Error(\"Bad AST\");\n  }\n}\n\nvar ast = parse(\"40 + 2\", \"add.js\");\nconsole.log(\n  compile(ast).toStringWithSourceMap({\n    file: \"add.js\"\n  })\n);\n// { code: '40 + 2',\n//   map: [object SourceMapGenerator] }\n```\n\n#### With SourceMapGenerator (low level API)\n\n```js\nvar map = new SourceMapGenerator({\n  file: \"source-mapped.js\"\n});\n\nmap.addMapping({\n  generated: {\n    line: 10,\n    column: 35\n  },\n  source: \"foo.js\",\n  original: {\n    line: 33,\n    column: 2\n  },\n  name: \"christopher\"\n});\n\nconsole.log(map.toString());\n// '{\"version\":3,\"file\":\"source-mapped.js\",\"sources\":[\"foo.js\"],\"names\":[\"christopher\"],\"mappings\":\";;;;;;;;;mCAgCEA\"}'\n```\n\n## API\n\nGet a reference to the module:\n\n```js\n// Node.js\nvar sourceMap = require(\"source-map\");\n\n// Browser builds\nvar sourceMap = window.sourceMap;\n\n// Inside Firefox\nconst sourceMap = require(\"devtools/toolkit/sourcemap/source-map.js\");\n```\n\n### SourceMapConsumer\n\nA `SourceMapConsumer` instance represents a parsed source map which we can query\nfor information about the original file positions by giving it a file position\nin the generated source.\n\n#### SourceMapConsumer.initialize(options)\n\nWhen using `SourceMapConsumer` outside of node.js, for example on the Web, it\nneeds to know from what URL to load `lib/mappings.wasm`. You must inform it by\ncalling `initialize` before constructing any `SourceMapConsumer`s.\n\nThe options object has the following properties:\n\n- `\"lib/mappings.wasm\"`: A `String` containing the URL of the\n  `lib/mappings.wasm` file, or an `ArrayBuffer` with the contents of `lib/mappings.wasm`.\n\n```js\nsourceMap.SourceMapConsumer.initialize({\n  \"lib/mappings.wasm\": \"https://example.com/source-map/lib/mappings.wasm\"\n});\n```\n\n#### new SourceMapConsumer(rawSourceMap)\n\nThe only parameter is the raw source map (either as a string which can be\n`JSON.parse`'d, or an object). According to the spec, source maps have the\nfollowing attributes:\n\n- `version`: Which version of the source map spec this map is following.\n\n- `sources`: An array of URLs to the original source files.\n\n- `names`: An array of identifiers which can be referenced by individual\n  mappings.\n\n- `sourceRoot`: Optional. The URL root from which all sources are relative.\n\n- `sourcesContent`: Optional. An array of contents of the original source files.\n\n- `mappings`: A string of base64 VLQs which contain the actual mappings.\n\n- `file`: Optional. The generated filename this source map is associated with.\n\nThe promise of the constructed souce map consumer is returned.\n\nWhen the `SourceMapConsumer` will no longer be used anymore, you must call its\n`destroy` method.\n\n```js\nconst consumer = await new sourceMap.SourceMapConsumer(rawSourceMapJsonData);\ndoStuffWith(consumer);\nconsumer.destroy();\n```\n\nAlternatively, you can use `SourceMapConsumer.with` to avoid needing to remember\nto call `destroy`.\n\n#### SourceMapConsumer.with\n\nConstruct a new `SourceMapConsumer` from `rawSourceMap` and `sourceMapUrl`\n(see the `SourceMapConsumer` constructor for details. Then, invoke the `async function f(SourceMapConsumer) -> T` with the newly constructed consumer, wait\nfor `f` to complete, call `destroy` on the consumer, and return `f`'s return\nvalue.\n\nYou must not use the consumer after `f` completes!\n\nBy using `with`, you do not have to remember to manually call `destroy` on\nthe consumer, since it will be called automatically once `f` completes.\n\n```js\nconst xSquared = await SourceMapConsumer.with(myRawSourceMap, null, async function(consumer) {\n  // Use `consumer` inside here and don't worry about remembering\n  // to call `destroy`.\n\n  const x = await whatever(consumer);\n  return x * x;\n});\n\n// You may not use that `consumer` anymore out here; it has\n// been destroyed. But you can use `xSquared`.\nconsole.log(xSquared);\n```\n\n#### SourceMapConsumer.prototype.destroy()\n\nFree this source map consumer's associated wasm data that is manually-managed.\n\n```js\nconsumer.destroy();\n```\n\nAlternatively, you can use `SourceMapConsumer.with` to avoid needing to remember\nto call `destroy`.\n\n#### SourceMapConsumer.prototype.computeColumnSpans()\n\nCompute the last column for each generated mapping. The last column is\ninclusive.\n\n```js\n// Before:\nconsumer.allGeneratedPositionsFor({ line: 2, source: \"foo.coffee\" });\n// [ { line: 2,\n//     column: 1 },\n//   { line: 2,\n//     column: 10 },\n//   { line: 2,\n//     column: 20 } ]\n\nconsumer.computeColumnSpans();\n\n// After:\nconsumer.allGeneratedPositionsFor({ line: 2, source: \"foo.coffee\" });\n// [ { line: 2,\n//     column: 1,\n//     lastColumn: 9 },\n//   { line: 2,\n//     column: 10,\n//     lastColumn: 19 },\n//   { line: 2,\n//     column: 20,\n//     lastColumn: Infinity } ]\n```\n\n#### SourceMapConsumer.prototype.originalPositionFor(generatedPosition)\n\nReturns the original source, line, and column information for the generated\nsource's line and column positions provided. The only argument is an object with\nthe following properties:\n\n- `line`: The line number in the generated source. Line numbers in\n  this library are 1-based (note that the underlying source map\n  specification uses 0-based line numbers -- this library handles the\n  translation).\n\n- `column`: The column number in the generated source. Column numbers\n  in this library are 0-based.\n\n- `bias`: Either `SourceMapConsumer.GREATEST_LOWER_BOUND` or\n  `SourceMapConsumer.LEAST_UPPER_BOUND`. Specifies whether to return the closest\n  element that is smaller than or greater than the one we are searching for,\n  respectively, if the exact element cannot be found. Defaults to\n  `SourceMapConsumer.GREATEST_LOWER_BOUND`.\n\nand an object is returned with the following properties:\n\n- `source`: The original source file, or null if this information is not\n  available.\n\n- `line`: The line number in the original source, or null if this information is\n  not available. The line number is 1-based.\n\n- `column`: The column number in the original source, or null if this\n  information is not available. The column number is 0-based.\n\n- `name`: The original identifier, or null if this information is not available.\n\n```js\nconsumer.originalPositionFor({ line: 2, column: 10 });\n// { source: 'foo.coffee',\n//   line: 2,\n//   column: 2,\n//   name: null }\n\nconsumer.originalPositionFor({\n  line: 99999999999999999,\n  column: 999999999999999\n});\n// { source: null,\n//   line: null,\n//   column: null,\n//   name: null }\n```\n\n#### SourceMapConsumer.prototype.generatedPositionFor(originalPosition)\n\nReturns the generated line and column information for the original source,\nline, and column positions provided. The only argument is an object with\nthe following properties:\n\n- `source`: The filename of the original source.\n\n- `line`: The line number in the original source. The line number is\n  1-based.\n\n- `column`: The column number in the original source. The column\n  number is 0-based.\n\nand an object is returned with the following properties:\n\n- `line`: The line number in the generated source, or null. The line\n  number is 1-based.\n\n- `column`: The column number in the generated source, or null. The\n  column number is 0-based.\n\n```js\nconsumer.generatedPositionFor({ source: \"example.js\", line: 2, column: 10 });\n// { line: 1,\n//   column: 56 }\n```\n\n#### SourceMapConsumer.prototype.allGeneratedPositionsFor(originalPosition)\n\nReturns all generated line and column information for the original source, line,\nand column provided. If no column is provided, returns all mappings\ncorresponding to a either the line we are searching for or the next closest line\nthat has any mappings. Otherwise, returns all mappings corresponding to the\ngiven line and either the column we are searching for or the next closest column\nthat has any offsets.\n\nThe only argument is an object with the following properties:\n\n- `source`: The filename of the original source.\n\n- `line`: The line number in the original source. The line number is\n  1-based.\n\n- `column`: Optional. The column number in the original source. The\n  column number is 0-based.\n\nand an array of objects is returned, each with the following properties:\n\n- `line`: The line number in the generated source, or null. The line\n  number is 1-based.\n\n- `column`: The column number in the generated source, or null. The\n  column number is 0-based.\n\n```js\nconsumer.allGeneratedPositionsFor({ line: 2, source: \"foo.coffee\" });\n// [ { line: 2,\n//     column: 1 },\n//   { line: 2,\n//     column: 10 },\n//   { line: 2,\n//     column: 20 } ]\n```\n\n#### SourceMapConsumer.prototype.hasContentsOfAllSources()\n\nReturn true if we have the embedded source content for every source listed in\nthe source map, false otherwise.\n\nIn other words, if this method returns `true`, then\n`consumer.sourceContentFor(s)` will succeed for every source `s` in\n`consumer.sources`.\n\n```js\n// ...\nif (consumer.hasContentsOfAllSources()) {\n  consumerReadyCallback(consumer);\n} else {\n  fetchSources(consumer, consumerReadyCallback);\n}\n// ...\n```\n\n#### SourceMapConsumer.prototype.sourceContentFor(source[, returnNullOnMissing])\n\nReturns the original source content for the source provided. The only\nargument is the URL of the original source file.\n\nIf the source content for the given source is not found, then an error is\nthrown. Optionally, pass `true` as the second param to have `null` returned\ninstead.\n\n```js\nconsumer.sources;\n// [ \"my-cool-lib.clj\" ]\n\nconsumer.sourceContentFor(\"my-cool-lib.clj\");\n// \"...\"\n\nconsumer.sourceContentFor(\"this is not in the source map\");\n// Error: \"this is not in the source map\" is not in the source map\n\nconsumer.sourceContentFor(\"this is not in the source map\", true);\n// null\n```\n\n#### SourceMapConsumer.prototype.eachMapping(callback, context, order)\n\nIterate over each mapping between an original source/line/column and a\ngenerated line/column in this source map.\n\n- `callback`: The function that is called with each mapping. Mappings have the\n  form `{ source, generatedLine, generatedColumn, originalLine, originalColumn, name }`\n\n- `context`: Optional. If specified, this object will be the value of `this`\n  every time that `callback` is called.\n\n- `order`: Either `SourceMapConsumer.GENERATED_ORDER` or\n  `SourceMapConsumer.ORIGINAL_ORDER`. Specifies whether you want to iterate over\n  the mappings sorted by the generated file's line/column order or the\n  original's source/line/column order, respectively. Defaults to\n  `SourceMapConsumer.GENERATED_ORDER`.\n\n```js\nconsumer.eachMapping(function(m) {\n  console.log(m);\n});\n// ...\n// { source: 'illmatic.js',\n//   generatedLine: 1,\n//   generatedColumn: 0,\n//   originalLine: 1,\n//   originalColumn: 0,\n//   name: null }\n// { source: 'illmatic.js',\n//   generatedLine: 2,\n//   generatedColumn: 0,\n//   originalLine: 2,\n//   originalColumn: 0,\n//   name: null }\n// ...\n```\n\n### SourceMapGenerator\n\nAn instance of the SourceMapGenerator represents a source map which is being\nbuilt incrementally.\n\n#### new SourceMapGenerator([startOfSourceMap])\n\nYou may pass an object with the following properties:\n\n- `file`: The filename of the generated source that this source map is\n  associated with.\n\n- `sourceRoot`: A root for all relative URLs in this source map.\n\n- `skipValidation`: Optional. When `true`, disables validation of mappings as\n  they are added. This can improve performance but should be used with\n  discretion, as a last resort. Even then, one should avoid using this flag when\n  running tests, if possible.\n\n```js\nvar generator = new sourceMap.SourceMapGenerator({\n  file: \"my-generated-javascript-file.js\",\n  sourceRoot: \"http://example.com/app/js/\"\n});\n```\n\n#### SourceMapGenerator.fromSourceMap(sourceMapConsumer)\n\nCreates a new `SourceMapGenerator` from an existing `SourceMapConsumer` instance.\n\n- `sourceMapConsumer` The SourceMap.\n\n```js\nvar generator = sourceMap.SourceMapGenerator.fromSourceMap(consumer);\n```\n\n#### SourceMapGenerator.prototype.addMapping(mapping)\n\nAdd a single mapping from original source line and column to the generated\nsource's line and column for this source map being created. The mapping object\nshould have the following properties:\n\n- `generated`: An object with the generated line and column positions.\n\n- `original`: An object with the original line and column positions.\n\n- `source`: The original source file (relative to the sourceRoot).\n\n- `name`: An optional original token name for this mapping.\n\n```js\ngenerator.addMapping({\n  source: \"module-one.scm\",\n  original: { line: 128, column: 0 },\n  generated: { line: 3, column: 456 }\n});\n```\n\n#### SourceMapGenerator.prototype.setSourceContent(sourceFile, sourceContent)\n\nSet the source content for an original source file.\n\n- `sourceFile` the URL of the original source file.\n\n- `sourceContent` the content of the source file.\n\n```js\ngenerator.setSourceContent(\"module-one.scm\", fs.readFileSync(\"path/to/module-one.scm\"));\n```\n\n#### SourceMapGenerator.prototype.applySourceMap(sourceMapConsumer[, sourceFile[, sourceMapPath]])\n\nApplies a SourceMap for a source file to the SourceMap.\nEach mapping to the supplied source file is rewritten using the\nsupplied SourceMap. Note: The resolution for the resulting mappings\nis the minimum of this map and the supplied map.\n\n- `sourceMapConsumer`: The SourceMap to be applied.\n\n- `sourceFile`: Optional. The filename of the source file.\n  If omitted, sourceMapConsumer.file will be used, if it exists.\n  Otherwise an error will be thrown.\n\n- `sourceMapPath`: Optional. The dirname of the path to the SourceMap\n  to be applied. If relative, it is relative to the SourceMap.\n\n  This parameter is needed when the two SourceMaps aren't in the same\n  directory, and the SourceMap to be applied contains relative source\n  paths. If so, those relative source paths need to be rewritten\n  relative to the SourceMap.\n\n  If omitted, it is assumed that both SourceMaps are in the same directory,\n  thus not needing any rewriting. (Supplying `'.'` has the same effect.)\n\n#### SourceMapGenerator.prototype.toString()\n\nRenders the source map being generated to a string.\n\n```js\ngenerator.toString();\n// '{\"version\":3,\"sources\":[\"module-one.scm\"],\"names\":[],\"mappings\":\"...snip...\",\"file\":\"my-generated-javascript-file.js\",\"sourceRoot\":\"http://example.com/app/js/\"}'\n```\n\n### SourceNode\n\nSourceNodes provide a way to abstract over interpolating and/or concatenating\nsnippets of generated JavaScript source code, while maintaining the line and\ncolumn information associated between those snippets and the original source\ncode. This is useful as the final intermediate representation a compiler might\nuse before outputting the generated JS and source map.\n\n#### new SourceNode([line, column, source[, chunk[, name]]])\n\n- `line`: The original line number associated with this source node, or null if\n  it isn't associated with an original line. The line number is 1-based.\n\n- `column`: The original column number associated with this source node, or null\n  if it isn't associated with an original column. The column number\n  is 0-based.\n\n- `source`: The original source's filename; null if no filename is provided.\n\n- `chunk`: Optional. Is immediately passed to `SourceNode.prototype.add`, see\n  below.\n\n- `name`: Optional. The original identifier.\n\n```js\nvar node = new SourceNode(1, 2, \"a.cpp\", [\n  new SourceNode(3, 4, \"b.cpp\", \"extern int status;\\n\"),\n  new SourceNode(5, 6, \"c.cpp\", \"std::string* make_string(size_t n);\\n\"),\n  new SourceNode(7, 8, \"d.cpp\", \"int main(int argc, char** argv) {}\\n\")\n]);\n```\n\n#### SourceNode.fromStringWithSourceMap(code, sourceMapConsumer[, relativePath])\n\nCreates a SourceNode from generated code and a SourceMapConsumer.\n\n- `code`: The generated code\n\n- `sourceMapConsumer` The SourceMap for the generated code\n\n- `relativePath` The optional path that relative sources in `sourceMapConsumer`\n  should be relative to.\n\n```js\nconst consumer = await new SourceMapConsumer(fs.readFileSync(\"path/to/my-file.js.map\", \"utf8\"));\nconst node = SourceNode.fromStringWithSourceMap(fs.readFileSync(\"path/to/my-file.js\"), consumer);\n```\n\n#### SourceNode.prototype.add(chunk)\n\nAdd a chunk of generated JS to this source node.\n\n- `chunk`: A string snippet of generated JS code, another instance of\n  `SourceNode`, or an array where each member is one of those things.\n\n```js\nnode.add(\" + \");\nnode.add(otherNode);\nnode.add([leftHandOperandNode, \" + \", rightHandOperandNode]);\n```\n\n#### SourceNode.prototype.prepend(chunk)\n\nPrepend a chunk of generated JS to this source node.\n\n- `chunk`: A string snippet of generated JS code, another instance of\n  `SourceNode`, or an array where each member is one of those things.\n\n```js\nnode.prepend(\"/** Build Id: f783haef86324gf **/\\n\\n\");\n```\n\n#### SourceNode.prototype.setSourceContent(sourceFile, sourceContent)\n\nSet the source content for a source file. This will be added to the\n`SourceMap` in the `sourcesContent` field.\n\n- `sourceFile`: The filename of the source file\n\n- `sourceContent`: The content of the source file\n\n```js\nnode.setSourceContent(\"module-one.scm\", fs.readFileSync(\"path/to/module-one.scm\"));\n```\n\n#### SourceNode.prototype.walk(fn)\n\nWalk over the tree of JS snippets in this node and its children. The walking\nfunction is called once for each snippet of JS and is passed that snippet and\nthe its original associated source's line/column location.\n\n- `fn`: The traversal function.\n\n```js\nvar node = new SourceNode(1, 2, \"a.js\", [\n  new SourceNode(3, 4, \"b.js\", \"uno\"),\n  \"dos\",\n  [\"tres\", new SourceNode(5, 6, \"c.js\", \"quatro\")]\n]);\n\nnode.walk(function(code, loc) {\n  console.log(\"WALK:\", code, loc);\n});\n// WALK: uno { source: 'b.js', line: 3, column: 4, name: null }\n// WALK: dos { source: 'a.js', line: 1, column: 2, name: null }\n// WALK: tres { source: 'a.js', line: 1, column: 2, name: null }\n// WALK: quatro { source: 'c.js', line: 5, column: 6, name: null }\n```\n\n#### SourceNode.prototype.walkSourceContents(fn)\n\nWalk over the tree of SourceNodes. The walking function is called for each\nsource file content and is passed the filename and source content.\n\n- `fn`: The traversal function.\n\n```js\nvar a = new SourceNode(1, 2, \"a.js\", \"generated from a\");\na.setSourceContent(\"a.js\", \"original a\");\nvar b = new SourceNode(1, 2, \"b.js\", \"generated from b\");\nb.setSourceContent(\"b.js\", \"original b\");\nvar c = new SourceNode(1, 2, \"c.js\", \"generated from c\");\nc.setSourceContent(\"c.js\", \"original c\");\n\nvar node = new SourceNode(null, null, null, [a, b, c]);\nnode.walkSourceContents(function(source, contents) {\n  console.log(\"WALK:\", source, \":\", contents);\n});\n// WALK: a.js : original a\n// WALK: b.js : original b\n// WALK: c.js : original c\n```\n\n#### SourceNode.prototype.join(sep)\n\nLike `Array.prototype.join` except for SourceNodes. Inserts the separator\nbetween each of this source node's children.\n\n- `sep`: The separator.\n\n```js\nvar lhs = new SourceNode(1, 2, \"a.rs\", \"my_copy\");\nvar operand = new SourceNode(3, 4, \"a.rs\", \"=\");\nvar rhs = new SourceNode(5, 6, \"a.rs\", \"orig.clone()\");\n\nvar node = new SourceNode(null, null, null, [lhs, operand, rhs]);\nvar joinedNode = node.join(\" \");\n```\n\n#### SourceNode.prototype.replaceRight(pattern, replacement)\n\nCall `String.prototype.replace` on the very right-most source snippet. Useful\nfor trimming white space from the end of a source node, etc.\n\n- `pattern`: The pattern to replace.\n\n- `replacement`: The thing to replace the pattern with.\n\n```js\n// Trim trailing white space.\nnode.replaceRight(/\\s*$/, \"\");\n```\n\n#### SourceNode.prototype.toString()\n\nReturn the string representation of this source node. Walks over the tree and\nconcatenates all the various snippets together to one string.\n\n```js\nvar node = new SourceNode(1, 2, \"a.js\", [\n  new SourceNode(3, 4, \"b.js\", \"uno\"),\n  \"dos\",\n  [\"tres\", new SourceNode(5, 6, \"c.js\", \"quatro\")]\n]);\n\nnode.toString();\n// 'unodostresquatro'\n```\n\n#### SourceNode.prototype.toStringWithSourceMap([startOfSourceMap])\n\nReturns the string representation of this tree of source nodes, plus a\nSourceMapGenerator which contains all the mappings between the generated and\noriginal sources.\n\nThe arguments are the same as those to `new SourceMapGenerator`.\n\n```js\nvar node = new SourceNode(1, 2, \"a.js\", [\n  new SourceNode(3, 4, \"b.js\", \"uno\"),\n  \"dos\",\n  [\"tres\", new SourceNode(5, 6, \"c.js\", \"quatro\")]\n]);\n\nnode.toStringWithSourceMap({ file: \"my-output-file.js\" });\n// { code: 'unodostresquatro',\n//   map: [object SourceMapGenerator] }\n```\n\n"
},
{
  "name": "rkv",
  "files": {
    "/": [
      ".gitattributes",
      ".github",
      ".gitignore",
      ".rustfmt.toml",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "examples",
      "run-all-examples.sh",
      "src",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# rkv\n\n[![CI Build Status](https://github.com/mozilla/rkv/actions/workflows/ci.yml/badge.svg?branch=main)](https://github.com/mozilla/rkv/actions/workflows/ci.yml)\n[![Documentation](https://docs.rs/rkv/badge.svg)](https://docs.rs/rkv/)\n[![Crate](https://img.shields.io/crates/v/rkv.svg)](https://crates.io/crates/rkv)\n\nThe [rkv Rust crate](https://crates.io/crates/rkv) is a simple, humane, typed key-value storage solution. It supports multiple backend engines with varying guarantees, such as [LMDB](http://www.lmdb.tech/doc/) for performance, or \"SafeMode\" for reliability.\n\n## \u26a0\ufe0f Warning \u26a0\ufe0f\n\nTo use rkv in production/release environments at Mozilla, you may do so with the \"SafeMode\" backend, for example:\n\n```rust\nuse rkv::{Manager, Rkv};\nuse rkv::backend::{SafeMode, SafeModeEnvironment};\n\nlet mut manager = Manager::<SafeModeEnvironment>::singleton().write().unwrap();\nlet shared_rkv = manager.get_or_create(path, Rkv::new::<SafeMode>).unwrap();\n\n...\n```\n\nThe \"SafeMode\" backend performs well, with two caveats: the entire database is stored in memory, and write transactions are synchronously written to disk (only on commit).\n\nIn the future, it will be advisable to switch to a different backend with better performance guarantees. We're working on either fixing some LMDB crashes, or offering more choices of backend engines (e.g. SQLite).\n\n## Use\n\nComprehensive information about using rkv is available in its [online documentation](https://docs.rs/rkv/), which can also be generated for local consumption:\n\n```sh\ncargo doc --open\n```\n\n## Build\n\nBuild this project as you would build other Rust crates:\n\n```sh\ncargo build\n```\n\n### Features\n\nThere are several features that you can opt-in and out of when using rkv:\n\nBy default, `db-dup-sort` and `db-int-key` features offer high level database APIs which allow multiple values per key, and optimizations around integer-based keys respectively. Opt out of these default features when specifying the rkv dependency in your Cargo.toml file to disable them; doing so avoids a certain amount of overhead required to support them.\n\nTo aid fuzzing efforts, `with-asan`, `with-fuzzer`, and `with-fuzzer-no-link` configure the build scripts responsible with compiling the underlying backing engines (e.g. LMDB) to build with these LLMV features enabled. Please refer to the official LLVM/Clang documentation on them for more informatiuon. These features are also disabled by default.\n\n## Test\n\nTest this project as you would test other Rust crates:\n\n```sh\ncargo test\n```\n\nThe project includes unit and doc tests embedded in the `src/` files, integration tests in the `tests/` subdirectory, and usage examples in the `examples/` subdirectory. To ensure your changes don't break examples, also run them via the run-all-examples.sh shell script:\n\n```sh\n./run-all-examples.sh\n```\n\nNote: the test fixtures in the `tests/envs/` subdirectory aren't included in the package published to crates.io, so you must clone this repository in order to run the tests that depend on those fixtures or use the `rand` and `dump` executables to recreate them.\n\n## Contribute\n\nOf the various open source archetypes described in [A Framework for Purposeful Open Source](https://medium.com/mozilla-open-innovation/whats-your-open-source-strategy-here-are-10-answers-383221b3f9d3), the rkv project most closely resembles the Specialty Library, and we welcome contributions. Please report problems or ask questions using this repo's GitHub [issue tracker](https://github.com/mozilla/rkv/issues) and submit [pull requests](https://github.com/mozilla/rkv/pulls) for code and documentation changes.\n\nrkv relies on the latest [rustfmt](https://github.com/rust-lang-nursery/rustfmt) for code formatting, so please make sure your pull request passes the rustfmt before submitting it for review. See rustfmt's [quick start](https://github.com/rust-lang-nursery/rustfmt#quick-start) for installation details.\n\nWe follow Mozilla's [Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/) while contributing to this project.\n\n## License\n\nThe rkv source code is licensed under the Apache License, Version 2.0, as described in the [LICENSE](https://github.com/mozilla/rkv/blob/master/LICENSE) file.\n"
},
{
  "name": "MOSS-Directory",
  "files": {
    "/": [
      ".gitignore",
      "COVID_19_Solutions_Fund.md",
      "Foundational_Technology.md",
      "Foundational_Technology_Committee.md",
      "LICENSE",
      "Mission_Partners.md",
      "Mission_Partners_Committee.md",
      "README.md",
      "SOS_Fund.md",
      "SOS_Fund_Audits",
      "Seed_Awards.md"
    ]
  },
  "makefile": null,
  "readme": "# MOSS-Directory\nA listing of projects which have received awards from the [Mozilla Open Source Support (MOSS) program](https://www.mozilla.org/moss/) and people who sit on the MOSS committees. MOSS broadens access, increases security, and empowers users by providing catalytic funding to open source technologists. Until late 2020, MOSS had three ongoing funding tracks: 1.) Foundational Technology; 2.) Mission Partners; and 3.) Secure Open Source (SOS). **Please note that the MOSS program is currently on hiatus and no new applications for funding are being accepted at this time.**\n* This repository is a work-in-progress and is not yet a complete listing of all MOSS projects\n* Projects are listed in individual files by Track and are ordered by the year in which the award was given\n* Reports and change/fix logs for the SOS Fund can be found in the directory \"SOS_Fund_Audits\"\n* MOSS committees are listed by Track and are ordered by the years in which the committee member served\n* If you are a MOSS awardee and would like us to change any information about your project, please open an issue or better yet, send us a pull request!\n* Want to learn more about MOSS or apply for an award? Please visit [the MOSS website](https://www.mozilla.org/moss/)\n"
},
{
  "name": "jsonschema-transpiler",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "build.rs",
      "docs",
      "scripts",
      "src",
      "tests"
    ],
    "/docs": [
      "development.md",
      "examples.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# jsonschema-transpiler\n\n[![CircleCI](https://circleci.com/gh/mozilla/jsonschema-transpiler.svg?style=svg)](https://circleci.com/gh/mozilla/jsonschema-transpiler)\n\nA tool for transpiling [JSON Schema](https://json-schema.org/) into schemas for\n[Avro](https://avro.apache.org/docs/current/index.html#schemas) and\n[BigQuery](https://cloud.google.com/bigquery/docs/schemas).\n\nJSON Schema is primarily used to validate incoming data, but contains enough\ninformation to describe the structure of the data. The transpiler encodes the\nschema for use with data serialization and processing frameworks. The main\nuse-case is to enable ingestion of JSON documents into BigQuery through an Avro\nintermediary.\n\nThis tool can handle many of the composite types seen in modern data processing\ntools that support a SQL interface such as lists, structures, key-value\nmaps, and type-variants.\n\nThis tool is designed for generating new schemas from\n[`mozilla-pipeline-schemas`](https://github.com/mozilla-services/mozilla-pipeline-schemas),\nthe canonical source of truth for JSON schemas in the Firefox Data Platform.\n\n## Installation\n\n```bash\ncargo install jsonschema-transpiler\n```\n\n## Usage\n\n```bash\nA tool to transpile JSON Schema into schemas for data processing\n\nUSAGE:\n    jsonschema-transpiler [FLAGS] [OPTIONS] [file]\n\nFLAGS:\n    -w, --allow-maps-without-value    Produces maps without a value field for incompatible or under-specified value\n                                      schema\n    -n, --force-nullable              Treats all columns as NULLABLE, ignoring the required section in the JSON Schema\n                                      object\n    -h, --help                        Prints help information\n    -c, --normalize-case              snake_case column-names for consistent behavior between SQL engines\n        --tuple-struct                Treats tuple validation as an anonymous struct\n    -V, --version                     Prints version information\n\nOPTIONS:\n    -r, --resolve <resolve>    The resolution strategy for incompatible or under-specified schema [default: cast]\n                               [possible values: cast, panic, drop]\n    -t, --type <type>          The output schema format [default: avro]  [possible values: avro, bigquery]\n\nARGS:\n    <file>    Sets the input file to use\n```\n\nJSON Schemas can be read from stdin or from a file.\n\n### Examples usage\n\n```bash\n# An object with a single, optional boolean field\n$ schema='{\"type\": \"object\", \"properties\": {\"foo\": {\"type\": \"boolean\"}}}'\n\n$ echo $schema | jq\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"foo\": {\n      \"type\": \"boolean\"\n    }\n  }\n}\n\n$ echo $schema | jsonschema-transpiler --type avro\n{\n  \"fields\": [\n    {\n      \"default\": null,\n      \"name\": \"foo\",\n      \"type\": [\n        {\n          \"type\": \"null\"\n        },\n        {\n          \"type\": \"boolean\"\n        }\n      ]\n    }\n  ],\n  \"name\": \"root\",\n  \"type\": \"record\"\n}\n\n$ echo $schema | jsonschema-transpiler --type bigquery\n[\n  {\n    \"mode\": \"NULLABLE\",\n    \"name\": \"foo\",\n    \"type\": \"BOOL\"\n  }\n]\n```\n\n## Building\n\nTo build and test the package:\n\n```bash\ncargo build\ncargo test\n```\n\nOlder versions of the package (<= 1.9) relied on the use of oniguruma for\nperforming snake-casing logic. To enable the use of this module, add a feature\nflag:\n\n```bash\ncargo test --features oniguruma\n```\n\n## Contributing\n\nContributions are welcome. The API may change significantly, but the\ntransformation between various source formats should remain consistent. To aid\nin the development of the transpiler, tests cases are generated from a language\nagnostic format under `tests/resources`.\n\n```json\n{\n    \"name\": \"test-suite\",\n    \"tests\": [\n        {\n            \"name\": \"test-case\",\n            \"description\": [\n                \"A short description of the test case.\"\n            ],\n            \"tests\": {\n                \"avro\": {...},\n                \"bigquery\": {...},\n                \"json\": {...}\n            }\n        },\n        ...\n    ]\n}\n```\n\nSchemas provide a type system for data-structures. Most schema languages support\na similar set of primitives. There are atomic data types like booleans,\nintegers, and floats. These atomic data types can form compound units of\nstructure, such as objects, arrays, and maps. The absence of a value is usually\ndenoted by a null type. There are type modifiers, like the union of two types.\n\nThe following schemas are currently supported:\n\n- JSON Schema\n- Avro\n- BigQuery\n\nIn the future, it may be possible to support schemas from similar systems like\nParquet and Spark, or into various interactive data languages (IDL) like\nAvro IDL.\n\n## Publishing\n\nThe jsonschema-transpiler is distributed as a crate via Cargo. Follow this\nchecklist for deploying to [crates.io](https://crates.io/crates/jsonschema-transpiler).\n\n1. Bump the version number in the `Cargo.toml`, as per [Semantic Versioning](https://semver.org/).\n2. Double check that `cargo test` and CI succeeds.\n3. Run `cargo publish`. It must be run with the `--no-verify` flag due to issue #59.\n4. Draft a new release in GitHub corresponding with the version bump.\n"
},
{
  "name": "janus-plugin-sfu",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "Makefile",
      "README.md",
      "client",
      "docs",
      "janus.plugin.sfu.cfg.example",
      "rustfmt.toml",
      "src"
    ],
    "/docs": [
      "api.md"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "PREFIX = /opt/janus/lib/janus/plugins\n\ninstall: release\n\tmkdir -p $(DESTDIR)$(PREFIX)\n\tcp target/release/libjanus_plugin_sfu.so $(DESTDIR)$(PREFIX)\n\nrelease:\n\tRUSTFLAGS=-g cargo build --release\n\tRUSTFLAGS=-g cargo test --release\n\ndebug:\n\tcargo build\n\tcargo test\n\nclean:\n\tcargo clean\n\n.PHONY: clean install release debug\n",
  "readme": "# janus-plugin-sfu\n\n[Janus](https://janus.conf.meetecho.com/) [plugin](https://janus.conf.meetecho.com/docs/plugin_8h.html) to serve as a WebRTC Selective Forwarding Unit (SFU) for game networking data. It was designed as the backend for [Mozilla Hubs](https://github.com/mozilla/hubs), although Hubs no longer uses it.\n\n[See here for API documentation on how to communicate with the plugin.](docs/api.md)\n\nPRs and GitHub issues are welcome.\n\n### How do I use this?\n\nThis is a plugin for Janus, so you'll need to install and run Janus first. The [installation instructions on GitHub](https://github.com/meetecho/janus-gateway#dependencies) are canonical. It's compatible with Janus version 0.10.9 and later, although sometimes Janus makes changes that break plugins.\n\nThis plugin should be compatible with any OS that can run Janus; that includes Linux, OS X, and Windows via WSL. If you use a version from a package manager, you might want to check to make sure it has data channel support, which is a compile-time option. (Debian and Ubuntu have it.)\n\n## Dependencies\n\nThese are the native dependencies necessary for building the Rust plugin. For Janus's dependencies, consult its documentation.\n```\n$ sudo apt install libglib2.0-dev libjansson-dev\n```\n\n## Building\n\n```\n$ cargo build [--release]\n```\n\n## Testing\n\n```\n$ cargo test\n```\n\n## Installing\n\nInstall the library output by the build process (e.g. ./target/release/libjanus_plugin_sfu.so) into the Janus plugins\ndirectory (e.g. /usr/lib/janus/plugins). Restart Janus to activate.\n\n## Configuration and usage\n\nThe plugin accepts a configuration file in the Janus configuration directory named `janus.plugin.sfu.cfg` containing key/value pairs in INI format. An example configuration file is provided as `janus.plugin.sfu.cfg.example`.\n\nYou can test your install by pointing a browser at the `tiny.html` client provided in the `client` directory. If you open two browser windows, you should be able to share your microphone, share your screen, and send data channel messages in one, and see the results in the other.\n\n## Using it with networked-aframe\n\nYou can use this plugin with [A-Frame](https://aframe.io) and [networked-aframe](https://github.com/networked-aframe/networked-aframe). Look at the [adapters table](https://github.com/networked-aframe/networked-aframe#adapters) for a maintained version of the naf-janus-adapter.\n"
},
{
  "name": "build-mar",
  "files": {
    "/": [
      ".coveragerc",
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      "AUTHORS.rst",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "docs",
      "get_mozilla_keys.sh",
      "requirements-py2.txt",
      "requirements.in",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "src",
      "test-requirements-py2.txt",
      "test-requirements.in",
      "test-requirements.txt",
      "tests",
      "tox.ini",
      "update-requirements.sh"
    ],
    "/docs": [
      "authors.rst",
      "changelog.rst",
      "conf.py",
      "contributing.rst",
      "index.rst",
      "installation.rst",
      "readme.rst",
      "reference",
      "requirements.txt",
      "spelling_wordlist.txt",
      "usage.rst"
    ]
  },
  "makefile": null,
  "readme": "========\nOverview\n========\n\n.. start-badges\n\n.. list-table::\n    :stub-columns: 1\n\n    * - docs\n      - |docs|\n    * - tests\n      - | |travis| |codecov|\n    * - package\n      - |version| |downloads| |wheel| |supported-versions| |supported-implementations|\n\n.. |docs| image:: https://readthedocs.org/projects/mar/badge/?style=flat\n    :target: https://readthedocs.org/projects/mar\n    :alt: Documentation Status\n\n.. |travis| image:: https://travis-ci.org/mozilla/build-mar.svg?branch=master\n    :alt: Travis-CI Build Status\n    :target: https://travis-ci.org/mozilla/build-mar\n\n.. |codecov| image:: https://codecov.io/github/mozilla/build-mar/coverage.svg?branch=master\n    :alt: Coverage Status\n    :target: https://codecov.io/github/mozilla/build-mar\n\n.. |version| image:: https://img.shields.io/pypi/v/mar.svg?style=flat\n    :alt: PyPI Package latest release\n    :target: https://pypi.org/project/mar/\n\n.. |downloads| image:: https://img.shields.io/pypi/dm/mar.svg?style=flat\n    :alt: PyPI Package monthly downloads\n    :target: https://pypi.org/project/mar/\n\n.. |wheel| image:: https://img.shields.io/pypi/wheel/mar.svg?style=flat\n    :alt: PyPI Wheel\n    :target: https://pypi.org/project/mar/\n\n.. |supported-versions| image:: https://img.shields.io/pypi/pyversions/mar.svg?style=flat\n    :alt: Supported versions\n    :target: https://pypi.org/project/mar/\n\n.. |supported-implementations| image:: https://img.shields.io/pypi/implementation/mar.svg?style=flat\n    :alt: Supported implementations\n    :target: https://pypi.org/project/mar/\n\n.. end-badges\n\nPackage for handling Mozilla Archive files. MAR file format is documented at https://wiki.mozilla.org/Software_Update:MAR\n\n* Free software: MPL 2.0 license\n\nUsage\n=====\n\nTo list the contents of a mar::\n\n    mar -t complete.mar\n\nTo list the contents of a mar with extra detail::\n\n    mar -T complete.mar\n\nTo extract a mar::\n\n    mar -x complete.mar\n\nTo extract, and uncompress a bz2 compressed mar::\n\n    mar -j -x complete.mar\n\nTo verify a mar::\n\n    mar -k :mozilla-nightly -v complete.mar\n\nTo create a mar, using bz2 compression::\n\n    mar -j -c complete.mar *\n\nTo create a mar, using xz compression::\n\n    mar -J -c complete.mar *\n\nTo create a signed mar::\n\n    mar -J -c complete.mar -k private.key -H nightly -V 123 tests\n\nInstallation\n============\n\n::\n\n    pip install mar\n\nDocumentation\n=============\n\nhttps://mar.readthedocs.io/en/latest/\n\nDevelopment\n===========\n\nTo run the all tests run::\n\n    tox\n"
},
{
  "name": "stoneridge",
  "files": {
    "/": [
      ".gitignore",
      ".hgignore",
      "INSTALL",
      "LICENSE",
      "README",
      "apache",
      "config.ini.example",
      "doc",
      "head.js",
      "installdmg.sh",
      "linux",
      "mqproxy_empty.db",
      "osx",
      "pageloader",
      "python",
      "requirements.txt",
      "run.ini.example",
      "srarchiver.py",
      "srarpfixer.py",
      "srcleaner.py",
      "srcloner.py",
      "srcollator.py",
      "srdata.js",
      "srdeferrer.py",
      "srdns.py",
      "srdnscheck.py",
      "srdnsupdater.py",
      "srdownloader.py",
      "sremailer.py",
      "srinfogatherer.py",
      "srmaster.py",
      "srnamed.py",
      "srpcap.py",
      "srpcapper.py",
      "srreporter.py",
      "srrun.py",
      "srrunner.py",
      "srscheduler.py",
      "srunpacker.py",
      "sruploader.py",
      "srworker.py",
      "stoneridge.py",
      "tests",
      "tools",
      "windows",
      "wpr"
    ]
  },
  "makefile": null,
  "readme": "This is the source for Stone Ridge (https://wiki.mozilla.org/Necko/Performance/AutomatedTesting).\n\nCurrently it consists of\n    * XPCShell JS to download network resources and report timings\n    * Python harness to control xpcshell and upload timings to a graph server\n\nThere are plans for more, but we're starting basic here, folks.\n"
},
{
  "name": "sumo",
  "files": {
    "/": [
      ".github",
      "README.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Sumo\n\nThis repository is used to track anything related to the platform that powers SuMo and any other projects the team might be working on.\n[Kitsune](https://github.com/mozilla/kitsune) is the platform that powers SuMo (support.mozilla.org).\n\n## Engineering Board\n\nWe are working mostly with projects. A project signifies the team's focus for a period of time. Usually there are one or two active projects at a time.\nA project is defined as a standalone chunk of work for a specific property. Recent examples of projects were the redesign of the SUMO site, integrating Firefox Accounts etc.\n\nAs a rule of thumb, a project is defined as work that requires more than one tasks to be completed and takes more than 3 working days. Anything that doesn't fall in the project category\nis a standalone issue. In most of the cases, these issues are bugs.\n\n_The board will only display either standalone issues or the issues of the active project(s) in order to avoid clutter._\n\n### GitHub Milestones\n\nWe are using milestones to better organize the work that a project needs in order to be completed. A milestone is a concrete chunk of work and each project can have more than one milestones.\n\nAn example of the above structure is:\n\n- Site Redesign - project\n  - create designs - Milestone1\n    - GH issue\n    - GH issue\n    - GH issue\n  - implement designs - Milestone2\n    - GH issue\n    - GH issue\n    - GH issue\n\nNot active projects may have a placeholder milestone in the form of a Inbox if there are already existing issues in board. This is purely for organization reasons.\n\n### Labels\n\nLabels are used to easily distinguish cards that have them attached. We should keep them to the minimum in order for the cards that hold them to easily stand out.\nWe are not using priority labels. Priority is determined by the position of a card in the board.\n\nWe have the following labels to highlight an issue:\n\n- Bug:\n  Something is not working properly.\n\n  _Any issue that has the bug label should also be assigned to the KTLO GitHub milestone regardless of the severity of the bug._\n\n- Pr-welcome:\n  Tasks friendly to new contributors. These tasks are only closed when they are done.\n\n- Project specific labels:\n\n  - oidc\n  - other\n\n  If an issue does not have a label then it belongs to kitsune.\n\n### Columns\n\n#### Triage\n\nDefault column for incoming issues. Only holds issues that have not been proccessed yet.\n\n#### Backlog column\n\nOur Parking Lot. Issues that are valid but not yet an immediate priority.\n\n#### Next Items\n\nHold the tasks of individual _active_ milestone(s) or standalone tickets that will be worked on within a few weeks.\n\n#### Projects, Epics and Blocked Items\n\nThis column holds all the tasks that require collaboration with project stakeholders or are blocked for any reason. It is also a placeholder for Project/Epic cards. Single cards that hold all the information relevant to a project, links to documentation and its milestones.\n\n#### In Progress column\n\nSpecific tasks that are actively worked on by the team.\n\nA card is moved to the next column only when a PR is opened.\n\n#### Review column\n\nAnything that waits for a review. An issue is moved to the next column only when merged.\n\n#### Merged column\n\nMerged in the default branch but pending deployment to a testing environment.\n\n#### QA column\n\nEverything that is already released to the testing environment and is ready to go under QA.\n\nIf an issue passes the QA process, it will be moved to the next column.\nOtherwise it will be moved back to the `In Progress` column.\n\nIf new issues are opened from the QA processes, they should go under the `Next Items` column without anyone assigned.\n\n#### Ready for Release column\n\nActs as a parking lot. This column is suitable for anything that successfully passed QA but is not yet released to production.\n\nAn effort should be put to keep this column short.\n\n#### Done column\n\nThat's all!\n\n## Release notes\n\nAfter each release, we are posting a summary of what happened in [Discourse](https://discourse.mozilla.org/c/sumo/22).\n"
},
{
  "name": "addon-review-helper",
  "files": {
    "/": [
      ".babelrc",
      ".gitignore",
      "README.md",
      "package-lock.json",
      "package.json",
      "public",
      "src",
      "webpack.dev.config.js",
      "webpack.production.config.js",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": "\n## Available Scripts\n\nIn the project directory, you can run:\n\n### `npm run dev`\n\nRuns the app in the development mode.<br />\n\n\n### `npm run build`\nBuilds the app for production to the `dist` folder.<br />\n\n\n"
},
{
  "name": "ensemble",
  "files": {
    "/": [
      ".editorconfig",
      ".env",
      ".eslintignore",
      ".eslintrc.extra.js",
      ".gitignore",
      ".stylintrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "nightwatch.conf.js",
      "package-lock.json",
      "package.json",
      "public",
      "scripts",
      "src"
    ]
  },
  "makefile": null,
  "readme": "Ensemble is the platform that powers the [Firefox Public Data\nReport](https://data.firefox.com), a weekly public report on the activity,\nbehavior, and hardware configuration of Firefox Desktop users.\n\nEnsemble fetches data from\n[ensemble-transposer](https://github.com/mozilla/ensemble-transposer), a JSON\nserver that adds metadata to the raw data hosted by Mozilla data engineers.\n\nEnsemble is written in React with the help of the wonderful\n[create-react-app](https://github.com/facebook/create-react-app) tool from\nFacebook. See the [create-react-app documentation](https://facebook.github.io/create-react-app/docs/getting-started)\nfor more information. Some highlights and some additional information are\nprovided here.\n\n## Run\n\n### For development\n\nRun `npm start`\n\nAny of the environment variables in *.env* can be overridden. For example:\n\n`REACT_APP_SITE_TITLE='Firefox Public Lore Report' npm start`\n\n### In production\n\nSee the [create-react-app documentation on\ndeployment](https://facebook.github.io/create-react-app/docs/deployment).\n\nAny of the environment variables in *.env* can be overridden.\n\n## Development\n\n### Testing\n\nRun `npm test` to run Jest, Nightwatch, and ESLint tests locally.\n\nNightwatch tests can optionally be run against the staging and production sites.\nRun `npm run test:nightwatch:stage` or `npm run test:nightwatch:prod`\nrespectively.\n\n### Analyzing\n\nTo analyze the size of the JavaScript bundle that will be served, run `npm run\nsize`.\n\n### Notes\n\n#### Versioning\n\nWe maintain a version number for this project. We try to update it when we\ndeploy new code. The version number is specified in package.json.\n\nThe number looks like a semantic version number, but [semver isn't suitable for\napplications](https://softwareengineering.stackexchange.com/a/255201). We\ninstead follow this basic guideline: the first number is incremented for major\nchanges, the second number is incremented for medium changes, and the third\nnumber is incremented for small changes.\n"
},
{
  "name": "cubeb-pulse-rs",
  "files": {
    "/": [
      ".editorconfig",
      ".github",
      ".gitignore",
      "AUTHORS",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "pulse-ffi",
      "pulse-rs",
      "src"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# cubeb-pulse-rs\n\nImplementation of PulseAudio backend for Cubeb written in Rust.\n\n[![Build Status](https://github.com/mozilla/cubeb-pulse-rs/actions/workflows/build.yml/badge.svg)](https://github.com/mozilla/cubeb-pulse-rs/actions/workflows/build.yml)\n"
},
{
  "name": "naf-janus-adapter",
  "files": {
    "/": [
      ".babelrc",
      ".eslintrc.json",
      ".gitignore",
      ".prettierrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "examples",
      "package-lock.json",
      "package.json",
      "src",
      "webpack.common.js",
      "webpack.dev.js",
      "webpack.prod.js"
    ]
  },
  "makefile": null,
  "readme": "# Networked-AFrame Janus Adapter\n\n[![npm](https://img.shields.io/npm/v/naf-janus-adapter.svg)](https://www.npmjs.com/package/naf-janus-adapter)\n\nNetwork adapter for [networked-aframe](https://github.com/haydenjameslee/networked-aframe) that uses the Janus WebRTC server as a backend.\n\n## Usage\n\nnaf-janus-adapter needs access to networked-aframe's `NAF` global variable. Include it **after** including networked-aframe but **before** the `networked-scene` is loaded.\n\n## Compatibility\n\nnaf-janus-adapter should support anything that supports recent WebRTC standards (right now, the `RTPSender`-based APIs for manipulating streams and tracks). At the time of this writing, that means that many browsers (e.g. Chrome) will require the use of the [WebRTC adapter shim](https://github.com/webrtc/adapter). If you're using NPM to build your A-Frame application, you should include [webrtc-adapter](https://www.npmjs.com/package/webrtc-adapter) as a dependency of your application; if you're using the [browser distribution](https://github.com/mozilla/naf-janus-adapter/tree/master/dist), you should include the WebRTC adapter prior to including naf-janus-adapter, as shown in the example code below.\n\n### Example\n\n```html\n<html>\n<head>\n  <script src=\"https://webrtc.github.io/adapter/adapter-latest.js\"></script>\n  <script src=\"https://aframe.io/releases/0.7.0/aframe.min.js\"></script>\n  <script src=\"https://rawgit.com/netpro2k/networked-aframe/feature/register-adapter/dist/networked-aframe.js\"></script>\n  <script src=\"https://unpkg.com/naf-janus-adapter/dist/naf-janus-adapter.min.js\"></script>\n</head>\n<body>\n   <a-scene networked-scene=\"\n        room: 1;\n        audio: true;\n        adapter: janus;\n        serverURL: ws://localhost:8080;\n      \">\n  </a-scene>\n</body>\n</html>\n```\n\n## Migrating to 4.0.0\n\n4.0.0 allows the application to control when occupants are subscribed to. Occupants are no longer automatically subscribed to when they join. The application is now required to call `syncOccupants` and pass an array of `occupantId`s that it wants to subscribe to. This list should contain all `occupantsId`s that are currently desired to be subscribed to- including any that have already been subscribed to and want to continue to be so. Any occupants not on the list that are already subscribed to will be unsubscribed from. \n```js\nNAF.connection.adapter.syncOccupants(arrayOfOccupantIds);\n```\n\nIf you want to automatically subscribe to occupants on join, you may call `syncOccupants` with the `availableOccupants` array as the first arugument once.\n```js\nNAF.connection.adapter.syncOccupants(NAF.connection.adapter.availableOccupants);\n```\n\n## Development\n\n- Dev: `npm run start`\n- Build: `npm run build`\n- Release: `npm run release`\n"
},
{
  "name": "lmdb-rs",
  "files": {
    "/": [
      ".appveyor.yml",
      ".gitignore",
      ".gitmodules",
      ".rustfmt.toml",
      ".travis.yml",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "azure-pipelines-template.yml",
      "azure-pipelines.yml",
      "benches",
      "lmdb-sys",
      "src"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/mozilla/lmdb-rs.svg?branch=master)](https://travis-ci.org/mozilla/lmdb-rs)\n[![Windows Build status](https://ci.appveyor.com/api/projects/status/id69kkymorycld55/branch/master?svg=true)](https://ci.appveyor.com/project/mykmelez/lmdb-rs-rrsb3/branch/master)\n\n# lmdb-rs\n\nIdiomatic and safe APIs for interacting with the\n[Symas Lightning Memory-Mapped Database (LMDB)](http://symas.com/mdb/).\n\nThis repo is a fork of [danburkert/lmdb-rs](https://github.com/danburkert/lmdb-rs)\nwith fixes for issues encountered by [mozilla/rkv](https://github.com/mozilla/rkv).\n\n## Building from Source\n\n```bash\ngit clone --recursive git@github.com:mozilla/lmdb-rs.git\ncd lmdb-rs\ncargo build\n```\n\n## Publishing to crates.io\n\nTo publish the lmdb-rkv-sys crate to crates.io:\n\n```bash\ngit clone --recursive git@github.com:mozilla/lmdb-rs.git\ncd lmdb-rs/lmdb-sys\n# Update the version string in lmdb-sys/Cargo.toml and lmdb-sys/src/lib.rs.\ncargo publish\ngit tag lmdb-rkv-sys-$VERSION # where $VERSION is the updated version string\ngit push git@github.com:mozilla/lmdb-rs.git --tags\n```\n\nTo publish the lmdb-rkv crate to crates.io:\n\n```bash\ngit clone --recursive git@github.com:mozilla/lmdb-rs.git\ncd lmdb-rs\n# Update the version string in Cargo.toml and src/lib.rs and temporarily change\n# the lmdb-rkv-sys dependency in Cargo.toml to the latest version on crates.io.\ncargo publish\ngit tag $VERSION # where $VERSION is the updated version string\ngit push git@github.com:mozilla/lmdb-rs.git --tags\n# Change the lmdb-rkv-sys dependency in Cargo.toml back to a path dependency\n# on the ./lmdb-sys directory.\n```\n\n## Features\n\n* [x] lmdb-sys.\n* [x] Cursors.\n* [x] Zero-copy put API.\n* [x] Nested transactions.\n* [x] Database statistics.\n"
},
{
  "name": "libmozevent",
  "files": {
    "/": [
      ".flake8",
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "VERSION",
      "docker",
      "libmozevent",
      "requirements-dev.txt",
      "requirements.txt",
      "setup.py",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "LibMozEvent\n===========\n\nA Python 3 library to build workflows that react to Mozilla events (pulse messages, http notifications) and publis results (mercurial pushes, Phabricator interactions, Taskcluster integration, ...).\n"
},
{
  "name": "sumo-data",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# SUMO (support.mozilla.org) Data Workflows\n\nThis repository contains code used in various SUMO data workflows.\n"
},
{
  "name": "grunt-l10n-lint",
  "files": {
    "/": [
      ".eslintrc",
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gruntfile.js",
      "LICENSE",
      "README.md",
      "lib",
      "package-lock.json",
      "package.json",
      "tasks",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# grunt-l10n-lint\n\n[![Build\nStatus](https://travis-ci.org/mozilla/grunt-l10n-lint.svg?branch=master)](https://travis-ci.org/mozilla/grunt-l10n-lint)\n\ngrunt-l10n-lint is a grunt task to check l10n `.po` files for\nunexpected/malformed HTML.\n\n## Getting Started\nThis plugin requires Grunt `>0.4.5`\n\nIf you haven't used [Grunt](http://gruntjs.com/) before, be sure to check out the [Getting Started](http://gruntjs.com/getting-started) guide, as it explains how to create a [Gruntfile](http://gruntjs.com/sample-gruntfile) as well as install and use Grunt plugins. Once you're familiar with that process, you may install this plugin with this command:\n\n```shell\nnpm install grunt-l10n-lint --save-dev\n```\n\nOnce the plugin has been installed, it may be enabled inside your Gruntfile with this line of JavaScript:\n\n```js\ngrunt.loadNpmTasks('grunt-l10n-lint');\n```\n\n## The \"l10n-lint\" task\n\nThe source files used to extract allowed HTML are defined using\nthe `untranslated` option.\n\nThe target files to be checked are in `files`.\n\n```js\n  grunt.initConfig({\n    'l10n-lint': {\n      test: {\n        options: {\n          untranslated: ['test/fixtures/templates/**/*.pot']\n        },\n        files: [{\n          src: 'files-to-check/**/*.po'\n        }]\n      }\n    }\n  });\n```\n\n## The lint process\n\nThe grunt task uses one or more `.po` or `.pot` files as the source\ntemplates from which to extract allowed HTML. The source files are\nto create a list of elements, attributes, and attribute values.\n\nThe grunt task then parses one or more translated `.po` files for strings\nthat contain HTML that is malformed, contains unexpected tags, attributes,\nor attribute values.\n\nThe translated strings are checked, one by one, for:\n\n1. Grossly malformed HTML\n  * Unnamed tags (e.g. `<>`, `</>`)\n  * Unclosed tags, mismatched `<` or `>` (e.g., `<span`, `span>`)\n  * Unclosed elements (e.g., `<span>This span is not closed`)\n  * Elements closed in the wrong order.\n    * (e.g., `<a><span>closed out of order</a></span>`)\n1. Unexpected tags. Tag names that are not used in the source `.pot` files may not be present in the translated `.po` files. For instance:\n  * If the source `.pot` files contain only `a` and `span` tags, the translated files:\n      1. Can contain 0 or more `a` and `span` tags.\n      1. All other tags cause an error.\n1. Unexpected attributes. Attributes that are not used on a type of element in the source `.pot` files may not be present on instances of that element type in the translated `.po` files. For instance:\n  * If the source `.pot` files contain `<a href=\"...\" target=\"...\">`,\n      `<button id=\"...\">` and `<button class=\"...\">`, the translated files:\n      1. Can have either, both or neither of the `href` and `target` attributes on each `a` element.\n      1. Can have either, both or neither of the `id` and `class` attributes on each `button` element.\n      1. All other attributes will cause an error.\n1. Unexpected attribute values. Attribute values that are not used on a type of element in the source `.pot` files may not be present on instances of that element type in the translated `.po` files. For instance:\n  * If the source `.pot` files contain `<a href=\"/signin\" target=\"_blank\">`\n      and `<button id=\"logout\">`, the translated files:\n      1. Can only have the value `/signin` for `href` attributes.\n      1. Can only have the value `_blank` for `target` attributes.\n      1. Can only have the value `logout` for `id` attributes.\n      1. All other attribute values cause an error.\n\nThe target translation checks are very coarse.\n\nFor example, if the source `.pot` file contains a single `a` element,\n_any_ translated string can contain an `a` element.\n\nIf the source `.pot` file contains two `a` elements, one\nwith `id=\"first-anchor\"` and another with `id=\"second-anchor\"`, _any_ translated\nstring could contain an `a` element with either `id`.\n\nAll translated strings are assumed to be independent items, and are checked\nindividually. Quotes that surround attribute values are not checked, as long\nas the tag correctly closes and the attribute value matches an expected value,\nthe value is accepted.\n\n## Running the tests\n\n```bash\nnpm test\n```\n\n## Contributing\nIn lieu of a formal styleguide, take care to maintain the existing coding style. Add unit tests for any new or changed functionality. Lint and test your code using [Grunt](http://gruntjs.com/).\n\n## Get involved:\n\nPlease see the [CONTRIBUTING.md](CONTRIBUTING.md) file.\n\n## License:\nThis software is available under version 2.0 of the MPL:\n\n  https://www.mozilla.org/MPL/\n"
},
{
  "name": "mozmoderator",
  "files": {
    "/": [
      ".editorconfig",
      ".env-dist",
      ".eslintrc.json",
      ".github",
      ".gitignore",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "bin",
      "bower.json",
      "buildspec.yml",
      "contribute.json",
      "docker-compose.yml",
      "gulpfile.js",
      "manage.py",
      "moderator",
      "package.json",
      "requirements",
      "setup.cfg"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Moderator\n\n[![Code CI](https://github.com/mozilla/mozmoderator/actions/workflows/ci.yaml/badge.svg)](https://github.com/mozilla/mozmoderator/actions/workflows/ci.yaml)\n\nMozilla Moderator is a panel moderation webapp that enables users to view, vote and ask questions on different events.\n\nThen panel moderators can export the questions and use them during panel discussions and Q&A.\n\n## License\n\nAll mozmoderator source files are made available under the terms of the GNU Affero General Public License (AGPL).\n\n## Frontend testing\n\nUse [npm](https://www.npmjs.com/) to install the necessary tools. If you use [docker](https://docker.com/) for development this step is not necessary.\n\n    npm -g install bower gulp-cli\n\nUse [bower](https://bower.io/) to download all Frontend libraries.\n\n    bower install\n\nInstall all required packages.\n\n    npm install\n\nFinally use [gulp](http://gulpjs.com/) to check in all main static files and run the tests.\n\n    gulp\n\n## CI & CD\n\nThis application is currently run through integration and deploy pipelines via both [GitHub Actions](https://github.com/mozilla/mozmoderator/actions/workflows/ci.yaml) & a background Kubernetes [Flux](https://fluxcd.io/) setup leveraging [Helm Charts](github.com/mozilla-it/helm-charts/).\n\nThrough those workflows, a Docker image is built, tagged, pushed to ECR, and deployed either to a staging [(itse-apps-stage-1)](https://github.com/mozilla-it/itse-apps-stage-1-infra/) or production [(itse-apps-prod-1)](https://github.com/mozilla-it/itse-apps-prod-1-infra/) Kubernetes cluster. \n\ntl;dr: Push commits to master branch for a stage deploy, cut GitHub releases (following v1.2.3 format) for a production deploy.\n\nThe pipelines work as followed:\n\nCI & Docker Builds:\n1. (manual) create your feature branch on this repository or a fork (_CI will run in our repository for PRs from forks now_) & add your work;\n2. (manual) push your feature branch up to GitHub & create your PR;\n3. (automated) upon push (if a branch off this repository) or PR (both our repository & forks), GitHub Actions will:\n    a. run linting & syntax checks on the code;\n    b. build the Docker image & tag it with the short git commit SHA of the latest commit to confirm the image can be built;\n4. (manual) create a PR from your feature branch to the master branch, have it reviewed, then merged into master;\n5. (automated) Upon merge into master, GitHub Actions will:\n    a. run linting & syntax checks on the code;\n    b. build the Docker image & tag it with \"stg-{the 7-digit short git commit SHA of the latest commit};\n    c. push that Docker image & tag to our ECR repository for Moderator;\n\nStage Deploy:\n6. (automated): upon creation & push of any Docker Images to our ECR moderator repository with the tag pattern `^(stg-[a-f0-9]{7})$`:\n    a. Flux will update our [Stage Helm Release of the Moderator Helm Chart](https://github.com/mozilla-it/itse-apps-stage-1-infra/blob/main/k8s/releases/moderator/moderator.yaml) with that new Stage image tag;\n    b. Flux will rollout that release;\n    c. Flux will update [Stage Helm Release of the Moderator Helm Chart](https://github.com/mozilla-it/itse-apps-stage-1-infra/blob/main/k8s/releases/moderator/moderator.yaml) with a git commit & the latest image changes upon successful deploy.\n\nProduction Deploy:\n6. (manual) test / QA the stage deploy as desired (moderator.allizom.org).\n7. (manual) Create a GitHub Release off of the master branch with appropriate semver updating (using the pattern `^(v[0-9]+.[0-9]+.[0-9]+)$`);\n8. (automated): upon Release, GitHub Actions will:\n    a. run linting & syntax checks on the code;\n    b. pull the docker image of the latest commit in that release & tag that image with the release version;\n    c. push that Docker image with release tag to our ECR repository for Moderator;\n10. (automated): upon creation & push of any Docker Images to our ECR moderator repository with the tag pattern `^(v[0-9]+.[0-9]+.[0-9]+)$`:\n    a. Flux will update our [Prod Helm Release of the Moderator Helm Chart](https://github.com/mozilla-it/itse-apps-prod-1-infra/blob/main/k8s/releases/moderator/moderator.yaml) with that new release tag;\n    b. Flux will rollout that release;\n    c. Flux will update [Prod Helm Release of the Moderator Helm Chart](https://github.com/mozilla-it/itse-apps-prod-1-infra/blob/main/k8s/releases/moderator/moderator.yaml) with a git commit & the latest image changes upon successful deploy.\n"
},
{
  "name": "origin-trial-token",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "src",
      "tools"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "web-lit-core",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "Gemfile.lock",
      "LICENSE.md",
      "README.md",
      "_articles",
      "_config.yml",
      "_includes",
      "_layouts",
      "canvas.md",
      "contents.html",
      "css",
      "images",
      "index.md"
    ]
  },
  "makefile": null,
  "readme": "# The WebLit Core Curriculum Overview README\n\nThe purpose of Mozilla\u2019s Core Web Literacy Curriculum is to provide learners with a basic conceptual understanding of how to read, write, and participate on the web. The Core Activities were written to be self-sufficient and developed from new as well as remixing existing activities with the goal of aligning with the [Web Literacy Map](https://learning.mozilla.org/en-US/web-literacy). \n\nWeb literacy for example helps people:\n* Understand how the Internet works and connects different parts of the web.\n* Share information with others in ways that make sense and protect their privacy and security.\n* Evaluate information, and spot misinformation and disinformation. \n* Empower themselves and their communities to participate online as citizens, learners, workers, creators, consumers, and people.\n\nDeveloping this core curriculum has truly been a community effort involving staff, volunteer contributors, and allied organizations from around the world. A special shout-out to the [library pilots](https://learning.mozilla.org/blog/mozilla-imls-web-literacy-for-library-staff-pilots-kick-off-meeting), and the web literacy leaders and their respective public library staff: Sherry Lehane, Davis Erin Anderson, Joanna Milner, Liz Dyer, Melanie Wilson, and Matthew Kopel for all their contributions, as well as Randy MacDonald and Iris Bond Gill. A big thank you to Matt Rogers and [Digitalme](https://digitalme.co.uk/) for making web literacy badges a reality. \n\nSee [crosswalk](https://docs.google.com/document/d/1MKxmLQMSyhDRCFwKcrGHZiHmPUGoFLmD5HFtHWBK7Yg/edit#) of the Web Literacy Skills with core activities. \n\nOther web literacy activities: https://learning.mozilla.org/en-US/activities\n\n#### Open Practices\nWorking open is one of the underlying tenants of the core curriculum, and also one of web literacy skills. One of the first steps in implementing the core curriculum is understanding what it means to work in the open. Thus, we encourage you to take this one hour, free [working in the open workshop](https://mozilla.teachable.com/p/open-leadership-101) to learn the basics of participation, collaboration, and sharing on community-driven projects. \n\n#### Spread, Grow, and Improve the Curriculum\nThis core curriculum is meant to be self-sufficient and catalyze communities around the world to use, spread, grow, and improve it. Thus, please spread the word on why it's important for people in their everyday lives to have these core skills, and let everyone know how the curriculum is being utilized and forked for continuous improvement. \n\nHere is how you can contribute:\n* Tell us [how and where you're using the curriculum](https://github.com/mozilla/web-lit-core/issues/8); we'd also like to know [what you've learned and what might you change](https://github.com/mozilla/web-lit-core/issues/9).\n* translate into different languages\n* remix for different audiences\n* continually improve the curriculum-- see this [list of tasks and modifications](https://github.com/mozilla/web-lit-core/issues) we need help with! \n\n[Fork](https://help.github.com/articles/fork-a-repo/) this repository. Forking is making a copy and creating your own version of this project you can edit and use. Here is also another resource for [forking](https://guides.github.com/activities/forking/#making-changes). \n\nFirst time contributing to open source? Check out this free series, [How to Contribute to an Open Source Project on GitHub](https://egghead.io/courses/how-to-contribute-to-an-open-source-project-on-github).\n\n#### Printout Curriculum from GitHub\nThe best way to do this is opening GitHub in the Chrome browser. \n\n#### Share your Remixes\nFor anyone remixing and creating own custom version of the curriculum, we encourage you to add your remixes to https://www.mozillapulse.org/issues/web-literacy.\n\n#### Earning Digital Badges\nDigitalme through their [Open Badges Academy](https://www.openbadgeacademy.com/mozilladirectory) is offering digital badges for the web literacy skills if you are interested in earning or having your learners earn badges. To help you with assessment of badges, we created these [guidelines](https://docs.google.com/document/d/19QAgcMiVkkAILcT8PZwohrC5A5Y-eUxEw6FUmsp7zRM/edit) to help organizations and programs that are implementing badging systems and systems for assessing their badges. \n\n#### Maintainers\nThis repository is currently maintained by @anmechung and @zee-moz. \n"
},
{
  "name": "www.ccadb.org",
  "files": {
    "/": [
      ".gitignore",
      "CNAME",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "Gemfile.lock",
      "README.txt",
      "_config.yml",
      "_layouts",
      "assets",
      "bundle",
      "cas",
      "documents",
      "images",
      "index.md",
      "policy.md",
      "resources.md",
      "rootstores",
      "salesforce"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "protocol-assets",
  "files": {
    "/": [
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "icons",
      "logos",
      "package.json",
      "zaps"
    ]
  },
  "makefile": null,
  "readme": "# Protocol Assets\n\n[![npm version](https://img.shields.io/npm/v/feather-icons.svg?style=flat-square)](https://www.npmjs.com/package/@mozilla-protocol/assets)\n\nThis repository contains a set of reusable assets for Mozilla's websites. These assets are available as both svg and png files. Assets include logos and icons.\n\nProtocol icons are a derivative of [Feather icons](https://feathericons.com/), used under the [MIT License](https://github.com/colebemis/feather/blob/master/LICENSE)\n\n\nWhat's included\n-----------------\n\nIcons\n=====\n\nFor the most part icons are black SVGs. There are a few white variations for historical reasons. These have the sufix `-white` in the file name.\n\nThere is also a set of colorful \"brand\" icons in SVG format, in four different color schemes drawn from the Firefox brand palette.\n\nLogos\n=====\n\nSVG logo and logo + wordmark files are included for a number of Mozilla and Firefox products. Sizing varies; you should declare the height and width you want when you use them.\n\n### Variations\n\n* `logo` = the logo is part of the file\n* `word` = the wordmark is in the file\n* `ver` = the logo is above the wordmark\n* `hor` = the logo is to the left of the wordmark\n* `stack` = each word in the wordmark is on a new line\n* `flat` = the image is a single, solid color with no shading or gradients\n* `white` = the image is intended for use on a dark background\n  * if the image does not have \"white\" in the file name it's meant for use on a light background\n* `og` = this is an open graph image for the product\n\nFavicons are not included at this time; we encourage you to generate them yourself from the files included here.\n\nCode of Conduct\n---------------\n\nThis repository is governed by Mozilla's code of conduct and etiquette guidelines.\nFor more details, please see the [Mozilla Community Participation Guidelines][participation]\nand [Developer Etiquette Guidelines][etiquette].\n\n[participation]: https://www.mozilla.org/about/governance/policies/participation/\n[etiquette]: https://bugzilla.mozilla.org/page.cgi?id=etiquette.html\n\n\nLicense\n-------\n\nThis software is licensed under the [MPL version 2.0][MPL]. For more\ninformation, read the file ``LICENSE``.\n\n[MPL]: https://www.mozilla.org/MPL/\n"
},
{
  "name": "sphinx-js",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "requirements_dev.txt",
      "setup.cfg",
      "setup.py",
      "sphinx_js",
      "tests",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "=========\nsphinx-js\n=========\n\nWhy\n===\n\nWhen you write a JavaScript library, how do you explain it to people? If it's a small project in a domain your users are familiar with, JSDoc's alphabetical list of routines might suffice. But in a larger project, it is useful to intersperse prose with your API docs without having to copy and paste things.\n\nsphinx-js lets you use the industry-leading `Sphinx <http://sphinx-doc.org/>`_ documentation tool with JS projects. It provides a handful of directives, patterned after the Python-centric `autodoc <www.sphinx-doc.org/en/latest/ext/autodoc.html>`_ ones, for pulling JSDoc-formatted documentation into reStructuredText pages. And, because you can keep using JSDoc in your code, you remain compatible with the rest of your JS tooling, like Google's Closure Compiler.\n\nsphinx-js also works with TypeScript, using the TypeDoc tool in place of JSDoc and emitting all the type information you would expect.\n\nSetup\n=====\n\n1. Install JSDoc (or TypeDoc if you're writing TypeScript). The tool must be on your ``$PATH``, so you might want to install it globally::\n\n        npm install -g jsdoc\n\n   ...or... ::\n\n        npm install -g typedoc\n\n   JSDoc 3.6.3 and TypeDoc 0.15.0 are known to work.\n\n2. Install sphinx-js, which will pull in Sphinx itself as a dependency::\n\n        pip install sphinx-js\n\n3. Make a documentation folder in your project by running ``sphinx-quickstart`` and answering its questions::\n\n        cd my-project\n        sphinx-quickstart\n\n          > Root path for the documentation [.]: docs\n          > Separate source and build directories (y/n) [n]:\n          > Name prefix for templates and static dir [_]:\n          > Project name: My Project\n          > Author name(s): Fred Fredson\n          > Project version []: 1.0\n          > Project release [1.0]:\n          > Project language [en]:\n          > Source file suffix [.rst]:\n          > Name of your master document (without suffix) [index]:\n          > Do you want to use the epub builder (y/n) [n]:\n          > autodoc: automatically insert docstrings from modules (y/n) [n]:\n          > doctest: automatically test code snippets in doctest blocks (y/n) [n]:\n          > intersphinx: link between Sphinx documentation of different projects (y/n) [n]:\n          > todo: write \"todo\" entries that can be shown or hidden on build (y/n) [n]:\n          > coverage: checks for documentation coverage (y/n) [n]:\n          > imgmath: include math, rendered as PNG or SVG images (y/n) [n]:\n          > mathjax: include math, rendered in the browser by MathJax (y/n) [n]:\n          > ifconfig: conditional inclusion of content based on config values (y/n) [n]:\n          > viewcode: include links to the source code of documented Python objects (y/n) [n]:\n          > githubpages: create .nojekyll file to publish the document on GitHub pages (y/n) [n]:\n          > Create Makefile? (y/n) [y]:\n          > Create Windows command file? (y/n) [y]:\n\n4. In the generated Sphinx conf.py file, turn on ``sphinx_js`` by adding it to ``extensions``::\n\n        extensions = ['sphinx_js']\n\n5. If you want to document TypeScript, add ``js_language = 'typescript'`` to conf.py as well.\n6. If your JS source code is anywhere but at the root of your project, add ``js_source_path = '../somewhere/else'`` on a line by itself in conf.py. The root of your JS source tree should be where that setting points, relative to the conf.py file. (The default, ``../``, works well when there is a ``docs`` folder at the root of your project and your source code lives directly inside the root.)\n7. If you have special JSDoc or TypeDoc configuration, add ``jsdoc_config_path = '../conf.json'`` (for example) to conf.py as well.\n8. If you're documenting only JS or TS and no other languages (like C), you can set your \"primary domain\" to JS in conf.py::\n\n        primary_domain = 'js'\n\n   (The domain is ``js`` even if you're writing TypeScript.) Then you can omit all the \"js:\" prefixes in the directives below.\n\nUse\n===\n\nIn short, write a folder full of reStructuredText files, use the following directives to pull in your JSDoc documentation, then tell Sphinx to render it all by running ``make html`` in your docs directory. If you have never used Sphinx or written reStructuredText before, here is `where we left off in its tutorial <http://www.sphinx-doc.org/en/stable/tutorial.html#defining-document-structure>`_. For a quick start, just add things to index.rst until you prove things are working.\n\nautofunction\n------------\n\nFirst, document your JS code using standard JSDoc formatting::\n\n    /**\n     * Return the ratio of the inline text length of the links in an element to\n     * the inline text length of the entire element.\n     *\n     * @param {Node} node - Types or not: either works.\n     * @throws {PartyError|Hearty} Multiple types work fine.\n     * @returns {Number} Types and descriptions are both supported.\n     */\n    function linkDensity(node) {\n        const length = node.flavors.get('paragraphish').inlineLength;\n        const lengthWithoutLinks = inlineTextLength(node.element,\n                                                    element => element.tagName !== 'A');\n        return (length - lengthWithoutLinks) / length;\n    }\n\nThen, reference your documentation using sphinx-js directives. Our directives work much like Sphinx's standard autodoc ones. You can specify just a function's name... ::\n\n    .. js:autofunction:: someFunction\n\n...and a nicely formatted block of documentation will show up in your docs.\n\nYou can also throw in your own explicit parameter list, if you want to note\noptional parameters::\n\n    .. js:autofunction:: someFunction(foo, bar[, baz])\n\nParameter properties and destructuring parameters also work fine, using `standard JSDoc syntax <https://jsdoc.app/tags-param.html#parameters-with-properties>`_::\n\n    /**\n     * Export an image from the given canvas and save it to the disk.\n     *\n     * @param {Object} options Output options\n     * @param {string} options.format The output format (``jpeg``,  ``png``, or\n     *     ``webp``)\n     * @param {number} options.quality The output quality when format is\n     *     ``jpeg`` or ``webp`` (from ``0.00`` to ``1.00``)\n     */\n    function saveCanvas({ format, quality }) {\n        // ...\n    }\n\nExtraction of default parameter values works as well. These act as expected, with a few caveats::\n\n    /**\n     * You must declare the params, even if you have nothing else to say, so\n     * JSDoc will extract the default values.\n     *\n     * @param [num]\n     * @param [str]\n     * @param [bool]\n     * @param [nil]\n     */\n    function defaultsDocumentedInCode(num=5, str=\"true\", bool=true, nil=null) {}\n\n    /**\n     * JSDoc guesses types for things like \"42\". If you have a string-typed\n     * default value that looks like a number or boolean, you'll need to\n     * specify its type explicitly. Conversely, if you have a more complex\n     * value like an arrow function, specify a non-string type on it so it\n     * isn't interpreted as a string. Finally, if you have a disjoint type like\n     * {string|Array} specify string first if you want your default to be\n     * interpreted as a string.\n     *\n     * @param {function} [func=() => 5]\n     * @param [str=some string]\n     * @param {string} [strNum=42]\n     * @param {string|Array} [strBool=true]\n     * @param [num=5]\n     * @param [nil=null]\n     */\n    function defaultsDocumentedInDoclet(func, strNum, strBool, num, nil) {}\n\nYou can even add additional content. If you do, it will appear just below any extracted documentation::\n\n    .. js:autofunction:: someFunction\n\n        Here are some things that will appear...\n\n        * Below\n        * The\n        * Extracted\n        * Docs\n\n        Enjoy!\n\n``js:autofunction`` has one option, ``:short-name:``, which comes in handy for chained APIs whose implementation details you want to keep out of sight. When you use it on a class method, the containing class won't be mentioned in the docs, the function will appear under its short name in indices, and cross references must use the short name as well (``:func:`someFunction```)::\n\n    .. js:autofunction:: someClass#someFunction\n       :short-name:\n\n``autofunction`` can also be used on callbacks defined with the `@callback tag <https://jsdoc.app/tags-callback.html>`_.\n\nThere is experimental support for abusing ``autofunction`` to document `@typedef tags <https://jsdoc.app/tags-typedef.html>`_ as well, though the result will be styled as a function, and ``@property`` tags will fall misleadingly under an \"Arguments\" heading. Still, it's better than nothing until we can do it properly.\n\nautoclass\n---------\n\nWe provide a ``js:autoclass`` directive which documents a class with the concatenation of its class comment and its constructor comment. It shares all the features of ``js:autofunction`` and even takes the same ``:short-name:`` flag, which can come in handy for inner classes. The easiest way to use it is to invoke its ``:members:`` option, which automatically documents all your class's public methods and attributes::\n\n    .. js:autoclass:: SomeEs6Class(constructor, args, if, you[, wish])\n       :members:\n\nYou can add private members by saying... ::\n\n    .. js:autoclass:: SomeEs6Class\n       :members:\n       :private-members:\n\nPrivacy is determined by JSDoc ``@private`` tags or TypeScript's ``private`` keyword.\n\nExclude certain members by name with ``:exclude-members:``::\n\n    .. js:autoclass:: SomeEs6Class\n       :members:\n       :exclude-members: Foo, bar, baz\n\nOr explicitly list the members you want. We will respect your ordering. ::\n\n    .. js:autoclass:: SomeEs6Class\n       :members: Qux, qum\n\nWhen explicitly listing members, you can include ``*`` to include all unmentioned members. This is useful to have control over ordering of some elements, without having to include an exhaustive list. ::\n\n    .. js:autoclass:: SomeEs6Class\n       :members: importMethod, *, uncommonlyUsedMethod\n\nFinally, if you want full control, pull your class members in one at a time by embedding ``js:autofunction`` or ``js:autoattribute``::\n\n    .. js:autoclass:: SomeEs6Class\n\n       .. js:autofunction:: SomeEs6Class#someMethod\n\n       Additional content can go here and appears below the in-code comments,\n       allowing you to intersperse long prose passages and examples that you\n       don't want in your code.\n\nautoattribute\n-------------\n\nThis is useful for documenting public properties::\n\n    class Fnode {\n        constructor(element) {\n            /**\n             * The raw DOM element this wrapper describes\n             */\n            this.element = element;\n        }\n    }\n\nAnd then, in the docs... ::\n\n    .. autoclass:: Fnode\n\n       .. autoattribute:: Fnode#element\n\nThis is also the way to document ES6-style getters and setters, as it omits the trailing ``()`` of a function. The assumed practice is the usual JSDoc one: document only one of your getter/setter pair::\n\n    class Bing {\n        /** The bong of the bing */\n        get bong() {\n            return this._bong;\n        }\n\n        set bong(newBong) {\n            this._bong = newBong * 2;\n        }\n    }\n\nAnd then, in the docs... ::\n\n   .. autoattribute:: Bing#bong\n\nDodging Ambiguity With Pathnames\n--------------------------------\n\nIf you have same-named objects in different files, use pathnames to disambiguate them. Here's a particularly long example::\n\n    .. js:autofunction:: ./some/dir/some/file.SomeClass#someInstanceMethod.staticMethod~innerMember\n\nYou may recognize the separators ``#.~`` from `JSDoc namepaths <https://jsdoc.app/about-namepaths.html>`_; they work the same here.\n\nFor conciseness, you can use any unique suffix, as long as it consists of complete path segments. These would all be equivalent to the above, assuming they are unique within your source tree::\n\n    innerMember\n    staticMethod~innerMember\n    SomeClass#someInstanceMethod.staticMethod~innerMember\n    some/file.SomeClass#someInstanceMethod.staticMethod~innerMember\n\nThings to note:\n\n* We use simple file paths rather than JSDoc's ``module:`` prefix or TypeDoc's ``external:`` or ``module:`` ones.\n* We use simple backslash escaping exclusively rather than switching escaping schemes halfway through the path; JSDoc itself `is headed that way as well <https://github.com/jsdoc3/jsdoc/issues/876>`_. The characters that need to be escaped are ``#.~(/``, though you do not need to escape the dots in a leading ``./`` or ``../``. A really horrible path might be... ::\n\n    some/path\\ with\\ spaces/file.topLevelObject#instanceMember.staticMember\\(with\\(parens\n* Relative paths are relative to the ``js_source_path`` specified in the config. Absolute paths are not allowed.\n\nBehind the scenes, sphinx-js will change all separators to dots so that...\n\n* Sphinx's \"shortening\" syntax works: ``:func:`~InwardRhs.atMost``` prints as merely ``atMost()``. (For now, you should always use dots rather than other namepath separators: ``#~``.)\n* Sphinx indexes more informatively, saying methods belong to their classes.\n\nSaving Keystrokes By Setting The Primary Domain\n-----------------------------------------------\n\nTo save some keystrokes, you can set ``primary_domain = 'js'`` in conf.py and then say (for example) ``autofunction`` rather than ``js:autofunction``.\n\nTypeScript: Getting Superclass and Interface Links To Work\n----------------------------------------------------------\n\nTo have a class link to its superclasses and implemented interfaces, you'll need to document the superclass (or interface) somewhere using ``js:autoclass`` or ``js:class`` and use the class's full (but dotted) path when you do::\n\n    .. js:autoclass:: someFile.SomeClass\n\nUnfortunately, Sphinx's ``~`` syntax doesn't work in these spots, so users will see the full paths in the documentation.\n\nConfiguration Reference\n-----------------------\n\n``js_language``\n  Use 'javascript' or 'typescript' depending on the language you use. The default is 'javascript'.\n\n``js_source_path``\n  A list of directories to scan (non-recursively) for JS or TS source files, relative to Sphinx's conf.py file. Can be a string instead if there is only one. If there is more than one, ``root_for_relative_js_paths`` must be specified as well. Defaults to '../'.\n\n``jsdoc_config_path``\n  A conf.py-relative path to a JSDoc config file, which is useful if you want to specify your own JSDoc options, like recursion and custom filename matching. If using TypeDoc, you can also point to a ``tsconfig.json`` file.\n\n``root_for_relative_js_paths``\n  Relative JS entity paths are resolved relative to this path. Defaults to ``js_source_path`` if it is only one item.\n\n``jsdoc_cache``\n  Path to a file where JSDoc output will be cached. If omitted, JSDoc will be run every time Sphinx is. If you have a large number of source files, it may help to configure this value. But be careful: the cache is not automatically flushed if your source code changes; you must delete it manually.\n\nExample\n=======\n\nA good example using most of sphinx-js's functionality is the Fathom documentation. A particularly juicy page is https://mozilla.github.io/fathom/ruleset.html. Click the \"View page source\" link to see the raw directives.\n\n`ReadTheDocs <https://readthedocs.org/>`_ is the canonical hosting platform for Sphinx docs and now supports sphinx-js as an opt-in beta. Put this in the ``.readthedocs.yml`` file at the root of your repo::\n\n    requirements_file: docs/requirements.txt\n    build:\n      image: latest\n\nThen put the version of sphinx-js you want in ``docs/requirements.txt``. For example... ::\n\n    sphinx-js==3.1.2\n\nOr, if you prefer, the Fathom repo carries a `Travis CI configuration <https://github.com/mozilla/fathom/blob/92304b8ad4768e90c167c3d93f9865771f5a6d80/.travis.yml#L41>`_ and a `deployment script <https://github.com/mozilla/fathom/blob/92304b8ad4768e90c167c3d93f9865771f5a6d80/tooling/travis-deploy-docs>`_ for building docs with sphinx-js and publishing them to GitHub Pages. Feel free to borrow them.\n\nCaveats\n=======\n\n* We don't understand the inline JSDoc constructs like ``{@link foo}``; you have to use Sphinx-style equivalents for now, like ``:js:func:`foo``` (or simply ``:func:`foo``` if you have set ``primary_domain = 'js'`` in conf.py.\n* So far, we understand and convert the JSDoc block tags ``@param``, ``@returns``, ``@throws``, ``@example`` (without the optional ``<caption>``), ``@deprecated``, ``@see``, and their synonyms. Other ones will go *poof* into the ether.\n\nTests\n=====\n\nRun the tests using tox, which will also install JSDoc and TypeDoc at pinned versions::\n\n    pip install tox\n    tox\n\nVersion History\n===============\n\n3.1.2\n  * Remove our declared dependency on ``docutils`` to work around the way pip's greedy dependency resolver reacts to the latest version of Sphinx. pip fails when pip-installing sphinx-js because pip sees our \"any version of docutils\" declaration first (which resolves greedily to the latest version, 0.17) but later encounters Sphinx's apparently new ``<0.17`` constraint and gives up. We can revert this when pip's ``--use-feature=2020-resolver`` becomes the default.\n\n3.1.1\n  * Rewrite large parts of the suffix tree that powers path lookup. This fixes several crashers.\n\n3.1\n  * Re-architect language analysis. There is now a well-documented intermediate representation between JSDoc- and TypeDoc-emitted JSON and the renderers. This should make it much faster to merge PRs.\n  * Rewrite much of the TypeScript analysis engine so it feeds into the new IR.\n\n    * TypeScript analysis used to crash if your codebase contained any overloaded functions. This no longer happens; we now arbitrarily use only the first function signature of each overloaded function.\n    * Add support for static properties on TS classes.\n    * Support variadic args in TS.\n    * Support intersection types (``foo & bar``) in TS.\n    * Remove the \"exported from\" module links from classes and interfaces. Functions never had them. Let's see if we miss them.\n    * Pathnames for TypeScript objects no longer spuriously use ``~`` after the filename path segment; now they use ``.`` as in JS.\n    * More generally, TS pathnames are now just like JS ones. There is no more ``external:`` prefix in front of filenames or ``module:`` in front of namespace names.\n    * TS analyzer no longer cares with the current working directory is.\n    * Tests now assert only what they care about rather than being brittle to the point of prohibiting any change.\n  * No longer show args in the arg list that are utterly uninformative, lacking both description and type info.\n  * Class attributes are now listed before methods unless manally ordered with ``:members:``.\n\n3.0.1\n  * Don't crash when encountering a ``../`` prefix on an object path. This can happen behind the scenes when ``root_for_relative_js_paths`` is set inward of the JS code.\n\n3.0\n  * Make compatible with Sphinx 3, which requires Python 3.\n  * Drop support for Python 2.\n  * Make sphinx-js not care what the current working directory is, except for the TypeScript analyzer, which needs further work.\n  * Properly RST-escape return types.\n\n2.8\n  * Display generic TypeScript types properly. Make fields come before methods. (Paul Grau)\n  * Combine constructor and class documentation at the top TypeScript classes. (Sebastian Weigand)\n  * Switch to pytest as the testrunner. (Sebastian Weigand)\n  * Add optional caching of JSDoc output, for large codebases. (Patrick Browne)\n  * Fix the display of union types in TypeScript. (Sebastian Weigand)\n  * Fix parsing breakage that began in typedoc 0.14.0. (Paul Grau)\n  * Fix a data-intake crash with TypeScript. (Cristiano Santos)\n\n2.7.1\n  * Fix a crash that would happen sometimes with UTF-8 on Windows. #67.\n  * Always use conf.py's dir for JSDoc's working dir. #78. (Thomas Khyn)\n\n2.7\n  * Add experimental TypeScript support. (Wim Yedema)\n\n2.6\n  * Add support for ``@deprecated`` and ``@see``. (David Li)\n  * Notice and document JS variadic params nicely. (David Li)\n  * Add linter to codebase.\n\n2.5\n  * Use documented ``@params`` to help fill out the formal param list for a\n    function. This keeps us from missing params that use destructuring. (flozz)\n  * Improve error reporting when JSDoc is missing.\n  * Add extracted default values to generated formal param lists. (flozz and erikrose)\n\n2.4\n  * Support the ``@example`` tag. (lidavidm)\n  * Work under Windows. Before, we could hardly find any documentation. (flozz)\n  * Properly unwrap multiple-line JSDoc tags, even if they have Windows line endings. (Wim Yedema)\n  * Drop support for Python 3.3, since Sphinx has also done so.\n  * Fix build-time crash when using recommonmark (for Markdown support) under Sphinx >=1.7.1. (jamrizzi)\n\n2.3.1\n  * Find the ``jsdoc`` command on Windows, where it has a different name. Then\n    patch up process communication so it doesn't hang.\n\n2.3\n  * Add the ability to say \"*\" within the ``autoclass :members:`` option, meaning \"and all the members that I didn't explicitly list\".\n\n2.2\n  * Add ``autofunction`` support for ``@callback`` tags. (krassowski)\n  * Add experimental ``autofunction`` support for ``@typedef`` tags. (krassowski)\n  * Add a nice error message for when JSDoc can't find any JS files.\n  * Pin six more tightly so ``python_2_unicode_compatible`` is sure to be around.\n\n2.1\n  * Allow multiple folders in ``js_source_path``. This is useful for gradually migrating large projects, one folder at a time, to JSDoc. Introduce ``root_for_relative_js_paths`` to keep relative paths unambiguous in the face of multiple source paths.\n  * Aggregate PathTaken errors, and report them all at once. This means you don't have to run JSDoc repeatedly while cleaning up large projects.\n  * Fix a bytes-vs-strings issue that crashed on versions of Python 3 before 3.6. (jhkennedy)\n  * Tolerate JS files that have filename extensions other than \".js\". Before, when combined with custom JSDoc configuration that ingested such files, incorrect object pathnames were generated, which led to spurious \"No JSDoc documentation was found for object ...\" errors.\n\n2.0.1\n  * Fix spurious syntax errors while loading large JSDoc output by writing it to a temp file first. (jhkennedy)\n\n2.0\n  * Deal with ambiguous object paths. Symbols with identical JSDoc longnames (such as two top-level things called \"foo\" in different files) will no longer have one shadow the other. Introduce an unambiguous path convention for referring to objects. Add a real parser to parse them rather than the dirty tricks we were using before. Backward compatibility breaks a little, because ambiguous references are now a fatal error, rather than quietly referring to the last definition JSDoc happened to encounter.\n  * Index everything into a suffix tree so you can use any unique path suffix to refer to an object.\n  * Other fallout of having a real parser:\n\n    * Stop supporting \"-\" as a namepath separator.\n    * No longer spuriously translate escaped separators in namepaths into dots.\n    * Otherwise treat paths and escapes properly. For example, we can now handle symbols that contain \"(\".\n  * Fix KeyError when trying to gather the constructor params of a plain old\n    object labeled as a ``@class``.\n\n1.5.2\n  * Fix crasher while warning that a specified longname isn't found.\n\n1.5.1\n  * Sort ``:members:`` alphabetically when an order is not explicitly specified.\n\n1.5\n  * Add ``:members:`` option to ``autoclass``.\n  * Add ``:private-members:`` and ``:exclude-members:`` options to go with it.\n  * Significantly refactor to allow directive classes to talk to each other.\n\n1.4\n  * Add ``jsdoc_config_path`` option.\n\n1.3.1\n  * Tolerate @args and other info field lines that are wrapped in the source code.\n  * Cite the file and line of the source comment in Sphinx-emitted warnings and errors.\n\n1.3\n  * Add ``autoattribute`` directive.\n\n1.2\n  * Always do full rebuilds; don't leave pages stale when JS code has changed but the RSTs have not.\n  * Make Python-3-compatible.\n  * Add basic ``autoclass`` directive.\n\n1.1\n  * Add ``:short-name:`` option.\n\n1.0\n  * Initial release, with just ``js:autofunction``\n"
},
{
  "name": "crash-stop-addon",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Procfile",
      "README.md",
      "bin",
      "config",
      "crashstop",
      "docker-compose-test.yml",
      "docker-compose.yml",
      "mozdata.ini",
      "requirements.txt",
      "runtime.txt",
      "static",
      "templates",
      "test-requirements.txt",
      "tests",
      "webextension"
    ]
  },
  "makefile": null,
  "readme": "# crash-stop-addon\n\n[![Build Status](https://api.travis-ci.org/mozilla/crash-stop-addon.svg?branch=master)](https://travis-ci.org/mozilla/crash-stop-addon)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/crash-stop-addon/badge.svg?branch=master)](https://coveralls.io/r/mozilla/crash-stop-addon)\n[![Updates](https://pyup.io/repos/github/mozilla/crash-stop-addon/shield.svg)](https://pyup.io/repos/github/mozilla/crash-stop-addon/)\n\n\ncrash-stop addon is used to display crash data and patch information in Bugzilla.\nThe addon gets an iframe containing these information from a server hosted on Heroku.\nSo here we've code for the server (Python) and the code for the addon (Javascript).\n\nCrash data are coming from https://crash-stats.mozilla.org and patch information are coming from https://hg.mozilla.org.\nThe WebExtension is available at https://addons.mozilla.org/firefox/addon/bugzilla-crash-stop/.\nYou can find more explanations here: https://crash-stop-addon.herokuapp.com/.\n\n## Setup\n\nInstall docker and docker-compose and then:\n```sh\ndocker-compose up --build\n```\nThen you can test in your browser: https://localhost:8081/sumup.html?s=OOM%20|%20small.\n\n## Running tests\n\nIn using docker:\n```sh\ndocker-compose -f docker-compose-test.yml run tests\n```\n\n## Bugs\n\nhttps://github.com/mozilla/crash-stop-addon/issues/new\n\n## Contact\n\nEmail: calixte@mozilla.com\n"
},
{
  "name": "release-blog",
  "files": {
    "/": [
      ".gitignore",
      "CNAME",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "Gemfile.lock",
      "README.md",
      "_config.yml",
      "_layouts",
      "_posts",
      "feed.xml",
      "images",
      "index.html",
      "scripts",
      "stylesheets",
      "themes"
    ]
  },
  "makefile": null,
  "readme": " # :warning: WARNING:\n \n # This Repository is in maintenance mode, issues are disabled and we don't take pull requests.\n\n\n### Test Release Blog Locally\n\n\nThe release blog runs on Jekyll. Jekyll is a Ruby framework, therefore the Ruby language should be installed on your OS.\n\nJekyll is often packaged by Linux distributions so the following instructions should work in most cases:\n\n```bash\n$ sudo apt install jekyll\n$ git clone the repo\n$ jekyll serve\n$ Go to http://localhost:4000\n```\n\nHowever, if you do not want to install a Web framework and all of its dependencies at the OS level, you can install Jekyll, Bundler (Ruby dependency manager) and all of the  dependencies (gems) in the project itself.\n\nFirst edit your ```~/.bashrc``` file and add the following lines:\n```bash\n# Find locally installed Ruby apps\nif which ruby >/dev/null && which gem >/dev/null; then\n    PATH=\"$(ruby -rubygems -e 'puts Gem.user_dir')/bin:$PATH\"\nfi\n```\n\nThen open a new terminal or reload your configuration in an existing terminal:\n```bash\nsource ~/.bashrc\n```\nThe installation process for the blog on Ubuntu/Debian is:\n\n```bash\n$ sudo apt install ruby-full\n$ sudo apt install zlib1g-dev # Dependency requirement for the nokogiri gem\n$ git clone https://github.com/mozilla/release-blog.git\n$ cd release-blog\n$ gem install bundler --user-install\n$ bundle config set path 'vendor/bundle'\n$ bundle install\n$ bundle exec jekyll serve\n$ Go to http://localhost:4000\n```\nYou may need to set up a personal token from GitHub even when running locally.\n\n### Flags\n\n* `$ --no-watch` (Do not watch for changes)\n* `$ --watch` (Make changes on the fly)\n* `$ --trace` (For debug)\n"
},
{
  "name": "open-leadership-training-series",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gemfile",
      "Gemfile.lock",
      "LICENSE",
      "README.md",
      "_articles",
      "_config.yml",
      "_includes",
      "_layouts",
      "contents.html",
      "css",
      "img",
      "index.md",
      "subs"
    ]
  },
  "makefile": null,
  "readme": "# Open Leadership Training Series\nHow to work openly to build collaboratively\n\nThis Open Leadership Training Series teaches you best practices in \u201cworking open\u201d -- a way of working where:\n\n* **_everyone_ is invited to collaborate on something amazing,**\n* **and any new product or knowledge is shared widely and freely.**\n\nThis is for anyone starting up or leading open projects-- project leads, collaborators, or small groups of co-leaders responsible for project success and growth.\n\n## Contributing\n\nThanks for your interest in contributing to the Open Leadership Training Series! There are many ways to contribute. To get started, take a look at [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Run locally\n\nYou'll need to install [Jekyll](https://jekyllrb.com/), [Ruby](https://www.ruby-lang.org/en/) and [Bundler](http://bundler.io/) to run this site locally.\n\n1. `bundle install`\n2. `bundle exec jekyll serve`\n3. Open [http://localhost:4000/open-leadership-training-series/](http://localhost:4000/open-leadership-training-series/) in your favourite browser!\n\n## Participation Guidelines\n\nThis project adheres to a [code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to uphold this code. Please report unacceptable behavior to abby@mozillafoundation.org.\n\n## Contributors\n\nThe material this repository is based on was written by [@zee-moz](https://github.com/zee-moz) and [@chadsansing](https://github.com/chadsansing). Many thanks to the wonderful contributors who've helped since to improve this series:\n\n* David Bild ([@dbild](https://github.com/dbild))\n* Zannah Marsh ([@zee-moz](https://github.com/zee-moz))\n* Tim Riches\n* Holly Robbins ([@hvrobbins](https://github.com/hvrobbins))\n* Chad Sansing ([@chadsansing](https://github.com/chadsansing))\n* Robert Schaefer ([@schae234](https://github.com/schae234))\n\nAs well as all [contributors][gh-contributors] on GitHub.\n\n## License\n\nNon-software content in this project is licensed under a [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n\n\n[gh-contributors]: https://github.com/mozilla/open-leadership-training-series/network/members\n"
},
{
  "name": "http-observatory",
  "files": {
    "/": [
      ".coveragerc",
      ".flake8",
      ".github",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "docker-compose.yml",
      "httpobs",
      "requirements.txt",
      "setup.py"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla HTTP Observatory - [![Build Status](https://travis-ci.org/april/http-observatory.svg?branch=master)](https://travis-ci.org/april/http-observatory) [![Requirements Status](https://requires.io/github/mozilla/http-observatory/requirements.svg?branch=master)](https://requires.io/github/mozilla/http-observatory/requirements/?branch=master)\n\nThe Mozilla HTTP Observatory is a set of tools to analyze your website and inform you if you are utilizing the many available methods to secure it.\n\nIt is split into three projects:\n\n* [http-observatory](https://github.com/mozilla/http-observatory) - scanner/grader\n* [observatory-cli](https://github.com/mozilla/observatory-cli) - command line interface\n* [http-observatory-website](https://github.com/mozilla/http-observatory-website) - web interface\n\n## Scanning sites with the HTTP Observatory\n\nSites can be scanned using:\n\n* [observatory.mozilla.org](https://observatory.mozilla.org/) - the online interface\n* [observatory-cli](https://github.com/mozilla/observatory-cli) - the official node.js command line interface\n* [java-http-observatory-api](https://github.com/stoennies/java-http-observatory-api) - a third party java library and command line interface\n\n## Contributing\n\n### Prerequisites\n* Python 3.7\n* Git\n* pip3\n\n#### Notes\n\nThese instructions assume that you have a working Python3.7 development environment with `pip3` installed and capable of building requirements, which may require installing an additional python OS package (`-dev`, `-devel`).\n\nIf this is not appropriate for your environment, you may install the appropriate requirements using your OS package manager (or other means) and skip the `pip3 -r requirements` command.\n\n## Running a scan from the local codebase, without DB, for continuous integration\n```bash\n# Install the HTTP Observatory\n$ git clone https://github.com/mozilla/http-observatory.git\n$ cd http-observatory\n$ pip3 install --upgrade .\n$ pip3 install --upgrade -r requirements.txt\n```\n\n### Using the local scanner function calls\n```python\n>>> from httpobs.scanner.local import scan\n>>> scan('observatory.mozilla.org')  # a scan with default options\n>>> scan('observatory.mozilla.org',  # all the custom options\n         http_port=8080,             # http server runs on port 8080\n         https_port=8443,            # https server runs on port 8443\n         path='/foo/bar',            # don't scan /, instead scan /foo/bar\n         cookies={'foo': 'bar'},     # set the \"foo\" cookie to \"bar\"\n         headers={'X-Foo': 'bar'},   # send an X-Foo: bar HTTP header\n         verify=False)               # treat self-signed certs as valid for tests like HSTS/HPKP\n```\n\n### The same, but with the local CLI\n```bash\n$ httpobs-local-scan --http-port 8080 --https-port 8443 --path '/foo/bar' \\\n    --cookies '{\"foo\": \"bar\"}' --headers '{\"X-Foo\": \"bar\"}' --no-verify mozilla.org\n```\n\n## Running a local scanner with Docker\n* Install [Docker Toolbox](https://docs.docker.com/toolbox/overview/) and [VirtualBox](https://www.virtualbox.org/wiki/Downloads)\n\n```bash\n# Install the HTTP Observatory client and requests library\n$ git clone https://github.com/mozilla/http-observatory.git\n$ cd http-observatory\n$ pip3 install .\n$ pip3 install --upgrade requests\n\n# Create docker machine\n$ docker-machine create --driver virtualbox --virtualbox-disk-size \"40000\" http-observatory\n\n# Save the URL to the API in your .profile, .bash_profile, or whatever\n$ echo export HTTPOBS_API_URL=http://$(docker-machine ip http-observatory):57001/api/v1 >> ~/.profile\n$ . ~/.profile\n\n# Start up the docker instance and install all the pieces\n$ eval $(docker-machine env http-observatory)\n$ docker-compose up -d\n```\n\n## Creating a local installation (tested on Ubuntu 15)\n```\n# Install git, postgresql, and redis\n# sudo -s\n# apt-get install -y git libpq-dev postgresql redis-server\n\n# Clone the repo\n# cd /opt\n# git clone https://github.com/mozilla/http-observatory.git\n# cd http-observatory\n\n# Install the observatory and scanner\n# pip install .\n# pip3 install -r requirements.txt\n\n# Install the database\n# su - postgres\n$ createdb http_observatory\n$ psql http_observatory < httpobs/database/schema.sql\n$ psql http_observatory\nhttp_observatory=# \\password httpobsapi\nhttp_observatory=# \\password httpobsscanner\n# vi /etc/postgresql/9.4/main/postgresql.conf (set max_connections = 512, shared_buffers = 256MB)\n# service postgresql restart\n\n# Create the httpobs user, and log/pid directories\n# useradd -m httpobs\n# install -m 750 -o httpobs -g httpobs -d /var/run/httpobs /var/log/httpobs\n\n# Update the environmental variables\n# su - httpobs\n$ echo export HTTPOBS_API_URL=\"http://localhost:57001/api/v1\" >> ~/.profile\n\n# Start the scanner\n$ cd /opt/http-observatory\n$ HTTPOBS_DATABASE_USER=\"httpobsscanner\" HTTPOBS_DATABASE_PASS=\".....\" \\\n    /opt/http-observatory/httpobs/scripts/httpobs-scan-worker\n\n# Start the API (in another terminal)\n# HTTPOBS_DATABASE_USER=\"httpobsapi\" HTTPOBS_DATABASE_PASS=\".....\" \\\n    uwsgi --http :57001 --wsgi-file httpobs/website/main.py --processes 8 --callable app --master\n```\n\n## Authors\n\n* April King\n\n## License\n\n* Mozilla Public License Version 2.0\n"
},
{
  "name": "pyo3-parsepatch",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE-MPL-2.0",
      "README.md",
      "pyproject.toml",
      "requirements-dev.txt",
      "src",
      "tests"
    ],
    "/.github": [
      "dependabot.yml"
    ]
  },
  "makefile": null,
  "readme": "# pyo3-parsepatch\n\nA Python wrapper for https://github.com/mozilla/rust-parsepatch.  \nThe goal of this library is to be able to parse the patches coming from mercurial.  \nAll the patches in https://hg.mozilla.org/mozilla-central/ have been successfully parsed !  \nIt's used in https://github.com/mozilla/bugbug to get some metrics on patches.\n\n## License\n\nPublished under the MPL 2.0 license.\n\n## Publish\n\nInstall docker and then:\n```sh\ndocker build -t rs_pp\n```\nIt will compile everything and run tests in a manylinux environment.\n\nAnd then:\n```sh\ndocker run -it rs_pp\n```\nto publish the packages on Pypi.\n\n## Contact\n\nEmail: calixte@mozilla.com\n"
},
{
  "name": "BrowserQuest",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "bin",
      "client",
      "package.json",
      "server",
      "shared",
      "tools"
    ]
  },
  "makefile": null,
  "readme": "BrowserQuest\n============\n\nBrowserQuest is a HTML5/JavaScript multiplayer game experiment.\n\n\nDocumentation\n-------------\n\nDocumentation is located in client and server directories.\n\n\nLicense\n-------\n\nCode is licensed under MPL 2.0. Content is licensed under CC-BY-SA 3.0.\nSee the LICENSE file for details.\n\n\nCredits\n-------\nCreated by [Little Workshop](http://www.littleworkshop.fr):\n\n* Franck Lecollinet - [@whatthefranck](http://twitter.com/whatthefranck)\n* Guillaume Lecollinet - [@glecollinet](http://twitter.com/glecollinet)"
},
{
  "name": "readability",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitattributes",
      ".gitignore",
      ".npmignore",
      ".release-it.json",
      ".taskcluster.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "JSDOMParser.js",
      "README.md",
      "Readability-readerable.js",
      "Readability.js",
      "SECURITY.md",
      "index.d.ts",
      "index.js",
      "package-lock.json",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# Readability.js\n\nA standalone version of the readability library used for Firefox Reader View.\n\n## Installation\n\nReadability is available on npm:\n\n```bash\nnpm install @mozilla/readability\n```\n\nYou can then `require()` it, or for web-based projects, load the `Readability.js` script from your webpage.\n\n## Basic usage\n\nTo parse a document, you must create a new `Readability` object from a DOM document object, and then call the [`parse()`](#parse) method. Here's an example:\n\n```javascript\nvar article = new Readability(document).parse();\n```\n\nIf you use Readability in a web browser, you will likely be able to use a `document` reference from elsewhere (e.g. fetched via XMLHttpRequest, in a same-origin `<iframe>` you have access to, etc.). In Node.js, you can [use an external DOM library](#nodejs-usage).\n\n## API Reference\n\n### `new Readability(document, options)`\n\nThe `options` object accepts a number of properties, all optional:\n\n* `debug` (boolean, default `false`): whether to enable logging.\n* `maxElemsToParse` (number, default `0` i.e. no limit): the maximum number of elements to parse.\n* `nbTopCandidates` (number, default `5`): the number of top candidates to consider when analysing how tight the competition is among candidates.\n* `charThreshold` (number, default `500`): the number of characters an article must have in order to return a result.\n* `classesToPreserve` (array): a set of classes to preserve on HTML elements when the `keepClasses` options is set to `false`.\n* `keepClasses` (boolean, default `false`): whether to preserve all classes on HTML elements. When set to `false` only classes specified in the `classesToPreserve` array are kept.\n* `disableJSONLD` (boolean, default `false`): when extracting page metadata, Readability gives precendence to Schema.org fields specified in the JSON-LD format. Set this option to `true` to skip JSON-LD parsing.\n* `serializer` (function, default `el => el.innerHTML`) controls how the the `content` property returned by the `parse()` method is produced from the root DOM element. It may be useful to specify the `serializer` as the identity function (`el => el`) to obtain a DOM element instead of a string for `content` if you plan to process it further.\n\n### `parse()`\n\nReturns an object containing the following properties:\n\n* `title`: article title;\n* `content`: HTML string of processed article content;\n* `textContent`: text content of the article, with all the HTML tags removed;\n* `length`: length of an article, in characters;\n* `excerpt`: article description, or short excerpt from the content;\n* `byline`: author metadata;\n* `dir`: content direction;\n* `siteName`: name of the site.\n* `lang`: content language\n\nThe `parse()` method works by modifying the DOM. This removes some elements in the web page, which may be undesirable. You can avoid this by passing the clone of the `document` object to the `Readability` constructor:\n\n```js\nvar documentClone = document.cloneNode(true);\nvar article = new Readability(documentClone).parse();\n```\n\n### `isProbablyReaderable(document, options)`\n\nA quick-and-dirty way of figuring out if it's plausible that the contents of a given document are suitable for processing with Readability. It is likely to produce both false positives and false negatives. The reason it exists is to avoid bogging down a time-sensitive process (like loading and showing the user a webpage) with the complex logic in the core of Readability. Improvements to its logic (while not deteriorating its performance) are very welcome.\n\nThe `options` object accepts a number of properties, all optional:\n\n* `minContentLength` (number, default `140`): the minimum node content length used to decide if the document is readerable;\n* `minScore` (number, default `20`): the minumum cumulated 'score' used to determine if the document is readerable;\n* `visibilityChecker` (function, default `isNodeVisible`): the function used to determine if a node is visible;\n\nThe function returns a boolean corresponding to whether or not we suspect `Readability.parse()` will suceeed at returning an article object. Here's an example:\n\n```js\n/*\n    Only instantiate Readability  if we suspect\n    the `parse()` method will produce a meaningful result.\n*/\nif (isProbablyReaderable(document)) {\n    let article = new Readability(document).parse();\n}\n```\n\n## Node.js usage\n\nSince Node.js does not come with its own DOM implementation, we rely on external libraries like [jsdom](https://github.com/jsdom/jsdom). Here's an example using `jsdom` to obtain a DOM document object:\n\n```js\nvar { Readability } = require('@mozilla/readability');\nvar { JSDOM } = require('jsdom');\nvar doc = new JSDOM(\"<body>Look at this cat: <img src='./cat.jpg'></body>\", {\n  url: \"https://www.example.com/the-page-i-got-the-source-from\"\n});\nlet reader = new Readability(doc.window.document);\nlet article = reader.parse();\n```\n\nRemember to pass the page's URI as the `url` option in the `JSDOM` constructor (as shown in the example above), so that Readability can convert relative URLs for images, hyperlinks etc. to their absolute counterparts.\n\n`jsdom` has the ability to run the scripts included in the HTML and fetch remote resources. For security reasons these are [disabled by default](https://github.com/jsdom/jsdom#executing-scripts), and we **strongly** recommend you keep them that way.\n\n## Security\n\nIf you're going to use Readability with untrusted input (whether in HTML or DOM form), we **strongly** recommend you use a sanitizer library like [DOMPurify](https://github.com/cure53/DOMPurify) to avoid script injection when you use\nthe output of Readability. We would also recommend using [CSP](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP) to add further defense-in-depth\nrestrictions to what you allow the resulting content to do. The Firefox integration of\nreader mode uses both of these techniques itself. Sanitizing unsafe content out of the input is explicitly not something we aim to do as part of Readability itself - there are other good sanitizer libraries out there, use them!\n\n## Contributing\n\nPlease see our [Contributing](CONTRIBUTING.md) document.\n\n## License\n\n    Copyright (c) 2010 Arc90 Inc\n\n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n"
},
{
  "name": "looker-ci-line-chart",
  "files": {
    "/": [
      ".gitignore",
      ".prettierignore",
      "LICENSE",
      "README.md",
      "dist",
      "manifest.lkml",
      "marketplace.json",
      "package-lock.json",
      "package.json",
      "src",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# Looker Line Chart with Confidence Interval\n\n> Work in progress\n\n## Development\n\nInstall dependencies: `npm install`\n\nTo build the visualization plugin run: `npm run build`\n\nAny changes to the source code will be automatically detected.\n\n## Testing\n\nTo test local changes run `npm run start`\n\nAll changes made locally will be reflected when selecting the _Line Chart with Confidence Interval - Development_ visualization.\n\nThis visualization has been configured by adding a new _Visualization_ through the Admin interface and setting the entry point to `https://localhost:3443/ciLineChart.js`\n\n## Installation\n\nTo install this plugin in Looker for use in production, go to _Marketplace_ \u2192 _Manage_ \u2192 _Install via git URL_:\n\n* Git Repository URL: `https://github.com/mozilla/looker-ci-line-chart.git`\n* Git Commit SHA: `main`\n\n## Updating\n\nWhenever changes have been pushed to the git repository, the plugin needs to be manually updated in Looker.\nGo to _Marketplace_ \u2192 _Manage_ and click on the settings icon of the installed plugin. Click _Update_ to install updates.\n"
},
{
  "name": "node-convict",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "assert_changelog_ready",
      "example",
      "jest.config.js",
      "lerna.json",
      "package-lock.json",
      "package.json",
      "packages"
    ]
  },
  "makefile": null,
  "readme": "# Node-convict\n\n[![Build Status](https://travis-ci.org/mozilla/node-convict.svg?branch=master)](https://travis-ci.org/mozilla/node-convict)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/node-convict/badge.svg?branch=master)](https://coveralls.io/github/mozilla/node-convict?branch=master)\n\nConvict expands on the standard pattern of configuring node.js applications in a\nway that is more robust and accessible to collaborators, who may have less\ninterest in digging through code in order to inspect or modify settings. By\nintroducing a configuration schema, convict gives project collaborators more\n**context** on each setting and enables **validation and early failures** for\nwhen configuration goes wrong.\n\nThis repository is a monorepo for the following packages managed through\n[Lerna-Lite](https://github.com/ghiscoding/lerna-lite).\n\n\n## Packages\n\n* [convict](/packages/convict/) :\n  the main package\n\n* [convict-format-with-validator](/packages/convict-format-with-validator/)\n  the optional package providing the `email`, `ipaddress` and `url` formats\n\n* [convict-format-with-moment](/packages/convict-format-with-moment/) :\n  the optional package providing the `duration` and `timestamp` formats\n\n\n## Migrating\n\n* [Migrating from Convict 5 to 6](/packages/convict/MIGRATING_FROM_CONVICT_5_TO_6.md)\n\n\n## Contributing, maintenance\n\nFor contributors and maintainers, read the [Contributing](./CONTRIBUTING.md) doc.\n"
},
{
  "name": "rust-android-gradle",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "build.gradle",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "plugin",
      "samples",
      "settings.gradle",
      "version.properties"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Rust Android Gradle Plugin\n\nCross compile Rust Cargo projects for Android targets.\n\n\n<p align=\"left\">\n    <a alt=\"Version badge\" href=\"https://plugins.gradle.org/plugin/org.mozilla.rust-android-gradle.rust-android\">\n        <img src=\"https://img.shields.io/maven-metadata/v/https/plugins.gradle.org/m2/org/mozilla/rust-android-gradle/rust-android/org.mozilla.rust-android-gradle.rust-android.gradle.plugin/maven-metadata.xml.svg?label=rust-android-gradle&colorB=brightgreen\" /></a>\n</p>\n\n# Usage\n\nAdd the plugin to your root `build.gradle`, like:\n\n```groovy\nbuildscript {\n    repositories {\n        maven {\n            url \"https://plugins.gradle.org/m2/\"\n        }\n    }\n    dependencies {\n        classpath 'org.mozilla.rust-android-gradle:plugin:0.9.3'\n    }\n}\n```\n\nor \n\n```groovy\nbuildscript {\n    //...\n}\n\nplugins {\n    id \"org.mozilla.rust-android-gradle.rust-android\" version \"0.9.3\"\n}\n```\n\nIn your *project's* build.gradle, `apply plugin` and add the `cargo` configuration:\n\n```groovy\nandroid { ... }\n\napply plugin: 'org.mozilla.rust-android-gradle.rust-android'\n\ncargo {\n    module  = \"../rust\"       // Or whatever directory contains your Cargo.toml\n    libname = \"rust\"          // Or whatever matches Cargo.toml's [package] name.\n    targets = [\"arm\", \"x86\"]  // See bellow for a longer list of options\n}\n```\n\nInstall the rust toolchains for your target platforms:\n\n```sh\nrustup target add armv7-linux-androideabi   # for arm\nrustup target add i686-linux-android        # for x86\nrustup target add aarch64-linux-android     # for arm64\nrustup target add x86_64-linux-android      # for x86_64\nrustup target add x86_64-unknown-linux-gnu  # for linux-x86-64\nrustup target add x86_64-apple-darwin       # for darwin x86_64 (if you have an Intel MacOS)\nrustup target add aarch64-apple-darwin      # for darwin arm64 (if you have a M1 MacOS)\nrustup target add x86_64-pc-windows-gnu     # for win32-x86-64-gnu\nrustup target add x86_64-pc-windows-msvc    # for win32-x86-64-msvc\n...\n```\n\nFinally, run the `cargoBuild` task to cross compile:\n```sh\n./gradlew cargoBuild\n```\nOr add it as a dependency to one of your other build tasks, to build your rust code when you normally build your project:\n```gradle\ntasks.whenTaskAdded { task ->\n    if ((task.name == 'javaPreCompileDebug' || task.name == 'javaPreCompileRelease')) {\n        task.dependsOn 'cargoBuild'\n    }\n}\n```\n\n## Configuration\n\nThe `cargo` Gradle configuration accepts many options.\n\n### Linking Java code to native libraries\n\nGenerated libraries will be added to the Android `jniLibs` source-sets, when correctly referenced in\nthe `cargo` configuration through the `libname` and/or `targetIncludes` options.  The latter\ndefaults to `[\"lib${libname}.so\", \"lib${libname}.dylib\", \"{$libname}.dll\"]`, so the following configuration will\ninclude all `libbackend` libraries generated in the Rust project in `../rust`:\n\n```\ncargo {\n    module = \"../rust\"\n    libname = \"backend\"\n}\n```\n\nNow, Java code can reference the native library using, e.g.,\n\n```java\nstatic {\n    System.loadLibrary(\"backend\");\n}\n```\n\n### Native `apiLevel`\n\nThe [Android NDK](https://developer.android.com/ndk/guides/stable_apis) also fixes an API level,\nwhich can be specified using the `apiLevel` option.  This option defaults to the minimum SDK API\nlevel.  As of API level 21, 64-bit builds are possible; and conversely, the `arm64` and `x86_64`\ntargets require `apiLevel >= 21`.\n\n### Cargo release profile\n\nThe `profile` option selects between the `--debug` and `--release` profiles in `cargo`.  *Defaults\nto `debug`!*\n\n### Extension reference\n\n### module\n\nThe path to the Rust library to build with Cargo; required.  `module` can be absolute; if it is not,\nit is interpreted as a path relative to the Gradle `projectDir`.\n\n```groovy\ncargo {\n    // Note: path is either absolute, or relative to the gradle project's `projectDir`.\n    module = '../rust'\n}\n```\n\n### libname\n\nThe library name produced by Cargo; required.\n\n`libname` is used to determine which native libraries to include in the produced AARs and/or APKs.\nSee also [`targetIncludes`](#targetincludes).\n\n`libname` is also used to determine the ELF SONAME to declare in the Android libraries produced by\nCargo.  Different versions of the Android system linker\n[depend on the ELF SONAME](https://android-developers.googleblog.com/2016/06/android-changes-for-ndk-developers.html).\n\nIn `Cargo.toml`:\n\n```toml\n[lib]\nname = \"test\"\n```\n\nIn `build.gradle`:\n\n```groovy\ncargo {\n    libname = 'test'\n}\n```\n\n### targets\n\nA list of Android targets to build with Cargo; required.\n\nValid targets for **Android** are:\n\n```\n'arm',\n'arm64',\n'x86',\n'x86_64'\n```\nValid targets for **Desktop** are:\n```\n'linux-x86-64',\n'darwin-x86-64',\n'darwin-aarch64',\n'win32-x86-64-gnu',\n'win32-x86-64-msvc'\n```\n\nThe desktop targets are useful for testing native code in Android unit tests that run on the host,\nnot on the target device.  Better support for this feature is\n[planned](https://github.com/ncalexan/rust-android-gradle/issues/13).\n\n```groovy\ncargo {\n    targets = ['arm', 'x86', 'linux-x86-64']\n}\n```\n\n### prebuiltToolchains\n\nWhen set to `true` (which requires NDK version 19+), use the prebuilt toolchains bundled with the\nNDK. When set to `false`, generate per-target architecture standalone NDK toolchains using\n`make_standalone_toolchain.py`.  When unset, use the prebuilt toolchains if the NDK version is 19+,\nand fall back to generated toolchains for older NDK versions.\n\nDefaults to `null`.\n\n```groovy\ncargo {\n    prebuiltToolchains = true\n}\n```\n\n### verbose\n\nWhen set, execute `cargo build` with or without the `--verbose` flag.  When unset, respect the\nGradle log level: execute `cargo build` with or without the `--verbose` flag according to whether\nthe log level is at least `INFO`.  In practice, this makes `./gradlew ... --info` (and `./gradlew\n... --debug`) execute `cargo build --verbose ...`.\n\nDefaults to `null`.\n\n```groovy\ncargo {\n    verbose = true\n}\n```\n\n### profile\n\nThe Cargo [release profile](https://doc.rust-lang.org/book/second-edition/ch14-01-release-profiles.html#customizing-builds-with-release-profiles) to build.\n\nDefaults to `\"debug\"`.\n\n```groovy\ncargo {\n    profile = 'release'\n}\n```\n\n### features\n\nSet the Cargo [features](https://doc.rust-lang.org/cargo/reference/manifest.html#the-features-section).\n\nDefaults to passing no flags to `cargo`.\n\nTo pass `--all-features`, use\n```groovy\ncargo {\n    features {\n        all()\n    }\n}\n```\n\nTo pass an optional list of `--features`, use\n```groovy\ncargo {\n    features {\n        defaultAnd(\"x\")\n        defaultAnd(\"x\", \"y\")\n    }\n}\n```\n\nTo pass `--no-default-features`, and an optional list of replacement `--features`, use\n```groovy\ncargo {\n    features {\n        noDefaultBut()\n        noDefaultBut(\"x\")\n        noDefaultBut \"x\", \"y\"\n    }\n}\n```\n\n### targetDirectory\n\nThe target directory into which Cargo writes built outputs. You will likely need to specify this\nif you are using a [cargo virtual workspace](https://doc.rust-lang.org/book/ch14-03-cargo-workspaces.html),\nas our default will likely fail to locate the correct target directory.\n\nDefaults to `${module}/target`.  `targetDirectory` can be absolute; if it is not, it is interpreted\nas a path relative to the Gradle `projectDir`.\n\nNote that if `CARGO_TARGET_DIR` (see https://doc.rust-lang.org/cargo/reference/environment-variables.html)\nis specified in the environment, it takes precedence over `targetDirectory`, as cargo will output\nall build artifacts to it, regardless of what is being built, or where it was invoked.\n\nYou may also override `CARGO_TARGET_DIR` variable by setting `rust.cargoTargetDir` in\n`local.properties`, however it seems very unlikely that this will be useful, as we don't pass this\ninformation to cargo itself. That said, it can be used to control where we search for the built\nlibrary on a per-machine basis.\n\n```groovy\ncargo {\n    // Note: path is either absolute, or relative to the gradle project's `projectDir`.\n    targetDirectory = 'path/to/workspace/root/target'\n}\n```\n\n### targetIncludes\n\nWhich Cargo outputs to consider JNI libraries.\n\nDefaults to `[\"lib${libname}.so\", \"lib${libname}.dylib\", \"{$libname}.dll\"]`.\n\n```groovy\ncargo {\n    targetIncludes = ['libnotlibname.so']\n}\n```\n\n### apiLevel\n\nThe Android NDK API level to target.  NDK API levels are not the same as SDK API versions; they are\nupdated less frequently.  For example, SDK API versions 18, 19, and 20 all target NDK API level 18.\n\nDefaults to the minimum SDK version of the Android project's default configuration.\n\n```groovy\ncargo {\n    apiLevel = 21\n}\n```\n\nYou may specify the API level per target in `targets` using the `apiLevels` option. At most one of\n`apiLevel` and `apiLevels` may be specified. `apiLevels` must have an entry for each target in\n`targets`.\n\n```groovy\ncargo {\n    targets = [\"arm\", \"x86_64\"]\n    apiLevels = [\n        \"arm\": 16,\n        \"x86_64\": 21,\n    ]\n}\n```\n\n### extraCargoBuildArguments\n\nSometimes, you need to do things that the plugin doesn't anticipate.  Use `extraCargoBuildArguments`\nto append a list of additional arguments to each `cargo build` invocation.\n\n```groovy\ncargo {\n    extraCargoBuildArguments = ['a', 'list', 'of', 'strings']\n}\n```\n\n### exec\n\nThis is a callback taking the `ExecSpec` we're going to use to invoke `cargo build`, and\nthe relevant toolchain. It's called for each invocation of `cargo build`. This generally\nis useful for the following scenarios:\n\n1. Specifying target-specific environment variables.\n1. Adding target-specific flags to the command line.\n1. Removing/modifying environment variables or command line options the rust-android-gradle plugin would\n   provide by default.\n\n```groovy\ncargo {\n    exec { spec, toolchain ->\n        if (toolchain.target != \"x86_64-apple-darwin\") {\n            // Don't statically link on macOS desktop builds, for some\n            // entirely hypothetical reason.\n            spec.environment(\"EXAMPLELIB_STATIC\", \"1\")\n        }\n    }\n}\n```\n\n## Specifying NDK toolchains\n\nThe plugin can either use prebuilt NDK toolchain binaries, or search for (and if missing, build)\nNDK toolchains as generated by `make_standalone_toolchain.py`.\n\nA prebuilt NDK toolchain will be used if:\n1. `rust.prebuiltToolchain=true` in the per-(multi-)project `${rootDir}/local.properties`\n1. `prebuiltToolchain=true` in the `cargo { ... }` block (if not overridden by `local.properties`)\n1. The discovered NDK is version 19 or higher (if not overridden per above)\n\nThe toolchains are rooted in a single Android NDK toolchain directory.  In order of preference, the\ntoolchain root directory is determined by:\n\n1. `rust.androidNdkToolchainDir` in the per-(multi-)project `${rootDir}/local.properties`\n1. the environment variable `ANDROID_NDK_TOOLCHAIN_DIR`\n1. `${System.getProperty(java.io.tmpdir)}/rust-android-ndk-toolchains`\n\nNote that the Java system property `java.io.tmpdir` is not necessarily `/tmp`, including on macOS hosts.\n\nEach target architecture toolchain is named like `$arch-$apiLevel`: for example, `arm-16` or `arm64-21`.\n\n## Specifying local targets\n\nWhen developing a project that consumes `rust-android-gradle` locally, it's often convenient to\ntemporarily change the set of Rust target architectures.  In order of preference, the plugin\ndetermines the per-project targets by:\n\n1. `rust.targets.${project.Name}` for each project in `${rootDir}/local.properties`\n1. `rust.targets` in `${rootDir}/local.properties`\n1. the `cargo { targets ... }` block in the per-project `build.gradle`\n\nThe targets are split on `','`.  For example:\n\n```\nrust.targets.library=linux-x86-64\nrust.targets=arm,linux-x86-64,darwin\n```\n\n## Specifying paths to sub-commands (Python, Cargo, and Rustc)\n\nThe plugin invokes Python, Cargo and Rustc.  In order of preference, the plugin determines what command to invoke for Python by:\n\n1. the value of `cargo { pythonCommand = \"...\" }`, if non-empty\n1. `rust.pythonCommand` in `${rootDir}/local.properties`\n1. the environment variable `RUST_ANDROID_GRADLE_PYTHON_COMMAND`\n1. the default, `python`\n\nIn order of preference, the plugin determines what command to invoke for Cargo by:\n\n1. the value of `cargo { cargoCommand = \"...\" }`, if non-empty\n1. `rust.cargoCommand` in `${rootDir}/local.properties`\n1. the environment variable `RUST_ANDROID_GRADLE_CARGO_COMMAND`\n1. the default, `cargo`\n\nIn order of preference, the plugin determines what command to invoke for `rustc` by:\n\n1. the value of `cargo { rustcCommand = \"...\" }`, if non-empty\n1. `rust.rustcCommand` in `${rootDir}/local.properties`\n1. the environment variable `RUST_ANDROID_GRADLE_RUSTC_COMMAND`\n1. the default, `rustc`\n\n(Note that failure to locate `rustc` is not fatal, however it may result in rebuilding the code more often than is necessary).\n\nPaths must be host operating system specific.  For example, on Windows:\n\n```properties\nrust.pythonCommand=c:\\Python27\\bin\\python\n```\n\nOn Linux,\n```shell\nenv RUST_ANDROID_GRADLE_CARGO_COMMAND=$HOME/.cargo/bin/cargo ./gradlew ...\n```\n\n## Specifying Rust channel\n\nRust is released to three different \"channels\": stable, beta, and nightly (see\nhttps://rust-lang.github.io/rustup/concepts/channels.html).  The `rustup` tool, which is how most\npeople install Rust, allows multiple channels to be installed simultaneously and to specify which\nchannel to use by invoking `cargo +channel ...`.\n\nIn order of preference, the plugin determines what channel to invoke `cargo` with by:\n\n1. the value of `cargo { rustupChannel = \"...\" }`, if non-empty\n1. `rust.rustupChannel` in `${rootDir}/local.properties`\n1. the environment variable `RUST_ANDROID_GRADLE_RUSTUP_CHANNEL`\n1. the default, no channel specified (which `cargo` installed via `rustup` generally defaults to the\n   `stable` channel)\n\nThe channel should be recognized by `cargo` installed via `rustup`, i.e.:\n- `\"stable\"`\n- `\"beta\"`\n- `\"nightly\"`\n\nA single leading `'+'` will be stripped, if present.\n\n(Note that Cargo installed by a method other than `rustup` will generally not understand `+channel`\nand builds will likely fail.)\n\n## Passing arguments to cargo\n\nThe plugin passes project properties named like `RUST_ANDROID_GRADLE_target_..._KEY=VALUE` through\nto the Cargo invocation for the given Rust `target` as `KEY=VALUE`.  Target should be upper-case\nwith \"-\" replaced by \"_\".  (See [the links from this Cargo issue](https://github.com/rust-lang/cargo/issues/5690).) So, for example,\n\n```groovy\nproject.RUST_ANDROID_GRADLE_I686_LINUX_ANDROID_FOO=BAR\n```\nand\n```shell\n./gradlew -PRUST_ANDROID_GRADLE_ARMV7_LINUX_ANDROIDEABI_FOO=BAR ...\n```\nand\n```\nenv ORG_GRADLE_PROJECT_RUST_ANDROID_GRADLE_ARMV7_LINUX_ANDROIDEABI_FOO=BAR ./gradlew ...\n```\nall set `FOO=BAR` in the `cargo` execution environment (for the \"armv7-linux-androideabi` Rust\ntarget, corresponding to the \"x86\" target in the plugin).\n\n# Development\n\nAt top-level, the `publish` Gradle task updates the Maven repository\nunder `build/local-repo`:\n\n```\n$ ./gradlew publish\n...\n$ ls -al build/local-repo/org/mozilla/rust-android-gradle/org.mozilla.rust-android-gradle.gradle.plugin/0.4.0/org.mozilla.rust-android-gradle.gradle.plugin-0.4.0.pom\n-rw-r--r--  1 nalexander  staff  670 18 Sep 10:09\nbuild/local-repo/org/mozilla/rust-android-gradle/org.mozilla.rust-android-gradle.gradle.plugin/0.4.0/org.mozilla.rust-android-gradle.gradle.plugin-0.4.0.pom\n```\n\n## Sample projects\n\nThe easiest way to get started is to run the sample projects.  The sample projects have dependency\nsubstitutions configured so that changes made to `plugin/` are reflected in the sample projects\nimmediately.\n\n```\n$ ./gradlew -p samples/library :assembleDebug\n...\n$ file samples/library/build/outputs/aar/library-debug.aar\nsamples/library/build/outputs/aar/library-debug.aar: Zip archive data, at least v1.0 to extract\n```\n\n```\n$ ./gradlew -p samples/app :assembleDebug\n...\n$ file samples/app/build/outputs/apk/debug/app-debug.apk\nsamples/app/build/outputs/apk/debug/app-debug.apk: Zip archive data, at least v?[0] to extract\n```\n\n## Testing Local changes\n\nAn easy way to locally test changes made in this plugin is to simply add this to your project's `settings.gradle`:\n\n```gradle\n// Switch this to point to your local plugin dir\nincludeBuild('../rust-android-gradle') {\n    dependencySubstitution {\n        // As required.\n        substitute module('gradle.plugin.org.mozilla.rust-android-gradle:plugin') with project(':plugin')\n    }\n}\n```\n\n# Publishing\n\n## Automatically via the Bump version Github Actions workflow\n\nYou will need to be a collaborator.  First, manually invoke the [Bump version Github Actions\nworkflow](https://github.com/mozilla/rust-android-gradle/actions/workflows/bump.yml).  Specify a\nversion (like \"x.y.z\", without quotes) and a single line changelog entry.  (This entry will have a\ndash prepended, so that it would look normal in a list.  This is working around [the lack of a\nmulti-line input in Github\nActions](https://github.community/t/multiline-inputs-for-workflow-dispatch/163906).)  This will push\na preparatory commit updating version numbers and the changelog like [this\none](https://github.com/mozilla/rust-android-gradle/commit/2a637d1797a5d0b5063b8d2f0a3d4a4938511154),\nand make a **draft** Github Release with a name like `vx.y.z`.  After verifying that tests pass,\nnavigate to [the releases panel](https://github.com/mozilla/rust-android-gradle/releases) and edit\nthe release, finally pressing \"Publish release\".  The release Github workflow will build and publish\nthe plugin, although it may take some days for it to be reflected on the Gradle plugin portal.\n\n## By hand\n\nYou will need credentials to publish to the [Gradle plugin portal](https://plugins.gradle.org/) in\nthe appropriate place for the [`plugin-publish`](https://plugins.gradle.org/docs/publish-plugin) to\nfind them.  Usually, that's in `~/.gradle/gradle.properties`.\n\nAt top-level, the `publishPlugins` Gradle task publishes the plugin for consumption:\n\n```\n$ ./gradlew publishPlugins\n...\nPublishing plugin org.mozilla.rust-android-gradle.rust-android version 0.8.1\nPublishing artifact build/libs/plugin-0.8.1.jar\nPublishing artifact build/libs/plugin-0.8.1-sources.jar\nPublishing artifact build/libs/plugin-0.8.1-javadoc.jar\nPublishing artifact build/publish-generated-resources/pom.xml\nActivating plugin org.mozilla.rust-android-gradle.rust-android version 0.8.1\n```\n\n## Real projects\n\nTo test in a real project, use the local Maven repository in your `build.gradle`, like:\n\n```gradle\nbuildscript {\n    repositories {\n        maven {\n            url \"file:///Users/nalexander/Mozilla/rust-android-gradle/build/local-repo\"\n        }\n    }\n\n    dependencies {\n        classpath 'org.mozilla.rust-android-gradle:plugin:0.9.0'\n    }\n}\n```\n"
},
{
  "name": "classify-client",
  "files": {
    "/": [
      ".cargo",
      ".circleci",
      ".codecov.yml",
      ".dockerignore",
      ".gitignore",
      ".therapist.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "app.yaml",
      "bors.toml",
      "src",
      "version.json"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Classify Client\n\nThis is an optimized version of the classify client endpoint in [Normandy](https://github.com/mozilla/normandy).\n\n## Dev instructions\n\nThis is a normal Cargo project, so after cloning the repository, you can build and run it with\n\n```shell\n$ cargo build\n$ cargo run\n```\n\nThis project should run on the latest stable version of Rust. Unstable features are not allowed.\n\n### GeoIP Database\n\nA GeoIP database will need to be provided. By default it is expected to be\nfound at `./GeoLite2-Country.mmdb`.\n\n## Configuration\n\nVia environment variables:\n\n- `DEBUG`: Set to `\"true\"` to enable extra debugging options, such as a `/debug`\n    endpoint that shows internal server state (default: `\"false\"`).\n- `GEOIP_DB_PATH`: path to GeoIP database (default: `\"./GeoLite2-Country.mmdb\"`)\n- `HOST`: host to bind to (default: `\"localhost\"`)\n- `HUMAN_LOGS`: set to `\"true\"` to use human readable logging (default: MozLog as JSON)\n- `METRICS_TARGET`: The host and port to send statsd metrics to. May be a\n    hostname like `\"metrics.example.com:8125\"` or an IP like\n    `\"127.0.0.1:8125\"`. Port is required. (default: `\"localhost:8125\"`)\n- `PORT`: port number to bind to (default: `\"8000\"`)\n- `SENTRY_DSN`: report errors to a Sentry instance (default: `\"\"`)\n- `TRUSTED_PROXY_LIST`: A comma-separated list of CIDR ranges that trusted\n    proxies will be in. Supports both IPv4 and IPv6.\n- `VERSION_FILE`: path to `version.json` file (default: `\"./version.json\"`)\n\n## Tests\n\nTests can be run with Cargo as well\n\n```shell\n$ cargo test\n```\n\n## Linting\n\nLinting is handled via\n[Therapist](https://therapist.readthedocs.io/en/latest/). After installing it,\nenable the git hooks using either `therapist install` or `therapist install\n--fix`. The `--fix` variant will automatically format your code upon commit.\nThe variant without `--fix` will simply show an error and ask you to reformat\nthe code using other means before committing.  Therapist runs in CI.\n\nThe checks Therapist runs are:\n\n* Rustfmt\n* Clippy, using the `clippy::all` preset\n"
},
{
  "name": "probe-dictionary",
  "files": {
    "/": [
      ".dockerignore",
      ".env",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "babel.config.js",
      "cypress.json",
      "cypress",
      "package.json",
      "public",
      "src",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": "## probe-dictionary\nA tool that makes the Telemetry probe data in Firefox more discoverable and searchable.\n\nThis front-end allows answering questions like\n* *\"do we have any probes in Firefox 55 that tell us about [tab usage](https://telemetry.mozilla.org/probe-dictionary/?search=tab&searchtype=in_name&optout=true&channel=release&constraint=is_in&version=55)?\"*\n* *\"which Firefox versions is this probe in anyway?\"*\n\nTo achieve this, it uses data extracted by the [probe-scraper](https://github.com/mozilla/probe-scraper) project.\nThis pulls probe registry files (`Histograms.json`, `Scalars.yaml`, `Events.yaml`) from different Firefox versions together into one dataset.\nAlso, probes outside of `Histograms.json` - like the CSS use counters - are included in the output data.\n\nCurrently this supports:\n* release, beta & nightly channels\n* major releases only\n* all probes registered in separate files (histograms, scalars, events)\n* some select environment data points (more to come)\n\n\nThis project was bootstrapped with [Create React App](https://github.com/facebook/create-react-app).\n\n## Development\nTo run locally:\n> run `npm start`\nTo launch the cypress test runner:\n> run `npm run test`\n\nThe production data endpoints should work just fine locally.\n"
},
{
  "name": "telemetry-dashboard",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "addon-perf",
      "addon-recommender",
      "dashboard-generator",
      "favicon.ico",
      "favicon.xcf",
      "histogram-simulator",
      "index.html",
      "new-pipeline",
      "probe-dictionary",
      "update-orphaning",
      "v2",
      "wrapper"
    ]
  },
  "makefile": null,
  "readme": "Telemetry Dashboard\n===================\n\nThis repository contains the **source code for [telemetry.mozilla.org](https://telemetry.mozilla.org)**. The dashboards on this site can be used for everything from checking measure values to figuring out common causes of Firefox hangs.\n\nThe main dashboards on [telemetry.mozilla.org](https://telemetry.mozilla.org) consume data from Telemetry's backend using Telemetry.js.\n\nThe dashboards that do not use Telemetry.js generally use [scheduled analysis jobs](https://analysis.telemetry.mozilla.org/) that regularly publish data on S3. The source code for these can be found in their respective repositories.\n\nThis repository also contains the **source code for Telemetry.js**. The specific files can be found under the `v2/` directory.\n\nDeploying Telemetry Dashboard\n-----------------------------\n\nThe [telemetry.mozilla.org](https://telemetry.mozilla.org) site is hosted in [Github Pages](https://pages.github.com/), so it may also be accessed via [mozilla.github.io/telemetry-dashboard](https://mozilla.github.io/telemetry-dashboard/). In front of Github Pages, there is also the CloudFront CDN (managed by :whd).\n\nUpdates to the `gh-pages` branch (also the default branch) will be reflected on [telemetry.mozilla.org](https://telemetry.mozilla.org) after a few moments.\n\nUsing Telemetry.js\n------------------\n\nCheck out the documentation!\n\n* [Telemetry.js v2](https://github.com/mozilla/telemetry-dashboard/blob/gh-pages/v2/doc.md)\n\nAdding Telemetry Probes\n-----------------------\n\nSee this [MDN article](https://developer.mozilla.org/en-US/docs/Mozilla/Performance/Adding_a_new_Telemetry_probe), which outlines the process and details for adding new Telemetry probes to Firefox which can be used with the dashboards.\n\nFor setting histogram properties, make sure to check out the [histogram simulator](https://telemetry.mozilla.org/histogram-simulator/), which might help with designing histograms that fit the expected data well.\n\nContributing to the Telemetry Dashboard\n---------------------------------------\n\nThis project is entirely open source, and licensed under the MPL to boot. Contributions welcome!\n\n### Getting started\n\nLooking for some task to get started on? Check the list of [mentored issues](https://github.com/mozilla/telemetry-dashboard/labels/mentored). If you're unsure what to choose, just get in touch.\n\nCommunication happens mostly on Github in the comments for issues or pull requests, but we're also active in IRC in [#tmo on irc.mozilla.org](https://client00.chat.mibbit.com/?server=irc.mozilla.org&channel=%23tmo).\n\n### Working on the dashboard\n\nA local webserver is really helpful for running a version of the site on the same machine you're developing on.\n* This can be done as follows (requires Python):\n\n          cd /PATH_TO_REPOSITORY_ROOT\n          python -m http.server\n          # now visit localhost:8000 in your browser to see the page\n\n* It can also be done with NPM:\n\n          cd /PATH_TO_REPOSITORY_ROOT\n          npm install http-server -g\n          # now visit localhost:8080 in your browser to see the page\n\nShortlink buttons (in the top right hand corner of the main dashboards) will not work when running the site on local servers. This is because they are shortened with bit.ly, which doesn't allow local links.\n\n### Submitting pull requests for the dashboard\n\nFor pull requests, it is recommended that you set up a live site hosting your branch. This makes it a lot easier for reviewers to check out the changes.\n\n[GitHub Pages](https://pages.github.com/) is great for this, you can enable this in your forked repository under \"Settings\", \"Github pages\" and then share the link in your pull requests comments. The [Github help pages](https://help.github.com/categories/github-pages-basics/) have more information.\n\nIf you need more than one test site up at a time, try [Divshot](https://divshot.com/), a static website hosting service.\n\n### Working on Telemetry.js\n\nTelemetry.js (both versions) is pretty straightforward to work on. However, note that sites that use Telemetry.js generally hotlink to the source files - make sure to preserve API backwards compatibility wherever possible.\n"
},
{
  "name": "hawk",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".taskcluster.yml",
      "API.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE.md",
      "README.md",
      "SECURITY.md",
      "lib",
      "package.json",
      "test"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# hawk\n\n#### HTTP Holder-Of-Key Authentication Scheme.\n\nDocumentation of the protocol, and the JS API, is in https://github.com/mozilla/hawk/blob/main/API.md.\n\n## Ownership Changes\n\nThis was once `hueniverse/hawk` and relased as `hawk`.\nThen, after the 7.0.10 release, it was moved to the `hapijs/hawk` repository and released as `@hapi/hawk`.\nHapi later de-supported the library, after releasing version 8.0.0.\nIt has since been moved to `mozilla/hawk` and is again released as `hawk`.\nAll of the intermediate versions are also relased as `hawk`.\n\nChanges are represented in GitHub releases on this repository.\n\nMozilla maintains several Hawk implementations in different langauages, so it is likely to stay at Mozilla for some time.\n\nThis library is in \"maintenance mode\" -- no features will be added, and only security-related bugfixes will be applied.\n"
},
{
  "name": "anti-tracking-test-pages",
  "files": {
    "/": [
      ".DS_Store",
      "CNAME",
      "LICENSE",
      "README.md",
      "test",
      "tracker-test"
    ]
  },
  "makefile": null,
  "readme": "# anti-tracking-test-pages\nPages to test Firefox/Gecko anti-tracking features.\n\nThis site was orignally self hosted by Steven Englehardt at `senglehardt.com` using this repo: https://github.com/englehardt/moz-test-pages\n\nThis site contained 2 subdomains which redirected back to `senglehardt.com`: `test.senglehardt.com`, `storage.senglehardt.com`\n\nThe sibling \"Tracker Test\" site was hosted at `anti-tracker-test.com` using this repo: https://github.com/englehardt/trackertest \n\nIt's subdomains are: `known-tracker.anti-tracker-test.com`\n\nThe current mappings are below, urls and subdomains are hosted in netlify:\n- senglehardt.com -> mozilla-anti-tracking.com\n- anti-tracker-test.com -> tracker-test.com\n\n\nJira issues related to this effort:\n- https://mozilla-hub.atlassian.net/browse/SE-3090\n- https://mozilla-hub.atlassian.net/browse/SE-3100\n"
},
{
  "name": "pdf.js.quickjs",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      "0001-Extra-stuff-for-use-with-PDF.js.patch",
      "Dockerfile",
      "README.md",
      "build.js",
      "compile.sh",
      "package-lock.json",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# pdf.js.quickjs\n\nProvide a sandbox to PDF.js.\n\n## Build\n\nRun:\n\n```sh\nnode build.js --compile --output my_output_dir\n```\n\nit will create a Docker image with emsdk and then run it. The generated `quickjs-eval.js` will be in `my_output_dir`.\n\n## Update\n\nIn order to update quickjs to a specific revision, change the commit hash in `Dockerfile` and then run:\n```sh\nnode build.js --create\n```\nto create a new docker image and then\n```sh\nnode build.js --compile --output my_output_dir\n```\nto compile. The short version is:\n```sh\nnode build.js -Cco my_output_dir\n```\n\n## Licensing\n\nThe code is released under [MIT license](https://opensource.org/licenses/MIT).\n"
},
{
  "name": "pkipolicy",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "ccadb",
      "rootstore"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla PKI Policy\n\nThis repository contains documents relating to Mozilla's PKI policies, such as\nits [certificate root program](https://wiki.mozilla.org/CA). The\nowner of these documents is the Module Owner of the \"[Mozilla CA Certificate\nPolicy](https://wiki.mozilla.org/Modules/Activities#Mozilla_CA_Certificate_Policy)\"\nmodule. Mozilla's PKI policies can be discussed in https://groups.google.com/a/mozilla.org/g/dev-security-policy.\nArchives of previous discussions are in https://groups.google.com/g/mozilla.dev.security.policy.\n\n## Root Program ##\n\nYou can find out [how to apply to be included in Mozilla's root\nprogram](https://wiki.mozilla.org/CA/Application_Process). There is also a\n[list of included roots](https://wiki.mozilla.org/CA/Included_Certificates).\n\n### Using Mozilla's Root Store ###\n\nThe decisions Mozilla makes with regards to the inclusion or exclusion of CA\ncertificates in its root store are directly tied to the capabilities and\nbehaviours of the software Mozilla distributes. Sometimes, a security change\nis made wholly or partly in the software instead of the root store. Further,\nMozilla does not promise to take into account the needs of other users of its\nroot store when making such decisions.\n\nTherefore, anyone considering bundling [Mozilla's root store](https://wiki.mozilla.org/CA/Included_Certificates) with other\nsoftware needs to be aware of the issues surrounding providing a root store,\nand committed to making sure that they maintain security for their users by\ncarefully observing Mozilla's actions and taking appropriate steps of their\nown. On a best-efforts basis, Mozilla maintains a\n[list of additional things](https://wiki.mozilla.org/CA/Additional_Trust_Changes)\nusers of our store might need to consider. \n\n[Data Usage Terms](https://www.ccadb.org/rootstores/usage#ccadb-data-usage-terms)\n"
},
{
  "name": "bugherder",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "bin",
      "bugherder",
      "karma.conf.js",
      "package.json",
      "runtime.txt",
      "test",
      "web-server.js"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/mozilla/bugherder.svg?branch=master)](https://travis-ci.org/mozilla/bugherder)\n[![Dependency Status](https://david-dm.org/mozilla/bugherder.svg)](https://david-dm.org/mozilla/bugherder)\n[![devDependency Status](https://david-dm.org/mozilla/bugherder/dev-status.svg)](https://david-dm.org/mozilla/bugherder#info=devDependencies)\n\nBugherder is a tool for marking bugs post-merge, created by [Graeme McCutcheon](http://www.graememcc.co.uk/).\n\nNOTES ON TESTING\n----------------\nAdding \"?debug=1\" shows how all changesets were identified, and shows what changesets Bugherder decided were affected by a backout.\n\nAdding \"?remap=1\" allows you to divert output to the bzapi sandbox bugzilla at [landfill.bugzilla.org](https://://landfill.bugzilla.org/bzapi_sandbox/).\nIf you use this option, you will be presented with a table of all the bugs associated with changesets. You can then enter a bug number from bzapi that output will be sent to. You don't need to enter a new bug number for every single bug shown, but only those you do enter will be transmitted, other bugs will be ignored.\n\nBugherder will not check the existence of any bugs you enter - be careful! Each bug number entered must be unique - that **will** be checked, as apparently I can't follow my own instructions. You will probably want to review the pushlog - if the push for the only bug you entered was backed out, then nothing will be sent.\n\nWhen testing target milestone setting, the equivalent bug in landfill must be filed in the *mcMerge Test Product* product, or the submission will fail.\n\nThere are various checkbox options at the bottom of the screen:\n* an option to add checkin-needed to some random bug whiteboards, to allow you to test it's removal on submission. Be careful of bugs with additional keywords - if they are not defined on landfill (and they probably won't be), the submission will fail\n* an option to throw up an alert - hacky, I know - partway through the submission process, to allow you to jump over to the landfill bug, and mid-air Bugherder\n* a useful option to ignore the real bug status, and set it to NEW, as you will likely be working with historic pushlogs when testing\n\nIf you do not enter any diversion bugs on the remap page, you will return to live mode, with changes going to [bugzilla.mozilla.org](https://bugzilla.mozilla.org/).\n\n\nRunning the Node.js static server locally\n-----------------------------------------\n1. Install [Node.js](https://nodejs.org/).\n2. $ ``npm install --production``\n(optionally omit ``--production`` to also install packages for the test suite).\n3. $ ``npm start``\n4. Navigate to [http://localhost:5000/](http://localhost:5000/).\n\n\nHeroku notes\n------------\n* For setup instructions/CLI guide, see the Heroku\n[getting started](https://devcenter.heroku.com/articles/getting-started-with-nodejs) tutorial.\n* The presence of `package.json` will cause Heroku to pick the\n[Node.js buildpack](https://github.com/heroku/heroku-buildpack-nodejs) automatically, however\nit's preferable to pin to a specific version tag of the buildpack, using eg:\n``heroku buildpacks:set git://github.com/heroku/heroku-buildpack-nodejs.git#v86``\n* During deploy, Heroku runs ``npm install --production``.\n* On start-up, the web dyno runs the command defined in the ``Procfile``.\n* On production, HTTP is 301 redirected to HTTPS, and all files are served\nwith a Cache-Control max-age set.\n* The \"auto-deploy from master when CI has passed\" option is enabled, to override\ndon't use ``git push`` - instead use the manual branch deploy controls\n[here](https://dashboard.heroku.com/apps/bugherder/deploy/github).\n\n\nSTANDING ON THE SHOULDERS OF GIANTS\n-----------------------------------\nBugherder uses the following third-party projects:\n\n* bz.js by Heather Arthur - [https://github.com/harthur/bz.js](https://github.com/harthur/bz.js)\n* jQuery - [http://jquery.com/](http://jquery.com)\n* Toast CSS framework by Dan Eden - [http://www.daneden.me/toast](http://www.daneden.me/toast)\n"
},
{
  "name": "bigquery-backfill",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "backfill",
      "script"
    ]
  },
  "makefile": null,
  "readme": "# bigquery-backfill\nScripts and historical records related to backfills in Mozilla's telemetry pipeline\n\n## Layout\n\nThere is a `script` directory containing relatively pristine reference scripts\nthat you can copy and paste into a new backfill scenario and modify for your\nparticular needs.\n\nThere is a `backfills` directory where each subdirectory should be a dated\nbackfill event, containing all the scripts used and a description of the\noverall scenario.\n\n## Setup\n\nMost of these backfill scenarios will assume that you have\n[`gcp-ingestion`](https://github.com/mozilla/gcp-ingestion) checked out\nlocally. `mvn` invocations in scripts likely assume that you're in the\n`ingestion-beam` directory of that repo.\n"
},
{
  "name": "reposado",
  "files": {
    "/": [
      ".gitignore",
      "CONTRIBUTING.md",
      "LICENSE.txt",
      "README.md",
      "code",
      "docs",
      "other",
      "setup.py"
    ],
    "/docs": [
      "URL_rewrites.md",
      "client_configuration.md",
      "getting_started.md",
      "reference.md",
      "reposado_metadata.md",
      "reposado_operation.md",
      "reposado_preferences.md",
      "reposado_py2exe.md"
    ]
  },
  "makefile": null,
  "readme": "**WARNING**\n\nThis is a fork of reposado used internally at Mozilla to scrape debug information from macOS builds. It contains a few changes compared with upstream that are needed for our use-cases:\n* Added the --no-download add --product-id options which allow to selectively download only the packages we require (or none at all)\n* The code has been converted to Python 3\n\n**macOS Big Sur important information**  \nIn macOS Big Sur, Apple has removed the ability for `softwareupdate` to be pointed to a non-Apple sucatalog. This means you cannot use a Reposado server to serve Apple software updates to Big Sur (and presumably later versions of macOS) clients.\n\n**INTRODUCTION**\n\nReposado is a set of tools written in Python that replicate the key functionality of Mac OS X Server's Software Update Service.\n\n**LICENSE**\n\nReposado is licensed under the new BSD license.\n\n**DISCUSSION GROUP**\n\nDiscussion for users and developers of Reposado is [here.](http://groups.google.com/group/reposado)\n\n**FEATURES AND CAPABILITIES**\n\nReposado, together with Python, the \"curl\" binary tool and a web server such as Apache 2, enables you to host a local Apple Software Update Server on any hardware and OS of your choice.\n\nReposado contains a tool (repo_sync) to download Software Update catalogs and (optionally) update packages from Apple's servers, enabling you to host them from a local web server.\n\nAdditionally, Reposado provides a command-line tool (repoutil) that enables you to create any arbitrary number of \"branches\" of the Apple catalogs. These branches can contain any subset of the available updates. For example, one could create \"testing\" and \"release\" branches, and then set some clients to use the \"testing\" branch catalog to test newly-released updates. You would set most of your clients to use the \"release\" branch catalog, which would contain updates that had been through the testing process.\n\nIf you configure Reposado to also download the actual updates as well as the catalogs, you can continue to offer updates that have been superseded by more recent updates. For example, if you are currently offering the 10.6.7 updates to your clients, and Apple releases a 10.6.8 update, you can continue to offer the (deprecated) 10.6.7 update until you are ready to release the newer update to your clients. You can even offer the 10.6.7 update to your \"release\" clients while offering the 10.6.8 update to your \"testing\" clients. Offering \"deprecated\" Apple Software Updates is a feature that is difficult with Apple's tools.\n\n**LIMITATIONS AND DEPENDENCIES**\n\nApple's Software Update Service does a few things. Primarily, it replicates software updates from Apple's servers, downloading them to a local machine. Secondly, it functions as a web server to actually serve these updates to client machines. Reposado does not duplicate the web server portion of Apple's Software Update Service. Instead you may use any existing web server you wish.\n\nReposado also currently relies on the command-line \"curl\" binary to download updates from Apple's servers. curl is available on OS X, RedHat Linux, and many other OSes, including Win32 and Win64 versions. See [http://curl.haxx.se](http://curl.haxx.se) for more information.\n\n**MORE INFO**\n\nMore information and basic documentation is available here: https://github.com/wdas/reposado/tree/master/docs\n"
},
{
  "name": "silme",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LEGAL.txt",
      "lib",
      "logo",
      "pyproject.toml",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "jexl-rs",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "jexl-eval",
      "jexl-parser",
      "parser-gen"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# jexl-rs [![CircleCI](https://circleci.com/gh/mozilla/jexl-rs/tree/main.svg?style=svg)](https://circleci.com/gh/mozilla/jexl-rs/tree/main)\n\n> [JEXL](https://www.npmjs.com/package/jexl) in Rust\n\n\n* jexl-eval: [![](https://img.shields.io/crates/v/jexl-eval.svg)](https://crates.io/crates/jexl-eval) [![](https://docs.rs/jexl-eval/badge.svg?v=2)](https://docs.rs/jexl-eval)\n* jexl-parser: [![](https://img.shields.io/crates/v/jexl-parser.svg)](https://crates.io/crates/jexl-parser) [![](https://docs.rs/jexl-parser/badge.svg?v=2)](https://docs.rs/jexl-parser)\n\n## Releases\n\nReleases are done using `cargo release`. Check [cargo-release](https://github.com/sunng87/cargo-release) for more information\n\n## License\n\nThis project is licensed under the [Mozilla Public License 2.0](https://github.com/tarikeshaq/update-notifier/blob/master/LICENSE)\n"
},
{
  "name": "usecounters",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "index.html",
      "loading.gif"
    ]
  },
  "makefile": null,
  "readme": "# usecounters\nPOC for a dashboard for Firefox usecounters\n"
},
{
  "name": "TTS",
  "files": {
    "/": [
      ".cardboardlint.yml",
      ".circleci",
      ".compute",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".pylintrc",
      "CODE_OF_CONDUCT.md",
      "CODE_OWNERS.rst",
      "CONTRIBUTING.md",
      "LICENSE.txt",
      "MANIFEST.in",
      "README.md",
      "TTS",
      "images",
      "notebooks",
      "pyproject.toml",
      "requirements.txt",
      "requirements_tests.txt",
      "run_tests.sh",
      "setup.cfg",
      "setup.py",
      "tests"
    ],
    "/.github": [
      "ISSUE_TEMPLATE.md",
      "PR_TEMPLATE.md",
      "stale.yml"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "<img src=\"https://user-images.githubusercontent.com/1402048/104139991-3fd15e00-53af-11eb-8640-3a78a64641dd.png\" data-canonical-src=\"![TTS banner](https://user-images.githubusercontent.com/1402048/104139991-3fd15e00-53af-11eb-8640-3a78a64641dd.png =250x250)\n\" width=\"256\" height=\"256\" align=\"right\" />\n\n# TTS: Text-to-Speech for all.\n\nTTS is a library for advanced Text-to-Speech generation. It's built on the latest research, was designed to achieve the best trade-off among ease-of-training, speed and quality.\nTTS comes with [pretrained models](https://github.com/mozilla/TTS/wiki/Released-Models), tools for measuring dataset quality and already used in **20+ languages** for products and research projects.\n\n[![CircleCI](<https://circleci.com/gh/mozilla/TTS/tree/dev.svg?style=svg>)]()\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n\n:loudspeaker: [English Voice Samples](https://erogol.github.io/ddc-samples/) and [SoundCloud playlist](https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2)\n\n:man_cook:  [TTS training recipes](https://github.com/erogol/TTS_recipes)\n\n:page_facing_up: [Text-to-Speech paper collection](https://github.com/erogol/TTS-papers)\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly, so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \u2754 **FAQ**                       | [TTS/Wiki](https://github.com/mozilla/TTS/wiki/FAQ)                              |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [Discourse Forum]                       |\n| \ud83d\uddef **General Discussion**        | [Discourse Forum] and [Matrix Channel]  |\n\n[github issue tracker]: https://github.com/mozilla/tts/issues\n[discourse forum]: https://discourse.mozilla.org/c/tts/\n[matrix channel]: https://matrix.to/#/!KTePhNahjgiVumkqca:matrix.org?via=matrix.org\n[Tutorials and Examples]: https://github.com/mozilla/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbe **Installation** | [TTS/README.md](https://github.com/mozilla/TTS/tree/dev#install-tts)|\n| \ud83d\udc69\ud83c\udffe\u200d\ud83c\udfeb **Tutorials and Examples**  | [TTS/Wiki](https://github.com/mozilla/TTS/wiki/TTS-Notebooks-and-Tutorials) |\n| \ud83d\ude80 **Released Models**         | [TTS/Wiki](https://github.com/mozilla/TTS/wiki/Released-Models)|\n| \ud83d\udcbb **Docker Image**            | [Repository by @synesthesiam](https://github.com/synesthesiam/docker-mozillatts)|\n| \ud83d\udda5\ufe0f **Demo Server**             | [TTS/server](https://github.com/mozilla/TTS/tree/master/TTS/server)|\n| \ud83e\udd16 **Running TTS on Terminal** | [TTS/README.md](https://github.com/mozilla/TTS#example-synthesizing-speech-on-terminal-using-the-released-models)|\n| \u2728 **How to contribute**       |[TTS/README.md](#contribution-guidelines)|\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://discourse-prod-uploads-81679984178418.s3.dualstack.us-west-2.amazonaws.com/optimized/3X/6/4/6428f980e9ec751c248e591460895f7881aec0c6_2_1035x591.png\" width=\"800\" /></p>\n\n\"Mozilla*\" and \"Judy*\" are our models.\n[Details...](https://github.com/mozilla/TTS/wiki/Mean-Opinion-Score-Results)\n\n## Features\n- High performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on console and Tensorboard.\n- Support for multi-speaker TTS.\n- Efficient Multi-GPUs training.\n- Ability to convert PyTorch models to Tensorflow 2.0 and TFLite for inference.\n- Released models in PyTorch, Tensorflow and TFLite.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Demo server for model testing.\n- Notebooks for extensive model benchmarking.\n- Modular (but not too much) code base enabling easy testing for new ideas.\n\n## Implemented Models\n### Text-to-Spectrogram\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1907.09006)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n\nYou can also help us implement more models. Some TTS related work can be found [here](https://github.com/erogol/TTS-papers).\n\n## Install TTS\nTTS supports **python >= 3.6, <3.9**.\n\nIf you are only interested in [synthesizing speech](https://github.com/mozilla/TTS/tree/dev#example-synthesizing-speech-on-terminal-using-the-released-models) with the released TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone TTS and install it locally.\n\n```bash\ngit clone https://github.com/mozilla/TTS\npip install -e .\n```\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- distribute.py              (train your TTS model using Multiple GPUs.)\n      |- compute_statistics.py      (compute dataset statistics for normalization.)\n      |- convert*.py                (convert target torch model to TF.)\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- tf/              (Tensorflow 2 utilities and model implementations)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n\n## Sample Model Output\nBelow you see Tacotron model state after 16K iterations with batch-size 32 with LJSpeech dataset.\n\n> \"Recent research at Harvard has shown meditating for as little as 8 weeks can actually increase the grey matter in the parts of the brain responsible for emotional regulation and learning.\"\n\nAudio examples: [soundcloud](https://soundcloud.com/user-565970875/pocket-article-wavernn-and-tacotron2)\n\n<img src=\"images/example_model_output.png?raw=true\" alt=\"example_output\" width=\"400\"/>\n\n## Datasets and Data-Loading\nTTS provides a generic dataloader easy to use for your custom dataset.\nYou just need to write a simple function to format the dataset. Check ```datasets/preprocess.py``` to see some examples.\nAfter that, you need to set ```dataset``` fields in ```config.json```.\n\nSome of the public datasets that we successfully applied TTS:\n\n- [LJ Speech](https://keithito.com/LJ-Speech-Dataset/)\n- [Nancy](http://www.cstr.ed.ac.uk/projects/blizzard/2011/lessac_blizzard2011/)\n- [TWEB](https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset)\n- [M-AI-Labs](http://www.caito.de/2019/01/the-m-ailabs-speech-dataset/)\n- [LibriTTS](https://openslr.org/60/)\n- [Spanish](https://drive.google.com/file/d/1Sm_zyBo67XHkiFhcRSQ4YaHPYM0slO_e/view?usp=sharing) - thx! @carlfm01\n\n## Example: Synthesizing Speech on Terminal Using the Released Models.\n\nAfter the installation, TTS provides a CLI interface for synthesizing speech using pre-trained models. You can either use your own model or the release models under the TTS project.\n\nListing released TTS models.\n```bash\ntts --list_models\n```\n\nRun a tts and a vocoder model from the released model list. (Simply copy and paste the full model names from the list as arguments for the command below.)\n```bash\ntts --text \"Text for TTS\" \\\n    --model_name \"<type>/<language>/<dataset>/<model_name>\" \\\n    --vocoder_name \"<type>/<language>/<dataset>/<model_name>\" \\\n    --out_path folder/to/save/output/\n```\n\nRun your own TTS model (Using Griffin-Lim Vocoder)\n```bash\ntts --text \"Text for TTS\" \\\n    --model_path path/to/model.pth.tar \\\n    --config_path path/to/config.json \\\n    --out_path output/path/speech.wav\n```\n\nRun your own TTS and Vocoder models\n```bash\ntts --text \"Text for TTS\" \\\n    --model_path path/to/config.json \\\n    --config_path path/to/model.pth.tar \\\n    --out_path output/path/speech.wav \\\n    --vocoder_path path/to/vocoder.pth.tar \\\n    --vocoder_config_path path/to/vocoder_config.json\n```\n\n**Note:** You can use ```./TTS/bin/synthesize.py``` if you prefer running ```tts``` from the TTS project folder.\n\n## Example: Training and Fine-tuning LJ-Speech Dataset\nHere you can find a [CoLab](https://gist.github.com/erogol/97516ad65b44dbddb8cd694953187c5b) notebook for a hands-on example, training LJSpeech. Or you can manually follow the guideline below.\n\nTo start with, split ```metadata.csv``` into train and validation subsets respectively ```metadata_train.csv``` and ```metadata_val.csv```. Note that for text-to-speech, validation performance might be misleading since the loss value does not directly measure the voice quality to the human ear and it also does not measure the attention module performance. Therefore, running the model with new sentences and listening to the results is the best way to go.\n\n```\nshuf metadata.csv > metadata_shuf.csv\nhead -n 12000 metadata_shuf.csv > metadata_train.csv\ntail -n 1100 metadata_shuf.csv > metadata_val.csv\n```\n\nTo train a new model, you need to define your own ```config.json``` to define model details, trainin configuration and more (check the examples). Then call the corressponding train script.\n\nFor instance, in order to train a tacotron or tacotron2 model on LJSpeech dataset, follow these steps.\n\n```bash\npython TTS/bin/train_tacotron.py --config_path TTS/tts/configs/config.json\n```\n\nTo fine-tune a model, use ```--restore_path```.\n\n```bash\npython TTS/bin/train_tacotron.py --config_path TTS/tts/configs/config.json --restore_path /path/to/your/model.pth.tar\n```\n\nTo continue an old training run, use ```--continue_path```.\n\n```bash\npython TTS/bin/train_tacotron.py --continue_path /path/to/your/run_folder/\n```\n\nFor multi-GPU training, call ```distribute.py```. It runs any provided train script in multi-GPU setting.\n\n```bash\nCUDA_VISIBLE_DEVICES=\"0,1,4\" python TTS/bin/distribute.py --script train_tacotron.py --config_path TTS/tts/configs/config.json\n```\n\nEach run creates a new output folder accomodating used ```config.json```, model checkpoints and tensorboard logs.\n\nIn case of any error or intercepted execution, if there is no checkpoint yet under the output folder, the whole folder is going to be removed.\n\nYou can also enjoy Tensorboard,  if you point Tensorboard argument```--logdir``` to the experiment folder.\n\n## Contribution Guidelines\nThis repository is governed by Mozilla's code of conduct and etiquette guidelines. For more details, please read the [Mozilla Community Participation Guidelines.](https://www.mozilla.org/about/governance/policies/participation/)\n\n1. Create a new branch.\n2. Implement your changes.\n3. (if applicable) Add [Google Style](https://google.github.io/styleguide/pyguide.html#381-docstrings) docstrings.\n4. (if applicable) Implement a test case under ```tests``` folder.\n5. (Optional but Prefered) Run tests. \n```bash\n./run_tests.sh\n```\n6. Run the linter.\n```bash\npip install pylint cardboardlint\ncardboardlinter --refspec master\n```\n7. Send a PR to ```dev``` branch, explain what the change is about.\n8. Let us discuss until we make it perfect :). \n9. We merge it to the ```dev``` branch once things look good. \n\nFeel free to ping us at any step you need help using our communication channels.\n\n## Collaborative Experimentation Guide\nIf you like to use TTS to try a new idea and like to share your experiments with the community, we urge you to use the following guideline for a better collaboration.\n(If you have an idea for better collaboration, let us know)\n- Create a new branch.\n- Open an issue pointing your branch.\n- Explain your idea and experiment.\n- Share your results regularly. (Tensorboard log files, audio results, visuals etc.)\n\n## Major TODOs\n- [x] Implement the model.\n- [x] Generate human-like speech on LJSpeech dataset.\n- [x] Generate human-like speech on a different dataset (Nancy) (TWEB).\n- [x] Train TTS with r=1 successfully.\n- [x] Enable process based distributed training. Similar to (https://github.com/fastai/imagenet-fast/).\n- [x] Adapting Neural Vocoder. TTS works with WaveRNN and ParallelWaveGAN (https://github.com/erogol/WaveRNN and https://github.com/erogol/ParallelWaveGAN)\n- [x] Multi-speaker embedding.\n- [x] Model optimization (model export, model pruning etc.)\n\n### Acknowledgement\n- https://github.com/keithito/tacotron (Dataset pre-processing)\n- https://github.com/r9y9/tacotron_pytorch (Initial Tacotron architecture)\n- https://github.com/kan-bayashi/ParallelWaveGAN (vocoder library)\n- https://github.com/jaywalnut310/glow-tts (Original Glow-TTS implementation)\n- https://github.com/fatchord/WaveRNN/ (Original WaveRNN implementation)\n"
},
{
  "name": "protocol-tokens",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "gulpfile.js",
      "package-lock.json",
      "package.json",
      "stylelint.config.js",
      "tokens"
    ],
    "/.github": [
      "PULL_REQUEST_TEMPLATE.md",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Protocol Tokens\n\nDesign tokens for Protocol, Mozilla\u2019s design system.\n\n<em>JavaScript \u00b7 JSON \u00b7 CSS \u00b7 SCSS</em>\n\n---\n\n## Information\n\n<table>\n<tr>\n<td>Package</td><td>@mozilla-protocol/tokens</td>\n</tr>\n<tr>\n<td>Description</td>\n<td>Design tokens for Protocol, Mozilla\u2019s design system</td>\n</tr>\n<tr>\n<td>Version</td>\n<td><a href=\"https://github.com/mozilla/protocol-tokens/blob/master/CHANGELOG.md\">5.0.5</a></td>\n</tr>\n</table>\n\n## Installation\n\nProtocol design tokens are available as an npm package (`@mozilla-protocol/tokens`) on [npm](https://www.npmjs.com/package/@mozilla-protocol/tokens).\n\nThe recommended way to use and install design tokens may vary depending on your project; the most common are documented below.\n\n### JavaScript package installation\n\nUsing [npm](https://www.npmjs.com/):\n\n```\nnpm install @mozilla-protocol/tokens --save\n```\n\nUsing [yarn](https://yarnpkg.com/en/):\n\n```\nyarn add @mozilla-protocol/tokens\n```\n\n### JavaScript\n\nIn JavaScript, design token names are formatted in [lower camelCase](http://wiki.c2.com/?CamelCase).\n\n```js\nconst tokens = require('@mozilla-protocol/tokens/dist/index');\nconsole.log(tokens.colorBlueLighter); // rgb(0, 0, 0)\n```\n\nIn JSON, design token names are formatted in [kebab-case](http://wiki.c2.com/?KebabCase).\n\n```js\nconst tokens = require('@mozilla-protocol/tokens/dist/index.json');\nconsole.log(tokens['color-black']); // rgb(0, 0, 0)\n```\n\n### Sass\n\nSass variables and map keys are formatted in [kebab-case](http://wiki.c2.com/?KebabCase).\n\n```scss\n// Using variables\n@import '~@mozilla-protocol/tokens/dist/index';\n\na {\n  color: $color-black;\n}\n```\n\n### Sass, with CSS Custom Properties\n\nCustom properties are formatted in [kebab-case](http://wiki.c2.com/?KebabCase).\n\n```scss\n// Omit .css at the end of the file\n@import '~@mozilla-protocol/tokens/dist/colors/colors.custom-properties';\n\na {\n  color: var(--color-black);\n}\n```\n\n## Publishing\n\nTo publish to the npmjs registry you'll need access to the mozilla-protocol org on npmjs.com.\nFirst run `gulp` to compile the package locally. You can check your local `dist`\nfolder to verify it has the up-to-date tokens. Then run `npm publish`.\n\n\n## Contributing\n\n### [Code of conduct](https://www.mozilla.org/en-US/about/governance/policies/participation/)\n\nWe have a [code of conduct](https://www.mozilla.org/en-US/about/governance/policies/participation/),\nplease follow it in all your interactions with the project.\n\n### [Contributing guide](https://github.com/mozilla/protocol-tokens/blob/master/CONTRIBUTING.md)\n\nRead the [contributing guide](https://github.com/mozilla/protocol-tokens/blob/master/CONTRIBUTING.md)\nto learn how to propose changes and understand our development process.\n\n### [License](https://github.com/mozilla/protocol-tokens/blob/master/LICENSE.md)\n\nThe protocol-tokens project is available under the [MPL-2.0](https://github.com/mozilla/protocol-tokens/blob/master/LICENSE.md).\n"
},
{
  "name": "fivetran-connectors",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gcloudignore",
      ".gitignore",
      ".isort.cfg",
      "LICENSE",
      "Makefile",
      "README.md",
      "connectors",
      "deploy.yaml",
      "docs",
      "fivetran",
      "requirements.in",
      "requirements.txt",
      "setup.py",
      "tools"
    ],
    "/docs": [
      "gcloud-function.png"
    ],
    "/.circleci": [
      "config.template.yml",
      "config.yml"
    ]
  },
  "makefile": "# Resources used when creating this Makefile:\n# - https://opensource.com/article/18/8/what-how-makefile\n# - https://swcarpentry.github.io/make-novice/08-self-doc/index.html\n# - https://stackoverflow.com/questions/20763629/test-whether-a-directory-exists-inside-a-makefile\n\n\n.DEFAULT_GOAL := help\n\nGREEN=\\033[0;32m\nRED=\\033[0;31m\nRESET=\\033[0m  # No Color\n\n\nPROJECT_ID=dev-fivetran\nREGION=us-central1\n\nVIRTUAL_ENV_FOLDER=venv\n\n\n##   Commands for interacting with GCP Cloud Functions service:\n##         auth                : For authenticating into gcloud service (browser prompt)\nauth:\n\t@echo \"$(GREEN)Authenticating into gcloud service...$(RESET)\"\n\tgcloud auth login\n\n##         list                : Lists all Cloud Functions found inside the GCP project,\n##                                 add PROJECT_ID=<gcp-project> to the end of the command to use other than default.\nlist:\n\tgcloud functions list --project=$(PROJECT_ID)\n\n\n##         deploy              : Deploys specified connector to target GCP project as a gcloud function\n## ------------------------------------------------------------------------------------------------------\nMEMORY ?= 256MB\nTIMEOUT ?= 60\nENTRYPOINT ?= main\ndeploy:\nifeq ($(CONNECTOR_NAME),)\n\t@echo \"$(RED)No connector_name was provided, please re-run the command and add CONNECTOR_NAME=<connector_name> at the end of the command$(RESET)\"\n\t@echo \"Example: 'make deploy CONNECTOR_NAME=my_connector'\"\n\texit 1\nendif\n\n# Values used for function deploy as specified by the official Fivetran cloud function guide:\n# https://fivetran.com/docs/functions/google-cloud-functions/setup-guide\n\t@echo \"$(GREEN)Deploying connector $(CONNECTOR_NAME) as a gcloud function (updates if already exists)$(RESET)\"\n\n# gcloud functions deploy reference: https://cloud.google.com/sdk/gcloud/reference/functions/deploy\n\tgcloud functions deploy $(CONNECTOR_NAME) \\\n\t\t--source=connectors/$(CONNECTOR_NAME) \\\n\t\t--ignore-file=`pwd`/.gcloudignore \\\n\t\t--project=$(PROJECT_ID) \\\n\t\t--region=$(REGION) \\\n\t\t--entry-point=$(ENTRYPOINT) \\\n\t\t--runtime=python38 \\\n\t\t--memory=$(MEMORY) \\\n\t\t--timeout=$(TIMEOUT)s \\\n\t\t--ingress-settings=all \\\n\t\t--security-level=secure-optional \\\n\t\t--trigger-http \\\n\t\t--service-account=gcloud-function-executor@dev-fivetran.iam.gserviceaccount.com\n\n\n##   Commands for environment management:\n##         setup               : Checks virtual env already exists and creates one (run @create-python-env command)\n.PHONY: setup\nsetup:\nifneq ($(wildcard $(VIRTUAL_ENV_FOLDER)/.),)\n\t@echo \"'$(VIRTUAL_ENV_FOLDER)/' directory already exists. Please delete it with 'make clean' and re-run this command.\"\nelse\n\t@make create-python-env\nendif\n\n\n##         create-python-env   : Creates virtual environment and installs requirements\ncreate-python-env:\n\t@echo \"Creating a new virtual environment in folder '$(VIRTUAL_ENV_FOLDER)/'\"\n\t@python3 -m venv $(VIRTUAL_ENV_FOLDER)\n\t@export SYSTEM_VERSION_COMPAT=1\n\n\t@venv/bin/pip install -r requirements.txt\n\t@venv/bin/pip install -e .\n\t@echo \"$(RED)fivetran CLI configured! It should now be ready for use.$(RESET)\"\n\n\n##         clean               : Removes virtual environment folder along with all installed packages\n.PHONY: clean\nclean:\n\t@rm -rf $(VIRTUAL_ENV_FOLDER)\n\n\n##         help                : Prints a list of all available commands\n.PHONY: help\nhelp: Makefile\n\t@echo\n\t@echo \"Commands for assisting in custom Fivetran connector development:\"\n\t@echo\n\t@sed -n 's/^##//p' $<\n\t@echo\n",
  "readme": "# fivetran-connectors\n\nCustom connectors for [Fivetran](https://fivetran.com/) implemented as Google Cloud Functions.\n\n## Development\n\nThe tools in this repository are intended for Python 3.8+.\n\nTo install the CLI:\n\n```\n./fivetran bootstrap\n```\n\n### Creating a New Connector\n\nTo add a new connector run:\n\n```\n./fivetran connector create <name_of_connector>\n```\n\n`<name_of_connector>` is the name of the new connector for which a new directory will be created\nin the `connectors/` directory. The new connector directory will be automatically populated with\nboilerplate code.\n\n### Deploying Connectors\n\nTo deploy a connector as a Google Cloud Function, the connector needs to be added to the `deploy.yaml` file:\n\n```yaml\n<connector-name>:     # name of the new Google Cloud Function (must be unique)\n  connector: <connector-type>   # name of the connector as specified in connectors/\n  environment: dev         # determines the GCP project the connector is deployed to\n```\n\nThe connector will be deployed after the CircleCI configuration has been updated and pushed to `main`.\n\nDuring development connectors can also be deployed quickly via:\n\n```\ncd connectors/new_connector\ngcloud functions deploy new_connector --entry-point main --runtime python38 --trigger-http --timeout=540 --memory=4096MB\n```\nThis does not require the code to be merged into `main`.\n\n### Updating the CircleCI Config\n\nTo Update the CircleCI `config.yml` and add new connectors to the CI workflow run:\n\n```\n./fivetran ci_config update\n```\n\n## Custom Connector Development\n\nThis is a collection of things to keep in mind/useful information when writing code for custom connectors.\n\nBy default the `main()` method in the `main.py` file for the connector will be they entry point for the Cloud Function.\n\n### Passing Data into Connectors\n\nTo pass configuration data, such as API keys, into a custom connector a JSON object can be specified in the Fivetran settings. The JSON object can be set as the \"Secret\". The connector can access the JSON data via the `request` object:\n\n```python\ndef main(request):\n    config = request.json['secrets']\n```\n\n### Connector Response Format\n\nThe expected response format for connectors is: \n\n```json\n{\n    \"state\": {\n        \"since_id\": \"2018-01-02T00:00:01Z\",\n        /* other data */\n    },\n    \"insert\": {\n        \"table_name\": [\n            {\"id\":1, \"value\": 100},\n            /* rows that will get added to \"table_name\" table in BigQuery */\n        ],\n        /* ... */\n    },\n    \"delete\": {},\n    \"schema\" : {\n        \"table_name\": {\n            \"primary_key\": [\"id\"]\n        },\n        /* .... */\n    },\n    \"has_more\" : true /* if there is more data available that can be imported; or false */\n}\n```\nMore about the response format can be found [here](https://fivetran.com/docs/functions/google-cloud-functions#responseformat)\n\n### Incremental Data Updates\n\nTo keep track of what data has already been imported in previous runs, Fivetran passes a `since_id` value as part of the `state` object. `since_id` needs to be updated by the connector and can be set, for example, to the date of the last data entry imported.\n\n```python\ndef main(request):\n    # [...]\n\n    since_id = request.json[\"state\"][\"since_id\"]\n\n    # make API request to get data added after since_id\n    data = api.get_data(last_modified=since_id)\n\n    # [...]\n    # update since_id\n    return {\n        \"state\": {\n            \"since_id\": max_date(data)\n            # [...]\n        },\n        # [...]\n    }\n```\n\n### Follow-up Calls to Fetch more Data\n\nGoogle Cloud Functions have a couple of limitations. For instance, the maximum runtime is around 10 minutes and available memory is limited. In some cases importing all data in one run might not be possible.\n\n`has_more` can be used to indicate to Fivetran to trigger the connector again to import more available data. To keep track of what data has already been imported in previous runs, `state` has an `offset` value. `offset` can, for example, be set to the date of the last data record imported.\n\n`has_more` and `offset` need to be updated every time the connector is triggered.\n\n```python\ndef main(request):\n    # [...]\n    offset = 1\n    if \"offset\" in request.json[\"state\"]:\n        offset = request.json[\"state\"][\"offset\"]\n\n    # fetch more data\n\n    # [...]\n\n    # udpate offset and has_more\n    return {\n        \"state\": {\n            \"offset\": offset + 1\n        },\n        \"has_more\": has_more_data(data),\n        # [...]\n    }\n```\n\n## Connector Deployment\n\nOnce a new connector has been added and committed to `main` CircleCI will automatically deploy it\nas a Google Cloud Function to the `dev-fivetran` project.\n\nWhen setting up a new custom connector in Fivetran for the first time, copy the trigger URL of the\ndeployed function:\n\n![Cloud Function Trigger URL](https://github.com/mozilla/fivetran-connectors/blob/main/docs/gcloud-function.png)\n\nIn Fivetran, add a new \"Google Cloud Function\" connector, set the \"Destination schema\" as the\ncreate BigQuery destination dataset. Paste the trigger URL in the \"Function Trigger\" field.\n\nThe \"Secrets\" field is an optional JSON object that the Cloud Function connector will have access to.\nThis JSON object can, for example, contain API access keys or other configuration values. Once the\nnew connector has been configured, hit \"Save and Test\". The connector will get triggered to run an\ninitial data import.\n\nGenerally, data imports should be scheduled via [telemetry-airflow](https://github.com/mozilla/telemetry-airflow) instead of Fivetrans built-in scheduling. Since Fivetran usually dumps data into a destination that is not accessible, it is also necessary to set up some ETL to transform the data and write it to an accessible destination. Both the ETL and the scheduling can be specified in [bigquery-etl](https://github.com/mozilla/bigquery-etl).\n\nWhen writing the query to create derived datasets in bigquery-etl add [the Fivetran import tasks to the scheduling config](https://github.com/mozilla/bigquery-etl/blob/b1a1f5a484ac8ab77c841d1c666bc02e3ccf9ee2/docs/reference/scheduling.md?plain=1#L57). Once changes are merged into main the DAG in Airflow will get updated automatically. Looking at the generated DAGs, each Fivetran task references a **Fivetran connector ID** that needs to be configured as an Airflow variable.\n\nTo configure this variable, in the **Airflow Admin - Variables** settings add a new entry. The **Key** needs to be set to the variable name as shown in the DAG source, the **Value** is the Fivetran connector ID which can be copied from the _Connection Details_ of the Fivetran connector _Setup_ tab.\n\nOnce configured, the Airflow DAG needs to be enabled.\n\n## How to Debug Issues\n\n1. Check Airflow errors to see which connector is affected.\n    * The Airflow logs will not offer a lot of details on what went wrong. Instead they will link to the logs in Fivetran which are a little more helpful.\n2. Check Fivetran logs of the connector.\n    * Go to [Fivetran dashboard](https://fivetran.com/dashboard/connectors)\n    * Check the connector logs. `reason` might provide some indication on what went wrong, whether it was a Fivetran specific issue or whether the connector encountered a problem.\n3. Check the connector logs.\n    * [Go to the deployed Google Cloud Function](https://console.cloud.google.com/functions/list?env=gen1&project=dev-fivetran) an check the logs for stack traces to determine what went wrong\n5. Fix connector\n    * If the connector needs to be fixed, deploy a new version (happens automatically when merged on `main`)\n6. Reconnect connector in Fivetran\n    * Connectors need to be manually reconnected in Fivetran\n    * Navigate to the _Setup_ tab of the connector and click on _Test Connection_. Wait for the tests to finish.\n"
},
{
  "name": "cubeb-coreaudio-rs",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".githooks",
      ".github",
      ".gitignore",
      ".travis.yml",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "build-audiounit-rust-in-cubeb.sh",
      "coreaudio-sys-utils",
      "install_git_hook.sh",
      "install_rustfmt_clippy.sh",
      "run_device_tests.sh",
      "run_sanitizers.sh",
      "run_tests.sh",
      "src",
      "todo.md"
    ],
    "/.github": [
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# cubeb-coreaudio-rs\n\n[![CircleCI](https://circleci.com/gh/mozilla/cubeb-coreaudio-rs.svg?style=svg)](https://circleci.com/gh/mozilla/cubeb-coreaudio-rs)\n[![Build & Test](https://github.com/mozilla/cubeb-coreaudio-rs/actions/workflows/test.yml/badge.svg)](https://github.com/mozilla/cubeb-coreaudio-rs/actions/workflows/test.yml)\n\n*Rust* implementation of [Cubeb][cubeb] on [the MacOS platform][cubeb-au].\n\n## Current Goals\n\n- Keep refactoring the implementation until it looks rusty! (it's translated from C at first.)\n  - Check the [todo list][todo]\n\n## Status\n\nThis is now the _Firefox_'s default audio backend on *Mac OS*.\n\n## Install\n\n### Install cubeb-coreaudio within cubeb\n\nRun the following command:\n```sh\ncurl https://raw.githubusercontent.com/mozilla/cubeb-coreaudio-rs/trailblazer/build-audiounit-rust-in-cubeb.sh | sh\n```\n\n### Other\n\nJust clone this repo\n\n## Test\n\nPlease run `sh run_tests.sh`.\n\nSome tests cannot be run in parallel.\nThey may operate the same device at the same time,\nor indirectly fire some system events that are listened by some tests.\n\nThe tests that may affect others are marked `#[ignore]`.\nThey will be run by `cargo test ... -- --ignored ...`\nafter finishing normal tests.\nMost of the tests are executed in `run_tests.sh`.\nOnly those tests commented with *FIXME* are left.\n\n### Git Hooks\n\nYou can install _git hooks_ by running `install_git_hook.sh`.\nThen _pre-push_ script will be run and do the `cargo fmt` and `cargo clippy` check\nbefore the commits are pushed to remote.\n\n### Run Sanitizers\n\nRun _AddressSanitizer (ASan), LeakSanitizer (LSan), MemorySanitizer (MSan), ThreadSanitizer (TSan)_\nby `sh run_sanitizers.sh`.\n\nThe above command will run all the test suits in *run_tests.sh* by all the available _sanitizers_.\nHowever, it takes a long time for finshing the tests.\n\n### Device Tests\n\nRun `run_device_tests.sh`.\n\nIf you'd like to run all the device tests with sanitizers,\nuse `RUSTFLAGS=\"-Z sanitizer=<SAN>\" sh run_device_tests.sh`\nwith valid `<SAN>` such as `address` or `thread`.\n\n#### Device Switching\n\nThe system default device will be changed during our tests.\nAll the available devices will take turns being the system default device.\nHowever, after finishing the tests, the default device will be set to the original one.\nThe sounds in the tests should be able to continue whatever the system default device is.\n\n#### Device Plugging/Unplugging\n\nWe implement APIs simulating plugging or unplugging a device\nby adding or removing an aggregate device programmatically.\nIt's used to verify our callbacks for minitoring the system devices work.\n\n### Manual Test\n\n- Output devices switching\n  - `$ cargo test test_switch_output_device -- --ignored --nocapture`\n  - Enter `s` to switch output devices\n  - Enter `q` to finish test\n- Device collection change\n  - `cargo test test_device_collection_change -- --ignored --nocapture`\n  - Plug/Unplug devices to see events log.\n- Manual Stream Tester\n  - `cargo test test_stream_tester -- --ignored --nocapture`\n    - `c` to create a stream\n    - `d` to destroy a stream\n    - `s` to start the created stream\n    - `t` to stop the created stream\n    - `r` to register a device changed callback to the created stream\n    - `v` to set volume to the created stream\n    - `q` to quit the test\n  - It's useful to simulate the stream bahavior to reproduce the bug we found,\n    with some modified code.\n\n## TODO\n\nSee [todo list][todo]\n\n## Issues\n\n- Atomic:\n  - We need atomic type around `f32` but there is no this type in the stardard Rust\n  - Using [atomic-rs](https://github.com/Amanieu/atomic-rs) to do this.\n- `kAudioDevicePropertyBufferFrameSize` cannot be set when another stream using the same device with smaller buffer size is active. See [here][chg-buf-sz] for details.\n\n### Test issues\n\n- Fail to run tests that depend on `AggregateDevice::create_blank_device` with the tests that work with the device event listeners\n  - The `AggregateDevice::create_blank_device` will add an aggregate device to the system and fire the device-change events indirectly.\n- `TestDeviceSwitcher` cannot work when there is an alive full-duplex stream\n  - An aggregate device will be created for a duplex stream when its input and output devices are different.\n  - `TestDeviceSwitcher` will cached the available devices, upon it's created, as the candidates for default device\n  - Hence the created aggregate device may be cached in `TestDeviceSwitcher`\n  - If the aggregate device is destroyed (when the destroying the duplex stream created it) but the `TestDeviceSwitcher` is still working,\n    it will set a destroyed device as the default device\n  - See details in [device_change.rs](src/backend/tests/device_change.rs)\n\n## Branches\n\n- [trailblazer][trailblazer]: Main branch\n- [plain-translation-from-c][from-c]: The code is rewritten from C code on a line-by-line basis\n- [ocs-disposal][ocs-disposal]: The first version that replace our custom mutex by Rust Mutex\n\n[cubeb]: https://github.com/mozilla/cubeb \"Cross platform audio library\"\n[cubeb-au]: https://github.com/mozilla/cubeb/blob/master/src/cubeb_audiounit.cpp \"Cubeb AudioUnit\"\n\n[chg-buf-sz]: https://cs.chromium.org/chromium/src/media/audio/mac/audio_manager_mac.cc?l=982-989&rcl=0207eefb445f9855c2ed46280cb835b6f08bdb30 \"issue on changing buffer size\"\n\n[todo]: todo.md\n\n[bmo1572273]: https://bugzilla.mozilla.org/show_bug.cgi?id=1572273\n[bmo1572273-c13]: https://bugzilla.mozilla.org/show_bug.cgi?id=1572273#c13\n\n[from-c]: https://github.com/mozilla/cubeb-coreaudio-rs/tree/plain-translation-from-c\n[ocs-disposal]: https://github.com/mozilla/cubeb-coreaudio-rs/tree/ocs-disposal\n[trailblazer]: https://github.com/mozilla/cubeb-coreaudio-rs/tree/trailblazer"
},
{
  "name": "tracking-protection-issues-exporter",
  "files": {
    "/": [
      ".eslintrc.cjs",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "docker-compose.yml",
      "package-lock.json",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Tracking Protection Issues Exporter\n\nNodeJS script to export GitHub issues generated from Firefox's ETP breakage\nreporting UI. Issues are created by the [tracking-protection-issues\nmiddleware](https://github.com/mozilla/tracking-protection-issues). This script\nfetches  issues (+comments) via the GitHub API, parses them into individual\nreports and stores them in a MongoDB database.\n\nDue to GitHub's API limits downloading a large amount of issues and comments can\ntake a long time. The script will gracefully handle API failures. Failed\nrequests will be retried. When hitting the rate limit the script will wait for\nthe next (hourly) reset before continuing.\n\n## Setup\nTested with NodeJS v12\n\nInstall dependencies via npm:\n```\nnpm install\n```\n\nThe script needs access to a MongoDB server to use as the storage backend. You\ncan use [Docker Compose](https://docs.docker.com/compose/) with the\n`docker-compose.yml` config to start a local development server:\n```\ndocker-compose up mongo\n```\n\n## Usage\n`cli.js` implements a command line interface to download and convert issues:\n\n```\nnode src/cli.js [command]\n\nCommands:\n  fetch    Fetch ETP issues from GitHub and import them into\n                               a database.\n  convert  Convert ETP GitHub issues into reports.\n\nOptions:\n  --help                     Show help                                 [boolean]\n  --version                  Show version number                       [boolean]\n  --dbHost                   MongoDB database host\n                                           [string] [default: \"localhost:27017\"]\n  --dbUser                   MongoDB username                [string] [required]\n  --dbPassword               MongoDB password                [string] [required]\n  --dbDatabaseName           MongoDB database name\n                                         [string] [default: \"etp-issues-export\"]\n  --dbCollectionNameIssues   Name of the db collection to store issues in.\n                                                    [string] [default: \"issues\"]\n  --dbCollectionNameReports  Name of the db collection to store reports in.\n                                                   [string] [default: \"reports\"]\n```\nPass `--help` to the sub-commands to see all options. For example `node src/cli.js fetch --help`.\n\n### Pass options via environment\nOptions can also be defined via environment variables, or an `.env` file in the repositories root directory:\n```\nEXPORT_GH_AUTH_TOKEN=<token>\nEXPORT_GH_REPO_USER=<user>\nEXPORT_GH_REPO_NAME=<repoName>\nEXPORT_GH_API_MAX_RETRY=50\nEXPORT_DB_HOST=localhost:27017\nEXPORT_DB_USER=root\nEXPORT_DB_PASSWORD=123456\nEXPORT_DB_DATABASE_NAME=etp-reports\nEXPORT_DB_COLLECTION_NAME_ISSUES=issues\nEXPORT_DB_COLLECTION_NAME_REPORTS=reports\n```"
},
{
  "name": "libdmg-hfsplus",
  "files": {
    "/": [
      ".gitignore",
      "CMakeLists.txt",
      "LICENSE",
      "README.markdown",
      "common",
      "dmg",
      "hdutil",
      "hfs",
      "includes"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "webrtc-sdp",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "examples",
      "fuzz",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# webrtc-sdp\n\n[![Crates.io](https://img.shields.io/crates/v/webrtc-sdp.svg)](https://crates.io/crates/webrtc-sdp)\n[![Build Status](https://travis-ci.org/mozilla/webrtc-sdp.svg?branch=master)](https://travis-ci.org/mozilla/webrtc-sdp)\n[![Codecov coverage status](https://codecov.io/gh/mozilla/webrtc-sdp/branch/master/graph/badge.svg)](https://codecov.io/gh/webrtc-sdp/webrtc-sdp)\n[![License: MPL 2.0](https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg)](#License)\n[![dependency status](https://deps.rs/repo/github/mozilla/webrtc-sdp/status.svg)](https://deps.rs/repo/github/mozilla/webrtc-sdp)\n\nA SDP parser written in Rust specifically aimed to handle WebRTC SDP offers and answers.\n\n## Dependecies\n\n* Rust >= 1.45.0\n* log module\n* serde module\n* serde-derive module\n\nCargo installs the missing modules automatically when building webrtc-sdp for the first time.\n\n## The webrtc-sdp API\n\nThe main function is:\n```rust\nfn parse_sdp(sdp: &str, fail_on_warning: bool) -> Result<SdpSession, SdpParserError>\n```\nThe `sdp` parameter is the string which will get parsed. The `fail_on_warning` parameter determines how to treat warnings encountered during parsing. Any problems encountered during are stored until the whole string has been parsed. Any problem during parsing falls into two catgeories:\n\n* Fatal error preventing further parsing or processing of the SDP\n* Warning which don't block further processing of the SDP\n\nWarnings will be for example unknown parameters in attributes. Setting `fail_on_warning` to `true` makes most sense during development, when you want to be aware of all potential problems. In production `fail_on_warning` is expected to be `false`.\n\n`parse_sdp()` returns either an `SdpSession` struct ([code](https://github.com/mozilla/webrtc-sdp/blob/master/src/lib.rs#L137)) which contains all the parsed information. Or in case a fatal error was encountered (or if `fail_on_warning` was set to `true` and any warnings were encountered) an `SdpParserError` ([code](https://github.com/mozilla/webrtc-sdp/blob/master/src/error.rs#L117)) will be returned as a `Result`.\n\n## Examples\n\nThe [file parser](https://github.com/mozilla/webrtc-sdp/blob/master/examples/file_parser.rs) in the webrtc-sdp package gives you an easy example of how to invoke the webrtc-sdp parser.\n\n## Contributing\n\nAs the Travis CI runs are checking for code formating and clippy warnings please run the following commands locally, before submitting a Pull Request.\n\nIf you haven't clippy and Rust format installed already you add them like this:\n```\nrustup component add rustfmt-preview\nrustup component add clippy\n```\n\nCheck with clippy for warnings in the code:\n```\ncargo clippy\n```\n\nAnd format all of the code according to Rust code style convention:\n```\ncargo fmt --all\n```\n\n## Fuzzing\n\nInstall cargo-fuzz like this:\n```\ncargo install cargo-fuzz\n```\n\nWith rust nightly you can start fuzzing like this:\n```\ncargo fuzz run fuzz_target_parse_sdp\n```\n\n## License\n\nLicensed under [MPL-2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "gpg.mozilla.org",
  "files": {
    "/": [
      ".travis.yml",
      "CNAME",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "docker-compose.yml",
      "docs",
      "nginx",
      "sks-cron",
      "sks-db",
      "sks-recon"
    ],
    "/docs": [
      "CNAME",
      "index.html",
      "static"
    ]
  },
  "makefile": "DUMP_URL := \"http://keys.niif.hu/keydump/\"\nVOLUME := \"gpgmozillaorg_sks\"\n\nall: build\n\nrun: docker-compose.yml cleanup\n\tdocker-compose -f docker-compose.yml up\n\nkill: docker-compose.yml\n\tdocker-compose -f docker-compose.yml down\n\ncleanup:\n\t@echo Removing old unix sockets...\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 gpgmozillaorg_sks-db \\\n\t    rm -f /var/sks/recon_com_sock && rm -f /var/sks/db_com_sock\n\nbuild: */Dockerfile\n\tdocker-compose -f docker-compose.yml build\n\nrebuild:\n\t@echo Rebuilding/updating images\n\tdocker-compose -f docker-compose.yml build --pull --no-cache\n\ninstall:\n\t@echo ensuring docker volume is present\n\tdocker volume create sks\n\t@echo Setting permissions\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 gpgmozillaorg_sks-db chown -R sks:sks /var/sks\n\n\t@echo \"Getting dump from $(DUMP_URL) (see sources at https://bitbucket.org/skskeyserver/sks-keyserver/wiki/KeydumpSources)\"\n\t@echo This will take forever. How much coffee can you drink before it\\'s done? tic-tac-tic-tac...\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 -w /var/sks/dump gpgmozillaorg_sks-db \\\n\t\twget -crp -e robots=off -l1 --no-parent --cut-dirs=3 -nH -A pgp,bz2,gz,xz,txt $(DUMP_URL)\n\n\t@echo Decompressing files...\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 -w /var/sks/dump gpgmozillaorg_sks-db \\\n\t        bzip2 -d \\*bz2\n\n\t@echo Creating KDB...\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 -w /var/sks/ gpgmozillaorg_sks-db \\\n                sks build\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 -w /var/sks/ gpgmozillaorg_sks-db \\\n                sh -c 'sks merge dump/*pgp'\n\n\t@echo Creating PTree...\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 -w /var/sks/ gpgmozillaorg_sks-db \\\n\t    \tsks pbuild\n\t@echo Fixing permissions for $(VOLUME)\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -u 0 -w /var/sks/ gpgmozillaorg_sks-db \\\n                chown -R sks:sks /var/sks\n\nupdate-web:\n\tdocker run -ti --mount source=$(VOLUME),target=/var/sks -w /var/sks/ --name gpgmozillaorg_tmp gpgmozillaorg_sks-db true\n\tdocker cp sks-db/etc/web/. gpgmozillaorg_tmp:/var/sks/web/\n\tdocker rm gpgmozillaorg_tmp\n\n.PHONY: all\n",
  "readme": "# gpg.mozilla.org SKS Keyserver\n\nThis was the setup for the SKS keyserver that used to run at https://gpg.mozilla.org.\nIt ran the [SKS Keyserver](https://github.com/SKS-Keyserver/sks-keyserver/wiki) software.\n\n# Status : gpg.mozilla.org service has ended\n\ngpg.mozilla.org was an OpenPGP Synchronizing Key Server (SKS) which \nparticipated in the global mesh of SKS servers that enabled OpenPGP users to \nretrieve and publish public keys.\n\nIn June of 2019 attackers showed the ease of a certificate spamming attack that\ncan poison clients' OpenPGP installations when the affected certificates are\nfetched.\n\nRobert J. Hansen, a member of the GnuPG project, was one of the people who's\ncertificate was affected. He has \n[a good writeup on the incident](https://gist.github.com/rjhansen/67ab921ffb4084c865b3618d6955275f)\nand the impact to the use of SKS keyservers like gpg.mozilla.org.\n\nThe vulnerability is recorded in [CVD-2019-13050](https://nvd.nist.gov/vuln/detail/CVE-2019-13050).\n\nAs a result of this type of attack being see in the wild, the problems Robert\nHansen identifies in his post about mitigating this vulnerability in SKS servers\nand unrelated operational challenges Mozilla has encountered in operating the\ngpg.mozilla.org SKS server, we've decided to stop hosting the gpg.mozilla.org\nSKS service as of September 2020.\n\nFor users that have configured their OpenPGP client to use gpg.mozilla.org, we\nrecommend you either stop using the keyserver based features of OpenPGP entirely\nby removing the `keyserver` directive in your gpg.conf configuration or you\nconfigure your client to use the `hkps://keys.openpgp.org` keyserver instead which\nis not part of the keyserver network. \n\n# How SKS works from an operational stand-point\n\n## Diagram\n\n```\n                          Ports:11371,11372,80,443\n    +--------------+         +----------------+            +----------------+                  +--------------------------+\n    |              |         |                |            |                |    Port 11370    |                          |\n    |   GnuPG CLI  +--------->                |            |   sks recon    <------------------>   (sks recon)            |\n    |              |         |  HTTP(S)       |            |                |                  |                          |\n    +--------------+         |  Load Balancer |            +----------------+                  |    3rd party             |\n                             |  (Nginx)       |                                                |                          |\n    +--------------+         |                |            +----------------+                  |    SKS Server            |\n    |              |         |                |            |                |                  |                          |\n    |  Web Browser +--------->                +------------>   sks db       |                  |                          |\n    |              |         |                | Port 11371 |                |                  |                          |\n    +--------------+         +----------------+            +----------------+                  +--------------------------+\n```\n\n## Ports\n\n- TCP 11370: Used by `sks recon` which is the key synchronization server. \"Recon\" stands for \"Reconciliation\". It uses\n  the SKS protocol and does not understand HTTP (i.e. cannot be served by an HTTP load balancer).\n- TCP 11371: Used by `sks db` and serves both a web interface for web browsers and for programs such as `gnupg`. It uses\n  the HTTP protocol (HKP port).\n- TCP 80: Not used by SKS, but generally used by a reverse proxy which hits back SKS's port 11371 (HTTP port).\n- TCP 443, 11372: Not used by SKS, but generally used by a reverse proxy which hits back SKS's port 11371 and performs TLS\n  termination (HTTPS, HKPS ports).\n\n## Addresses\n\nSince port TCP 11370 is not HTTP, it can be difficult to load-balance, and to have on a co-existing domain-name with the\nother ports, depending on your hosting provider.\n\nIn our case:\n\n- TCP 11370 listens on keyserver.mozilla.org\n- TCP 11371, 11372, 80, 443 listen on gpg.mozilla.org (HTTP load balancer)\n\n# Setup\n\n## Volume (persistent data)\n\nDue to how SKS works, the whole `/var/sks` directory is mounted as a persistent volume.\nThis includes the configuration, key database, web site, logs, etc.\n\n## Docker files\n\n### sks-db\n\n`sks-db` is the database server for SKS. It also listens on TCP port `11371` and exposes a web-server on that port\n(serving [sks-db/etc/web](sks-db/etc/web).\n\n### sks-recon\n`sks-recon` is the recon(aissance) server for SKS, which sync with other SKS peers. It listens on TCP port `11370` for\nother recon servers to contact and transfer data with.\n\n### sks-cron\n`sks-cron` simply run cron style tasks for SKS such as log rotation and database vacuuming.\n\n## Web server\n\nNote that there is currently no \"real\" web-server included with this repository at this time.\nServing directly from `sks-db` is not recommended and you should front it with your choice of load-balancer (nginx,\napache, AWS ELB, you name it)\n\n\n## Building\n\nType `make build`.\n\nTo build individual docker files, simply go to the right directory and type `make`.\n\n## First time use\n\nFor the first time use you'll probably want to establish a database with all (or most) PGP known-keys. This database is\nseveral Gb in size and you'll want to import for a database dump. You can find sources on the [SKS\nwebsite](https://bitbucket.org/skskeyserver/sks-keyserver/wiki/KeydumpSources)\n\nConvenience functions are also provided here: `make install` will automatically grab, decompress the dump and import it\nfor you. You can change the default sources as such: `DUMP_URL=https://example.net/dump make install`\n\nNote that all configuration, database and website go to the `sks` volume and thus, if you need to start from scratch,\nyou'll want to delete that volume.\n\nFinally, make sure you configure as per the [SKS Documentation](https://bitbucket.org/skskeyserver/sks-keyserver/wiki/)\nand change [sks-db/etc/](sks-db/etc/) files such as `sksconf`, `membership`, etc. to your liking.\n\n## Website update\n\nA convenience function is provided to update the website on the volume: \n- edit [sks-db/etc/web/](sks-db/etc/web/)\n- type `make update-web`\n\n## Containers update\n\nType `make rebuild`.\n\n## Run everything in docker-compose\n\nType `make`.\n"
},
{
  "name": "filter-cascade",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".pre-commit-config.yaml",
      "LICENSE",
      "README.md",
      "filtercascade",
      "pytest.ini",
      "requirements.txt",
      "setup.py"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# filter-cascade\nA python filter cascade implementation\n"
},
{
  "name": "l10nregistry-rs",
  "files": {
    "/": [
      ".gitignore",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "benches",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "Please note that development of l10nregistry has moved to\n[mozilla-central](https://hg.mozilla.org/mozilla-central/file/tip/intl/l10n/rust/l10nregistry-rs).\n\nYou can [file a bug](https://bugzilla.mozilla.org/enter_bug.cgi?product=Core&component=Internationalization)\nor [see open bugs](https://bugzilla.mozilla.org/buglist.cgi?product=Core&component=Internationalization&bug_status=__open__)\n"
},
{
  "name": "geckodriver",
  "files": {
    "/": [
      ".github",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "README.md"
    ],
    "/.github": [
      "lock.yml"
    ]
  },
  "makefile": null,
  "readme": "geckodriver\n===========\n\nProxy for using W3C [WebDriver] compatible clients to interact with\nGecko-based browsers.\n\nThis program provides the HTTP API described by the [WebDriver\nprotocol] to communicate with Gecko browsers, such as Firefox.  It\ntranslates calls into the [Marionette remote protocol] by acting\nas a proxy between the local- and remote ends.\n\n[WebDriver protocol]: https://w3c.github.io/webdriver/#protocol\n[Marionette remote protocol]: https://firefox-source-docs.mozilla.org/testing/marionette/\n[WebDriver]: https://developer.mozilla.org/en-US/docs/Web/WebDriver\n\n\nDownloads\n---------\n\n* [Releases](https://github.com/mozilla/geckodriver/releases/latest)\n* [Change log](https://searchfox.org/mozilla-central/source/testing/geckodriver/CHANGES.md)\n\n\nDocumentation\n-------------\n\n* [WebDriver] (work in progress)\n  * [Commands](https://developer.mozilla.org/en-US/docs/Web/WebDriver/Commands)\n  * [Errors](https://developer.mozilla.org/en-US/docs/Web/WebDriver/Errors)\n  * [Types](https://developer.mozilla.org/en-US/docs/Web/WebDriver/Types)\n\n* [Cross browser testing](https://developer.mozilla.org/en-US/docs/Learn/Tools_and_testing/Cross_browser_testing)\n\n* [Selenium](https://seleniumhq.github.io/docs/) (work in progress)\n  * [C# API](https://seleniumhq.github.io/selenium/docs/api/dotnet/)\n  * [JavaScript API](https://seleniumhq.github.io/selenium/docs/api/javascript/)\n  * [Java API](https://seleniumhq.github.io/selenium/docs/api/java/)\n  * [Perl API](https://metacpan.org/pod/Selenium::Remote::Driver)\n  * [Python API](https://seleniumhq.github.io/selenium/docs/api/py/)\n  * [Ruby API](https://seleniumhq.github.io/selenium/docs/api/rb/)\n\n* [geckodriver usage](https://firefox-source-docs.mozilla.org/testing/geckodriver/Usage.html)\n  * [Supported platforms](https://firefox-source-docs.mozilla.org/testing/geckodriver/Support.html)\n  * [Firefox capabilities](https://firefox-source-docs.mozilla.org/testing/geckodriver/Capabilities.html)\n  * [Capabilities example](https://firefox-source-docs.mozilla.org/testing/geckodriver/Capabilities.html#capabilities-example)\n  * [Enabling trace logs](https://firefox-source-docs.mozilla.org/testing/geckodriver/TraceLogs.html)\n  * [Analyzing crash data from Firefox](https://firefox-source-docs.mozilla.org/testing/geckodriver/CrashReports.html)\n\n* [Contributing](https://firefox-source-docs.mozilla.org/testing/geckodriver/#for-developers)\n  * [Building](https://firefox-source-docs.mozilla.org/testing/geckodriver/Building.html)\n  * [Testing](https://firefox-source-docs.mozilla.org/testing/geckodriver/Testing.html)\n  * [Releasing](https://firefox-source-docs.mozilla.org/testing/geckodriver/Releasing.html)\n  * [Self-serving an ARM build](https://firefox-source-docs.mozilla.org/testing/geckodriver/ARM.html)\n\n\nSource code\n-----------\n\ngeckodriver is made available under the [Mozilla Public License].\n\nIts source code can be found in [mozilla-central] under testing/geckodriver.\nThis GitHub repository is only used for issue tracking and making releases.\n\n[source code]: https://hg.mozilla.org/mozilla-unified/file/tip/testing/geckodriver\n[Mozilla Public License]: https://www.mozilla.org/en-US/MPL/2.0/\n[mozilla-central]: https://hg.mozilla.org/mozilla-central/file/tip/testing/geckodriver\n\nCustom release builds\n---------------------\n\nIf a binary is not available for your platform, it's possibe to create a custom\nbuild using the [Rust] toolchain. To do this, checkout the release tag for the\nversion of interest and run `cargo build`. Alternatively the latest version may\nbe built and installed from `crates.io` using `cargo install geckodriver`.\n\n[Rust]: https://rustup.rs/\n\nContact\n-------\n\nThe mailing list for geckodriver discussion is https://groups.google.com/a/mozilla.org/g/dev-webdriver.\n\nThere is also a [Matrix](https://wiki.mozilla.org/Matrix) channel to talk about using and developing\ngeckodriver in [webdriver](https://chat.mozilla.org/#/room/#webdriver:mozilla.org) on chat.mozilla.org.\n"
},
{
  "name": "react-content-marker",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "jest.config.js",
      "package-lock.json",
      "package.json",
      "src",
      "tsconfig.json"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Content Marker for React\n\n`react-content-marker` is a tool to replace content in strings with HTML tags.\nIt can match simple text, or use the full power of regex.\n\n**Key features:**\n\n- Can replace text with anything (other text, any React node).\n- Supports any number of parsers (so you can mark several patterns\n  in the same text easily).\n- Works on strings and arrays of strings (it ignores non-string items),\n  meaning you can combine it with other parsing tools.\n\n| | |\n|-|-|\nCode          | https://github.com/mozilla/react-content-marker\nIssues        | https://github.com/mozilla/react-content-marker/issues\nLicense       | 3-Clause BSD\nDocumentation | https://github.com/mozilla/react-content-marker#content-marker-for-react\n\n\n## Install\n\n`npm i -P react-content-marker` or `yarn add react-content-marker`\n\n\n## Basic usage\n\n```js\nimport createMarker from 'react-content-marker';\n\nconst parsers = [\n    {\n        rule: 'world',\n        tag: x => <mark title='Target'>{ x }</mark>,\n    },\n    {\n        rule: /(hello)/i,\n        tag: x => <mark title='Greeting'>{ x }</mark>,\n    },\n];\n\nconst MyMarker = createMarker(parsers);\n\nrender(<MyMarker>Hello, world!</MyMarker>);\n\n// Renders:\n<mark title='Greeting'>Hello</mark>, <mark title='Target'>world</mark>!\n```\n\n\n## Advanced usage\n\n`react-content-marker` exposes only one function: `createMarker`. It takes\na list of parsers and returns a React component. That component only accepts\na string or an array of strings \u2014 if you pass it a React Component, nothing will\nhappen.\n\nParsers are simple objects. They must define two attributes: `rule` and\n`tag`. `rule` is either a string or a regex expressing what is to be matched\nin the content. `tag` is a function that takes the matched content and returns\na React Node (a string, null, a React Component, etc. ).\n\nYou can use as many parsers as you want. However, note that once a part of your\ninput has been marked by a rule, it will be ignored for all following rules.\nThat means that the order of your parsers is very important.\n\nWhen using regex, you will need to have at least one pair of capturing\nparentheses, as that is what is used to extract the matched content. If your\nregex is complex and uses several capturing parentheses, by default this library\nwill choose the last non-null match available. If you want to match a different\ngroup, you can define a `matchIndex` attribute in your parser. That integer\nwill be used to choose the captured group to return. Here are examples:\n\n```js\n// Without `matchIndex`.\nconst parsers = [\n    {\n        rule: /(hello (world|folks))/i,\n        tag: x => <mark>{ x }</mark>,\n    },\n];\nconst MyMarker = createMarker(parsers);\nrender(<MyMarker>Hello, world!</MyMarker>);\n\n// Renders:\nHello, <mark>world</mark>!\n```\n\n```js\n// With `matchIndex`.\nconst parsers = [\n    {\n        rule: /(hello (world|folks))/i,\n        tag: x => <mark>{ x }</mark>,\n        matchIndex: 0,\n    },\n];\nconst MyMarker = createMarker(parsers);\nrender(<MyMarker>Hello, world!</MyMarker>);\n\n// Renders:\n<mark>Hello, world</mark>!\n```\n\n### The `mark` function\n\nYou can also directly access the `mark` function. That can be useful if you\nneed to combine different stacks of parsers, and don't want, or cannot, just\nmerge the lists of rules (which should almost always be a better and simpler\nsolution). For example, if you want to create a Higher-Order Marker that\ncombines with another Marker.\n\n`mark` takes the content to mark and all properties of a rule as parameters,\nand outputs the marked content as an array of strings and React nodes.\nSee its definition:\n\n```js\nfunction mark(\n    content: string | Array<string | React.Node>,\n    rule: string | RegExp,\n    tag: (string) => React.Node,\n    matchIndex: ?number,\n): Array<string | React.Node>\n```\n\nNote however that this function doesn't perform some of the niceties\n`createMarker` does. For example, it doesn't automatically add a `key` to the\ntagged elements, which might create warnings in your code.\n\n\n## Contributing\n\nThis code relies on unit tests (with Jest) and type checking (with TypeScript).\n\n### Running tests\n\n`npm test`\n\n### Building\n\n`npm run build`\n"
},
{
  "name": "fxa-pairing-channel",
  "files": {
    "/": [
      ".circleci",
      ".eslintignore",
      ".eslintrc.yml",
      ".gitignore",
      ".npmignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "demo",
      "dist",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "webpack.config.js"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# fxa-pairing-channel [![CircleCI](https://circleci.com/gh/mozilla/fxa-pairing-channel/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla/fxa-pairing-channel/tree/master)\n\nThis repo implements a shared library for two javascript environments\nto create an encrypted and authenticated communication channel, by\nsharing a secret key and by relaying messages through a websocket server.\n\nIt is used by the [Firefox Accounts pairing flow](\nhttps://mozilla.github.io/ecosystem-platform/docs/features/firefox-accounts/pairing),\nwith one side of the channel being web content from https://accounts.firefox.com and\nthe other side of the channel being a signed-in Firefox instance.\n\n\nAPI\n===\n\nTo use this library, each side needs to agree on the URL of a\n[channelserver](https://github.com/mozilla-services/channelserver)\ninstance which they can use to exchange messages via WebSocket:\n\n```\nconst CHANNEL_SERVER_URL = \"wss://channelserver.services.mozilla.com\"\n```\n\nThe main abstraction is the `PairingChannel` class.\nOne side of the connection can create a new channel like this:\n\n```\nconst channel = await PairingChannel.create(CHANNEL_SERVER_URL);\nconsole.log(channel.channelId, channel.channelKey);\n```\n\nThis produces a `channelId` and `channelKey` that need to be transferred to\nthe intended client, perhaps by scanning them from a QR code.\n\nThe client can then connect to the channel like this:\n\n```\nconst {channelId, channelKey} = OBTAIN_THESE_BY_SOME_OUT_OF_BAND_MECHANISM();\nconst channel = await PairingChannel.connect(CHANNEL_SERVER_URL, channelId, channelKey);\n```\n\nBoth ends of the channel can then send and receive messages using a websocket-like\ninterface:\n\n```\nchannel.send(\"ping\")\n\nchannel.addEventListener(\"message\", event => {\n  const {msg} = event.detail.data;\n  console.log(msg); // \"pong\"\n}\n```\n\nYou can try out a more complete demo of this API by loading\n`./demo/test_client.html` and `./demo/test_server.html` in\nparallel webpages and watching them pass messages back and forth.\n\n\nCrypto\n======\n\nUnder the hood, the `PairingChannel` implements the \"externally-provisioned\npre-shared key\" mode of [TLS1.3](https://tools.ietf.org/html/rfc8446).\nEach side of the channel can thus be assured that its peer is in possession\nof the `channelKey`, and that their traffic is protected from anyone who\ndoes not possess this key, even from the intermediary channelserver through\nwhich they exchange messages.\n\nAs a consumer of this library, you shouldn't need to care about any of those\ndetails though - just use the provided APIs to handle all the Crypto bits\nfor you.\n\n\nDevelopment and Maintenance\n===========================\n\nThis library is basically \"done\", inasmuch as it works and we are not planning\nany feature additions. We are still maintaining it to keep up-to-date with tooling\nand dependency changes, security fixes, etc.\n\n### Source Code\n\nThe main source code for the library is found under `./src`. Since the code gets\nvendored into the Firefox client distribution, we try to avoid taking any external\ndependencies (currently the only dependency is the `event-target-shim` package).\n\n### Tests\n\nThere is an extensive test suite under `./test`, covering many of the crypto edge-cases\nthat motived the choice to use TLS rather than an ad-hoc protocol. To run them use the\nusual npm test runner:\n\n```\nnpm test\n```\n\nThis uses the [Karma](https://karma-runner.github.io) test runner to execute\nthe tests in a live browser.\n\nRunning the tests will also produces coverage reports under `./coverage`,\nwhich can be viewed with a web browser. We strive for very close to 100%\ntest coverage, because this is crypto-related code where even the edgiest\nof edge-cases can have serious consequences if mis-handled.\n\n### Build Artifacts\n\nThe main sources get transpiled into two built distribution artifacts, one for use by\nweb content and one for vendoring into the Firefox source tree:\n\n* `./dist/FxAccountsPairingChannel.babel.umd.js` is transpiled to ES5-compatible JavaScript\n  using [Babel](https://babeljs.io/), and is suitable for use in web content that must\n  run in any (within reason!) browser. This is the artifact that gets used on the FxA\n  website for the client side of the pairing flow.\n* `./dist/FxAccountsPairingChannel.js` is Firefox browser code, suitable only for use\n  within the Firefox browser application itself. This gets manually copied over into\n  [`mozilla-central/services/fxaccounts/FxAccountsPairingChannel.js`](\n  https://searchfox.org/mozilla-central/source/services/fxaccounts/FxAccountsPairingChannel.js)\n  when making a new release.\n\nTo generate these files from the current `./src` tree, run:\n\n```\nnpm run build\n```\n\nFor ease of consumption, we check the resulting built artifacts in to the Git repo.\n\n### Release Process\n\nUse `npm version` and `npm publish` to cut a new release, after ensuring that the tests pass\nand the correct artifacts have been built:\n\n```\nnpm run build\nnpm test\nnpm version [major | minor | patch] -m \"Prepare version %s\"\nnpm publish\ngit push origin vX.Y.Z\n```\n\nThen, open a [Bugzilla Bug](https://bugzilla.mozilla.org/enter_bug.cgi?product=Firefox&component=Firefox%20Accounts) to update the vendored copy of the library in mozilla-central.\nCopy `./dist/FxAccountsPairingChannel.js` to [`services/fxaccounts/FxAccountsPairingChannel.js`](\nhttps://searchfox.org/mozilla-central/source/services/fxaccounts/FxAccountsPairingChannel.js) in\nthe mozilla-central source tree. The details of how to submit the resulting patch to mozilla-central\nare outside the scope of this document.\n"
},
{
  "name": "taar-experiment-v2-shield-study",
  "files": {
    "/": [
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "DEV.md",
      "LICENSE",
      "README.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "addon",
      "analysis",
      "bin",
      "dist",
      "fetch_translations.py",
      "generate_html.py",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "schemas",
      "templates",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# TAAR Experiment v2 - Shield Study\n\nTests the new [Telemetry-Aware Add-on Recommender](https://github.com/mozilla/taar) (TAAR).\n\nFor more information, see [the PHD](https://docs.google.com/document/d/1ZrfxNfBiEiAkqz4ZW9wmWfJF5sdfQg-Xq6_2mY1EXtI/edit)\n\n(The add-on for the previous version of this experiment is found [here](https://github.com/benmiroglio/taar-experiment))\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Analyzing data\n\nTelemetry pings are loaded into S3 and re:dash. Sample query:\n\n * [All pings](https://sql.telemetry.mozilla.org/queries/50057/source#table)\n\nSee [TELEMETRY.md](./TELEMETRY.md) for more details on what pings are sent by this add-on.  \n\n## Improving this add-on\n\nSee [DEV.md](./DEV.md) for more details on how to work with this add-on as a developer.  \n"
},
{
  "name": "esper-pioneer-shield-study",
  "files": {
    "/": [
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "LICENSE",
      "README.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "addon",
      "bin",
      "dist",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "schemas",
      "templates",
      "test"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# The ESPER Experiment - Firefox Pioneer Study\n\nEvaluating Similarity of Pioneers as Exemplars of Release\n\nAssess the degree and sense in which users in opt-in cohort of Firefox Pioneer differ from the Firefox release channel population. This should be a one time collection of data that focuses on fields for which we have aggregate statistics pertaining to the firefox release population in Telemetry.\n\nFor more information, see [the ESPER Product Hypothesis Doc](https://docs.google.com/document/d/1AhPGfCUs8lafrs9EznhL80NmiL0z4tUKb7HCMXlPxH8/edit)\n\n# Getting started\n\nSee [TESTPLAN.md](./TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n# Analysing Data\n\nTelemetry ping payloads are encrypted and not decrypted until it is on a server that is not connected to the wider internet. See [TELEMETRY.md](./TELEMETRY.md) for more details.\n\n# Bugzilla\n\n* [Initial launch](https://bugzilla.mozilla.org/show_bug.cgi?id=1414900)\n* [Re-launch](https://bugzilla.mozilla.org/show_bug.cgi?id=1450951)\n"
},
{
  "name": "speelycaptor",
  "files": {
    "/": [
      ".eslintrc.js",
      ".github",
      ".gitignore",
      ".prettierrc.json",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "app.js",
      "index.js",
      "package-lock.json",
      "package.json",
      "polycosm-publish.sh",
      "run-serverless.sh",
      "serverless.prod.yml",
      "serverless.public.yml",
      "serverless.yml",
      "template.yaml"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# speelycaptor\nAWS Lambda for performing video conversions via `ffmpeg`\n\nAPI:\n- GET `/init` to get a signed S3 URL to post a video to. Response is JSON with two keys, `uploadUrl` and `key`.\n- POST your video to `uploadUrl`.\n- GET `/convert?key=<key from /init>&args=<ffmpeg args>` will return JSON with `url` key with output\n\nRelies upon https://github.com/serverlesspub/ffmpeg-aws-lambda-layer being deployed under the layer name `ffmpeg` (see https://github.com/mozilla/hubs-ops/blob/master/terraform/modules/speelycaptor/main.tf for relevant terraform)\n"
},
{
  "name": "data-review",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "appendix.md",
      "renewal_request.md",
      "renewal_review.md",
      "request.md",
      "review.md"
    ]
  },
  "makefile": null,
  "readme": "# Forms for Firefox Data Collection Review Process\n\nThis respository contains templates for the Firefox data collection review process.  \n\nNew Firefox data collection (for the client, e.g. telemetry) and services (e.g. Firefox Accounts) must be reviewed and approved prior to deployment of collection code. Our data collection review process is designed to ensure that data collection meets our data and privacy policies and that there is sufficient documentation for all data collection in Firefox.  \n\nIf you are seeking review for new data collection, please use the request.md form in this repository.  Data stewards should fill out the review.md form in this repository in response to a request.  We provide both forms so that requesters know what stewards are looking for when performing a review of a request for data collection.\n\nYou can read more about the process and view a current list of data steward peers here: (https://wiki.mozilla.org/Data_Collection)\n"
},
{
  "name": "discourse-mozilla-gcm",
  "files": {
    "/": [
      ".deploy",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "api.yml",
      "app",
      "config",
      "db",
      "lib",
      "plugin.rb",
      "spec"
    ]
  },
  "makefile": null,
  "readme": "# Discourse Mozilla Group and Category Management\n*API to create/manage namespaced groups and categories on a Discourse instance*\n\n[![Build Status](https://travis-ci.org/mozilla/discourse-mozilla-gcm.svg?branch=master)](https://travis-ci.org/mozilla/discourse-mozilla-gcm)\n\n## API\n\n* [Documentation](https://mozilla.github.io/discourse-mozilla-gcm/)\n* [OpenAPI document](api.yml)\n\n## Dependencies\n\nThis plugin has a dependency on other Discourse plugins, these are:\n\n- [`discourse-mozilla-iam`](https://github.com/mozilla/discourse-mozilla-iam/)\n- [`discourse-group-category-notification`](https://github.com/mozilla/discourse-group-category-notification)\n\nAnd these plugins are recommended:\n\n- [`discourse-auto-email-in`](https://github.com/mozilla/discourse-auto-email-in)\n\n### Archives category\n\nThis plugin requires a category called \"Archives\" to moved deleted categories to.\n\n## Usage\n\nCurrently, this plugin doesn't have a UI. New clients must be created using `rails c`:\n\n`MozillaGCM::Client.create!(name: \"Test Client\", namespace: \"test\", category: category, key: \"12345\")`\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Running tests\n\nClone this plugin into `plugins/discourse-mozilla-gcm` in the root of your Discourse source dir.\n\nUse `RAILS_ENV=test rake plugin:spec[discourse-mozilla-gcm]` to run the tests.\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "server-side-tls",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "Cipher_Suites.mediawiki",
      "LICENSE",
      "README.md",
      "Server_Side_TLS.mediawiki",
      "json",
      "ssl-config-generator"
    ]
  },
  "makefile": null,
  "readme": "Server Side TLS\n===============\n\nThis repository contains the MediaWiki source for Mozilla's Server Side TLS\ndocument at [Server Side TLS](https://wiki.mozilla.org/Security/Server_Side_TLS).\n\nHistorically Mozilla's SSL/TLS Configuration Generator was housed in this\nrepository. Access the current generator by following this link: [SSL Configuration Generator](https://ssl-config.mozilla.org).\nThe current code for the configuration generator is located at https://github.com/mozilla/ssl-config-generator\n\nThe original code [can be found at this revision](https://github.com/mozilla/server-side-tls/tree/last-revision-before-move).\n"
},
{
  "name": "twemoji-colr",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTE.md",
      "Gruntfile.js",
      "LICENSE.md",
      "Makefile",
      "README.md",
      "extras",
      "fixDirection.py",
      "layerize.js",
      "overrides",
      "package.json",
      "tests",
      "twe-svg.zip",
      "twe-svg.zip.version.txt"
    ]
  },
  "makefile": "NPM        ?= npm\nNODE       ?= node\nPERL       ?= perl\nPYTHON     ?= python3\nTTX        ?= ttx\n\nFONT_NAME  = Twemoji\\ Mozilla\n\nBUILD_DIR  = build\n\nFINAL_TARGET = $(BUILD_DIR)/$(FONT_NAME).ttf\n\nSVGS         = twe-svg.zip\nOVERRIDE_DIR = overrides\nEXTRA_DIR    = extras\n\nGRUNTFILE  = Gruntfile.js\nLAYERIZE   = layerize.js\n\nCODEPOINTS          = $(BUILD_DIR)/codepoints.js\nOT_SOURCE  \t        = $(BUILD_DIR)/$(FONT_NAME).ttx\nRAW_FONT            = $(BUILD_DIR)/raw-font/$(FONT_NAME).ttf\nRAW_FONT_TEMPORARY\t= $(BUILD_DIR)/raw-font/$(FONT_NAME).temporary.ttf\n\n$(FINAL_TARGET) : $(RAW_FONT) $(OT_SOURCE)\n\trm -f $(FINAL_TARGET)\n\t# remove illegal <space> from the PostScript name in the font\n\t$(TTX) -t name -o $(RAW_FONT).names $(RAW_FONT)\n\t$(PERL) -i -e 'my $$ps = 0;' \\\n\t        -e 'while(<>) {' \\\n\t        -e '  $$ps = 1 if m/nameID=\"6\"/;' \\\n\t        -e '  $$ps = 0 if m|</namerecord>|;' \\\n\t        -e '  s/Twemoji Mozilla/TwemojiMozilla/ if $$ps;' \\\n\t        -e '  print;' \\\n\t        -e '}' $(RAW_FONT).names\n\t$(TTX) -m $(RAW_FONT) -o $(RAW_FONT_TEMPORARY) $(RAW_FONT).names\n\t$(PYTHON) fixDirection.py $(RAW_FONT_TEMPORARY)\n\t$(TTX) -m $(RAW_FONT_TEMPORARY) -o $(FINAL_TARGET) $(OT_SOURCE)\n\n$(RAW_FONT) : $(CODEPOINTS) $(GRUNTFILE)\n\t$(NPM) run grunt webfont\n\n$(CODEPOINTS) $(OT_SOURCE) : $(LAYERIZE) $(SVGS) $(OVERRIDE_DIR) $(EXTRA_DIR)\n\t$(NODE) $(LAYERIZE) $(SVGS) $(OVERRIDE_DIR) $(EXTRA_DIR) $(BUILD_DIR) $(FONT_NAME)\n",
  "readme": "# twemoji-colr\n\nProject to create a COLR/CPAL-based color OpenType font\nfrom the [Twemoji](https://twitter.github.io/twemoji/) collection of emoji images.\n\nNote that the resulting font will **only** be useful on systems that support\nlayered color TrueType fonts; this includes Windows 8.1 and later,\nas well as Mozilla Firefox and other Gecko-based applications running on\nany platform.\n\nSystems that do not support such color fonts will show blank glyphs\nif they try to use this font.\n\n## Getting started\n\nThis project makes use of [grunt-webfont](https://github.com/sapegin/grunt-webfont)\nand an additional [node.js](https://nodejs.org/en/) script.\nTherefore, installation of Node.js (and its package manager [npm](https://www.npmjs.com/)) is a prerequisite.\nGrunt will be installed as a package dependency \u2014 no need to install it globally.\n\nThe necessary tools can be installed via npm:\n\n    # install dependencies from packages.json, including `grunt-webfont`.\n    npm install\n\nThe build process also requires [fontforge](https://fontforge.github.io/)\nand the TTX script from the [font-tools](https://github.com/behdad/fonttools/) package to be installed, and assumes standard Perl and Python are available.\n\nBoth FontForge and font-tools can be installed via `homebrew` on OS X, or package managers on Linux:\n\n    # OS X\n    brew install fonttools fontforge\n\n    # Ubuntu, for example\n    sudo apt-get install fonttools fontforge python-fontforge\n\n## Building the font\n\nOnce the necessary build tools are all in place, simply running\n\n    make\n\nshould build the color-emoji font `build/Twemoji Mozilla.ttf` from the source SVG files found in `twe-svg.zip` file and `extras`, `overrides` directories.\n"
},
{
  "name": "midir",
  "files": {
    "/": [
      ".gitignore",
      "CHANGELOG.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "azure-pipelines-template.yml",
      "azure-pipelines.yml",
      "examples",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# midir [![crates.io](https://img.shields.io/crates/v/midir.svg)](https://crates.io/crates/midir) [![Build Status](https://dev.azure.com/Boddlnagg/midir/_apis/build/status/Boddlnagg.midir?branchName=master)](https://dev.azure.com/Boddlnagg/midir/_build/latest?definitionId=1)\n\nCross-platform, realtime MIDI processing in Rust. This is a friendly fork with\nsmall changes required for vendoring the crate in Firefox. It will go away as\nsoon as we will be able to vendor the upstream crate.\n\n## Features\n**midir** is inspired by [RtMidi](https://github.com/thestk/rtmidi) and supports the same features*, including virtual ports (except on Windows) and full SysEx support \u2013 but with a rust-y API!\n\n<sup>* With the exception of message queues, but these can be implemented on top of callbacks using e.g. Rust's channels.</sup>\n\n**midir** currently supports the following platforms/backends: \n- [x] ALSA (Linux)\n- [x] WinMM (Windows)\n- [x] CoreMIDI (macOS, iOS (untested))\n- [x] WinRT (Windows 8+), enable the `winrt` feature\n- [x] Jack (Linux, macOS), enable the `jack` feature\n- [x] Web MIDI (Chrome, Opera, perhaps others browsers)\n\nA higher-level API for parsing and assembling MIDI messages might be added in the future.\n\n## Documentation & Example\nAPI docs can be found at [docs.rs](https://docs.rs/crate/midir/). You can find some examples in the [`examples`](examples/) directory. Or simply run `cargo run --example test_play` after cloning this repository.\n"
},
{
  "name": "addons-moz-compare",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      ".npmignore",
      ".npmrc",
      ".prettierignore",
      ".prettierrc",
      "LICENSE.txt",
      "README.md",
      "package.json",
      "src",
      "tests",
      "yarn.lock"
    ],
    "/.github": [
      "CODE_OF_CONDUCT.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# addons-moz-compare\n\n[![CircleCI](https://circleci.com/gh/mozilla/addons-moz-compare.svg?style=svg)](https://circleci.com/gh/mozilla/addons-moz-compare) [![npm version](https://badge.fury.io/js/addons-moz-compare.svg)](https://www.npmjs.com/package/addons-moz-compare)\n\nA JavaScript library to compare Mozilla add-on versions that follow the [Manifest Version Format](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/manifest.json/version/format).\n\n## API\n\nThis library exposes a `mozCompare()` function that takes two (string) versions `A` and `B` and returns:\n\n- `-1` if `A < B`\n- `0` if `A == B`\n- `1` if `A > B`\n\nThis implementation matches the Firefox implementation except that there are only 3 different possible return values (Firefox returns strictly negative and strictly positive values instead of `-1` and `1`).\n\n## Usage\n\n```\nnpm i addons-moz-compare\n```\n\nor\n\n```\nyarn add addons-moz-compare\n```\n\n### Node\n\n```\nconst { mozCompare } = require('addons-moz-compare');\n```\n\n### Browser\n\nUse `window.mozCompare` after having included the source of this library.\n\n## License\n\nThis plugin is released under the Mozilla Public License Version 2.0. See the bundled [LICENSE](./LICENSE.txt) file for details.\n"
},
{
  "name": "webvision",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "build.js",
      "docs",
      "input",
      "package-lock.json",
      "package.json"
    ],
    "/docs": [
      "CNAME",
      "bg.jpg",
      "favicon.png",
      "full",
      "index.html",
      "mozilla-protocol",
      "styles.css"
    ]
  },
  "makefile": null,
  "readme": "\n# Instructions for publishing a new version\n\n- Update `input/summary.md` and `input/full.md`\n- Run: `node build.js`\n- To test locally: `python3 -m http.server --directory docs`\n\n# Protocol design library\n\nThis uses https://protocol.mozilla.org. To re-vendor it in:\n\n```\nrm -r docs/mozilla-protocol\nmkdir -p docs/mozilla-protocol/css docs/mozilla-protocol/fonts docs/mozilla-protocol/img/logos/mozilla\ncp node_modules/@mozilla-protocol/core/protocol/css/protocol.css docs/mozilla-protocol/css/\ncp node_modules/@mozilla-protocol/core/protocol/css/protocol.css.map docs/mozilla-protocol/css/\ncp node_modules/@mozilla-protocol/core/protocol/css/protocol-components.css docs/mozilla-protocol/css/\ncp -R node_modules/@mozilla-protocol/core/protocol/fonts docs/mozilla-protocol/\ncp node_modules/@mozilla-protocol/core/protocol/img/logos/mozilla/*.svg docs/mozilla-protocol/img/logos/mozilla\n```\n"
},
{
  "name": "identity-pubkeys",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "bchen.pub",
      "jbuck.pub",
      "lzoog.pub",
      "vbudhram.pub"
    ]
  },
  "makefile": null,
  "readme": "Firefox Accounts developers can add their public key to this repo to get SSH access to [fxa-dev](https://github.com/mozilla/fxa-dev) instances, including the [stable dev environment](https://developer.mozilla.org/en-US/docs/Mozilla/Tech/Firefox_Accounts/Introduction#Stable_development_(production_clone)).\n\n"
},
{
  "name": "matchmaker",
  "files": {
    "/": [
      ".eslintrc.json",
      ".gitignore",
      ".prettierignore",
      ".prettierrc",
      "README.md",
      "config",
      "examples",
      "package-lock.json",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n\n- [matchmaker](#matchmaker)\n  - [development](#development)\n  - [usage](#usage)\n    - [donut mode](#donut-mode)\n      - [optional args](#optional-args)\n    - [mentorship mode](#mentorship-mode)\n      - [optional args](#optional-args-1)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n# matchmaker\n\nMatchmaking utility serving a few purposes:\n\n- `--mode=donut` - Can create donut style groups of varying sizes. Fun for remote teams.\n- `--mode=mentorship` - Can create mentor/mentee matches. Makes for a good randomized starting point to review/make personalized adjustments as needed.\n\nSee examples/ for required input format and sample output.\n\n## development\n\n1. Make sure you have node installed. See 'engines' in package.json for version details.\n1. `npm i` - install dependencies.\n\n## usage\n\n1. Make sure you have node installed.\n1. `npm i && npm run build` - install dependencies and build.\n1. `cd dist` - run from dist dir. See options for running below:\n\n### donut mode\n\nThis will split a list of users into randomly selected groups. The default group size is `3` but you can configure this.\n\nFrom the dist folder, run:\n\n- `cd dist && node app.js --type=donut`.\n\nBy default, this will use the examples/donut/input.csv file and generate a `donut-output-local.csv` file.\n\n#### optional args\n\nUsage example: `node app.js --type=donut --size=2 --output=foo.csv` to set group size to 2 and change the name of output file.\n\n- `size` - control group size. defaults to 3.\n- `input` - set input file. defaults to `examples/donut/input.csv`.\n- `output` - set output file. defaults to `donut-output-local.csv`.\n\n### mentorship mode\n\nThis will split a list of people who have signalled their desire to be mentors/mentees into randomized mentor/mentee pairs. By default, uses the [sample input file](examples/mentorship/input.csv) and will create a file called `mentorship-output-local.csv`.\n\nFrom the dist folder, run:\n\n- `node app.js --type=mentorship`.\n\n#### optional args\n\nUsage example: `node app.js --type=mentorship --addPeopleData --input=input.csv --output=matches.csv --inputPeople=people-data.csv`. Input and output paths are related to the repo root.\n\n- `addPeopleData` - boolean - defaults to false. When enabled, this will use content from people.mozilla.org and combine timezone, and reporting chain information into final results. Up to date content to use here can be collected via an add-on. Contact [Rachel](https://github.com/tublitzed/) for access.\n  - Usage - `node app.js --type=mentorship --addPeopleData`.\n- `input` - set input file. Defaults to `examples/mentorship/input.csv`.\n- `inputPeople` - set file to use with the `addPeopleData` flag. Defaults to `examples/mentorship/input-people-data.csv`.\n- `output` - set output file. Defaults to `mentorship-output-local.csv`.\n"
},
{
  "name": "script_review_utils",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "wet-script"
    ]
  },
  "makefile": null,
  "readme": "# Script Review Utils\n\nA collection of tools for reviewing javascript scripts found on websites.\n\nThis is currently very adhoc, and is starting as a collection of tools\nas they are used.\n\nAs tools get more used and the use cases emerge, the code can formalize (get\ntests, packaged etc.)\n\nFor now, the intention is that this repository is cloned and then hacked on\nlocally to facilitate the analysis needed.\n\n## wet-script\n\nThe principle of dry code is to not repeat yourself. And, among other things,\noften involves using variables to replace hardcoded values like strings or\nconstants.\n\nSome javascript code uses this idea to an extreme to obfuscate the code. For \nexample:\n\n```js\nH = new RegExp(g8z.o8J + v8J + e8J + h8J + z8J + (O8J === (S8J, V9J) ? U8J : g8z.R8J), j8J);\n```\n\nIn this example, `v8J`, `e8J`, `h8J`, `z8J`, and `j8J` are all strings that can be replaced.\n``g8z`` is a reference to an object which has a series of constant properties.\nSo ``g8z.o8J`` and ``g8z.R8J`` are also strings that can be replaced.\n``O8J``, ``S8J``, ``V9J``, and ``U8J`` are all numbers that can be replaced.\n\nDoing these replacements we get:\n\n```js\nH = new RegExp(\"cd\" + \"n4.for\" + \"ter.co\" + \"m.+scr\" + \"ipt.j\" + (36.31 === (256.62, 1851) ? 722.46 : \"s\"), \"gm\");\n```\n\nWe can concatenate all the strings together and get:\n\n```js\nH = new RegExp(`cdn4.forter.com.+script.j${36.31 === (256.62, 1851) ? 722.46 : \"s\"}`, \"gm\");\n```\n\nWe can now see that the numbers in the middle are a conditional expression that will always\nreturn false and will return the letter \"s\". So we can eval the expression and\ntry again with the string concatenation\n\n```js\nH = new RegExp(\"cdn4.forter.com.+script.js\", \"gm\");\n```\n\nAs you can see the last line is quite version is readable, the initial version not at\nall especially with the variable declarations spread over thousands of lines of code.\n\n\nThe wet-script library contains scripts to manipulate scripts to remove as many variables\nas possible and insert their values in the code and tidy up where possible.\n\n"
},
{
  "name": "cloudtrail-resource-tracker",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "cloudtrail-resource-tracker.yml",
      "functions"
    ]
  },
  "makefile": null,
  "readme": "# cloudtrail-resource-tracker\n\n## Problem Statement\nDuring investigation and incident response, it is difficult to determine an owner/operator/admin for a given AWS resource (account, EC2 instance, database, etc.).\n\n## Possible Solution\nDevelop a searchable database that automatically maps resources to humans that create those resources, and build a simple way to query that database\n\n## How?\n* We consume CloudTrail logs for most of Mozilla\u2019s AWS infrastructure (excluding Firefox Services). These logs contain information on AWS users and resources\n  * Once MAWS is deployed, the AWS users will map to canonical LDAP identities for staff\n* CloudTrail logs in MozDef are only around for a month, meaning history doesn\u2019t persist for as long as we might need during investigation and incident\n\n## Approach\n1. At a regular schedule, query CloudTrail for the following event types:\n   * Resource Creation\n   * Resource Deletion\n1. Transform returned Creation events and inject into relational database\n   * Map AWS Resource to\n     * Associated AWS account\n     * Each human that touched it\n       * Last time they touched it (we only care about recency, because we\u2019re trying to find the most relevant human)\n       * Last action they took or preserve who created it\n   * Additionally while recording a newly created resource, it would make sense to also write a record of a new IP address for resources with IP addresses. This would allow us to do reverse IP lookups like ipquery/ip2instance/cloudhealth used to provide, enabling someone doing incident response to answer the question \"Is the IP address X one of ours and if so what AWS account and resource is it?\"\n1. For returned Deletion events, ~~remove the resource from the database~~ mark as deleted, but keep for historical reasons.\n1. Additionally there should be some capability to establish a baseline of existing resources that were created before this tool was enabled (as it will only detect newly created and deleted resources in CloudTrail, not existing resources).\n"
},
{
  "name": "inclusion",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "README.md",
      "code-of-conduct-enforcement",
      "code-of-conduct-events",
      "community-health",
      "curriculum",
      "data-metrics",
      "docs",
      "evaluation_tools",
      "leadership"
    ],
    "/docs": [
      "index.html"
    ]
  },
  "makefile": null,
  "readme": "# Diversity & Inclusion in Open Source\n\nWelcome!  This repository contains a number of resources, templates, standards and other useful things for open (source, education, knowledge, science) projects.  \n\n   * [Table of Contents](#diversity--inclusion-in-open-source)\n      * [Code of Conduct  Enforcement](#code-of-conduct--enforcement)\n      * [Education &amp; Training](#education--training)\n      * [Data &amp; Metrics Standards](#data--metrics-standards)\n      * [Evaluation Resources/Tools](#evaluation-resourcestools)\n      * [Research](#research)\n      * [Media / Articles / Blog Posts](#media--articles--blog)\n\n## Code of Conduct  Enforcement\n\n* [Mozilla Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/)\n* [Mozilla 'How to Report' Page](https://www.mozilla.org/en-US/about/governance/policies/participation/reporting/)\n\n#### Related Blog Posts\n\n* [Frameworks for Governance, Incentive and Consequence](https://medium.com/mozilla-open-innovation/frameworks-for-governance-incentive-and-consequence-in-foss-e1de6c091bdc) (2017)\n* [How We're Making Code of Conduct Enforcement Real, and Scaling it](https://medium.com/mozilla-open-innovation/how-were-making-code-of-conduct-enforcement-real-and-scaling-it-3e382cf94415) (2018)\n\n#### Process Standards for Enforcement\n* [Rolling out an all project/systems ban](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/process_documentation/community/ban-rollout.md)\n* [Consequence Ladder](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/consequence-ladder.md)\n* [Template - CPG/Code of Conduct 'Onboarding'](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/cpg-onboarding.md)\n* [Template - Decision Making](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/investigation/working-group/role-groups.md)\n\n#### Investigation & Decision Making\n* [Template - Decision Tracking Doc](https://github.com/mozilla/inclusion/blob/master/code-of-conduct-enforcement/investigation/working-group/decision.md)\n* [Template - Working Group 1st Agenda](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/investigation/working-group/working-group-first-agenda.md)\n* [Template - Working Group Standard Agenda](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/investigation/working-group/working-group-standard-agenda.md)\n* [Template - Investigation documentation](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/investigation/working-group/incident-investigation-template.md)\n\n##### Investigation Communication\n* [Template - Request for Clarification(from reporter)](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/triage/communications/more-information.md)\n* [Template - Receipt of Report (to reporter)](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/investigation/communication/reporter-investigation-started.md)\n* [Template - Request for more information(to reported)](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/investigation/communication/reported-request-for-clarification.md)\n* [Templates - Decision (to reported) by Level](https://github.com/mozilla/diversity/tree/master/code-of-conduct-enforcement/decisions/communication/decision-comms/reported)\n* [Templates - Decision(to reporter) by Level](https://github.com/mozilla/diversity/tree/master/code-of-conduct-enforcement/decisions/communication/decision-comms/reporter)\n* [Template - Request to Systems Administrator](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/decisions/communication/decision-comms/systems/level-7.md)\n* [Template - External Ban Decision](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/decisions/communication/decision-comms/reported/decision-matrix-ban.md)\n\n#### \n\n* [Enforcement FAQ (WIP)](https://github.com/mozilla/diversity/blob/master/code-of-conduct-enforcement/decisions/communication/community_comms/FAQ%20-%20Contributor.md)\n* [Template - Pre-Event Reminder](https://github.com/mozilla/diversity/blob/master/code-of-conduct-events/comms/mozillians-pre-event-reminder.md)\n\n\n## Education & Training\n\n* [CPG Enforcement Training for Staff](https://mozilla.teachable.com/courses/enrolled/634901) - which is basically like a 'first aid course', but for enforcement.\n* CPG Enforcement Training for Communities & Contributors  - also a 'first aid course' for enforcement.  (coming soon).\n* [Open Source Maintainer](https://mozilla.github.io/maintainer-cohort/) - designed to deliver (over 4 weeks or more) training for underrepresented people with goals for leadership in their careers.\n\n## Data & Metrics Standards\n* [How to Apply Metrics for Inclusion to Your Open Source Project](https://medium.com/@sunnydeveloper/how-to-apply-metrics-for-inclusion-to-your-open-source-project-71b4e31a7b0c)\n* [Best Practices  (asking about diversity demographics)](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/best-practices-diverse-data.md)\n* [Code of Conduct Experience (Survey questions)](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/cpg-follow-up.md)\n* [Standard - Dis/Ability](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/disability.md)\n* [Standard - Gender Identity](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/gender-identity.md)\n* [Standard - Pronouns](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/gender-pronouns.md)\n* [Standard - Language](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/language.md)\n* [Standard - Race/Ethnicity](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/race-ethnicity.md)\n* [Standard - Sexual Orientation](https://github.com/mozilla/diversity/blob/master/data-metrics/surveys/en/sexual-orientation.md)\n* [Standard - Github CODE_OF_CONDUCT](https://github.com/mozilla/repo-templates/blob/master/templates/CODE_OF_CONDUCT.md)\n* [Standard - Inclusive Leadership 'Team Page'](https://github.com/mozilla/diversity/blob/master/leadership/inclusive-leadership-template.md)\n\n\n## Evaluation Resources/Tools\n\n* [Open Source Governance Checklist](https://github.com/mozilla/diversity/blob/master/evaluation_tools/governance-basic.md)\n* [Inclusive Leadership Principles](https://github.com/emmairwin/wg-diversity-inclusion/blob/master/focus-areas/leadership/assets/leadership-principles.md)\n* [Inclusive Leadership Checklist - Maintainer Roles](https://github.com/mozilla/diversity/blob/master/leadership/leadership-principles-checklist-maintainer-tasks.md)\n* [Template - Contributor Testing](https://github.com/mozilla/diversity/blob/master/evaluation_tools/contributor-testing-steps.md)\n* [Open Source Project Inclusion Checklist (for contributors and projects)](https://github.com/mozilla/diversity/blob/master/evaluation_tools/contributor-assessment-basic.md.md)\n* [Code of Conduct Enforcement (Evaluation Tool)](https://mozilla.github.io/diversity-coc-review.io/modules/assessment/protected-groups/)\n\n## Research\n\n* [First-Language Interviews](https://medium.com/mozilla-open-innovation/celebrating-mother-language-day-in-open-source-5bd254890094)\n* [D&I in Open Source Survey (2018) - What we Learned](https://docs.google.com/presentation/d/13UxBGj2lI66SLjl6sp4NE3DH2ndT0k5QM0pPyyzZXuY/edit#slide=id.g25275a8168_3_275)\n* [Mozilla & the Rebel Alliance](https://report.mozilla.community/)\n* [What we Learned about Gender Identity in Open Source](https://medium.com/@sunnydeveloper/what-we-learned-about-gender-identity-in-open-source-d9acea0b7586)\n* ['Finding Inclusion Bugs'](https://medium.com/@sunnydeveloper/technical-volunteer-needed-help-me-find-inclusivity-bugs-b13644bf583a)  - [Inclusion Bug Campaign](https://medium.com/@sunnydeveloper/squash-inclusion-bugs-982a3e5ee29d)\n\n\n## Media / Articles / Blog\n\n* [Weaving Safety into Open Source Collaboration](https://blog.mozilla.org/community/2020/09/10/weaving-safety-into-the-fabric-of-open-source/) How Mozilla built a cross-organiztional program for CPG enforcement and safety.\n* [Open Source is only Ajar without Inclusion (Internet Citizen)](https://blog.mozilla.org/internetcitizen/2019/03/04/open-source-inclusion/)\n* [Inclusion at Scale in the Mozilla and Kubernetes Open Source Communities](https://thenewstack.io/inclusion-at-scale-in-the-mozilla-and-kubernetes-open-source-communities/)\n* [Innovating for Diversity & Inclusion (research report part #1)](https://medium.com/mozilla-open-innovation/a-time-for-action-innovating-for-diversity-inclusion-in-open-source-communities-6922fef4675e)\n* [Inclusive Organizing for Open Source Communities](https://medium.com/mozilla-open-innovation/reflection-inclusive-organizing-for-open-source-communities-9c44f0b689c1)\n* [We See You - Reaching Diverse Audiences in FOSS](https://medium.com/mozilla-open-innovation/we-see-you-reaching-diverse-audiences-in-foss-4e83efc86425)\n* [Innovation for Inclusion in the Mozilla Open Source Support (MOSS) Program](https://blog.mozilla.org/careers/innovating-for-inclusion-in-the-mozilla-open-source-support-program/)\n* [Words Matter - Mozilla Removes Meritocracy from our Governance](https://blog.mozilla.org/careers/words-matter-moving-beyond-meritocracy/)\n"
},
{
  "name": "xar",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      "README.md",
      "README.txt",
      "XarCMPlugIn",
      "XarKit",
      "python",
      "tools",
      "xar",
      "xarmdimport",
      "xarql"
    ]
  },
  "makefile": null,
  "readme": "**WARNING**\n\nThis is a fork of xar used internally at Mozilla to scrape debug information from macOS builds. It contains minimal changes required to build within our automation environment.\n"
},
{
  "name": "gcsfuse",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "benchmarks",
      "docs",
      "flags.go",
      "flags_test.go",
      "go.mod",
      "go.sum",
      "internal",
      "main.go",
      "mount.go",
      "tools",
      "vendor",
      "version.go"
    ],
    "/docs": [
      "installing.md",
      "mounting.md",
      "releasing.md",
      "semantics.md"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "gcsfuse is a user-space file system for interacting with [Google Cloud\nStorage][gcs].\n\n[gcs]: https://cloud.google.com/storage/\n\n# Current status\n\nPlease treat gcsfuse as beta-quality software. Use it for whatever you like, but\nbe aware that bugs may lurk, and that we reserve the right to make small\nbackwards-incompatible changes.\n\nThe careful user should be sure to read [semantics.md][] for information on how\ngcsfuse maps file system operations to GCS operations, and especially on\nsurprising behaviors. The list of [open issues][issues] may also be of interest.\n\n[semantics.md]: docs/semantics.md\n[issues]: https://github.com/GoogleCloudPlatform/gcsfuse/issues\n\n\n# Installing\n\nSee [installing.md][] for full installation instructions for Linux and Mac OS X.\n\n[installing.md]: docs/installing.md\n\n\n# Mounting\n\n## Prerequisites\n\n* Before invoking gcsfuse, you must have a GCS bucket that you want to mount. If\nyour bucket doesn't yet exist, create one using the\n[Google Developers Console][console].\n\n[console]: https://console.cloud.google.com\n\n* Make sure [the Google Cloud Storage JSON API is enabled][enableAPI].\n\n[enableAPI]: https://cloud.google.com/storage/docs/json_api/#activating\n\n* GCS credentials are automatically loaded using [Google application default\ncredentials][app-default-credentials], or a JSON key file can be specified\nexplicitly using `--key-file`. If you haven't already done so, the easiest way\nto set up your credentials for testing is to run the [gcloud tool][]:\n\n```\n    gcloud auth login\n```\n  See [mounting.md][] for more information on credentials.\n\n[gcloud tool]: https://cloud.google.com/sdk/gcloud/\n[app-default-credentials]: https://developers.google.com/identity/protocols/application-default-credentials#howtheywork\n[mounting.md]: /docs/mounting.md\n\n## Invoking gcsfuse\n\nTo mount a bucket using gcsfuse over an existing directory `/path/to/mount`,\ninvoke it like this:\n\n```\ngcsfuse my-bucket /path/to/mount\n```\n\n**Important**: You should run gcsfuse as the user who will be using the file\nsystem, not as root. Do not use `sudo`.\n\nThe gcsfuse tool will exit successfully after mounting the file system. Unmount\nin the usual way for a fuse file system on your operating system:\n\n    umount /path/to/mount         # OS X\n    fusermount -u /path/to/mount  # Linux\n\nIf you are mounting a bucket that was populated with objects by some other means\nbesides gcsfuse, you may be interested in the `--implicit-dirs` flag. See the\nnotes in [semantics.md][semantics-implicit-dirs] for more information.\n\n[semantics-implicit-dirs]: docs/semantics.md#implicit-directories\n\nSee [mounting.md][] for more detail, including notes on running in the\nforeground and fstab compatibility.\n\n[mounting.md]: /docs/mounting.md\n\n\n# Performance\n\n## Latency and rsync\n\nWriting files to and reading files from GCS has a much higher latency than using\na local file system. If you are reading or writing one small file at a time,\nthis may cause you to achieve a low throughput to or from GCS. If you want high\nthroughput, you will need to either use larger files to smooth across latency\nhiccups or read/write multiple files at a time.\n\nNote in particular that this heavily affects `rsync`, which reads and writes\nonly one file at a time. You might try using [`gsutil -m rsync`][gsutil rsync]\nto transfer multiple files to or from your bucket in parallel instead of plain\n`rsync` with gcsfuse.\n\n[gsutil rsync]: https://cloud.google.com/storage/docs/gsutil/commands/rsync\n\n## Rate limiting\n\nIf you would like to rate limit traffic to/from GCS in order to set limits on\nyour GCS spending on behalf of gcsfuse, you can do so:\n\n*   The flag `--limit-ops-per-sec` controls the rate at which gcsfuse will send\n    requests to GCS.\n*   The flag `--limit-bytes-per-sec` controls the egress\n    bandwidth from gcsfuse to GCS.\n\nAll rate limiting is approximate, and is performed over an 8-hour window. By\ndefault, there are no limits applied.\n\n## Upload procedure control\n\nAn upload procedure is implemented as a retry loop with exponential backoff \nfor failed requests to the GCS backend. Once the backoff duration exceeds this \nlimit, the retry stops. Flag `--max-retry-sleep` controls such behavior.\nThe default is 1 minute. A value of 0 disables retries.\n\n## GCS round trips\n\nBy default, gcsfuse uses two forms of caching to save round trips to GCS, at the\ncost of consistency guarantees. These caching behaviors can be controlled with\nthe flags `--stat-cache-capacity`, `--stat-cache-ttl` and `--type-cache-ttl`. See\n[semantics.md](docs/semantics.md#caching) for more information.\n\n## Timeouts\n\nIf you are using [FUSE for macOS](https://osxfuse.github.io/), be aware that by\ndefault it will give gcsfuse only 60 seconds to respond to each file system\noperation. This means that if you write and then flush a large file and your\nupstream bandwidth is insufficient to write it all to GCS within 60 seconds,\nyour gcsfuse file system may become unresponsive. This behavior can be tuned\nusing the [`daemon_timeout`][timeout] mount option. See [issue #196][] for\ndetails.\n\n[timeout]: https://github.com/osxfuse/osxfuse/wiki/Mount-options#daemon_timeout\n[issue #196]: https://github.com/GoogleCloudPlatform/gcsfuse/issues/196\n\n\n## Downloading object contents\n\nBehind the scenes, when a newly-opened file is first modified, gcsfuse downloads\nthe entire backing object's contents from GCS. The contents are stored in a\nlocal temporary file whose location is controlled by the flag `--temp-dir`.\nLater, when the file is closed or fsync'd, gcsfuse writes the contents of the\nlocal file back to GCS as a new object generation.\n\nFiles that have not been modified are read portion by portion on demand. gcsfuse\nuses a heuristic to detect when a file is being read sequentially, and will\nissue fewer, larger read requests to GCS in this case.\n\nThe consequence of this is that gcsfuse is relatively efficient when reading or\nwriting entire large files, but will not be particularly fast for small numbers\nof random writes within larger files, and to a lesser extent the same is true of\nsmall random reads. Performance when copying large files into GCS is comparable\nto gsutil (see [issue #22][issue-22] for testing notes). There is some overhead\ndue to the staging of data in a local temporary file, as discussed above.\n\n[issue-22]: https://github.com/GoogleCloudPlatform/gcsfuse/issues/22\n\nNote that new and modified files are also fully staged in the local temporary\ndirectory until they are written out to GCS due to being closed or fsync'd.\nTherefore the user must ensure that there is enough free space available to\nhandle staged content when writing large files.\n\n## Other performance issues\n\nIf you notice otherwise unreasonable performance, please [file an\nissue][issues].\n\n[issues]: https://github.com/googlecloudplatform/gcsfuse/issues\n\n# Support\n\ngcsfuse is open source software, released under the [Apache license](LICENSE).\nIt is distributed as-is, without warranties or conditions of any kind.\n\nFor support, visit [Server Fault][sf]. Tag your questions with `gcsfuse` and\n`google-cloud-platform`, and make sure to look at\n[previous questions and answers][previous] before asking a new one. For bugs and\nfeature requests, please [file an issue][issues].\n\n[sf]: http://serverfault.com/\n[previous]: http://serverfault.com/questions/tagged/gcsfuse\n\n\n# Versioning\n\ngcsfuse version numbers are assigned according to [Semantic\nVersioning][semver]. Note that the current major version is `0`, which means\nthat we reserve the right to make backwards-incompatible changes.\n\n[semver]: http://semver.org/\n"
},
{
  "name": "tippy-top-sites",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "data",
      "make_manifest.py",
      "manifest-viewer",
      "nsfw.py",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# tippy-top-sites\nScripts and tools related to tippy top services \n\n## make_manifest.py\nTo run the manifest generator script (note this script only supports Python3). In a new virtualenv:\n\n```\n$ pip install -r requirements.txt\n$ python make_manifest.py --help\nUsage: make_manifest.py [OPTIONS]\n\nOptions:\n  --count INTEGER         Number of sites from Alexa Top Sites\n  --loadrawsitedata TEXT  Load the full data from the filename specified\n  --saverawsitedata TEXT  Save the full data to the filename specified\n  --help                  Show this message and exit.\n\n$ python make_manifest.py --count 100 > icons.json\n```\n"
},
{
  "name": "properties-to-ftl",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "README.md",
      "cli.js",
      "lib",
      "package-lock.json",
      "package.json",
      "resolve-chrome-uri",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Properties-to-Fluent Migration Helper\n\nThis tool is intended to help automate most parts of moving messages\nfrom `.properties` files in `mozilla-central` to Fluent `.ftl` files.\nOn the way there, it can also update `.js`, `.jsm` and `.xhtml` files that use these messages,\nas well as writing a `.py` migration script for non-English locales.\n\nBecause this migration includes a move from indexed to named placeholders/variables,\nit's run as a **two-step process**.\nOn the first run, a `.migration.yaml` config file is generated next to each `.properties` file.\nThis may then be manually verified and updated before running the same command again,\nwhich then applies the full migration.\n\n## Install & Setup\n\nYou will need Node.js version 14 or greater to use this tool. Then:\n\n```\nnpm install --global @mozilla/properties-to-ftl\n```\n\nAfter this, the script may be run from anywhere as `properties-to-ftl`.\nIf you install it locally, use `npx properties-to-ftl` instead.\n\nTo verify that setup was successful and see a list of command-line options, run:\n\n```\nproperties-to-ftl --help\n```\n\n## Usage\n\nWhen migrating legacy messages, multiple things change:\n\n1. The message file extension changes, possible as well as its name.\n2. The file's location within `mozilla-central` changes.\n3. Message keys change, and often gain an identifying prefix in addition to being kebab-cased.\n4. The syntax for referring to variables in messages changes.\n5. The JavaScript API for formatting messages changes.\n\nTo help with the first three, you need to either use the `--ftl-path` and `--ftl-prefix` options\nor add some metadata comments to each `.properties` file that you're migrating:\n\n```ini\n# FTL path: foo/bar/baz.ftl\n# FTL prefix: foobar\n```\n\nThese comments don't need to be stored in the repo,\nbut keeping them there might help if a properties file is migrated in multiple patches.\nIf using the corresponding command-line arguments\nand the `.properties` file is only partially migrated,\nthese metadata comments will be added to it automatically.\n\n- The `FTL path` may use either the repo root or the `locales/en-US/` directory as its root.\n- An `FTL prefix` is not required, but if set, may only contain lower-case letters and dashes: `^[a-z-]+$`.\n  If set, it will be included as a prefix for all FTL message keys.\n\nOn the first run, a `.migration.yaml` config file is generated next to each `.properties` file.\nThis may then be manually verified and updated before running the same command again,\nwhich then applies the full migration.\n\n### Command-line arguments\n\nFor full usage, run this command:\n\n```ini\nproperties-to-ftl --help\n```\n\nWhen targeting a JS file, it is parsed for `chrome://` references to `.properties` and `.xhtml` files,\nwhich are then parsed in turn.\nXHTML may include `<stringbundle>` elements which are detected (and their source `.properties` also parsed),\nand properties files may include `FTL path` references, which are also parsed.\nAll of those files are then modified in-place\nonce the migration config has been reviewed and the CLI command is run again.\n\nWhen targeting a `.properties` file, all of its strings are migrated to Fluent.\nIn this use, JS and XHTML files are not parsed or migrated,\nand the placeholder variables are forced to use `var#` names.\nThese should be individually fixed in the migration config; they will have `# FIXME` comments.\n\n### Your Attention is Required\n\nBecause so many things change, it's unlikely that the script will catch everything.\nWhere possible, a comment `/* L10N-FIXME */` is injected\nimmediately after points in the JS source that require human attention.\n\nIn the generated FTL file, particular care should be given to reviewing the comments,\nwhich will at least approximate the recommended\n[metadata structurefor placeholders](https://firefox-source-docs.mozilla.org/l10n/fluent/review.html#comments),\nbut may not match exactly or be complete.\n\nYou will also need to manually make any necessary updates to `jar.mn` manifest files\nif a `.properties` file is removed.\nMigration config files should not be added to the soruce repository;\nthey may be safely removed at the end of the migration.\n\n## Tutorials\n\nTo best learn how all of this works, **play around with it!**\nFollow the setup instructions,\nthen find a JS file in `mozilla-central` that calls `Services.strings.createBundle()`,\nand run:\n\n```\nproperties-to-ftl path/to/file.jsm\n```\n\nBased on the CLI output,\nyou might need to first `--include` or `--exclude` some `.properties` file paths\nand provide an `--ftl-path` argument in order to generate a `.migration.yaml` file\nnext to its source `.properties` file.\nOpen it and the `.js` or `.jsm` file in an editor,\nand see if you can resolve the `FIXME` comments.\nThen run the same CLI command again to apply your transformation.\nSometimes, everything is already perfect,\nbut often additional manual work is required to polish up the migration patch.\n\n### An Example Migration\n\nAs an example, the file `browser/locales/en-US/chrome/browser/feeds/subscribe.properties`\n[currently](https://searchfox.org/mozilla-central/rev/131f3af9a49d2203adb7b7ef30dcc37c9f1aa10b/browser/locales/en-US/chrome/browser/feeds/subscribe.properties) contains these messages:\n\n```properties\naddProtocolHandlerMessage=Add \u201c%1$S\u201d as an application for %2$S links?\naddProtocolHandlerAddButton=Add application\naddProtocolHandlerAddButtonAccesskey=A\n```\n\nRunning the following command will generate a config file `subscribe.migration.yaml` next to it:\n\n```sh\nproperties-to-ftl --ftl-path protocolhandler.ftl \\\n  browser/locales/en-US/chrome/browser/feeds/subscribe.properties\n```\n\n```yaml\nmeta:\n  bug: xxxxxx # FIXME\n  title: Convert subscribe.properties to Fluent\n\nftl:\n  root: browser/locales/en-US\n  path: protocolhandler.ftl\n\nmigrate:\n  addProtocolHandlerMessage: # Add \u201c%1$S\u201d as an application for %2$S links?\n    key: add-protocol-handler-message\n    varNames:\n      - var1 # FIXME\n      - var2 # FIXME\n\n  addProtocolHandlerAddButton: # Add application\n    key: add-protocol-handler-add-button\n\n  addProtocolHandlerAddButtonAccesskey: # A\n    key: add-protocol-handler-add-button-accesskey\n```\n\nFor a proper migration, a few things ought to be fixed here:\n\n- The bug id needs to be included; this'll be a part of the generated Python migration script's filename.\n- The `add-protocol-handler-message` variable names need to be specified.\n  In many cases, if the command is run against a `.js` or `.jsm` file, these can be autodetected.\n  In this case, based on an inspection of [WebProtocolHandlerRegistrar.jsm](https://searchfox.org/mozilla-central/rev/131f3af9a49d2203adb7b7ef30dcc37c9f1aa10b/browser/components/protocolhandler/WebProtocolHandlerRegistrar.jsm),\n  these should probably be `host` and `protocol`.\n- The access key ought to be an attribute rather than a separate message.\n\nAfter these changes, the migration config will look like this:\n\n```yaml\nmeta:\n  bug: 123456\n  title: Convert subscribe.properties to Fluent\n\nftl:\n  root: browser/locales/en-US\n  path: protocolhandler.ftl\n\nmigrate:\n  addProtocolHandlerMessage: # Add \u201c%1$S\u201d as an application for %2$S links?\n    key: add-protocol-handler-message\n    varNames:\n      - host\n      - protocol\n\n  addProtocolHandlerAddButton: # Add application\n    key: add-protocol-handler-add-button\n\n  addProtocolHandlerAddButtonAccesskey: # A\n    key: add-protocol-handler-add-button\n    attr: accesskey\n```\n\nNow running the `properties-to-ftl` command again:\n\n```\nproperties-to-ftl browser/locales/en-US/chrome/browser/feeds/subscribe.properties\n```\n\nfinds the config file, and generates a new `protocolhandler.ftl`:\n\n```ftl\nadd-protocol-handler-message = Add \u201c{ $host }\u201d as an application for { $protocol } links?\nadd-protocol-handler-add-button = Add application\n    .accesskey = A\n```\n\nas well as a corresponding `bug_123456_protocolhandler.py` migration script for other locales.\n\n**NOTE:** If these commands were run against `WebProtocolHandlerRegistrar.jsm`\n(The only JS file that uses these messages) instead of `subscribe.properties`,\nthat would have modified its source as well,\nautomating some of the changes needed there and marking the rest with `/* L10N-FIXME */` comments.\n\n## Hacking It\n\nThis is an imperfect tool, because there's a limit to how much it makes sense to automate it.\nIf/when you encounter issues with it,\nyou are invited and expected to gauge for yourself how much it's really helping you,\nand whether it might make sense to either 1) submit a PR with a fix or 2) just deal with it.\n\nIn total, at the time of writing this, there are only about 5000 messages in properties files in mozilla-central,\nand many of the corner cases are relatively rarely used.\nSo if you're encounter a problem,\nit may well be easier to fix it directly rather than improving this tool.\n\nSome specific situations are recognised:\n\n- Often moving from `.properties` to Fluent should include a switch from using\n  imperative formatting methods to e.g. DOM localization.\n  That's a transform that can't really be automated,\n  so the best we can do is provide a much more Fluent-ish base for your work.\n  Applying the transformation via the JS file should also allow for decent variable name mapping,\n  which you'd have to otherwise do manually.\n\n- Much of the code under `devtools/` is using custom wrappers for localization code.\n  While these wrappers are not directly supported,\n  but it's still possible to force the `properties-to-ftl` JS processor to transform at least the message keys\n  by adding a line like this to the file:\n\n  ```js\n  Services.strings.createBundle('chrome://fake/locale/foo/bar.properties')\n  ```\n\n  and as long as the `bar.properties` filename is unique,\n  literal key value strings in that file can get appropriately transformed\n\n- When migrating messages with plural forms,\n  the JS calls targeting the `PluralForm` global are not automatically migrated.\n  If such messages include `#1`/`#2` variables,\n  you need to include their mapping to Fluent variables manually\n  in the generated FTL file as well as the Python migration script,\n  and remove the wrapping JS code that applies `.replace(\"#1\", ...)` transformations on the result.\n\n- When migrating messages that are used from C++,\n  you'll probably need to target the `.properties` file directly,\n  and manually fill out more variable names in the migration config.\n  Some examples for manually constructing the C++ arguments required by the `Localization` class are available in\n  [`TestLocalization.cpp`](https://searchfox.org/mozilla-central/source/intl/l10n/test/gtest/TestLocalization.cpp).\n\n## Development\n\nIf you do find a reason to fix/improve this tool,\nplease do file a PR to this repository with your work.\n\nWhen getting started you'll need to run:\n\n```\ngit submodule update --init\nnpm install\n```\n\nThe `resolve-chrome-uri` dependency is vendored in as a git submodule\nbecause it's honestly too hacky to release for wider use.\n\nThe \"tests\" that are included are a couple of example migration files.\n"
},
{
  "name": "addons-wp-headless",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      ".prettierignore",
      ".prettierrc",
      "LICENSE.txt",
      "Makefile",
      "README.md",
      "addons-wp-headless.php",
      "bin",
      "composer.json",
      "composer.lock",
      "package.json",
      "phpunit.xml.dist",
      "tests",
      "yarn.lock"
    ],
    "/.github": [
      "CODE_OF_CONDUCT.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "ZIP_FILE = addons-wp-headless.zip\n\ndefault: $(ZIP_FILE)\n\n$(ZIP_FILE): LICENSE.txt *.php\n\tzip $@ $^\n\nclean:\n\trm -f $(ZIP_FILE)\n",
  "readme": "# addons-wp-headless\n\n[![CircleCI](https://circleci.com/gh/mozilla/addons-wp-headless.svg?style=svg)](https://circleci.com/gh/mozilla/addons-wp-headless)\n\nA WordPress plugin for the AMO blog.\n\n## Usage\n\nInstall the plugin in the WordPress admin panel by uploading the zip file.\n\n## Development\n\nImportant: you'll need [composer](https://getcomposer.org/) to install the project's dev dependencies.\n\n### Running the test suite\n\nThis is a bit involved because there is no nice way to write \"unit\" tests with WordPress. We follow the recommended approach, which requires both a WordPress instance and MySQL database. The `bin/install-wp-tests.sh` script can be used to download and setup the WP instance as well as create the \"test\" database, which requires a local MySQL server, e.g. with Docker:\n\n```\n$ docker run --name mysql_addons_wp_headless -e MYSQL_ROOT_PASSWORD=pass -e MYSQL_DATABASE=addons_wp_headless -p 55001:3306 --rm mysql\n```\n\nAssuming you have a MySQL server running locally using the docker command above, run the following commands once:\n\n```\n# `5.8.3` is the WordPress version, use `latest` for the latest version or any\n# other version if you like.\n# `true` at the end of the command below skips the database creation since it is\n# created when the docker container starts\n$ ./bin/install-wp-tests.sh addons_wp_headless root pass '127.0.0.1:55001' 5.8.3 true\n$ composer install\n```\n\nThen, you can run the test suite with PHPUnit:\n\n```\n$ ./vendor/bin/phpunit\n```\n\n### Build the plugin (zip file)\n\n```\n$ make\n```\n\n## License\n\nThis plugin is released under the Mozilla Public License Version 2.0. See the bundled [LICENSE](./LICENSE.txt) file for details.\n"
},
{
  "name": "django-post-request-task",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "AUTHORS",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "post_request_task",
      "runtests.py",
      "setup.py",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "testenv:\n\tpip install --upgrade pip wheel tox\n\tpip install -e .\n\tpip install -e .[tests]\n\nflake8:\n\tflake8 post_request_task\n\nruntests:\n\tcoverage run --branch --source=post_request_task ./runtests.py\n\ncoveragereport:\n\tcoverage report --omit=post_request_task/test*\n\ntest: flake8 runtests coveragereport\n\n.PHONY: test runtests flake8 coveragereport\n",
  "readme": "django-post-request-task\n========================\n\n.. image:: https://circleci.com/gh/mozilla/django-post-request-task.svg?style=svg\n    :target: https://circleci.com/gh/mozilla/django-post-request-task\n\nA celery task class whose execution is delayed until after the request\nfinishes, using ``request_started`` and ``request_finished`` signals from django\nand thread locals.\n\nThis is useful if your views are wrapped in transactions (as they should if\nyou're making database modifications in them), as you can end up triggering a\ncelery task too soon before the transaction has been committed (or even trigger\na task when the corresponding transaction has been rolled back).\n\nBy listening to the `request_started` and `request_finished` django signals, we\ncan safely trigger a task after all transactions created from ``@atomic`` or\n``ATOMIC_REQUESTS`` have been committed.\n\nUsage\n-----\n\n.. code-block:: python\n\n    from celery import Celery\n    from post_request_task.task import PostRequestTask\n\n\n    app = Celery('myapp', task_cls=PostRequestTask)\n\n    @app.task\n    def my_task():\n        # If .delay() is called on this task inside a django request-response\n        # cycle it will be called once the request is finished, and not before.\n        pass\n\n\nOr, if you are using the decorator directly:\n\n.. code-block:: python\n\n    from post_request_task.task import shared_task\n\n    @shared_task\n    def my_task():\n        pass\n\n\nThat's it. If the task is called from outside the django request-response\ncycle, then it will be triggered normally.\n\nAs a bonus feature, if the same task is called with the same argument several\ntimes during a request-response cycle, it will only be queued up once.\n\n\nRunning tests\n-------------\n\n.. code-block:: sh\n\n    $ make testenv\n    $ make test\n\nBy default, tests are run with whatever django version is installed. If you want to run tests for other versions\nuse tox:\n\n\n.. code-block:: sh\n\n    $ make testenv\n    $ tox -e 3.7-2.0.x # or any other environment defined in our tox.ini\n"
},
{
  "name": "fxa-crypto-relier",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc",
      ".gitignore",
      ".npmignore",
      ".nvmrc",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "dist",
      "docs",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "webpack.config.js"
    ],
    "/docs": [
      "PRIVATE.md",
      "README.md",
      "template.hbs"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# fxa-crypto-relier [![npm version](https://badge.fury.io/js/fxa-crypto-relier.svg)](https://www.npmjs.com/package/fxa-crypto-relier) [![CircleCI](https://circleci.com/gh/mozilla/fxa-crypto-relier/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla/fxa-crypto-relier/tree/master)\n\nScoped Encryption Keys for Firefox Accounts Utility Library\n\n![](http://imgur.com/QH7eDUj.jpg)\n\n\n## Installation\n\n```sh\nnpm install fxa-crypto-relier --save\n```\n\n## Usage\n\nSee the [documentation](docs/README.md).\n\n## Local Development\n\n### Scripts\n\n* `npm run build` - build library\n* `npm run dev` - development mode\n* `npm test` - run tests\n\n## Dependencies\n\n- [base64url](https://github.com/brianloveswords/base64url): For encoding to/from base64urls\n- [node-hkdf](https://github.com/zaach/node-hkdf): HKDF key derivation function\n- [node-jose](https://github.com/cisco/node-jose.git): A JavaScript implementation of the JSON Object Signing and Encryption (JOSE) for current web browsers and node.js-based servers\n\n## License\n\nMPL-2.0\n"
},
{
  "name": "jestr-pioneer-shield-study",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "content.js",
      "dist",
      "docs",
      "feature.js",
      "karma.conf.js",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "src",
      "test",
      "tsconfig.json",
      "tslint.json",
      "web-ext-config.js",
      "webpack.config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# JESTr Pioneer Shield Study\n\n[![CircleCI badge](https://img.shields.io/circleci/project/github/motin/jestr-pioneer-shield-study/master.svg?label=CircleCI)](https://circleci.com/gh/motin/jestr-pioneer-shield-study/)\n[![Coverage Status](https://coveralls.io/repos/github/motin/jestr-pioneer-shield-study/badge.svg)](https://coveralls.io/github/motin/jestr-pioneer-shield-study)\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Analyzing data\n\nTelemetry pings are loaded into the encrypted Pioneer pipeline.\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n## History\n\n- Experiment v1 relaunch: [launch bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1559430) - [Experimenter entry](https://experimenter.services.mozilla.com/experiments/jestr-relaunch-identifying-web-browsing-patterns-in-the-pioneer-population/)\n- Experiment v1: [launch bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1496154) - [phd](https://docs.google.com/document/d/10JEZ9WgAqqsveYGMgSs8xJMPxQoSExAXx5pf_Fya3sY/edit) - [code](https://github.com/motin/jestr-pioneer-shield-study)\n"
},
{
  "name": "taar-experiment-v3-shield-study",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "analysis",
      "dist",
      "docs",
      "fetch_translations.py",
      "generate_html.py",
      "karma.conf.js",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "schemas",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# TAAR Experiment v3 - Shield Study\n\nTests the [Telemetry-Aware Add-on Recommender](https://github.com/mozilla/taar) (TAAR).\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Analyzing data\n\nTelemetry pings are loaded into S3 and re:dash. Sample query:\n\n* [All pings](https://sql.telemetry.mozilla.org/queries/55519/source#table)\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n# History\n\n* Experiment v1: [launch bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1400900) - [code](https://github.com/benmiroglio/taar-experiment)\n* Experiment v2: [launch bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1428308) - [phd](https://docs.google.com/document/d/1ZrfxNfBiEiAkqz4ZW9wmWfJF5sdfQg-Xq6_2mY1EXtI/edit) - [code](https://github.com/motin/taar-experiment-v2-shield-study)\n* Experiment v3: [launch bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1469546) - [phd](https://docs.google.com/document/d/1r8RC8BBjS9DQSBfanlD-MlCS4XWN3-bozyWOIFlD8aA/edit) - [code](https://github.com/motin/taar-experiment-v3-shield-study)\n"
},
{
  "name": "certificate-certainty",
  "files": {
    "/": [
      ".pre-commit-config.yaml",
      ".secrets.baseline",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Purpose\n\nThe purpose of this tooling is to automate the detection of expiring certs, and\nthen check if the renewal has been generated and deployed. Ultimately, tickets\nwill be automatically opened.\n\n# Approach\n\nThe tool is invoked with a future date and a list of hosts (or domains). The\ntool processes all hosts looking for certificates that are currently unexpired,\nand checks to see if they _will_ be expired on the specified future date. If\nanother cert will be valid on the future date (i.e. the cert has been\nreissued), then the host is checked to see if the newer cert is deployed which\nwill be valid on the future date.\n\nFor each host, certificate tuple, the following attributes are then known:\n- `needs renewal` - true if this cert will expire before the future date\n- `host reachable` - true if we can connect to the host from the tool\n- `certificate deployed` - true if the deployed certificate will be valid on\n  the future date. Only meaningful if the host is reachable\n\nCertificate information for a host is retrieved from a Certificate Transparency\ndata store. Note that there may be multiple unexpired certificates for a host.\nUsually, just 2: one that will be expired on the future date, and one that will\nbe valid.\n\nAn exemption list, containing items to skip (domains, hosts) is consulted, so\nthat false positives can be removed from the next report.\n\nTODO: create exemption list maintenance tool to remove decommissioned exemptions after the current cert expires.\n\nTODO: some domains have multiple valid certs at any point in time. Consider treating as \"deployed\" only the cert being tested. (Rather than any valid at future date.)\n\nTODO: date reported for expiration in \"re-issued, unreachable\" should be stable, and the \"soonest\" expiration date. (This may not be meaningful when >2 certs exist.)\n\n# Operation\n\n## Configuration\n\nWhile you can enter host names on the command line, you may prefer to keep all\nrelevant hosts in a file.\n\nYou may also want to develop and maintain a list of exceptions. The exceptions\nfile is formatted as yaml, and is described in the source file. The default\nname is `exceptions.yaml` and there is a schema available as\n`exceptions_schema.json` which is helpful if your editor supports them. (See\ndevelopment section for more information on the schema.)\n\nThe exceptions file supports specifying:\n- Certificates for hosts we know are being decommissioned, so no renewal is\n  needed. These exceptions are manually entered after someone confirms the\n  decom.\n- Certificates for hosts that are not on the public net, and thus can not be\n  verified as installed. These exceptions are manually entered after someone\n  with access confirms the installation.\n- Domains that never contain long lived production domains. E.g. a development\n  domain with short lived instances.\n\n\n## Execution\n\nSet up a virtual environment, then invoke \"`report-tls-\"certs --help`\" to view the options.\n\n# Development\n\nThis is a normal Python script, with one embellishment. All structured data is\ndefined in the source, using [Pydantic][pydantic]. This allows a schema to be\ngenerated via:\n```bash\n$ ./report-tls-certs --generate-schema >exceptions_schema.json\n```\nHopefully, you're using an editor that can make use of the schema.\n\n\n[pydantic]: https://pydantic-docs.helpmanual.io/\n"
},
{
  "name": "tf-actions",
  "files": {
    "/": [
      "README.md",
      "ci",
      "matrixify"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Terraform GitHub Actions\n\nThis repository contains GitHub Actions Composite Actions used for Terraform Automation.\n\n## Desired CI Workflow\n\n1. Discover each Terraform module or project directory & if it has changed & iterate rest of steps on this;\n2. Terraform init --backend=false, format, validate checks;\n3. Required metadata checks:\n    i. terraform.required_version (if Terraform instantiation code) exists;\n    i. terraform.backend.gcs.prefix (if Terraform instantiation code) is same as that Terraform\u2019s directory name;\n4. Required files & documentation checks;\n    i. Has README.md (though not necessarily that the README matches a terraform-docs output, to allow for edits);\n    i. Warning if .terraform-version doesn\u2019t exist;\n    i. Warning if .terraform.lock.hcl doesn\u2019t exist;\n5. Security checks:\n    i. secrets scanning;\n    i. approved providers & modules only being used (maybe via checkov or conftest);\n6. (someday goal) tflint run with cloud provider support;\n7. (someday goal) Terratest style Terraform configuration unit tests demoed at least.\n\n## Matrixify\n\nThe `mozilla/tf-actions/matrixify` action will return a list of directories that contain a versions.tf & where there has been a change in the latest git commit(s). This is useful for running a set of commands in each Terraform root directory under a given project.\n\nInputs:\n* `default_branch`: The default branch of the repository. Defaults to main. Used for first commit on a new branch diffs.\n* `filter_file`: Files to filter for as a representation of a Terraform module or project direcotry. Defaults to \"version.tf\".\n* `ignore_dir`: Directories to filter out of the resulting matrix. Defaults to \"ignoreme\".\n* `ignore_path`: Filepaths to filter out of the resulting matrix. Defaults to \"*.ignore\".\n\n```\njobs:\n  matrixify:\n    name: Matrixify\n    runs-on: ubuntu-latest\n\n    steps:\n    - name: Get Changed Terraform directories\n      id: search\n      uses: mozilla/tf-actions/matrixify@main\n    - name: Outputs\n      run: echo \"${{ steps.search.outputs.matrix }}\"\n```\n\n## CI\n\nThe `mozilla/tf-actions/ci` action will run stateless Terraform automated checks on a given Terraform root directory.\n\nInputs:\n* `tfpath`: The directory of the Terraform project or module to run the checks on. Defaults to \".\".\n* `tfmodule`: Whether or not the Terrform checks are running on a module. Matches on \"true\" or \"false\". Defaults to \"false\".\n* `workspace`: The Terraform workspace to run the checks on. Defaults to \"default\".\n\n```\njobs:\n  terraform-ci:\n    name: Terraform CI on \"${{ matrix.directory }}\"\n    needs: matrixify\n    runs-on: ubuntu-latest\n    strategy:\n        matrix: ${{fromJson(needs.matrixify.outputs.matrix)}}\n\n    steps:\n    - id: terraform-checks\n      name: Terraform Checks\n      uses: mozilla/tf-actions/ci@main\n      with:\n        tfpath: ${{ matrix.directory }}\n```\n"
},
{
  "name": "mozilla-campus-clubs",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "LICENSE",
      "README.md",
      "_config-dev.yml",
      "_config.yml",
      "_includes",
      "_layouts",
      "_pages",
      "google82794cfa7bbf94c1.html",
      "index.html",
      "static"
    ]
  },
  "makefile": null,
  "readme": "# [Mozilla Campus Clubs](https://campus.mozilla.community/)\n\n## Hack the site\n\nThank you for contributing! Just follow these simple steps\n\nClone the repo\n\n```sh\n$ git clone https://github.com/mozilla/mozilla-campus-clubs\n```\n\nTo install the dependencies. First you need change your directory to mozilla-campus-clubs\n\n```sh\n$ cd mozilla-campus-clubs\n$ bundle install\n```\n\nBuild the site using the following developer config\n\n```sh\n$ bundle exec jekyll build --config ./_config-dev.yml\n```\n\nThis builds the website under the `_site` folder. The simplest way to browse it is to use python's http server. For that. you can follow the following steps\n\n```sh\ncd _site\npython3 -m http.server\n```\n\nYou will be able to access the site at [http://127.0.0.1:8000/](http://127.0.0.1:8000/)\n\nIf you want to contribute, Just Clone this repo, create an issue first and then a pull request to submit changes.\n"
},
{
  "name": "ldap-access-log-humanizer",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "Makefile",
      "README.md",
      "examples",
      "humanizer-logrotate",
      "humanizer-rsyslog.conf",
      "humanizer.py",
      "humanizer.service",
      "humanizer_settings.json.default",
      "ldap_access_log_humanizer",
      "requirements.txt",
      "restart.sh",
      "setup.py",
      "tests"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n# Copyright (c) 2014 Mozilla Corporation\n#\nPACKAGE := ldap_access_log_humanizer\n\n.PHONY:list ## List all make targets\nlist:\n\t@echo 'Available make targets:'\n\t@grep '^[^#[:space:]^\\.PHONY.*].*:' Makefile\n\n.PHONY: dependencies ## install all dependencies\ndependencies:\n\tpip install -e .\n\tpip install -r requirements.txt\n\n.PHONY: tests ## run all unit tests\ntests:\n\tpytest tests/unit-tests/\npep8:\n\t@find ./* `git submodule --quiet foreach 'echo -n \"-path ./$$path -prune -o \"'` -type f -name '*.py' -exec pep8 --show-source --max-line-length=100 {} \\;\n\npylint:\n\t@find ./* `git submodule --quiet foreach 'echo -n \"-path ./$$path -prune -o \"'` -type f -name '*.py' -exec pylint -r no --disable=locally-disabled --rcfile=/dev/null {} \\;\n\nrpm:\n\tfpm -s python -t rpm --after-install restart.sh --after-upgrade restart.sh --rpm-dist \"$$(rpmbuild -E '%{?dist}' | sed -e 's#^\\.##')\" --iteration 2 setup.py\n\t@rm -rf build $(PACKAGE).egg-info\n\nclean:\n\trm -f humanizer/*.pyc tests/unit-tests/*.pyc\n\trm -rf build humanizer.egg-info\n",
  "readme": "# ldap-access-log-humanizer\nA script to help make OpenLDAP access logs more readable for humans and machines\n\n## Example\n_____\nThis will convert LDAP access logs in this format:\n```\nOct 26 03:30:53 ldap.example.com slapd[11086]: conn=6832973 fd=24 ACCEPT from IP=192.168.1.1:43050 (IP=0.0.0.0:389)\nOct 26 03:30:53 ldap.example.com slapd[11086]: conn=6832973 op=0 EXT oid=1.3.6.1.4.1.1466.20037 \nOct 26 03:30:53 ldap.example.com slapd[11086]: conn=6832973 op=0 STARTTLS \nOct 26 03:30:53 ldap.example.com slapd[11086]: conn=6832973 op=0 RESULT oid= err=0 text=  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 fd=24 TLS established tls_ssf=256 ssf=256  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=1 BIND n=\"uid=bind-generateusers,ou=logins,dc=example\" method=128  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=1 BIND dn=\"uid=bind-generateusers,ou=logins,dc=example\" mech=SIMPLE ssf=0  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=1 RESULT tag=97 err=0 text=  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=2 SRCH base=\"ou=groups,dc=example\" scope=2 deref=0 filter=\"(cn=group_name)\"  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=2 SRCH attr=memberUid  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=2 SEARCH RESULT tag=101 err=0 nentries=1 text=  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=3 SRCH base=\"o=net,dc=example\" scope=2 deref=0 filter=\"(objectClass=posixAccount)\"  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=3 SRCH attr=sshPublicKey loginShell homeDirectory mail uidNumber uid  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=3 SEARCH RESULT tag=101 err=0 nentries=1626 text=  \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 op=4 UNBIND \nOct 26 03:30:54 ldap.example.com slapd[11086]: conn=6832973 fd=24 closed  \n```\nto something that looks like this:\n\n```\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:53', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': False, 'fd_id': 24, 'verb': 'ACCEPT', 'details': 'from IP=192.168.1.1:43050 (IP=0.0.0.0:389)'}\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:53', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': False, 'op_id': 0, 'requests': [{'verb': 'EXT', 'details': ['oid=1.3.6.1.4.1.1466.20037']}, {'verb': 'STARTTLS', 'details': []}], 'response': {'verb': 'RESULT', 'details': ['err=0', 'oid=', 'text='], 'error': 'LDAP_SUCCESS'}}\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:54', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': True, 'fd_id': 24, 'verb': 'TLS', 'details': 'established tls_ssf=256 ssf=256'}\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:54', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': True, 'op_id': 1, 'requests': [{'verb': 'BIND', 'details': ['dn=\"uid=bind-generateusers,ou=logins,dc=example\"', 'method=128']}, {'verb': 'BIND', 'details': ['dn=\"uid=bind-generateusers,ou=logins,dc=example\"', 'mech=SIMPLE', 'ssf=0']}], 'response': {'verb': 'RESULT', 'details': ['err=0', 'tag=97', 'text='], 'error': 'LDAP_SUCCESS'}}\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:54', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': True, 'op_id': 2, 'requests': [{'verb': 'SRCH', 'details': ['base=\"ou=groups,dc=example\"', 'scope=2', 'deref=0', 'filter=\"(cn=group_name)\"']}, {'verb': 'SRCH', 'details': ['attr=memberUid']}], 'response': {'verb': 'SEARCH RESULT', 'details': ['err=0', 'nentries=1', 'tag=101', 'text='], 'error': 'LDAP_SUCCESS'}}\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:54', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': True, 'op_id': 3, 'requests': [{'verb': 'SRCH', 'details': ['base=\"o=net,dc=example\"', 'scope=2', 'deref=0', 'filter=\"(objectClass=posixAccount)\"']}, {'verb': 'SRCH', 'details': ['attr=sshPublicKey', 'loginShell', 'homeDirectory', 'mail', 'uidNumber', 'uid']}], 'response': {'verb': 'SEARCH RESULT', 'details': ['err=0', 'nentries=1626', 'tag=101', 'text='], 'error': 'LDAP_SUCCESS'}}\n{'conn_id': '6832973', 'time': 'Oct 26 03:30:54', 'client': '192.168.1.1', 'server': 'ldap.example.com', 'tls': True, 'fd_id': 24, 'verb': 'closed', 'details': ''}\n```\n\nwhich is more readable by humans and machines. The benefit to this format is that every operation gets its own line of log output, with all of the relevant metadata included on that line, such as connection number, whether TLS is used for this operation, the client IP, the request and the response.\n\n## Usage\n____\nFor testing purposes, use just the command line utility like this:\n```\npython humanizer.py --noconfig --input_file_name /var/log/ldap/ldap.log --output_stdout\n```\nThis will read the specified OpenLDAP log file and dump the humanized output to stdout\n\nTo start a syslog daemon:\n```\npython humanizer.py --noconfig --input_type syslog --daemonize --host 0.0.0.0 --port 1514 --output_file /var/log/humanizer.log\n```\nThis will open a listener on 0.0.0.0:1514 (udp) and accept syslog messages and write the humanized logs to the specified log file.\n\nQuick and dirty:\n```\ncat /var/log/ldap/ldap.log | python humanizer.py --noconfig --output_stdout\n```\n\nFor production usage, use the humanizer_settings.json file to pass the configuration and use systemd or other tool to start the listener\n## Supported inputs and outputs\n___________________________\nThe humanizer can read logs via stdin, a specified file or from syslog over UDP.\n\nThe humanizer can output humanized logs to stdout, stderr, a specified file, or forward to another syslog server. It can do any combination of output types, so you can have one instance write a local file, dump to stdout, stderr, or any combination of these.\n"
},
{
  "name": "moz_crlite_query",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "crlite_query",
      "requirements.txt",
      "setup.py"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Query CRLite data\n\nThis tool queries the published Mozilla CRLite database to determine certificate status.\n\nIt maintains a local database in your `~/.crlitedb/` folder, which is updated when older than six hours.\n\nIt works on a best-effort basis, and certificates with malformed serial numbers or other serious encoding issues might not be identified correctly, which would lead to false negatives. For a more bulletproof implementation of a CRLite decoder, you might want to consider building one atop [the rust-cascade](https://github.com/mozilla/rust-cascade) project, or simply rework the ASN.1 parsing here to reveal the exact values from the encoding without converting to intermediate Python types.\n\nInstall from [PyPi](https://pypi.org/project/moz-crlite-query/):\n\n```sh\npip install moz_crlite_query\n```\n\nCurrently, it expects PEM-formatted certificate data, and can process many at once:\n\n```sh\nfor id in 77575263 1988442812 1485147627 2680822568; do\n  curl --silent https://crt.sh/?d=${id} > /tmp/${id}.pem\ndone\nmoz_crlite_query /tmp/*.pem --hosts getfirefox.com\nINFO:query_cli:Database was updated at 2020-04-08 16:06:39.400780, skipping.\nINFO:query_cli:Status: 2195 Intermediates, Current filter: 2020-04-02T06:00:00Z-full with 18 layers and 12922536 bit-count, 2 stash files with 3307 stashed revocations, up-to-date as of 2020-04-02 12:00:00.\n/tmp/1485147627.pem      Issuer: CN=Let's Encrypt Authority X3,O=Let's Encrypt,C=US\n                         Enrolled in CRLite: \u274c\n                         Result: \u274c Not Enrolled \u274c\n/tmp/1988442812.pem      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                         Enrolled in CRLite: \u2705\n                         Revoked via CRLite filter: 2020-04-02T06:00:00Z-full\n                         Result: \u26d4\ufe0f Revoked \u26d4\ufe0f\n/tmp/2680822568.pem      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                         Enrolled in CRLite: \u2705\n                         Result: \ud83d\udc07 Too New \ud83d\udc07\n/tmp/77575263.pem      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                       Enrolled in CRLite: \u2705\n                       Result: \u23f0 Expired \u23f0\ngetfirefox.com:443      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                        Enrolled in CRLite: \u2705\n                        Result: \ud83d\udc4d Valid \ud83d\udc4d\n```\n\nYou can also pipe in PEM data:\n\n```sh\ncurl --silent https://crt.sh/?d=1988442812 https://crt.sh/?d=1871771575 | moz_crlite_query -v -\nINFO:query_cli:Database was updated at 2020-04-08 16:06:39.400780, skipping.\nDEBUG:query_cli:Database was last updated 2:27:19.869039 ago.\nINFO:query_cli:Status: 2195 Intermediates, Current filter: 2020-04-02T06:00:00Z-full with 18 layers and 12922536 bit-count, 2 stash files with 3307 stashed revocations, up-to-date as of 2020-04-02 12:00:00.\n<stdin>      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n             Enrolled in CRLite: \u2705\n             CertID(e6426f344330d0a8eb080bbb7976391d976fc824b5dc16c0d15246d5148ff75c-0371b58a86f6ce9c3ecb7bf42f9208fc)\n             Revoked via CRLite filter: 2020-04-02T06:00:00Z-full\n             Result: \u26d4\ufe0f Revoked \u26d4\ufe0f\n<stdin>      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n             Enrolled in CRLite: \u2705\n             CertID(e6426f344330d0a8eb080bbb7976391d976fc824b5dc16c0d15246d5148ff75c-0f7d9e589e0dd146f55bc6530139d3a6)\n             Result: \ud83d\udc4d Valid \ud83d\udc4d\n```\n\nYou can feed in files containing individual lines of the form `host:port`:\n\n```sh\ncat >/tmp/top4.txt <<EOF\napple.com\nyoutube.com\nwww.google.com:443\n# This is definitely half of my top 8 spaces\nwww.blogger.com\nEOF\n\nmoz_crlite_query --hosts mozilla.com firefox.com --hosts getfirefox.net --hosts-file /tmp/top4.txt\nINFO:query_cli:Database was updated at 2020-07-16 16:10:41.545092, skipping.\nINFO:query_cli:Status: 2084 Intermediates, Current filter: 2020-06-18T18:00:18+00:00Z-full with 27 layers and 41536664 bit-count, 0 stash files with 0 stashed revocations, up-to-date as of 2020-06-18 18:00:18+00:00 (28 days, 5:34:39.044502 ago).\nmozilla.com:443      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                     Enrolled in CRLite: \u2705\n                     CertID(e6426f344330d0a8eb080bbb7976391d976fc824b5dc16c0d15246d5148ff75c-019d2b994ec99445c735d2a6d739e43a)\n                     Result: \ud83d\udc4d Valid \ud83d\udc4d\nfirefox.com:443      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                     Enrolled in CRLite: \u2705\n                     CertID(e6426f344330d0a8eb080bbb7976391d976fc824b5dc16c0d15246d5148ff75c-019d2b994ec99445c735d2a6d739e43a)\n                     Result: \ud83d\udc4d Valid \ud83d\udc4d\ngetfirefox.net:443      Issuer: CN=DigiCert SHA2 Secure Server CA,O=DigiCert Inc,C=US\n                        Enrolled in CRLite: \u2705\n                        CertID(e6426f344330d0a8eb080bbb7976391d976fc824b5dc16c0d15246d5148ff75c-019d2b994ec99445c735d2a6d739e43a)\n                        Result: \ud83d\udc4d Valid \ud83d\udc4d\napple.com:443      Issuer: CN=DigiCert SHA2 Extended Validation Server CA-3,OU=www.digicert.com,O=DigiCert\\, Inc.,C=US\n                   Enrolled in CRLite: \u2705\n                   CertID(9704cf37ad50839fb5a8053e32293db056835f984ba360073fcd1847e22037a3-0e7b3ab429e183d07a4fc4dbe9c4c191)\n                   Result: \ud83d\udc07 Too New \ud83d\udc07\nyoutube.com:443      Issuer: CN=GTS CA 1O1,O=Google Trust Services,C=US\n                     Enrolled in CRLite: \u2705\n                     CertID(6193e04d9fb0a0d0820885b72c7d82c5078bcc1ff59b8d907024c149d81aca3b-7e10d901f7ac03cd080000000047ef8e)\n                     Result: \ud83d\udc4d Valid \ud83d\udc4d\nwww.google.com:443      Issuer: CN=GTS CA 1O1,O=Google Trust Services,C=US\n                        Enrolled in CRLite: \u2705\n                        CertID(6193e04d9fb0a0d0820885b72c7d82c5078bcc1ff59b8d907024c149d81aca3b-25eb382df564aeb608000000004aaba0)\n                        Result: \ud83d\udc07 Too New \ud83d\udc07\nwww.blogger.com:443      Issuer: CN=GTS CA 1O1,O=Google Trust Services,C=US\n                         Enrolled in CRLite: \u2705\n                         CertID(6193e04d9fb0a0d0820885b72c7d82c5078bcc1ff59b8d907024c149d81aca3b-be84ce8731c637490200000000715c1a)\n```\n"
},
{
  "name": "pontoon-intro",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "locales"
    ]
  },
  "makefile": null,
  "readme": "Pontoon Intro\n=============\n\nThese are the strings for the in-context localization demo page of [Pontoon](https://github.com/mozilla/pontoon).\n"
},
{
  "name": "perf-youtube-playback",
  "files": {
    "/": [
      ".circleci",
      ".utils",
      "2019",
      "2020",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# perf-youtube-playback\n\nThis repository is a mirror of the 2019 branch from the\nhttps://github.com/youtube/js_mse_eme/ repository,\nwith some customization for Raptor applied.\n\nWith the *raptor* branch checked out, it is used for \nexecuting the *raptor-youtube-playback* performance tests.\n\nThe description of the original repository is:\n\n> js_mse_eme is an externally-published tool that is aimed to test the validity\nof a browser's HTML5 Media Source Extension and Encrypted Media Extension\nimplementations. As an added bonus, this tool is used by YouTube to certify\nhardware electronic devices (e.g. TVs, settop boxes, etc...).\n>\n> Year specific tests and tools are in the respective branches.\n"
},
{
  "name": "dockutil",
  "files": {
    "/": [
      "README.md",
      "build.sh",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "**INTRODUCTION**\n\ndockutil is a command line utility for managing Mac OS X dock items.\nIt is currently written in Python and makes use of plistlib module included in Mac OS X.\n- Compatible with Mac OS X 10.9.x thru 10.13 (use 1.x version for older\n  OSes)\n- Add, List, Move, Find, Remove Dock Items\n- Supports Applications, Folders, Stacks, URLs. \n- Can act on a specific dock plist or every dock plist in a folder of home directories\n\n**LICENSE**\n\n[Apache 2](http://www.apache.org/licenses/LICENSE-2.0)\n\n**CHANGELOG**\n\nVersion 2.0.5\n- 10.12 fix for Dock restart\n- Add and remove spacer tiles\n\nVersion 2.0.4\n- Allow removal by app bundle identifier (credit to Yoann Gini)\n- Bug fixes for plist quoting (credit to VitosX)\n\nVersion 2.0.3\n- Wait for dock to be setup by Apple before modifying (useful for first login scripts)\n\nVersion 2.0.2\n- Bug Fix for 10.9.x\n\nVersion 2.0.1\n\n- Yosemite compatibility\n- Support for multiple removals\n\nVersion 2.0.0\n\n- Remove restart of cfprefsd in favor of using defaults\n- Bumped to version 2 because some backend changes may break compatibility with older OS versions\n- Please test and report any issues\n\nVersion 1.1.4\n\n- Restart cfprefsd before restarting Dock to ensure settings are read\n\nVersion 1.1.3\n\n- fix issue with missing labels and removals\n\nVersion 1.1.2\n\n- fix issue with replacing a url dock item\n- add legacy support --hupdock option for backward compatibility\n- fix paths with spaces when passing full path to plist\n\n\nVersion 1.1\n\n- fixes many issues with paths (should now work with Default User Template)\n- adds option to not restart the dock (--no-restart)\n- fixes issue where item would be added multiple times\n(use --replacing to update an existing item)\n- resolves deprecation warnings\n- adds option to remove all items (--remove all)\n- fix issue with removals when a url exists in a dock\n- adds option --version to output version\n\n\n**USAGE**\n\n    usage:     dockutil -h\n    usage:     dockutil --add <path to item> | <url> [--label <label>] [ folder_options ] [ position_options ] [--no-restart] [ plist_location_specification ]\n    usage:     dockutil --remove <dock item label> | <app bundle id> | all | spacer-tiles [--no-restart] [ plist_location_specification ]\n    usage:     dockutil --move <dock item label>  position_options [ plist_location_specification ]\n    usage:     dockutil --find <dock item label> [ plist_location_specification ]\n    usage:     dockutil --list [ plist_location_specification ]\n    usage:     dockutil --version\n\n    position_options:\n      --replacing <dock item label name>                            replaces the item with the given dock label or adds the item to the end if item to replace is not found\n      --position [ index_number | beginning | end | middle ]        inserts the item at a fixed position: can be an position by index number or keyword\n      --after <dock item label name>                                inserts the item immediately after the given dock label or at the end if the item is not found\n      --before <dock item label name>                               inserts the item immediately before the given dock label or at the end if the item is not found\n      --section [ apps | others ]                                   specifies whether the item should be added to the apps or others section\n\n    plist_location_specifications:\n      <path to a specific plist>                                    default is the dock plist for current user\n      <path to a home directory>\n      --allhomes                                                    attempts to locate all home directories and perform the operation on each of them\n      --homeloc                                                     overrides the default /Users location for home directories\n\n    folder_options:\n      --view [grid|fan|list|auto]                                   stack view option\n      --display [folder|stack]                                      how to display a folder's icon\n      --sort [name|dateadded|datemodified|datecreated|kind]         sets sorting option for a folder view\n\n    Examples:\n      The following adds TextEdit.app to the end of the current user's dock:\n               dockutil --add /Applications/TextEdit.app\n\n      The following replaces Time Machine with TextEdit.app in the current user's dock:\n               dockutil --add /Applications/TextEdit.app --replacing 'Time Machine'\n\n      The following adds TextEdit.app after the item Time Machine in every user's dock on that machine:\n               dockutil --add /Applications/TextEdit.app --after 'Time Machine' --allhomes\n\n      The following adds ~/Downloads as a grid stack displayed as a folder for every user's dock on that machine:\n               dockutil --add '~/Downloads' --view grid --display folder --allhomes\n\n      The following adds a url dock item after the Downloads dock item for every user's dock on that machine:\n               dockutil --add vnc://miniserver.local --label 'Mini VNC' --after Downloads --allhomes\n\n      The following removes System Preferences from every user's dock on that machine:\n               dockutil --remove 'System Preferences' --allhomes\n\n      The following moves System Preferences to the second slot on every user's dock on that machine:\n               dockutil --move 'System Preferences' --position 2 --allhomes\n\n      The following finds any instance of iTunes in the specified home directory's dock:\n               dockutil --find iTunes /Users/jsmith\n\n      The following lists all dock items for all home directories at homeloc in the form: item<tab>path<tab><section>tab<plist>\n               dockutil --list --homeloc /Volumes/RAID/Homes --allhomes\n\n      The following adds Firefox after Safari in the Default User Template without restarting the Dock\n               dockutil --add /Applications/Firefox.app --after Safari --no-restart '/System/Library/User Template/English.lproj'\n\n      The following adds a spacer tile in the apps section after Mail\n               dockutil --add '' --type spacer --section apps --after Mail\n\n      The following removes all spacer tiles\n               dockutil --remove spacer-tiles\n\n    Notes:\n      When specifying a relative path like ~/Documents with the --allhomes option, ~/Documents must be quoted like '~/Documents' to get the item relative to each home\n\n\n\n**LIMITATIONS AND DEPENDENCIES**\n\nRequires plistlib\n\n"
},
{
  "name": "spikes",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "bin",
      "config",
      "requirements.txt",
      "runtime.txt",
      "spikes",
      "static",
      "templates",
      "test-requirements.txt",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# spikes\n> Library to detect spikes in data coming from Socorro.\n\n\n[![Build Status](https://api.travis-ci.org/mozilla/spikes.svg?branch=master)](https://travis-ci.org/mozilla/spikes)\n[![codecov.io](https://img.shields.io/codecov/c/github/mozilla/spikes/master.svg)](https://codecov.io/github/mozilla/spikes?branch=master)\n\n## Setup\n\nInstall the prerequisites via `pip`:\n```sh\nsudo pip install -r requirements.txt\n```\n\n## Running tests\n\nInstall test prerequisites via `pip`:\n```sh\nsudo pip install -r test-requirements.txt\n```\n\nRun tests:\n```sh\ncoverage run --source=spikes -m unittest discover tests/\n```\n\n## Bugs\n\nhttps://github.com/mozilla/spikes/issues/new\n\n## Contact\n\nEmail: release-mgmt@mozilla.com\n"
},
{
  "name": "language-mapping-list",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bower.json",
      "language-mapping-list.js",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "language-mapping-list\n=====================\n\nList of all the known languages in their English and Native name with locales.\n\nThere are over 200 languages available in the list.\n\n`$ npm install langmap`\n\nUsage:\n\n```\nvar langmap = require(\"langmap\");\n\n\"Native\" => English (US)\nvar native = langmap[\"en-US\"][\"nativeName\"];\n\"Native\" => \u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22\nvar native = langmap[\"th\"][\"nativeName\"];\n\"English\" => Thai\nvar native = langmap[\"th\"][\"englishName\"];\n\n```"
},
{
  "name": "pretty-fast",
  "files": {
    "/": [
      ".circleci",
      ".eslintrc",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE.md",
      "README.md",
      "__snapshots__",
      "bin",
      "package-lock.json",
      "package.json",
      "pretty-fast.js",
      "test.js"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Pretty Fast\n\nPretty Fast is a source-map-generating JavaScript pretty printer, that is pretty\nfast.\n\n[![Build Status](https://travis-ci.org/mozilla/pretty-fast.png?branch=master)](https://travis-ci.org/mozilla/pretty-fast)\n\n## Install\n\n    npm install pretty-fast\n\n## Usage\n\n    var prettyFast = require(\"pretty-fast\");\n\n    var uglyJS = \"function ugly(){code()}\";\n\n    var pretty = prettyFast(uglyJS, {\n      url: \"test.js\",\n      indent: \"  \"\n    });\n\n    console.log(pretty.code);\n    // function ugly() {\n    //   code()\n    // }\n\n    console.log(pretty.map);\n    // [object SourceMapGenerator]\n\n(See the [mozilla/source-map][0] library for information on SourceMapGenerator\ninstances, and source maps in general.)\n\n[0]: https://github.com/mozilla/source-map\n\n## Options\n\n* `url` - The URL of the JavaScript source being prettified. Used in the\n  generated source map. If you are prettifying JS that isn't from a file or\n  doesn't have a URL, you can use a dummy value, such as \"(anonymous)\".\n\n* `indent` - The string that you want your code indented by. Most people want\n  one of `\"  \"`, `\"    \"`, or `\"\\t\"`.\n\n* `ecmaVersion` - Indicates the ECMAScript version to parse. \n   See acorn.parse documentation for more details. Defaults to `\"latest\"`.\n\n## Issues\n\n[Please use Bugzilla](https://bugzilla.mozilla.org/enter_bug.cgi?product=Firefox&component=Developer%20Tools%3A%20Debugger)\n\n## Goals\n\n* To be pretty fast, while still generating source maps.\n\n* To avoid fully parsing the source text; we should be able to get away with\n  only a tokenizer and some heuristics.\n\n* Preserve comments.\n\n* Pretty Fast should be able to run inside Web Workers.\n\n## Non-goals\n\n* Extreme configurability of types of indentation, where curly braces go, etc.\n\n* To be the very fastest pretty printer in the universe. This goal is\n  unattainable given that generating source maps is a requirement. We just need\n  to be Pretty Fast.\n\n* To perfectly pretty print *exactly* as you would have written the code. This\n  falls out from both not wanting to support extreme configurability, and\n  avoiding full on parsing. We just aim to pretty print Pretty Well.\n"
},
{
  "name": "eslint-plugin-fxa",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "lib",
      "package-lock.json",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# eslint-plugin-fxa\n\nLicense MPL-2.0\n"
},
{
  "name": "janus-plugin-rs",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "jansson-sys",
      "janus-plugin-sys",
      "rustfmt.toml",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# janus-plugin-rs\n\n[![Documentation](https://docs.rs/janus-plugin/badge.svg)](https://docs.rs/janus-plugin/)\n[![janus-plugin](https://img.shields.io/crates/v/janus-plugin.svg)](https://crates.io/crates/janus-plugin)\n\nLibrary for creating Rust plugins and event handlers for [Janus](https://janus.conf.meetecho.com/). Still moderately unstable.\n\n``` toml\n[dependencies]\njanus-plugin = \"0.13.0\"\n```\n\n## Compatibility\n\nCurrently compatible with Janus versions >= 0.10.9; Janus makes breaking changes relatively frequently to\nthe plugin API, so expect this library to require updating and recompilation for plugins to continue to work with new\nJanus versions.\n\n## Building\n\nRequires the [Jansson](http://www.digip.org/jansson/) native library (Ubuntu: `libjansson-dev`) to link against; tested as compatible with versions >= 2.5.\n\n```\n$ cargo build --all\n```\n\n## Testing\n\n```\n$ cargo test --all\n```\n\n## Basic usage\n\nJanus expects to dynamically link plugins as libraries and then call a `create` function on them to return a\n`janus_plugin` struct, which has a variety of function pointers that Janus will call when plugin-related events in the\ncore happen.\n\nThese bindings provide a `build_plugin!` macro that accepts as arguments plugin metadata and a set of (`extern C`) Rust\nfunctions, producing a Rust version of the `janus_plugin` struct, and an `export_plugin!` macro that defines the\n`create` function to return that struct. So to implement a plugin, you should write some handler functions, and then use\nthose macros like so:\n\n``` Rust\nuse std::os::raw::c_char;\n\n// helper macro for generating C-style strings from Rust string literals at compile time\nmacro_rules! c_str {\n    ($lit:expr) => {\n        unsafe {\n            std::ffi::CStr::from_ptr(concat!($lit, \"\\0\").as_ptr() as *const c_char)\n        }\n    }\n}\n\nextern \"C\" fn init(callbacks: *mut PluginCallbacks, config_path: *const c_char) -> c_int {\n    janus_info!(\"Plugin loaded!\");\n    0\n}\n\nextern \"C\" fn destroy() {\n    janus_info!(\"Plugin destroyed!\");\n}\n\n// ...other handlers omitted: see\n// https://janus.conf.meetecho.com/docs/plugin_8h.html#details\n\nconst PLUGIN: Plugin = build_plugin!(\n    LibraryMetadata {\n        // The Janus plugin API version. The version compiled into the plugin\n        // must be identical to the version in the Janus which loads the plugin.\n        api_version: 15,\n        // Incrementing plugin version number for your own use.\n        version: 1,\n        // Human-readable metadata which Janus can query.\n        name: c_str!(\"My plugin name\"),\n        package: c_str!(\"My plugin package name\"),\n        version_str: c_str!(env!(\"CARGO_PKG_VERSION\")),\n        description: c_str!(env!(\"CARGO_PKG_DESCRIPTION\")),\n        author: c_str!(env!(\"CARGO_PKG_AUTHORS\")),\n    },\n    init,\n    destroy,\n    // ...other handlers omitted: see\n    // https://janus.conf.meetecho.com/docs/plugin_8h.html#details\n);\n\nexport_plugin!(&PLUGIN);\n```\n\n## Examples\n\nHere are some projects which are using these bindings:\n\n* https://github.com/mozilla/janus-plugin-sfu\n* https://github.com/ivanovaleksey/janus-echotest-rs\n* https://github.com/netology-group/janus-conference/\n"
},
{
  "name": "libwebrtc",
  "files": {
    "/": [
      ".clang-format",
      ".git-blame-ignore-revs",
      ".gitignore",
      ".gn",
      ".style.yapf",
      ".vpython",
      ".vpython3",
      "AUTHORS",
      "BUILD.gn",
      "CODE_OF_CONDUCT.md",
      "DEPS",
      "DIR_METADATA",
      "ENG_REVIEW_OWNERS",
      "LICENSE",
      "OWNERS",
      "PATENTS",
      "PRESUBMIT.py",
      "README.chromium",
      "README.md",
      "WATCHLISTS",
      "api",
      "audio",
      "build_overrides",
      "call",
      "codereview.settings",
      "common_audio",
      "common_video",
      "data",
      "docs",
      "examples",
      "g3doc.lua",
      "g3doc",
      "license_template.txt",
      "logging",
      "media",
      "modules",
      "native-api.md",
      "net",
      "p2p",
      "pc",
      "presubmit_test.py",
      "presubmit_test_mocks.py",
      "pylintrc",
      "resources",
      "rtc_base",
      "rtc_tools",
      "sdk",
      "stats",
      "system_wrappers",
      "test",
      "tools_webrtc",
      "video",
      "webrtc.gni",
      "webrtc_lib_link_test.cc",
      "whitespace.txt"
    ],
    "/docs": [
      "OWNERS",
      "bug-reporting.md",
      "faq.md",
      "native-code",
      "release-notes.md"
    ]
  },
  "makefile": null,
  "readme": "**WebRTC is a free, open software project** that provides browsers and mobile\napplications with Real-Time Communications (RTC) capabilities via simple APIs.\nThe WebRTC components have been optimized to best serve this purpose.\n\n**Our mission:** To enable rich, high-quality RTC applications to be\ndeveloped for the browser, mobile platforms, and IoT devices, and allow them\nall to communicate via a common set of protocols.\n\nThe WebRTC initiative is a project supported by Google, Mozilla and Opera,\namongst others.\n\n### Development\n\nSee [here][native-dev] for instructions on how to get started\ndeveloping with the native code.\n\n[Authoritative list](native-api.md) of directories that contain the\nnative API header files.\n\n### More info\n\n * Official web site: http://www.webrtc.org\n * Master source code repo: https://webrtc.googlesource.com/src\n * Samples and reference apps: https://github.com/webrtc\n * Mailing list: http://groups.google.com/group/discuss-webrtc\n * Continuous build: https://ci.chromium.org/p/webrtc/g/ci/console\n * [Coding style guide](g3doc/style-guide.md)\n * [Code of conduct](CODE_OF_CONDUCT.md)\n * [Reporting bugs](docs/bug-reporting.md)\n * [Documentation](g3doc/sitemap.md)\n\n[native-dev]: https://webrtc.googlesource.com/src/+/main/docs/native-code/index.md\n"
},
{
  "name": "avocado",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "setupJest.js",
      "src"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# \ud83e\udd51avocado\nAvocado is a dashboard for rich insights into the health of the Firefox Experiments Program\n\nMaintainer: Jenny Zhang\n"
},
{
  "name": "fxa-common-password-list",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc",
      ".gitignore",
      ".npmrc",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "package-lock.json",
      "package.json",
      "scripts",
      "source_data",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# fxa-common-password-list\n\nCheck whether a password is common.\n\n## Installation:\n> npm install fxa-common-password-list\n\n## Usage:\n```js\nconst commonPassworList = require('fxa-common-password-list');\n\n// returns true\ncommonPassworList.test('password');\n\n// returns false\ncommonPasswordList.test('@!#^GDSAQ@#^Q#@^$YAESFDAS');\n```\n\n## Tagging a release\n\nOne command to do it all:\n\n> npm version &lt;version&gt;\n\n* Creates a release branch\n* Updates version number in package.json, package-lock.json\n* Updates CHANGELOG.md\n* Commits changes\n* Creates a tag\n* Pushes release branch and tag to origin\n\n## Generating password list\nA new encoded password list can be created\n> node scripts/generate-encoded-passwords-module.js ./source_data/10_million_password_list_top_1M.txt ./src/encoded-passwords.js 50000 8\n\n## License:\nMPL-2.0\n\n"
},
{
  "name": "fathom-training-server",
  "files": {
    "/": [
      ".dockerignore",
      ".eslintrc.json",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "docker-compose.yml",
      "docker",
      "fathom_server",
      "js",
      "manage.py",
      "package-lock.json",
      "package.json",
      "pytest.ini",
      "setup.cfg",
      "setup.py",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# Fathom Training Server\n\nThis is a prototype service for maintaining a set of training webpages for Fathom.\n\n## Development Setup\n\nPrerequisites:\n\n- Docker 18.03.0 or higher\n- docker-compose 1.21.0 or higher\n- Recent node/npm\n\n1. Clone the repository:\n\n   ```sh\n   git clone https://github.com/osmose/fathom-training-server.git\n   cd fathom-training-server\n   ```\n2. Build the Docker image:\n\n   ```sh\n   docker-compose build\n   ```\n3. Run migrations:\n\n   ```sh\n   docker-compose run webserver pipenv run python manage.py migrate\n   ```\n4. Create an admin account:\n\n   ```sh\n   docker-compose run webserver pipenv run python manage.py createsuperuser\n   ```\n5. Build frontend files:\n\n   ```sh\n   npm install\n   npm run build # or `npm run watch`\n   ```\n6. Start everything:\n\n   ```sh\n   docker-compose up\n   ```\n\n## What can it do?\n\n- Add webpages in the admin interface (http://localhost:8000/admin/) and then use the \"freeze\" admin action to freeze and persist their frozen HTML.\n  - Once frozen, view the webpage by clicking the \"View on Site\" button on the webpage's admin page.\n\n## License\n\nFathom Training Server is licensed under the MPL 2.0. See the `LICENSE` file for details.\n"
},
{
  "name": "amazon-transcribe-proxy",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "app"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "firefox-public-data-report-etl",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "Dockerfile",
      "Makefile",
      "README.md",
      "public_data_report",
      "requirements.txt",
      "scripts",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: help test build\n\nhelp:\n\t@echo \"  lint -  check style with flake8\"\n\t@echo \"  test -  run tests quickly with the default Python version\"\n\t@echo \"  build - build the docker image\"\n\nlint:\n\tdocker run -t firefox-public-data-report-etl:0.1 -m flake8 public_data_report tests --max-line-length 100\n\ntest:\n\tdocker run -t firefox-public-data-report-etl:0.1 -m tox\n\nbuild:\n\tdocker build -t firefox-public-data-report-etl:0.1 .\n",
  "readme": "# Firefox Public Data Report ETL\nThe [Firefox Public Data](https://data.firefox.com) project is a public facing website which tracks various metrics over time and helps the general public understand what kind of data is being tracked by Mozilla and how it is used.\n\nThis repository contains the code used to pull and process the data for the _Hardware_ section of the report.\n\nThe website itself is generated by the [Ensemble](https://github.com/mozilla/ensemble) and [Ensemble Transposer](https://github.com/mozilla/ensemble-transposer) repositories.\n\n## Data\n### Hardware report\n[Hardware report job](public_data_report/hardware_report) uses data from [Main pings](https://firefox-source-docs.mozilla.org/toolkit/components/telemetry/data/main-ping.html), pulled from main ping BigQuery table.\n\nIt produces weekly aggregates organized by various dimensions, which are stored in BigQuery and exported to S3 where they can be consumed by Transposer.\n\n### User Activity\n[User activity job](public_data_report/user_activity) exports a BigQuery table containing following metrics:\n\nUser Activity (`fxhealth.json`):\n* Monthly Active users (MAU) - number of clients that used Firefox in the past 28 days\n* Average daily usage hours - average daily use of a typical client from the past 7 days. Calculated by getting the average daily use for each client from the last week (on days they used), and then averaging across all clients\n* Average intensity - average daily intensity of use of a typical client from the past 7 days. Intensity shows how many days per week do users use the product\n* New profile rate - percentage of WAU (clients who used Firefox in the past 7 days) that are new clients (created profile that week)\n* Latest version ratio - percentage of WAU on the newest version (or newer) of Firefox (for that week). Note, Firefox updates are often released with different throttling rates (i.e. 10% of population in week 1, etc.).\n\nUsage Behavior (`webusage.json`):\n* Top languages - percentage of WAU on each of the top 5 language setting (locale).\n* Has Add-on - Percentage of WAU with at least 1 user installed addon.\n* Top Add-ons - The top 10 most common user installed addons from the last 7 days.\n\nYou can run this job locally from a Docker container:\n```shell script\nmake build && \\\ndocker run \\\n    -v PATH_TO_SERVICE_CREDENITIALS.json:/app/.credentials \\\n    -e GOOGLE_APPLICATION_CREDENTIALS=/app/.credentials \\\n    -e AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID \\\n    -e AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY \\\n    firefox-public-data-report-etl:0.1 \\\n    -m public_data_report.cli \\\n    user_activity \\\n    --bq_table moz-fx-data-shared-prod.analysis.public_data_report_user_activity \\\n    --s3_bucket telemetry-public-analysis-2 \\\n    --s3_path public-data-report-dev/user_activity\n```\n\n## Development\n### Testing\nTo run the tests, ensure you have Docker installed. First build the container using:\n```shell script\nmake build\n```\nthen run the tests with:\n```shell script\nmake test\n```\n"
},
{
  "name": "cia-tasks",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "scripts",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "# CIA Tasks\n\nThis is a project to support a number of cron jobs and other tasks that support the CIA team\n\nFor scheduling tasks read the [scripts](scripts/README.md) README.\n\n\n## Problem \n\nThere are a number of small tasks that must be run to process and transform data around our various services.  These are too small, too ephemeral, and too indepenedent of other code to put into the main source tree.  At the same time we do not want to be managing the comput resources requires to support these tasks.\n\n## Solution\n\n[The Community TaskCluster!](https://community-tc.services.mozilla.com/)  This will repo will contain the source code for a number of scripts that can run as a task on the community cluster\n\n\n"
},
{
  "name": "surveillance.mozilla.org",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "action",
      "apple-touch-icon-precomposed.png",
      "close-window",
      "css",
      "fonts",
      "img",
      "index.html",
      "js",
      "netlify.toml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Surveillance\n\nSource for surveillance.mozilla.org.\n\n### Static site. No build steps necessary."
},
{
  "name": "mozilla-donate-content-staging",
  "files": {
    "/": [
      "l10n.toml",
      "locales",
      "templates"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "thunderbird-donate-content-staging",
  "files": {
    "/": [
      "l10n.toml",
      "locales",
      "templates"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "morgoth-cli",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "morgoth",
      "requirements",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "# morgoth-cli\nA command line tool to help with shipping system addons in Balrog\n\n### Installation\n\nClone the repo locally:\n\n```\n$ git clone https://github.com/rehandalal/morgoth-cli.git\n```\n\nUse pip to install from the repo:\n\n```\n$ cd morgoth-cli\n$ pip install --editable .\n```\n\n***Note:** You can leave out the `--editable` flag however you will\nthen need to reinstall any time you update the repo.*\n\n### Configuration\n\nTo configure this tool simply run the following command:\n\n```\n$ morgoth init\n```\n\nIf you plan on running multiple commands you may find it useful to\nstore a bearer token by running:\n\n```\n$ morgoth auth\n```\n\nAdditionally if you need to change any other configurations you\ncan simply run:\n\n```\n$ morgoth config [KEY] [VALUE]\n```\n\nFor example if you use AWS profile you can configure the profile\nto use with the following:\n\n```\n$ morgoth config aws.profile my-profile-name\n```\n\n#### Configuration options\n\n`balrog_url`: The URL of the Balrog server to use.\n\n`aws.profile`: The name of the AWS profile to use.\n\n`aws.prefix`: The prefix to be added to the filename in the S3 bucket.\n\n`aws.bucket_name`: The name of the S3 bucket to use.\n\n`aws.base_url`: The base public URL for the S3 bucket.\n\n\n### Usage\n\n##### Make releases:\n\n```\n$ morgoth make release [PATH_TO_XPI]\n```\n\nThis command is used to create a new release from an XPI file. It will \ncheck if the XPI has been uploaded to S3 and if not upload it for you \nwith the correctly formatted file name.\n\nIt will then give you the option to directly upload the release to \nBalrog, or save it to a file, or simply output it to stdout.\n\n##### Make superblobs:\n\n```\n$ morgoth make superblob [RELEASES] \n```\n\nA superblob is a type of release that represents a group of releases.\n\nThe releases passed to this command may either be the name of a release\nor the path to a JSON file with the data for a release.\n\nIt will then give you the option to directly upload the release to \nBalrog, or save it to a file, or simply output it to stdout.\n\n##### Modify rules:\n\n```\n$ morgoth modify rules [RULES]\n```\n\nThis command lets you modify one or more rules.\n\nThe `--add` option allows you to add a release to each of the rules if \nit does not already exist.\n\nThe `--remove` option allows you to remove a release from each of the\nrules, if it exists.\n\nThe rule changes are added to Balrog as \"Scheduled Changes\" as this is\nhow sign-offs are handled on live channels. The changes are scheduled\nfor 5 seconds in the future so they should take effect immediately after\nany required sign-offs are received.\n\n### Related documentation\n\n- [Go Faster process](https://wiki.mozilla.org/Firefox/Go_Faster/System_Add-ons/Process).\n- [Balrog Wiki](https://wiki.mozilla.org/Balrog)\n- [Balrog documentation](https://mozilla-balrog.readthedocs.io/en/latest/index.html)\n"
},
{
  "name": "lmdb",
  "files": {
    "/": [
      "libraries"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "firefox-quality-metrics",
  "files": {
    "/": [
      "quality_metrics.js"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "fix-stacks",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "src",
      "tests"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# fix-stacks\n\nThis program post-processes (\"fixes\") the stack frames produced by\n`MozFormatCodeAddress()`, which often lack one or more of: function name, file\nname, line number. It relies on the `symbolic` and `goblin` crates to read\ndebug info from files.\n\nIt reads from standard input and writes to standard output. Lines matching the\nspecial stack frame format are modified appropriately. For example, a line\nlike this in the input:\n```\n#01: ???[tests/example +0x43a0]\n```\nis changed to something like this in the output:\n```\n#01: main (/home/njn/moz/fix-stacks/tests/example.c:24)\n```\nLines that do not match the special stack frame format are passed through\nunchanged.\n\nBy default, `fix-stacks` uses native debug info present in binary files. In\nthis case, because the stack frames produced by `MozFormatCodeAddress()` refer\nto build files (such as libxul), `fix-stacks` must run on the same machine that\nproduced the stack frames and the build files. Furthermore, the build files\nmust not have changed since the stack frames were produced. Otherwise, source\nlocations in the output may be missing or incorrect.\n\nAlternatively, you can use the `-b` option to tell `fix-stacks` to read\nBreakpad symbol files, as packaged by Firefox. The argument must contain two\npaths, separated by a comma: the first path points to the Breakpad symbols\ndirectory, the second path points to the `fileid` executable in the Firefox\nobjdir. In this case, the processed output will contain square brackets instead\nof parentheses, to make it detectable from the output that breakpad symbols\nwere used. \n\n`fix-stacks` works on Linux, Windows, and Mac.\n\n# Shortcomings\n\nOn Linux, use with debuginfo sections in separate files is untested and\nprobably does not work.\n"
},
{
  "name": "infosec-risk-management-bugzilla",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assigner.py",
      "casa.py",
      "config.yaml",
      "contrib",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# Setup\n\n- Bugzilla credentials are passed as environment variable `BUGZILLA_API_KEY`\n- CASA credentials are passed as environment variable `CASA_API_KEY`\n\n\nFor Bugzilla, you need a \"personal token\" that you can generate in your user profile.\n\nFor CASA, you need a \"personal OAuth token\" that you can generate at https://biztera.com/developer/tokens for a user that has\n\"security moderator\" privileges (generally you want this to be a dedicated user).\n\nScopes required:\n- `write_project` (to change the security tab values)\n- `read_project` (to check the security tab values, so that we know if it needs to be changed)\n- `read_friend` (to lookup the user in Biztera and notify that a change has been posted on their behalf)\n\n## Usage\n\nSee `config.yaml` for configuration and `./assigner.py --help` for available commands.\n\nExample:\n`$ ./assigner.py -d --dry-run --module rra,va`\n\n# Features\n\n## Bugzilla auto-assignment\n\nThis is a simple script using `simple_bugzilla` in order to assign VA, RRA bugs automatically.\nIt aims to be as simple as possible.\n\n## Casa-Bugzilla sync\n\nThis is another simple script which scans recent Bugzilla changes for VA, RRA components, and updates CASA automatically\nif a bug has been closed, with the correct status. This updates CASA's security tab, and uses a special account that has\n\"security moderator\" privileges.\n\nNote that CASA is short for Contracts and Spends Approval, and runs on biztera.com\n"
},
{
  "name": "pyjexl",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "pyjexl",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# PyJEXL\n\n[![CircleCI](https://circleci.com/gh/mozilla/pyjexl.svg?style=svg)](https://circleci.com/gh/mozilla/pyjexl)\n\nA Python-based JEXL parser and evaluator.\n\n**NOTE:** This library handles the JEXL from\n[TomFrost's JEXL library][jexl]. It does **NOT** handle the\nsimilarly-named Apache Commons JEXL language.\n\n[jexl]: https://github.com/TomFrost/Jexl\n\n## Limitations and Differences from JEXL\n\n* JavaScript-style implicit type conversions aren't supported, but may be added\n  in the future. Instead, Python type semantics are used.\n* Property access is only supported for mapping objects currently.\n* Several odd edge-cases in JEXL are ignored because they are unintuitive,\n  difficult to implement, or a bad pattern:\n  * Implicitly using the first element in an array when chaining identifiers\n    is not supported. In JEXL, if `foo.bar` is a list, the expression\n    `foo.bar.baz` is equivalent to `foo.bar[0].baz`.\n  * Conditional expressions (AKA ternary expressions) cannot have a missing\n    consequent, i.e. `\"foo\" ?: 4` is invalid.\n\n## License\n\nLicensed under the MIT License. See `LICENSE` for details.\n"
},
{
  "name": "normandy-edi",
  "files": {
    "/": [
      ".gitignore",
      ".therapist.yml",
      "README.md",
      "edi",
      "poetry.lock",
      "pyproject.toml",
      "setup.cfg"
    ]
  },
  "makefile": null,
  "readme": "# normandy-edi\n\nA CLI for working with the Normandy API.\n\n## Usage\n\nEdi can currently only be used from a development environment. To manage Python requirements, [Poetry](https://python-poetry.org) is used. After installing Poetry, run the following command to install the requirements for Edi:\n\n```bash\n$ poetry install\n```\n\nAfter this, to run Edi\n\n```bash\n$ poetry run edi\n```\n\nWhen run without arguments, or with not enough, Edi will print a help message detailing what further options you can use. For example:\n\n```bash\n$ poetry run edi recipes\n\nUsage: edi recipes [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  -v, --verbose\n  -s, --server [prod|stage|dev|local|prod-admin|stage-admin|dev-admin]\n  --help                          Show this message and exit.\n\nCommands:\n  all\n  classify-filters\n  count-filters\n  delete\n  empty\n  get\n  revise\n  summarize         Show recipes enabled during a time range\n```\n\n## Examples\n\n### The name of all heartbeat recipes that were active in 2019\n\n```bash\n$ poetry run edi recipes all \\\n    --action show-heartbeat \\\n    --enabled-begin 2019-01-01 \\\n    --enabled-end 2020-01-01 \\\n    --jq-query '.[].latest_revision.name'\n```\n"
},
{
  "name": "rust-ece",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "src"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# rust-ece &emsp; [![Build Status]][circleci] [![Latest Version]][crates.io]\n\n[Build Status]: https://circleci.com/gh/mozilla/rust-ece.svg?style=svg\n[circleci]: https://circleci.com/gh/mozilla/rust-ece\n[Latest Version]: https://img.shields.io/crates/v/ece.svg\n[crates.io]: https://crates.io/crates/ece\n\n*This crate has not been security reviewed yet, use at your own risk\n([tracking issue](https://github.com/mozilla/rust-ece/issues/18))*.\n\nThe [ece](https://crates.io/crates/ece) crate is a Rust implementation of Message Encryption for Web Push\n([RFC8291](https://tools.ietf.org/html/rfc8291)) and the HTTP Encrypted Content-Encoding scheme\n([RFC8188](https://tools.ietf.org/html/rfc8188)) on which it is based.\n\nIt provides low-level cryptographic \"plumbing\" and is destined to be used by higher-level Web Push libraries, both on\nthe server and the client side. It is a port of the [ecec](https://github.com/web-push-libs/ecec) C library.\n\n[Full Documentation](https://docs.rs/ece/)\n\n## Implemented schemes\n\nThis crate implements both the published Web Push Encryption scheme, and a legacy scheme from earlier drafts\nthat is still widely used in the wild:\n\n* `aes128gcm`: the scheme described in [RFC8291](https://tools.ietf.org/html/rfc8291) and\n  [RFC8188](https://tools.ietf.org/html/rfc8188)\n* `aesgcm`: the draft scheme described in\n  [draft-ietf-webpush-encryption-04](https://tools.ietf.org/html/draft-ietf-webpush-encryption-04) and\n  [draft-ietf-httpbis-encryption-encoding-03](https://tools.ietf.org/html/draft-ietf-httpbis-encryption-encoding-03_)\n\nIt does not support, and we have no plans to ever support, the obsolete `aesgcm128` scheme\nfrom [earlier drafts](https://tools.ietf.org/html/draft-thomson-http-encryption-02).\n\n## Usage\n\nTo receive messages via WebPush, the receiver must generate an EC keypair and a symmetric authentication secret,\nthen distribute the public key and authentication secret to the sender:\n\n```rust\nlet (keypair, auth_secret) = ece::generate_keypair_and_auth_secret()?;\nlet pubkey = keypair.pub_as_raw();\n// Base64-encode the `pubkey` and `auth_secret` bytes and distribute them to the sender.\n```\n\nThe sender can encrypt a Web Push message to the receiver's public key:\n\n```rust\nlet ciphertext = ece::encrypt(&pubkey, &auth_secret, b\"payload\")?;\n```\n\nAnd the receiver can decrypt it using their private key:\n\n```rust\nlet plaintext = ece::decrypt(&keypair, &auth_secret, &ciphertext)?;\n```\n\nThat's pretty much all there is to it! It's up to the higher-level library to manage distributing the encrypted payload,\ntypically by arranging for it to be included in a HTTP response with `Content-Encoding: aes128gcm` header.\n\n### Legacy `aesgcm` encryption\n\nThe legacy `aesgcm` scheme is more complicated, because it communicates some encryption parameters in HTTP header fields\nrather than as part of the encrypted payload.  When used for encryption, the sender must deal with `Encryption` and\n`Crypto-Key` headers in addition to the ciphertext:\n\n```rust\nlet encrypted_block = ece::legacy::encrypt_aesgcm(pubkey, auth_secret, b\"payload\")?;\nfor (header, &value) in encrypted_block.headers().iter() {\n  // Set header to corresponding value\n}\n// Send encrypted_block.body() as the body\n```\n\nWhen receiving an `aesgcm` message, the receiver needs to parse encryption parameters from the `Encryption`\nand `Crypto-Key` fields:\n\n```rust\n// Parse `rs`, `salt` and `dh` from the `Encryption` and `Crypto-Key` headers.\n// You'll need to consult the spec for how to do this; we might add some helpers one day.\nlet encrypted_block = ece::AesGcmEncryptedBlock::new(dh, rs, salt, ciphertext);\nlet plaintext = ece::legacy::decrypt_aesgcm(keypair, auth_secret, encrypted_block)?;\n```\n\n### Unimplemented Features\n\n* We do not implement streaming encryption or decryption, although the ECE scheme is designed to permit it.\n* We only support encrypting or decrypting across multiple records for `aes128gcm`; messages using the\n  legacy `aesgcm` scheme must fit in a single record.\n* We do not support customizing the record size parameter during encryption, but do check it during decryption.\n  * The default record size is 4096 bytes.\n* We do not support customizing the number of padding bytes added during encryption.\n  * We currently select the padding length at random for each encryption, but this is an implementation detail and\n    should not be relied on.\n\nThese restrictions might be lifted in future, if it turns out that we need them.\n\n## Cryptographic backends\n\nThis crate is designed to use pluggable backend implementations of low-level crypto primitives. different crypto\nbackends. At the moment only [openssl](https://github.com/sfackler/rust-openssl) is supported.\n\n## Release process\n\nWe use [`cargo-release`](https://crates.io/crates/cargo-release) to manage releases. To cut a new release,\nmake sure you have it installed and then:\n\n1. Start a new branch for the release:\n     * `git checkout -b release-vX.Y.Z`\n     * `git push -u origin release-vX.Y.Z`\n2. Run `cargo release --dry-run -vv [major|minor|patch]` and check that the things\n   it's proposing to do seem sensible.\n3. Run `cargo release [major|minor|patch]` to prepare, commit, tag and publish the release.\n4. Make a PR from your `release-vX.Y.Z` branch to request it be merged to the main branch.\n"
},
{
  "name": "protodash",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "auth.go",
      "config.go",
      "config.yml",
      "dash.go",
      "go.mod",
      "go.sum",
      "helpers.go",
      "index.gohtml",
      "logging.go",
      "main.go",
      "pkce",
      "server.go"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/protodash.svg?style=shield&circle-token=742fb1108f7e6e5a28c11d43b21f62605037f5a4)](https://circleci.com/gh/mozilla/protodash)\n\n# ProtoDash\n\nProtoDash is a tool to aid the rapid development of prototype dashboards and enable data engineering and data science to deploy static sites without the need to engage with ops.\n\n## Dashboard Config\n\nThe config for the dashboards is stored in `config.yml` and is a map of slugs (the path that the dashboard will serve from) and the config options for that specific dashboard.\n\nA verbose example of the file with all available options is below.\n\n```yaml\n---\ndashboard-slug:\n  gcs_bucket: my-sandbox-bucket # required\n  single_page_app: true # optional\n  prefix: sub-dir-in-gcs # optional\n  public: true # optional\n```\n\n| Key               | Description                                                                                                                 | Default | Required |\n| ----------------- | --------------------------------------------------------------------------------------------------------------------------- | ------- | -------- |\n| `gcs_bucket`      | The bucket to serve the files from                                                                                          |         | `yes`    |\n| `single_page_app` | Whether the app is an SPA, when this is set to `true` and a path would return a 404, we serve the root `index.html` instead | `false` | `no`     |\n| `prefix`          | A prefix in the bucket to serve from, this would allow you to run multiple apps from the same bucket                        |         | `no`     |\n| `public`          | Whether the dashboard should be publicly accessible                                                                         | `false` | `no`     |\n| `subdomain`       | Whether the dashboard should serve from a path or a subdomain                                                               | `false` | `no`     |\n\n## Adding a dashboard\n\nProtodash has access to GCS buckets created in projects in the `dataops/sandbox` hierarchy: for more information on creating such a project see [Creating a Prototype Data Project on Google Cloud Platform](https://docs.telemetry.mozilla.org/cookbooks/gcp-projects.html).\n\nOnce you have a dashboard ready to go, open a PR against `config.yml` with the required info. After it's approved and merged we auto-deploy the changes and from that point on you can edit the files in your GCS bucket and the changes should be instant.\n\n## Local Development\n\nYou'll want to set the `GOOGLE_CLOUD_CREDENTIALS` to point at either the json keyfile for a service account, or your local application default credentials json keyfile (usually `~/.config/gcloud/application_default_credentials.json`).\n\n```\ngo build -o protodash\n./protodash\n```\n\n## Environment Config\n\nThese environment variables control how ProtoDash operates in production. It should not normally be necessary to modify these.\n\n| Env Variable                    | Description                                                                                             | Default          |\n| ------------------------------- | ------------------------------------------------------------------------------------------------------- | ---------------- |\n| `PROTODASH_LISTEN`              | Address to bind the server                                                                              | `:8080`          |\n| `PROTODASH_LOG_LEVEL`           | Logging level                                                                                           | `debug`          |\n| `PROTODASH_PROXY_TIMEOUT`       | Defines the maximum time in serving the proxy requests, this is a hard timeout and includes retries     | `10s`            |\n| `PROTODASH_CLIENT_TIMEOUT`      | Hard timeout on requests that protodash sends to the Google Storage API                                 | `2s`             |\n| `PROTODASH_IDLE_CONN_TIMEOUT`   | Maximum duration of idle connections between protodash and the Google Storage API                       | `120s`           |\n| `PROTODASH_MAX_IDLE_CONNS`      | Maximum number of idle connections to keep open. This doesn't control the maximum number of connections | `10`             |\n| `PROTODASH_OAUTH_ENABLED`       | Toggles whether authentication is on or off                                                             | `false`          |\n| `PROTODASH_OAUTH_DOMAIN`        | The OAuth domain that the authentication layer will use, currently only supports Auth0                  |                  |\n| `PROTODASH_OAUTH_CLIENT_ID`     | Client ID of the OAuth application                                                                      |                  |\n| `PROTODASH_OAUTH_CLIENT_SECRET` | Client Secret of the OAuth application, if not defined use the PKCE flow                                |                  |\n| `PROTODASH_OAUTH_REDIRECT_URI`  | Callback URI to redirect to after authenticating                                                        |                  |\n| `PROTODASH_SESSION_SECRET`      | Secret to usse for encrypting the session cookie                                                        |                  |\n| `PROTODASH_SHOW_PRIVATE`        | Whether to show the list of private dashboards if not authenticated                                     | `false`          |\n| `PROTODASH_REDIRECT_TO_LOGIN`   | Whether to redirect to the login pagee if a user is not authenticated and accesses a private dashboard  | `false`          |\n| `PROTODASH_BASE_DOMAIN`         | The domain to use when building subdomains and handling redirects                                       | `localhost:8080` |\n| `PROTODASH_DEFAULT_BUCKET`      | Default GCS bucket to use for dashboards if none is defined in the config                               |                  |\n| `PROTODASH_CONFIG_FILE`         | Config file for the dashboards                                                                          | `config.yml`     |\n\n## Thanks\n\n- [nytimes/gcs-helper](https://github.com/nytimes/gcs-helper) - Portions of the code here were heavily inspired by the gcs-helper project from the NY Times, particularly the method of proxying requests to GCS without having to use the GCS storage APIs.\n- [markbates/goth](https://github.com/markbates/goth) - The core of the authentication / the interfaces we use for the auth are based on those in Goth.\n"
},
{
  "name": "compare-locales",
  "files": {
    "/": [
      ".arcconfig",
      ".github",
      ".gitignore",
      ".hgignore",
      ".hgtags",
      "README.md",
      "compare_locales",
      "contrib",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "![Build tests](https://github.com/mozilla/compare-locales/workflows/test/badge.svg)\n# compare-locales\nLint Mozilla localizations\n\nFinds\n* missing strings\n* obsolete strings\n* errors on runtime errors without false positives\n* warns on possible runtime errors\n\nIt also includes `l10n-merge` functionality, which pads localizations with\nmissing English strings, and replaces entities with errors with English.\n\nIf you want to check your original code for errors like duplicated messages,\nuse `moz-l10n-lint`, which is also part of this package. You can also use\nthis to check for conflicts between your strings and those already exposed\nto l10n.\n\n# Configuration\n\nYou configure `compare-locales` (and `moz-l10n-lint`) through a\n[project configuration](https://moz-l10n-config.readthedocs.io/en/latest/fileformat.html)\nfile, `l10n.toml`.\n\n# Examples\n\nTo check all locales in a project use\n\n```bash\ncompare-locales l10n.toml .\n```\n\nTo check Firefox against a local check-out of l10n-central, use\n\n```bash\ncompare-locales browser/locales/l10n.toml ../l10n-central\n```\n\nIf you just want to check particular locales, specify them as additional\ncommandline parameters.\n\nTo lint your local work, use\n\n```bash\nmoz-l10n-lint l10n.toml\n```\n\nTo check for conflicts against already existing strings:\n\n```bash\nmoz-l10n-lint --reference-project ../android-l10n/mozilla-mobile/fenix l10n.toml\nmoz-l10n-lint --l10n-reference ../gecko-strings browser/locales/l10n.toml\n```\n\nto check for a monolithic project like Fenix or a gecko project like Firefox,\nresp.\n"
},
{
  "name": "moz-gfx-telemetry",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "analyses",
      "samples",
      "tools",
      "www"
    ]
  },
  "makefile": null,
  "readme": "# moz-gfx-telemetry\nMozilla Graphics Telemetry Scripts\n\nLayout:\n\n * `analyses` - Telemetry scripts that drive the dashboard.\n * `samples` - Sample scripts to help with custom graphics-related telemetry analyses.\n * `tools` - Scripts to assist in doing analyses.\n * `www` - Frontend code for the graphics telemetry dashboard.\n\nSee the [wiki](https://wiki.mozilla.org/Platform/GFX/Telemetry) for a tutorial using the sample script.\n\nThe frontend dashboard is hosted [here](https://firefoxgraphics.github.io/telemetry/).\n"
},
{
  "name": "side-view",
  "files": {
    "/": [
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "addon",
      "docs",
      "package-lock.json",
      "package.json"
    ],
    "/docs": [
      "release.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Side View\n\nAn experiment with opening mobile views of pages in the sidebar.\n\n[**Install from addons.mozilla.org**](https://addons.mozilla.org/en-US/firefox/addon/side-view/)\n\n## Installing\n\nUse `npm install`, then `npm start`.\n\n## Installing manually\n\nCheck out the repository. Go to `about:debugging` in Firefox, and select **Load Temporary Add-on**. Select a file in the `addon/` directory.\n\nOr: install [`web-ext`](https://github.com/mozilla/web-ext) (like `npm i -g web-ext`) and run `web-ext run -s addon/ --browser-console -f nightly`\n\n## Using\n\nThis adds a context menu item: **Open in sidebar** or **Open link in sidebar**. Select that, and the sidebar will be opened with a mobile view of the page.\n\n## Credits\n\n[Anthony_f](https://addons.mozilla.org/en-US/firefox/user/Anthony_f/)'s [Sidebar for Google Search](https://addons.mozilla.org/en-US/firefox/addon/sidebar-for-google-search/) inspired this add-on's approach.\n"
},
{
  "name": "webrtc-landing",
  "files": {
    "/": [
      "Cloth.js",
      "DataChannel_changes.html",
      "README.md",
      "Test.js",
      "audio.html",
      "bootstrap.css",
      "bootstrap.js",
      "canvas_demo.html",
      "canvas_filter_demo.html",
      "canvas_swap.html",
      "compat_gum.html",
      "constraints_test.html",
      "data_latency_test.html",
      "data_test.html",
      "data_test_old.html",
      "detect.js",
      "effects",
      "enumerate.html",
      "gum_iframe.html",
      "gum_picture.html",
      "gum_test.html",
      "gum_test_2_camera.html",
      "gum_test_aec.html",
      "index.html",
      "jquery.js",
      "muting.html",
      "pc_audio_only_test.html",
      "pc_forward.html",
      "pc_test.html",
      "pc_test_callbacks.html",
      "pc_test_fps.html",
      "pc_test_h264.html",
      "pc_test_loop.html",
      "pc_test_loop_h264.html",
      "pc_test_multi.html",
      "pc_test_no_h264.html",
      "pc_test_swap.html",
      "seriously.js",
      "stun_test.html",
      "stun_test_ip.html",
      "temp.html",
      "test.ogg"
    ]
  },
  "makefile": null,
  "readme": "# Simple WebRTC Tests\n\n[Try for yourself](https://mozilla.github.io/webrtc-landing/)\n"
},
{
  "name": "mozmonument-sf",
  "files": {
    "/": [
      "4PanelNamesLayout.pdf",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "mozmonument-sf\n==============\n\nThis will be a repository for all the assets and content on the Mozilla monument in San Francisco.\n"
},
{
  "name": "rra2json",
  "files": {
    "/": [
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.rst",
      "VERSION",
      "parselib.py",
      "requirements.txt",
      "rra2json.inc.json",
      "rra2json.py",
      "rra_parsers",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "This program integrates with service-map: https://github.com/mozilla/service-map\nIt posts a JSON version of the Gdocs RRA documents to service-map, to be precise.\n\nSee also: https://wiki.mozilla.org/Security/Risk_management/Rapid_Risk_Assessment\n\nGet oauth2 credentials\n======================\n\nSee http://gspread.readthedocs.org/en/latest/oauth2.html for a guided version of this.\n\nAs your user, login to https://console.developers.google.com/project/ and create a new project.\nGo to \"API&Auth/APIs\".\nGive that project API rights for the Drive API.\nGo to \"API&Auth/Credentials\".\nClick \"Create client ID\" as \"Service ID\".\nYou'll get a JSON key back (JWT), that's your credentials.\n\n\n.. note::\n\n\tMake sure you authorize your Service email to all the spreadsheets you'll want to have access to! By default it\nhas no accesses.\n\nJSON Format\n===========\n\nSee the file rra2json.inc.json that is included for the exact and complete format. Here's an approximation of what\nyou'll get though:\n\n  ::\n\n  {\n  'source': '1deadbeef-Mju0niB5gZaxy5uZ24_kuJiN6wOSyIx3JJRAyks',\n  'timestamp': '2015-05-11T15:50:13.185754+00:00',\n  'summary': 'RRA for <something>',\n  'tags': ['RRA', 'service', '1.0.0'],\n  'severity': 'INFO',\n  'lastmodified': '2015-05-09T01:18:55.850000+00:00',\n  'category': 'rra_data',\n  'details': {\n        'risk': {\n                'availability': {\n                        'reputation':   {'impact': 'Unknown', 'probability': ''},\n                        'finances':     {'impact': 'Unknown', 'probability': ''},\n                        'productivity': {'impact': 'Unknown', 'probability': ''}\n                },\n                'integrity': {\n                        'reputation':   {'impact': 'Unknown', 'probability': ''},\n                        'finances':     {'impact': 'Unknown', 'probability': ''},\n                        'productivity': {'impact': 'Unknown', 'probability': ''}\n                },\n                'confidentiality': {\n                        'reputation':   {'impact': 'Unknown', 'probability': ''},\n                        'finances':     {'impact': 'Unknown', 'probability': ''},\n                        'productivity': {'impact': 'Unknown', 'probability': ''}\n                },\n        'metadata': {\n                'service': '<something>',\n                'owner': 'IT Team, J.Doe',\n                'description': 'A service to do <something>',\n                'developer': 'Dev Team, J.Doe',\n                'operator': 'IT Team, J.Doe',\n                'RRA_version': '2.5.0',\n                'scope': 'The <something> part of the <something service>'\n        },\n        'data': {\n                'Unknown': [],\n                'PUBLIC': [],\n                'INTERNAL': [],\n                'SECRET': [],\n                'RESTRICTED': [],\n                'default': ''\n        }\n  }\n\n"
},
{
  "name": "patches",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "TODO.md",
      "clair_config",
      "cmd",
      "docker-compose.yml",
      "docs",
      "internal",
      "patches.go",
      "pkg",
      "reportapi.go"
    ],
    "/docs": [
      "arch.png",
      "features.md"
    ]
  },
  "makefile": "unit-test:\n\tgo test -v ./internal/clients/\n\tgo test -v ./internal/limit/\n\tgo test -v ./internal/scanners/\n\tgo test -v ./internal/sources/\n\tgo test -v ./internal/sources/clair/\n\tgo test -v ./internal/servers/\n\tgo test -v ./cmd/scanner/\n\ntest: unit-test\n\ndependencies:\n\tgo get github.com/Sirupsen/logrus\n\nserver:\n\tgo build -o patchesserver ./cmd/server\n\nscanner:\n\tgo build -o patchesscanner ./cmd/scanner\n\ndocker-image:\n\tdocker build -t patches .\n",
  "readme": "# patches\n\nPatches scans your endpoints and servers for vulnerable package installations.\n\nUsing the [Clair](https://coreos.com/clair/docs/latest/) API as a source of\ninformation about packages with known vulnerabilities, patches periodically\nperforms a scan on a host for packages containing any vulnerabilities affecting\nthe host.\n\n\n\u26a0\ufe0f  **WARNING** \u26a0\ufe0f\n\nWhile it's extremely unlikely that this software would cause harm to your\ncomputer, it is worth noting that this software is considered to be in _alpha_.\n\nRun at your own risk.\n\n## Architecture\n\n![Architecture diagram](https://raw.githubusercontent.com/Atelier-Arcadia/patches/master/docs/arch.png)\n\n1. The patches-server reads information about vulnerabilities from Clair.\n2. The patches-scanner streams that information from the server.\n3. The patches-scanner then scans the host it runs on for vulnerable packages.\n4. Finally, the patches-scanner reports anything it finds to a reporter API.\n\n## Setup\n\n### Prerequisites\n\n#### Go compiler\n\nFirst, you must have a working [Go](https://golang.org/) compiler at this time\nas packages are not yet being built for patches.  Follow the instructions on\nthe official site to get a working compiler installed.\n\n#### Debian or Ubuntu host\n\nYou will see in the patches-scanner's usage output that it claims to support a\nnumber of different versions of Linux distributions.  At present, patches only\nactually includes support for Debian and Ubuntu hosts.  Support for the\nremaining hosts will come pretty soon, with any luck, but I'd be happy to help\ncontributors.\n\n#### Docker\n\nRunning a patches-server locally will require running the Clair vulnerability\nAPI and its datbase inside of [Docker](https://www.docker.com/). So you'll\nneed that installed.\n\n### Running a patches-server\n\n```bash\n# Inside the patches/ directory\n\n# 1. Build the server\nmake server\n\n# 2. Run Clair\ndocker-compose up -d\n\n# (1) Wait about ten or fifteen minutes for the Clair database to fill up.\n\n# 3. Run the patches server\n./patchesserver\n```\n\n_Notes:_\n\n(1) - Clair, running in Docker, automatically updates its database with\ninformation about vulnerabilities affecting packages on a variety of\nLinux hosts.  This process takes some time but can be checked on manually if\nyou want to be sure.\n\n```bash\n# 1. Get the identifier of the Clair API container.\nclairid=`docker ps | grep \"clair_clair\" | sed \"s/ .*//g\" | sed \"s/ //g\"`\n\n# 2. Open a shell instance inside of the Clair API container.\ndocker exec -it $clairid sh\n\n# 3. Make a request to the Clair API to see if vulnerabilities have been\n#    obtained for your platform. If you are running Ubuntu 18.04 for example,\n#    look for its name in the output of the following.\n#    Note that you may see `{\"namespaces\": null}` for some time before any\n#    data loads.\nwget http://127.0.0.1:6060/v1/namespaces; cat namespaces; rm namespaces\n\n# 4. If your platform has data loaded for it, exit.\nexit\n```\n\n### Running the mock reporter API\n\nThe patches-scanner will report any vulnerabilities affecting he host it's\nrunning on to a \"reporter API.\" This API is essentially expected only to\naccept requests containing vulnerability information encoded as JSON in the\nbody and to indicate success with status code 200.\n\n```bash\n# 1. Run the reporter API.\ngo run reportapi.go\n```\n\n### Running a patches-scanner\n\nThe patches-scanner is the host utility that periodically streams information\nabout vulnerabilities from a patches-server.  When it finds a vulnerability\naffecting an unpatched package on the host, it will report this vulnerability\nto a reporter API.\n\n```bash\n# 1. Build the scanner.\nmake scanner\n\n# 2. Run the scanner.\n#    This step assumes you have a patches-server and Clair running locally.\nplatform=\"ubuntu-18.04\" # Replace with appropriate platform name for you\n\n./patchesscanner\\\n\t-platform $platform\\\n\t-patches-server http://127.0.0.1:8080\\\n\t-mozdef-proxy http://127.0.0.1:9001\\\n\t-scan-frequency 1\n```\n\nAfter about one minute, the scanner should start logging information about\nrequests it's making to the vulnerability server and any findings it produces.\n\n## Testing\n\nAll of the patches' unit tests can be run by invoking `make`.\n\n```bash\nmake test\n```\n\n## Contributing\n\nNo formal process has been established for this yet.\nIf you'd like to help out, please feel welcome to open an issue with any\nquestions.\n"
},
{
  "name": "iraas",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "example-client",
      "serverless-project-iraas",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# Incident Response as a Service - Work in Progress\n\nThis toolkit is a serverless framework application that builds on the\ncapabilities in aws_ir.  It can do host_compromises, key_compromises, and\ncloudtrail_compromises.\n\n## Dev Environment Setup\n\n1. `cd serverless-project-iraas`\n1. `npm install --save serverless`\n1. `npm install --save serverless-python-requirements`\n1. `npm install --save https://github.com/vortarian/serverless-sqs-fifo`\n\n## Incident Flow\n\n1. CIM ( MozDef in our case ) generates an alert to the service by outputting to an SQS Queue for\nincidents in.\n\n2. CloudWatch events poll the SQS queue every 1-minute and if there's work to do spin up the credential\nhelper function.  The credential helper looks at the list of roles and attempts to match the account ID\nwith the resource we're remediating.  Once a role is matched that role is assumed and the credentials are\npassed to the IR function.\n\n3. The IR function determines the type of resource it is taking actions on and follows the plan\nfor that type of resource as determined by the configuration of the deployment.\n\n## Sample event to output to SQS\n\n_CloudTrail Disabled_\n\n_Instance Suspected of Malicious Activity_\n(Future)\n\n_Access Key Suspected Leak or Anomaly_\n(Future)\n\n## Resources to Create\n\n1. SQS Input FIFO Queue\n2. SQS Output FIFO Queue\n3. CloudWatch metric + CloudWatch Event to invoke lambda if SQS.ApproxMessagesVisible > 0\n4. Credential Assumption Function\n5. IR Function\n\n## Deploying this Service\nA Dockerfile is present in the project to facilitate deployment with the serverless framework.\n\n```bash\ndocker run --rm -ti \\\n-v ~/.aws:/root/.aws \\\n-v ~/workspace/iraas/:/workspace \\\nmozillaiam/docker-sls:latest \\\n/bin/bash\n```\n\n## Sample Client Output\n\nA sample client for event mocking purposes has been provided in the example-client directory.\n"
},
{
  "name": "http-observatory-dashboard",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "httpobsdashboard",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# HTTP Observatory Dashboard\n\nThis dashboard provides a metrics dashboard to see the status of the [Observatory](https://observatory.mozilla.org/) for each link.\n\n## Prerequisites\n\nThis site requires:\n\n- Python >3.4\n- Make\n  - **Linux:** most package managers have *build-essential* which provide make\n  - **MacOS:** installing Xcode provides make\n\n## Install\n\n```\npip install -r requirements.txt\n```\n\n**Note:** Some linux setups may require *pip3* instead of *pip* in the above command.\n\n## Running\n\nRunning `make devserver` will make a live dev environment that can be loaded in the browser and refreshes every time you change templates or run `make generate`.\n\nRunning `make generate` fetches the reports from the Observatory.\n\n```\ncd httpobsdashboard;\nmake generate;\nmake devserver;\n```\n\n## Configuring\n\nConfiguring the dashboard is simple, edit the JSON files `httpobsdashboard/conf/` the main files is `sites.json`.\n\nTo modify the meaning of scores for websites edit `deviations.json`. \n"
},
{
  "name": "vaporized_meerkat",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "ansible.cfg",
      "ansible.cfg-host",
      "ansible.cfg-nat1",
      "awsmfa.sh",
      "cloudformation",
      "open_the_vault.sh",
      "playbooks",
      "roles",
      "suricata.yml",
      "vars",
      "vault_passphrase.gpg"
    ]
  },
  "makefile": null,
  "readme": "# vaporized_meerkat\n\nSuricata on an AWS NAT instance\n"
},
{
  "name": "scribe",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "chain.go",
      "concat.go",
      "document.go",
      "evr.go",
      "evrops.go",
      "evrops_test.go",
      "exactmatch.go",
      "filecontent.go",
      "filename.go",
      "fileops_test.go",
      "hasline.go",
      "meta_test.go",
      "noop.go",
      "object.go",
      "package.go",
      "package_test.go",
      "parser.go",
      "pkgmgr.go",
      "raw.go",
      "regexp.go",
      "result.go",
      "scribe.go",
      "scribe_test.go",
      "scribecmd",
      "scribevulnpolicy",
      "test.go",
      "test",
      "variable.go",
      "vendor"
    ]
  },
  "makefile": "PROJS = scribe scribecmd scribevulnpolicy\nGO = GO15VENDOREXPERIMENT=1 go\nGOLINT = golint\n\nall: $(PROJS) runtests\n\nscribe:\n\t$(GO) install github.com/mozilla/scribe\n\nscribecmd:\n\t$(GO) install github.com/mozilla/scribe/scribecmd\n\nscribevulnpolicy:\n\t$(GO) install github.com/mozilla/scribe/scribevulnpolicy\n\nruntests: gotests\n\ngotests:\n\t$(GO) test -v -covermode=count -coverprofile=coverage.out github.com/mozilla/scribe\n\nshowcoverage: gotests\n\t$(GO) tool cover -html=coverage.out\n\nlint:\n\t$(GOLINT) $(PROJECT)\n\nvet:\n\t$(GO) vet $(PROJECT)\n\ngo_vendor_dependencies:\n\tgovend -u\n\trm -rf vendor/github.com/mozilla/scribe\n\t[ $$(ls -A vendor/github.com/mozilla) ] || rm -r vendor/github.com/mozilla\n\t[ $$(ls -A vendor/github.com) ] || rm -r vendor/github.com\n\nclean:\n\trm -rf pkg\n\trm -f bin/*\n\tcd test && $(MAKE) clean\n\n.PHONY: $(PROJS) runtests gotests showcoverage lint vet clean\n",
  "readme": "# scribe\n\nscribe is a host policy evaluator written in Go.\n\n[![Build Status](https://travis-ci.org/mozilla/scribe.svg?branch=master)](https://travis-ci.org/mozilla/scribe)\n[![Go Report Card](https://goreportcard.com/badge/mozilla/scribe \"Go Report Card\")](https://goreportcard.com/report/mozilla/scribe)\n\n## Overview\n\nscribe is a Go library and frontend used to evaluate policies on systems.\nPolicies are specified as a JSON or YAML document containing a series of tests, and\nthese tests return a status indicating if the test criteria matched or not.\n\nTests reference objects in the policy file. An object can be considered an abstraction\nof some data from the system, for example a package version or the contents of a specific\nfile. The tests also specify criteria that will be applied to the referenced object. For example,\nif an object returns a line from a given file, the test could indicate that the data must\nmatch specific content. If the match succeeeds, the test returns true.\n\nIt is intended to perform functions such as:\n\n* Identification of software versions that do not meet a specific requirement\n* Evaluation of hardening criteria or other system security policies\n* Any other functions involving extraction and analysis of host information\n\nThe software is designed to return only test status criteria, and meta-data\nassociated with the test. It runs directly on the system being evaluated, and\nrequires no data from the system to be returned to a central server for\nadditional processing.\n\nIt's primary purpose is integration with Mozilla MIG which allows\ninvestigators to perform system evaluation by sending a policy to the MIG\nagent for execution. It is also suited to executing policies as part of an\ninstance build and testing process, or periodically on an installed system.\n\n## Usage\n\nScribe policies can be evaluated using the scribecmd command line tool, or alternatively the scribe\nlibrary can be included in another go application.\n\nThis example shows evaluation of a given policy file, where only tests that return\ntrue are displayed in the results.\n\n```bash\n$ ./scribecmd -f mypolicy.json -T\n```\n\nscribecmd supports other runtime options, see the usage output for details.\n\n## Vulnerability scanning\n\nscribe can be used to perform vulnerability scanning directly on the system using a suitable\npolicy file. The library implements various criteria specifications such as\nEVR (epoch/version/release) testing that can be used to determine if a given package\nversion is less than what is required.\n\nscribevulnpolicy is a policy generator that integrates with [clair](https://github.com/coreos/clair)\nfor vulnerability data. This tool can be used to generate scribe vulnerability check\npolicies for supported platforms. For details on usage see the\n[documentation for scribevulnpolicy](./scribevulnpolicy/README.md).\n\n## Additional documentation\n\nAdditional documentation on the library is available at [godoc.org](https://godoc.org/github.com/mozilla/scribe/).\n"
},
{
  "name": "patches-server",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "clair",
      "docs",
      "patches"
    ],
    "/docs": [
      "Patches-Server-Architecture.png",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Patches Server\n\nThe Patches-Server is responsible for handling Patches-Scanner sessions and\nserving those scanners with active sessions a complete list of vulnerabilities\nfrom one or more sources.\n\nThis document describes the function of the Patches-Server with reference to\nits [architecture diagram](docs/Patches-Server-Architecture.png)\n\n![Architecture Diagram](docs/Patches-Server-Architecture.png)\n\n## Component Definitions\n\n**Vuln Source**\n\n> Any database or API that Patches-Server can read from to retrieve a list of\n> vulnerabilities affecting a particular platform, such as Ubuntu 18.04.\n\n**Vulnerability**\n\n> A description of an exploitable malfunction present in a software package.\n\n**Cache Window**\n\n> A data structure that contains a strict subset of a larger total set of data.\n> A cache window \"slides\" forward through the set of data it provides read\n> access to as readers retrieve the data contained by the cache.\n\n**Patches Server**\n\n> A web server that handles active and queued sessions for scanners. Scanners\n> that are members of the \"active sessions\" collection are granted read access\n> to the contents of the cache window maintained by the server.\n\n**Active Sessions**\n\n> A subset of sessions connected to scanners reading from the cache window.\n\n**Queued Sessions**\n\n> A subset of sessions connected to scanners waiting to be granted read access.\n> Note that the union of active and queued sessions is equal to the set of all\n> sessions.\n\n**Patches Scanner**\n\n> Also called scanners. The client/agent component of Patches.  Scanners read\n> information about vulnerabilities served by Patches-Server and scan their\n> host for the vulnerable packages they are informed about.\n\n## Memory Efficiency Problem\n\nA source such as [CoreOS' Clair](https://github.com/coreos/clair/) may contain \ngigabytes of information about vulnerabilities affecting different platforms.\nIn light of this, Patches-Server must be able to reliably serve scanners\ninformation from such sources without exhausting its host's memory resources\neach time a session becomes active.\n\nStated more concretely, Patches-Server must satisfy the following constraints:\n\n1. All scanners granted active sessions must be served all vulnerabilities\npresent in vulnerability sources supported by Patches-Server.\n2. All scanners that attempt to open a session with Patches-Server must\neventually be granted an active session or be denied explicitly.\n3. No more than a pre-defined amount of memory may be consumed by the server at\nany given point in time.\n\nTo accomplish this, Patches-Server employs two tactics:\n\n1. Maintain a cache window that serves as a read-only view into the complete\ncollection of vulnerabilities. Active sessions will be blocked on reads to the\nwindow to guarantee that all active sessions' scanners retreive all vulns.\n2. Maintain \"active sessions\" and \"queued sessions\" collections, granting read\naccess to a cache window to active sessions' scanners before moving queued\nsessions into active sessions once all active sessions complete or timeout.\n\nThis approach requires that Patches-Scanners frequently poll the\nPatches-Scanner to both retrieve vulnerabilities and prevent their session from\ntiming out and being expelled from either collection of sessions. While a\nscanner's session is \"queued,\" it will be informed that no vulnerabilities are\nprepared for it until that session is made \"active\" and a new window is created.\n"
},
{
  "name": "nsm-build",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "build_bro_pkgs.sh",
      "build_bro_plugin_afpacket.sh",
      "build_bro_plugin_myricom.sh",
      "build_bro_support_pkgs.sh",
      "build_caf_pkgs.sh",
      "build_heka_lua.pkgs.sh",
      "build_heka_lua_pkgs.sh",
      "build_heka_pkgs.sh",
      "build_myri_pkgs.sh",
      "build_nsm-mozdef-frontend.sh",
      "build_pulledpork.sh",
      "build_supervisor_pkgs.sh",
      "build_supervisor_support_pkgs.sh",
      "build_suricata_pkgs.sh",
      "common.sh",
      "myri-snf-scripts"
    ]
  },
  "makefile": null,
  "readme": "# nsm-build\nAll kinds of scripts that build NSM packages\n"
},
{
  "name": "clearlinux",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "autospec",
      "create_rpmbuild.sh",
      "formix",
      "patches",
      "specs"
    ]
  },
  "makefile": null,
  "readme": "# clearlinux\nClear Linux scripts and specs and bits\n"
},
{
  "name": "basket-client",
  "files": {
    "/": [
      ".coveragerc",
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "basket",
      "docs",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "_static",
      "_templates",
      "change_log.rst",
      "conf.py",
      "index.rst",
      "install.rst",
      "usage.rst"
    ]
  },
  "makefile": null,
  "readme": "=============\nBasket Client\n=============\n\nThis is a client for Mozilla's email subscription service,\n`basket <https://basket.mozilla.org/>`_. Basket is not a real subscription service, but it talks to a\nreal one and we don't really care who/what it is.\n\n.. image:: https://travis-ci.org/mozilla/basket-client.svg?branch=master\n    :target: https://travis-ci.org/mozilla/basket-client\n.. image:: https://img.shields.io/pypi/v/basket-client.svg\n    :target: https://pypi.python.org/pypi/basket-client\n\n\nDocs\n----\n\nDocumentation can be found at http://basket-client.rtfd.org/\n\n\nLicense\n-------\n\nThis software is licensed under the BSD License. For more information, read the file ``LICENSE``.\n"
},
{
  "name": "mozci-tools",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "citools",
      "docs",
      "poetry.lock",
      "pyproject.toml",
      "tests",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "dump_failures.rst",
      "index.rst",
      "make.bat"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Tests](https://github.com/mozilla/mozci-tools/actions/workflows/test.yml/badge.svg)](https://github.com/mozilla/mozci-tools/actions/workflows/test.yml)\n[![PyPI version](https://badge.fury.io/py/mozci-tools.svg)](https://badge.fury.io/py/mozci-tools)\n[![Docs](https://readthedocs.org/projects/mozci-tools/badge/?version=latest)](https://mozci-tools.readthedocs.io/en/latest/?badge=latest)\n\n# mozci-tools\n\nA suite of high-level commandline tools to assist with Firefox's CI.\n\n## Installation\n\n```bash\n$ pip install mozci-tools\n$ citools --help\n```\n\n## Usage\n\nThe `citools` binary provides subcommands that can interact with Firefox's CI for a variety of purposes. See `citools --help` or [the documentation](https://mozci-tools.readthedocs.io/en/latest/) for more details on available commands.\n\n## Get Involved\n\nSee the [contribution docs](https://github.com/mozilla/mozci-tools/blob/main/CONTRIBUTING.md) if you would like to get involved!\n"
},
{
  "name": "html5-lint",
  "files": {
    "/": [
      ".gitignore",
      ".npmignore",
      "README.md",
      "bad.html",
      "good.html",
      "html5-lint.js",
      "html5check.py",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "html5-lint - HTML Validation using Mozilla's HTML5 Validator instance\n==========\n\nThis is a node.js and Python front-end to the Mozilla Labs' HTML Validator Web Service, located at https://validator.mozillalabs.com/.  It was setup in order to be used in the build system of various Mozilla projects, without spamming the main validator (i.e., http://validator.nu) --see https://bugzilla.mozilla.org/show_bug.cgi?id=763804.  You can read more about the validator at https://validator.mozillalabs.com/.\n\nYou can and should use it in your own Mozilla project's build system in order to automatically check your HTML for errors.\n\nUsage - node.js\n-------\n\nThe `html5-lint` module can be installed via npm:\n\n`$ npm install html5-lint`\n\nOnce installed, it can be used like so:\n\n```javascript\nvar fs = require( 'fs' ),\n    html5Lint = require( 'html5-lint' );\n\nfs.readFile( 'index.html', 'utf8', function( err, html ) {\n  if ( err )\n    throw err;\n\n  html5Lint( html, function( err, results ) {\n    results.messages.forEach( function( msg ) {\n      var type = msg.type, // error or warning\n          message = msg.message;\n\n      console.log( \"HTML5 Lint [%s]: %s\", type, message );\n    });\n  });\n});\n````\n\n##### gulp.js\n\nIf you are using the [gulp.js build system](http://gulpjs.com/) you may wish to use the `gulp-html5-lint` plugin. Documentation is available at https://www.npmjs.com/package/gulp-html5-lint.\n\nUsage - Python\n-------\n\n`html5check.py -h file.html`\n\nYou can test the parser with the supplied files:\n\n```bash\n$ ./html5check.py -h good.html\nThe document is valid HTML5 + ARIA + SVG 1.1 + MathML 2.0 (subject to the utter previewness of this service).\n```\n\n```bash\n$ ./html5check.py bad.html\nError: Start tag seen without seeing a doctype first. Expected\n```\n\nOptions\n--------\n\n* -h : force text/html\n* -x : force application/xhtml+xml\n* -g : GNU output\n* -e : errors only (no info or warnings)\n* --encoding=foo : declare encoding foo\n* --service=url  : the address of the HTML5 validator (defaults to https://html5.validator.nu/)\n\nTODO\n--------\n\n* error/warning filtering based on types, categories of errors/warnings\n"
},
{
  "name": "webext-generator",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "css",
      "index.html",
      "js"
    ]
  },
  "makefile": null,
  "readme": "# Webext-Generator\n\nA tool for quickly generating minimum viable web-extensions for testing submissions etc.\n\nThis tool uses purely client-side technologies with a markov chain name generator to build a unique \nminimal web-extension for upload to https://addons.mozilla.org.\n\nThe web version is available at https://staticfil.es/webext-generator/\n"
},
{
  "name": "botio-files-pdfjs",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "config.json",
      "loop",
      "on_cmd_browsertest.js",
      "on_cmd_fonttest.js",
      "on_cmd_integrationtest.js",
      "on_cmd_lint.js",
      "on_cmd_makeref.js",
      "on_cmd_preview.js",
      "on_cmd_test.js",
      "on_cmd_unittest.js",
      "on_cmd_xfatest.js",
      "on_push.js",
      "on_release.js",
      "package.json",
      "test-files"
    ]
  },
  "makefile": null,
  "readme": "# Bot.io scripts for the PDF.js project\n \nThese scripts are used in Mozilla's PDF.js project to run regression tests.\n\n+ Bot.io: http://github.com/arturadib/botio\n+ PDF.js: http://github.com/mozilla/pdf.js\n\nNOTE: the npm package for botio mightnot be updated, please use/clone the\nproject directly from the location above.\n"
},
{
  "name": "node-client-sessions",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "ChangeLog",
      "LICENSE",
      "README.md",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "[![build status](https://secure.travis-ci.org/mozilla/node-client-sessions.png)](http://travis-ci.org/mozilla/node-client-sessions)\n\nclient-sessions is connect middleware that implements sessions in encrypted tamper-free cookies.  For a complete introduction to encrypted client side sessions, refer to [Francois Marier's blog post on the subject][];\n\n[Francois Marier's blog post on the subject]: https://hacks.mozilla.org/2012/12/using-secure-client-side-sessions-to-build-simple-and-scalable-node-js-applications-a-node-js-holiday-season-part-3/\n\n**NOTE:** It is not recommended using both this middleware and connect's built-in session middleware.\n\n## Installation\n`npm install client-sessions`\n\n## Usage\n\nBasic usage:\n\n```js\nvar sessions = require(\"client-sessions\");\napp.use(sessions({\n  cookieName: 'mySession', // cookie name dictates the key name added to the request object\n  secret: 'blargadeeblargblarg', // should be a large unguessable string\n  duration: 24 * 60 * 60 * 1000, // how long the session will stay valid in ms\n  activeDuration: 1000 * 60 * 5 // if expiresIn < activeDuration, the session will be extended by activeDuration milliseconds\n}));\n\napp.use(function(req, res, next) {\n  if (req.mySession.seenyou) {\n    res.setHeader('X-Seen-You', 'true');\n  } else {\n    // setting a property will automatically cause a Set-Cookie response\n    // to be sent\n    req.mySession.seenyou = true;\n    res.setHeader('X-Seen-You', 'false');\n  }\n});\n```\n\nYou can control more specific cookie behavior during setup:\n\n```js\napp.use(sessions({\n  cookieName: 'mySession', // cookie name dictates the key name added to the request object\n  secret: 'blargadeeblargblarg', // should be a large unguessable string\n  duration: 24 * 60 * 60 * 1000, // how long the session will stay valid in ms\n  cookie: {\n    path: '/api', // cookie will only be sent to requests under '/api'\n    maxAge: 60000, // duration of the cookie in milliseconds, defaults to duration above\n    ephemeral: false, // when true, cookie expires when the browser closes\n    httpOnly: true, // when true, cookie is not accessible from javascript\n    secure: false // when true, cookie will only be sent over SSL. use key 'secureProxy' instead if you handle SSL not in your node process\n  }\n}));\n```\n\nYou can have multiple cookies:\n\n```js\n// a 1 week session\napp.use(sessions({\n  cookieName: 'shopping_cart',\n  secret: 'first secret',\n  duration: 7 * 24 * 60 * 60 * 1000\n}));\n\n// a 2 hour encrypted session\napp.use(sessions({\n  cookieName: 'authenticated',\n  secret: 'first secret',\n  duration: 2 * 60 * 60 * 1000\n}));\n```\n\nIn this example, there's a 2 hour authentication session, but shopping carts persist for a week.\n\nFinally, you can use requestKey to force the name where information can be accessed on the request object.\n\n```js\nvar sessions = require(\"client-sessions\");\napp.use(sessions({\n  cookieName: 'mySession',\n  requestKey: 'forcedSessionKey', // requestKey overrides cookieName for the key name added to the request object.\n  secret: 'blargadeeblargblarg', // should be a large unguessable string or Buffer\n  duration: 24 * 60 * 60 * 1000, // how long the session will stay valid in ms\n}));\n\napp.use(function(req, res, next) {\n  // requestKey forces the session information to be\n  // accessed via forcedSessionKey\n  if (req.forcedSessionKey.seenyou) {\n    res.setHeader('X-Seen-You', 'true');\n  }\n  next();\n});\n```\n\n## Cryptography\n\nA pair of encryption and signature keys are derived from the `secret` option\nvia HMAC-SHA-256; the `secret` isn't used directly to encrypt or compute the\nMAC.\n\nThe key-derivation function, in pseudocode:\n\n```text\n  encKey := HMAC-SHA-256(secret, 'cookiesession-encryption');\n  sigKey := HMAC-SHA-256(secret, 'cookiesession-signature');\n```\n\nThe **AES-256-CBC** cipher is used to encrypt the session contents, with an\n**HMAC-SHA-256** authentication tag (via **Encrypt-then-Mac** composition).  A\nrandom 128-bit Initialization Vector (IV) is generated for each encryption\noperation (this is the AES block size regardless of the key size).  The\nCBC-mode input is padded with the usual PKCS#5 scheme.\n\nIn pseudocode, the encryption looks like the following, with `||` denoting\nconcatenation. The `createdAt` and `duration` parameters are decimal strings.\n\n```text\n  sessionText := cookieName || '=' || sessionJson\n  iv := secureRandom(16 bytes)\n  ciphertext := AES-256-CBC(encKey, iv, sessionText)\n  payload := iv || '.' || ciphertext || '.' || createdAt || '.' || duration\n  hmac := HMAC-SHA-256(sigKey, payload)\n  cookie := base64url(iv) || '.' ||\n    base64url(ciphertext) || '.' ||\n    createdAt || '.' ||\n    duration || '.' ||\n    base64url(hmac)\n```\n\nFor decryption, a constant-time equality operation is used to verify the HMAC\noutput to avoid the plausible timing attack.\n\n### Advanced Cryptographic Options\n\nThe defaults are secure, but may not suit your requirements. Some example scenarios:\n- You want to use randomly-generated keys instead of using the key-derivation\n  function used in this module.\n- AES-256 is overkill for the type of data you store in the session (e.g. not\n  personally-identifiable or sensitive) and you'd like to trade-off decreasing\n  the security level for CPU economy.\n- SHA-256 is maybe too weak for your application and you want to have more\n  MAC security by using SHA-512, which grows the size of your cookies slightly.\n\nIf the defaults don't suit your needs, you can customize client-sessions.\n**Beware: Changing keys and/or algorithms will make previously-generated\nCookies invalid!**\n\n#### Configuring Keys\n\nTo configure independent encryption and signature (HMAC) keys:\n\n```js\napp.use(sessions({\n  encryptionKey: loadFromKeyStore('session-encryption-key'),\n  signatureKey: loadFromKeyStore('session-signature-key'),\n  // ... other options discussed above ...\n}));\n```\n\n#### Configuring Algorithms\n\nTo specify custom algorithms and keys:\n\n```js\napp.use(sessions({\n  // use WEAKER-than-default encryption:\n  encryptionAlgorithm: 'aes128',\n  encryptionKey: loadFromKeyStore('session-encryption-key'),\n  // use a SHORTER-than-default MAC:\n  signatureAlgorithm: 'sha256-drop128',\n  signatureKey: loadFromKeyStore('session-signature-key'),\n  // ... other options discussed above ...\n}));\n```\n\n#### Encryption Algorithms\n\nSupported CBC-mode `encryptionAlgorithm`s (and key length requirements):\n\n| Cipher | Key length |\n| ------ | ---------- |\n| aes128 | 16 bytes   |\n| aes192 | 24 bytes   |\n| aes256 | 32 bytes   |\n\nThese key lengths are exactly as required by the [Advanced Encryption\nStandard](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard).\n\n#### Signature (HMAC) Algorithms\n\nSupported HMAC `signatureAlgorithm`s (and key length requirements):\n\n| HMAC           | Minimum Key Length | Maximum Key Length |\n| -------------- | ------------------ | ------------------ |\n| sha256         | 32 bytes           | 64 bytes           |\n| sha256-drop128 | 32 bytes           | 64 bytes           |\n| sha384         | 48 bytes           | 128 bytes          |\n| sha384-drop192 | 48 bytes           | 128 bytes          |\n| sha512         | 64 bytes           | 128 bytes          |\n| sha512-drop256 | 64 bytes           | 128 bytes          |\n\nThe HMAC key length requirements are derived from [RFC 2104 section\n3](https://tools.ietf.org/html/rfc2104#section-3). The maximum key length can\nbe exceeded, but it doesn't increase the security of the signature.\n\nThe `-dropN` algorithms discard the latter half of the HMAC output, which\nprovides some additional protection against SHA2 length-extension attacks on\ntop of HMAC. The same technique is used in the upcoming [JSON Web Algorithms\n`AES_CBC_HMAC_SHA2` authenticated\ncipher](http://tools.ietf.org/html/draft-ietf-jose-json-web-algorithms-19#section-5.2).\n\n#### Generating Keys\n\nOne can easily generate both AES and HMAC-SHA2 keys via command line: `openssl\nrand -base64 32` for a 32-byte (256-bit) key.  It's easy to then parse that\noutput into a `Buffer`:\n\n```js\nfunction loadKeyFromStore(name) {\n  var text = myConfig.keys[name];\n  return Buffer.from(text, 'base64');\n}\n```\n\n#### Key Constraints\n\nIf you specify `encryptionKey` or `signatureKey`, you must supply the other as\nwell.\n\nThe following constraints must be met or an `Error` will be thrown:\n\n1. both keys must be `Buffer`s.\n2. the keys must be _different_.\n3. the encryption key are _exactly_ the length required (see above).\n4. the signature key has _at least_ the length required (see above).\n\nBased on the above, please note that if you specify a `secret` _and_ a\n`signatureAlgorithm`, you need to use `sha256` or `sha256-drop128`.\n\n## License\n\n> This Source Code Form is subject to the terms of the Mozilla Public\n> License, v. 2.0. If a copy of the MPL was not distributed with this\n> file, You can obtain one at http://mozilla.org/MPL/2.0/.\n"
},
{
  "name": "dns-packet",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintrc",
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "LICENSE",
      "README.md",
      "classes.js",
      "examples",
      "index.js",
      "opcodes.js",
      "optioncodes.js",
      "package.json",
      "rcodes.js",
      "test.js",
      "types.js"
    ]
  },
  "makefile": null,
  "readme": "# dns-packet\n[![](https://img.shields.io/npm/v/dns-packet.svg?style=flat)](https://www.npmjs.org/package/dns-packet) [![](https://img.shields.io/npm/dm/dns-packet.svg)](https://www.npmjs.org/package/dns-packet) [![](https://api.travis-ci.org/mafintosh/dns-packet.svg?style=flat)](https://travis-ci.org/mafintosh/dns-packet) [![Coverage Status](https://coveralls.io/repos/github/mafintosh/dns-packet/badge.svg?branch=master)](https://coveralls.io/github/mafintosh/dns-packet?branch=master)\n\nAn [abstract-encoding](https://github.com/mafintosh/abstract-encoding) compliant module for encoding / decoding DNS packets. Lifted out of [multicast-dns](https://github.com/mafintosh/multicast-dns) as a separate module.\n\n```\nnpm install dns-packet\n```\n\n## UDP Usage\n\n``` js\nconst dnsPacket = require('dns-packet')\nconst dgram = require('dgram')\n\nconst socket = dgram.createSocket('udp4')\n\nconst buf = dnsPacket.encode({\n  type: 'query',\n  id: 1,\n  flags: dnsPacket.RECURSION_DESIRED,\n  questions: [{\n    type: 'A',\n    name: 'google.com'\n  }]\n})\n\nsocket.on('message', message => {\n  console.log(dnsPacket.decode(message)) // prints out a response from google dns\n})\n\nsocket.send(buf, 0, buf.length, 53, '8.8.8.8')\n```\n\nAlso see [the UDP example](examples/udp.js).\n\n## TCP, TLS, HTTPS\n\nWhile DNS has traditionally been used over a datagram transport, it is increasingly being carried over TCP for larger responses commonly including DNSSEC responses and TLS or HTTPS for enhanced security. See below examples on how to use `dns-packet` to wrap DNS packets in these protocols:\n\n- [TCP](examples/tcp.js)\n- [DNS over TLS](examples/tls.js)\n- [DNS over HTTPS](examples/doh.js)\n\n## API\n\n#### `var buf = packets.encode(packet, [buf], [offset])`\n\nEncodes a DNS packet into a buffer containing a UDP payload.\n\n#### `var packet = packets.decode(buf, [offset])`\n\nDecode a DNS packet from a buffer containing a UDP payload.\n\n#### `var buf = packets.streamEncode(packet, [buf], [offset])`\n\nEncodes a DNS packet into a buffer containing a TCP payload.\n\n#### `var packet = packets.streamDecode(buf, [offset])`\n\nDecode a DNS packet from a buffer containing a TCP payload.\n\n#### `var len = packets.encodingLength(packet)`\n\nReturns how many bytes are needed to encode the DNS packet\n\n## Packets\n\nPackets look like this\n\n``` js\n{\n  type: 'query|response',\n  id: optionalIdNumber,\n  flags: optionalBitFlags,\n  questions: [...],\n  answers: [...],\n  additionals: [...],\n  authorities: [...]\n}\n```\n\nThe bit flags available are\n\n``` js\npacket.RECURSION_DESIRED\npacket.RECURSION_AVAILABLE\npacket.TRUNCATED_RESPONSE\npacket.AUTHORITATIVE_ANSWER\npacket.AUTHENTIC_DATA\npacket.CHECKING_DISABLED\n```\n\nTo use more than one flag bitwise-or them together\n\n``` js\nvar flags = packet.RECURSION_DESIRED | packet.RECURSION_AVAILABLE\n```\n\nAnd to check for a flag use bitwise-and\n\n``` js\nvar isRecursive = message.flags & packet.RECURSION_DESIRED\n```\n\nA question looks like this\n\n``` js\n{\n  type: 'A', // or SRV, AAAA, etc\n  class: 'IN', // one of IN, CS, CH, HS, ANY. Default: IN\n  name: 'google.com' // which record are you looking for\n}\n```\n\nAnd an answer, additional, or authority looks like this\n\n``` js\n{\n  type: 'A', // or SRV, AAAA, etc\n  class: 'IN', // one of IN, CS, CH, HS\n  name: 'google.com', // which name is this record for\n  ttl: optionalTimeToLiveInSeconds,\n  (record specific data, see below)\n}\n```\n\n## Supported record types\n\n#### `A`\n\n``` js\n{\n  data: 'IPv4 address' // fx 127.0.0.1\n}\n```\n\n#### `AAAA`\n\n``` js\n{\n  data: 'IPv6 address' // fx fe80::1\n}\n```\n\n#### `CAA`\n\n``` js\n{\n  flags: 128, // octet\n  tag: 'issue|issuewild|iodef',\n  value: 'ca.example.net',\n  issuerCritical: false\n}\n```\n\n#### `CNAME`\n\n``` js\n{\n  data: 'cname.to.another.record'\n}\n```\n\n#### `DNAME`\n\n``` js\n{\n  data: 'dname.to.another.record'\n}\n```\n\n#### `DNSKEY`\n\n``` js\n{\n  flags: 257, // 16 bits\n  algorithm: 1, // octet\n  key: Buffer\n}\n```\n\n#### `DS`\n\n``` js\n{\n  keyTag: 12345,\n  algorithm: 8,\n  digestType: 1,\n  digest: Buffer\n}\n```\n\n#### `HINFO`\n\n``` js\n{\n  data: {\n    cpu: 'cpu info',\n    os: 'os info'\n  }\n}\n```\n\n#### `MX`\n\n``` js\n{\n  preference: 10,\n  exchange: 'mail.example.net'\n}\n```\n\n#### `NS`\n\n``` js\n{\n  data: nameServer\n}\n```\n\n#### `NSEC`\n\n``` js\n{\n  nextDomain: 'a.domain',\n  rrtypes: ['A', 'TXT', 'RRSIG']\n}\n```\n\n#### `NSEC3`\n\n``` js\n{\n  algorithm: 1,\n  flags: 0,\n  iterations: 2,\n  salt: Buffer,\n  nextDomain: Buffer, // Hashed per RFC5155\n  rrtypes: ['A', 'TXT', 'RRSIG']\n}\n```\n\n#### `NULL`\n\n``` js\n{\n  data: Buffer('any binary data')\n}\n```\n\n#### `OPT`\n\n[EDNS0](https://tools.ietf.org/html/rfc6891) options.\n\n``` js\n{\n  type: 'OPT',\n  name: '.',\n  udpPayloadSize: 4096,\n  flags: packet.DNSSEC_OK,\n  options: [{\n    // pass in any code/data for generic EDNS0 options\n    code: 12,\n    data: Buffer.alloc(31)\n  }, {\n    // Several EDNS0 options have enhanced support\n    code: 'PADDING',\n    length: 31,\n  }, {\n    code: 'CLIENT_SUBNET',\n    family: 2, // 1 for IPv4, 2 for IPv6\n    sourcePrefixLength: 64, // used to truncate IP address\n    scopePrefixLength: 0,\n    ip: 'fe80::',\n  }, {\n    code: 'TCP_KEEPALIVE',\n    timeout: 150 // increments of 100ms.  This means 15s.\n  }, {\n    code: 'KEY_TAG',\n    tags: [1, 2, 3],\n  }]\n}\n```\n\nThe options `PADDING`, `CLIENT_SUBNET`, `TCP_KEEPALIVE` and `KEY_TAG` support enhanced de/encoding. See [optionscodes.js](https://github.com/mafintosh/dns-packet/blob/master/optioncodes.js) for all supported option codes. If the `data` property is present on a option, it takes precedence. On decoding, `data` will always be defined.\n\n#### `PTR`\n\n``` js\n{\n  data: 'points.to.another.record'\n}\n```\n\n#### `RP`\n\n``` js\n{\n  mbox: 'admin.example.com',\n  txt: 'txt.example.com'\n}\n```\n\n#### `RRSIG`\n\n``` js\n{\n  typeCovered: 'A',\n  algorithm: 8,\n  labels: 1,\n  originalTTL: 3600,\n  expiration: timestamp,\n  inception: timestamp,\n  keyTag: 12345,\n  signersName: 'a.name',\n  signature: Buffer\n}\n```\n\n#### `SOA`\n\n``` js\n{\n  data:\n    {\n      mname: domainName,\n      rname: mailbox,\n      serial: zoneSerial,\n      refresh: refreshInterval,\n      retry: retryInterval,\n      expire: expireInterval,\n      minimum: minimumTTL\n    }\n}\n```\n\n#### `SRV`\n\n``` js\n{\n  data: {\n    port: servicePort,\n    target: serviceHostName,\n    priority: optionalServicePriority,\n    weight: optionalServiceWeight\n  }\n}\n```\n\n#### `TXT`\n\n``` js\n{\n  data: 'text' || Buffer || [ Buffer || 'text' ]\n}\n```\n\nWhen encoding, scalar values are converted to an array and strings are converted to UTF-8 encoded Buffers. When decoding, the return value will always be an array of Buffer.\n\nIf you need another record type, open an issue and we'll try to add it.\n\n## License\n\nMIT\n"
},
{
  "name": "pocket-help-center",
  "files": {
    "/": [
      "README.md",
      "assets"
    ]
  },
  "makefile": null,
  "readme": "# pocket-help-center\nCSS for Pocket's Help Center\n"
},
{
  "name": "looker-spoke-dev",
  "files": {
    "/": [
      "README.md",
      "ascholtz-dev-ujet",
      "ascholtz-dev",
      "emtwo-dev",
      "frank-dev",
      "jthomas-perf",
      "manifest.lkml",
      "mozilla-vpn-dev",
      "shong-dev"
    ]
  },
  "makefile": null,
  "readme": "# looker-spoke-dev\nDevelopment repository for Looker spoke projects. \n"
},
{
  "name": "certspotter-cloudformation",
  "files": {
    "/": [
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "certspotter-dynamodb.yml",
      "certspotter-ses.yml",
      "certspotter-sqs.yml",
      "example_record.json",
      "test.bash"
    ]
  },
  "makefile": null,
  "readme": "# certspotter-cloudformation\n\nThis [AWS CloudFormation](https://aws.amazon.com/cloudformation/) template,\n[`certspotter-sqs.yml`](certspotter-sqs.yml) will create a hosted version of the \n[SSLMate certspotter](https://github.com/SSLMate/certspotter) app.\n\nThis installation of certspotter will\n* create an events in the [Splunk certificate format](https://docs.splunk.com/Documentation/CIM/4.20.2/User/Certificates)\n* send events to an [AWS Message Queuing Service (SQS)](https://aws.amazon.com/sqs/)\n  queue for every matching certificate. These reports can then be consumed by a SIEM (e.g. Splunk).\n* store all matching certificate transparency events in a DynamoDB \n\n## Installation\n\nCreate the DynamoDB to store certificate transparency records in. This can be\ndone manually or with the [`certspotter-dynamodb.yml`](certspotter-dynamodb.yml)\ntemplate.\n\nDeploy the `certspotter-sqs.yml` CloudFormation template in AWS. The template\nparameters let you set\n* which SQS queue and which DynamoDB to send new certificate transparency \n  records to\n* the S3 URL of the watchlist to use which contains the domains that you want to\n  filter for\n* the cron schedule for how often to run certspotter\n* whether or not to start from the end of the certificate transparency logs.\n  Starting from the end of the logs will allow certspotter to complete quickly.\n  Starting from the beginning will require a very long time to process all of\n  the historical logs.\n* the optional Elastic IP to assign to the certspotter EC2 instance\n\n## Logging\n\nVerbose logs from the past 4 weeks of certspotter runs can be found in\n`/var/log/certspotter.log` along with the other weekly logrotated files. The \nstart and end of runs can be found with\n\n    grep \"cron initiated run\" /var/log/certspotter.log\n\nA log of every matched certificate is kept in `/home/centos/certificates_matched.log`\n\n## Files\n\n### `certspotter-dynamodb.yml`\n\nThis CloudFormation template creates a simple DynamoDB table to store certificate\ntransparency records for matching domain names.\n\n### `certspotter-sqs.yml`\n\nThis CloudFormation template creates an EC2 instance that will run certspotter.\nIt also provisions an IAM Role for the EC2 instance to use to enable it to\n* Read the watchlist from S3\n* Send matching records to SQS\n* Put matching records in the DynamoDB table\n\n### `send_to_sqs.py`\n\nThe `certspotter-sqs.yml` CloudFormation template creates a simple python tool \ncalled `send_to_sqs.py` which sends certificates discovered by certspotter to \nSQS as well as storing them in a DynamoDB.\n\n#### Experimenting with `send_to_sqs.py`\n\nTo display the message before sending it to SQS, change the\n`client.send_message` command to something which prints the message first like this\n\n    queue_url = client.get_queue_url(QueueName=ARGS[1], QueueOwnerAWSAccountId=ARGS[2])['QueueUrl']\n    print(json.dumps(data))\n    client.send_message(QueueUrl=queue_url, MessageBody=json.dumps(data, sort_keys=True))\n\n#### Testing `send_to_sqs.py`\n\nThe `test.bash` script will set example environment variables and run \n`send_to_sqs.py`\n\n### `certspotter-ses.yml`\n\nThis CloudFormation template deploys certspotter configured to emit matching\ndomain names via email using SES.\n\n## Deploying\n\nWhen updating an existing deployment, try retaining the `/home/centos/.certspotter/certs/`\ndirectory as it contains a copy of all the certs it finds that match the watchlist\nwhich might be interesting down the road as well as the current position in all\nthe logs which will allow you to pick up from where certspotter last was in the \nlogs.\n\n### Reading from the end of the log\n\nWhen starting certspotter for the first time, you will probably want to begin\nconsuming the CT logs from the end of all current logs. Consuming from the\nbeginning of the logs would take a very long time.\n\nTo do this, set the `StartFromEndOfCTLogs` CloudFormation parameter to `true`\n\nThis will index all of the logs at their tails.\n\n## DynamoDB Table\n\nEach matching certificate transparency record is written to the DynamoDB table.\nEach record has a unique `id` field which is a combination of the certificate \nissuer and the serial number of the certificate. Example\n\n    C=US, O=DigiCert Inc, CN=DigiCert SHA2 Secure Server CA, 858cee155d7179ea7b50054e670ab51\n\nThe `date` field is the [unix time](https://en.wikipedia.org/wiki/Unix_time) of\nthe `not_before_unixtime` field. This will allow sorting the records by the date\non which the certificate is first valid.\n\nThe `record` field is the JSON record from certspotter. See the format in \n[`example_record.json`](example_record.json)"
},
{
  "name": "ssm-acquire",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      "AUTHORS.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "bin",
      "cloudformation",
      "conf",
      "docker",
      "docs",
      "lambda_handler",
      "requirements_dev.txt",
      "setup.cfg",
      "setup.py",
      "ssm_acquire",
      "tests"
    ],
    "/docs": [
      "Makefile",
      "authors.rst",
      "conf.py",
      "contributing.rst",
      "history.rst",
      "index.rst",
      "installation.rst",
      "make.bat",
      "readme.rst",
      "requirements.txt",
      "usage.rst"
    ]
  },
  "makefile": ".PHONY: clean clean-test clean-pyc clean-build docs help\n.DEFAULT_GOAL := help\n\ndefine BROWSER_PYSCRIPT\nimport os, webbrowser, sys\n\ntry:\n\tfrom urllib import pathname2url\nexcept:\n\tfrom urllib.request import pathname2url\n\nwebbrowser.open(\"file://\" + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\n\ndefine PRINT_HELP_PYSCRIPT\nimport re, sys\n\nfor line in sys.stdin:\n\tmatch = re.match(r'^([a-zA-Z_-]+):.*?## (.*)$$', line)\n\tif match:\n\t\ttarget, help = match.groups()\n\t\tprint(\"%-20s %s\" % (target, help))\nendef\nexport PRINT_HELP_PYSCRIPT\n\nBROWSER := python -c \"$$BROWSER_PYSCRIPT\"\n\nhelp:\n\t@python -c \"$$PRINT_HELP_PYSCRIPT\" < $(MAKEFILE_LIST)\n\nclean: clean-build clean-pyc clean-test ## remove all build, test, coverage and Python artifacts\n\nclean-build: ## remove build artifacts\n\trm -fr build/\n\trm -fr dist/\n\trm -fr .eggs/\n\tfind . -name '*.egg-info' -exec rm -fr {} +\n\tfind . -name '*.egg' -exec rm -f {} +\n\nclean-pyc: ## remove Python file artifacts\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -fr {} +\n\nclean-test: ## remove test and coverage artifacts\n\trm -fr .tox/\n\trm -f .coverage\n\trm -fr htmlcov/\n\trm -fr .pytest_cache\n\nlint: ## check style with flake8\n\tflake8 ssm_acquire tests\n\ntest: ## run tests quickly with the default Python\n\tpy.test\n\ntest-all: ## run tests on every Python version with tox\n\ttox\n\ncoverage: ## check code coverage quickly with the default Python\n\tcoverage run --source ssm_acquire -m pytest\n\tcoverage report -m\n\tcoverage html\n\t$(BROWSER) htmlcov/index.html\n\ndocs: ## generate Sphinx HTML documentation, including API docs\n\trm -f docs/ssm_acquire.rst\n\trm -f docs/modules.rst\n\tsphinx-apidoc -o docs/ ssm_acquire\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C docs html\n\t$(BROWSER) docs/_build/html/index.html\n\nservedocs: docs ## compile the docs watching for changes\n\twatchmedo shell-command -p '*.rst' -c '$(MAKE) -C docs html' -R -D .\n\nrelease: dist ## package and upload a release\n\ttwine upload dist/*\n\ndist: clean ## builds source and wheel package\n\tpython setup.py sdist\n\tpython setup.py bdist_wheel\n\tls -l dist\n\ninstall: clean ## install the package to the active Python's site-packages\n\tpython setup.py install\n\nlambda-package:\n\tmkdir -p lambda_package\n\tdocker run -v `pwd`:/files threatresponse/rekall:latest pip install . --upgrade -t lambda_package/\n\tdocker run -v `pwd`:/files threatresponse/rekall:latest pip install rekall-agent rekall -t lambda_package/\n\tdocker run -v `pwd`:/files threatresponse/rekall:latest pip install future==0.16.0 --upgrade -t lambda_package/\n\tcp lambda_handler/*.py lambda_package/\n\nupload-lambda:\n\tcp lambda_handler/*.py lambda_package/\n\tzip -g ssm_acquire.zip lambda_package/*\n\taws s3 cp ssm_acquire.zip s3://ssm-acquire-us-west-2.threatresponse.cloud/ssm_acquire.zip --acl public-read\n",
  "readme": "===========\nssm-acquire\n===========\n\n\n.. image:: https://img.shields.io/pypi/v/ssm_acquire.svg\n        :target: https://pypi.python.org/pypi/ssm_acquire\n\n.. image:: https://readthedocs.org/projects/ssm-acquire/badge/?version=latest\n        :target: https://ssm-acquire.readthedocs.io/en/latest/?badge=latest\n        :alt: Documentation Status\n\nA python module for orchestrating content acquisitions and analysis via amazon ssm.  Note:  This is a pre-release.\n\n* Free software: MPL 2.0 License\n* Documentation: https://ssm-acquire.readthedocs.io.\n\nFeatures\n--------\n\n* Acquire memory from a linux instance to an S3 bucket using SSM.\n* Interrogate an instance for top-10 IOCs using OSQuery and save the jsonified output.\n* Analyze a memory sample on a machine using docker.\n* Create a rekall profile using an instance as a build target running the Amazon SSM Agent.\n\n\nUsage\n--------\n\nSample Cli Usage\n^^^^^^^^^^^^^^^^^\n::\n\n    pip install ssm_acquire\n    Usage: ssm_acquire [OPTIONS]\n\n    ssm_acquire a rapid evidence preservation tool for Amazon EC2.\n\n    Options:\n      --instance_id TEXT  The instance you would like to operate on.\n      --region TEXT       The aws region where the instance can be found.\n      --build             Specify if you would like to build a rekall profile with\n                          this capture.\n      --acquire           Use linpmem to acquire a memory sample from the system\n                          in question.\n      --interrogate       Use OSQuery binary to preserve top 10 type queries for\n                          rapid forensics.\n      --analyze           Use docker and rekall to autoanalyze the memory capture.\n      --deploy            Create a lambda function with a handler to take events\n                          from AWS GuardDuty.\n      --help              Show this message and exit.\n\nGetting Started\n^^^^^^^^^^^^^^^^^\n\nDeploy Responder Role into AWS Account with the CloudFormation Template: cloudformation/responder_role.yml. (Note: this role requires 2FA to assume) This will create a role with the required permissions to run ssm commands on ec2 instances and an s3 bucket to store the memory assets. You will need the bucket name and the ARN of the role in the next step.\n\nSetup a config file in your home directory. It should be named `.threatresponse.ini` There is a sample config file in conf/settings.ini - it has three required parameters.\n\n* mfa_serial_number: the serial number for your MFA device for assuming the role.\n* asset_bucket: the name of the bucket to store the assets. This was created in step 1.\n* ssm_acquire_role_arn: the ARN of the Responder Role you created in step 1.\n\n``pip install ssm_acquire``\n\nTo acquire memory and build a rekall profile from an instance:\n\n``ssm_acquire --instance_id i-xxxxxxxx --region us-west-2 --build --acquire``\n\nYou can analyze your memory capture right away with:\n\n``ssm_acquire --instance_id i-xxxxxxx --analyze``\n\nThis will analyze the memory dump with the most common rekall plugins: [psaux, pstree, netstat, ifconfig, pidhashtable]\nWhen the analysis is done it will upload the results back to the asset store.\n\n\nCredits\n-------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n"
},
{
  "name": "wikimo_content",
  "files": {
    "/": [
      ".gitignore",
      "300px-OpSec.png",
      "CODE_OF_CONDUCT.md",
      "File",
      "IAM",
      "LICENSE",
      "README.md",
      "Security.mediawiki",
      "Security",
      "requirements.txt",
      "rm.png",
      "sync.json.inc",
      "sync.py"
    ]
  },
  "makefile": null,
  "readme": "# Wikimo Git documentation\n\nThis repository stores  documentation for https://wiki.mozilla.org/ and in particular the\nhttps://wiki.mozilla.org/Security pages. It is possible to both pull existing pages, or changes made to pages\ndirectly back to Git as well as commit changes to Git first then push them to the wiki. In any case, the Git repository\n has authority and, for pages tracked here, it is highly recommended to ensure that your changes are sent via a\npull-request.\n\nPlease do not change the main repository directly (i.e always fork and send a pull-request, get at least one\npeer-review and merge).\n\nAll documentation is licensed under the MPL (see LICENSE).\n\n# Notes on infosec.mozilla.org (no longer hosted on wikimo)\n\nCertain pages used to be hosted on wiki.mozilla.org are now hosted on https://infosec.mozilla.org.\nThe wiki.mozilla.org pages redirect there automatically. Please use https://github.com/mozilla/infosec.mozilla.org if\nyou wish to improve these pages.\n\n# Drafting new documentation, or changes to documents\n\nSometimes, it's a little hard to edit documents directly from Git in your own text editor.\nWhen or if that is the case, the currently recommended practice is:\n\n1. Create a draft in your Wikimo space, such as: https://wiki.mozilla.org/User:Gdestuynder/mydraft\n  a. Optionally: Create an issue in https://github.com/mozilla/wikimo_content/issues to indicate you're working on this.\n     This may be particularly useful if you're going to work on this for a long time to avoid work duplication.\n2. Edit, preview, etc. to your heart's content.\n3. Copy paste the result to Git and push to your local/personal GitHub fork of this very repository\n4. Create a PR (Pull-request) with the new contents and also refer to your Wikimo space for a rendered version.\n\n# Usage of the sync.py tool\n\nThis tool synchronizes Git to Mediawiki automatically (both ways!) and should be run after you merged commits to GitHub\nfor changes to appear on Mediawiki (or to pull changes from Mediawiki into GitHub!).\n\n**Recommended:** Install/use virtual envs for Python. See the example below if you've never done that before.\n\n```\n# Place these in your $HOME/.bashrc or favorite shell RC file\n# then call 'pyvenv2' or 'pyvenv' when entering the directory with the virtual\n# python environment.\n\n# Python 2.x - 3.5\nfunction pyvenv2() {\n[ -d venv2 ] || virtualenv2 --no-site-packages venv2\nsource venv2/bin/activate\n}\n# Python 3.6+\nfunction pyvenv() {\n\t[ -d venv ] || python3 -m venv venv\n\tsource venv/bin/activate\n}\n```\n\n## Install & configure sync.py\n\n```\n$ git clone https://github.com/mozilla/wikimo_content && cd wikimo_content\n# <activate your venv>\n$ pip install -r requirements.txt\n$ cp sync.json.inc sync.json\n# <edit sync.json>\n```\n\n## Using sync.py\n\nChecking for differences/changes between your local Git copy and Wikimo. Nothing will be written to Wikimo.\n\n```\n$ ./sync.py\n# This also shows the unified diff for the pages\n$ ./sync.py -d\n```\n\n\nPush changes - this **will** write to Wikimo.\n\n```\n$ ./sync.py --push\n```\n\nTo sync the wiki back to git (for example when someone forgot to use git... ;-).\n\n```\n$ ./sync.py --get\n# To send back to GitHub as well:\n$ git commit -a -m \"your commit msg here\"\n$ git push origin master:myfix\n# Create a pull-request after this to get your changes integrated upstream\n```\n"
},
{
  "name": "fx-crash-sig",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bigquery-etl.py",
      "bin",
      "docker-compose.yml",
      "example.py",
      "fx_crash_sig",
      "requirements.dev.txt",
      "sample.json",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "DEFAULT_GOAL := help\n\n# Include .env and export it so variables set in there are available in the\n# Makefile.\ninclude .env\nexport\n\n# Set these in the environment to override them. This is helpful for\n# development if you have file ownership problems because the user in the\n# container doesn't match the user on your host.\nUSER_ID ?= 10001\nGROUP_ID ?= 10001\n\n.PHONY: help\nhelp:\n\t$(info Available rules:)\n\t$(info )\n\t@fgrep -h \"##\" Makefile | fgrep -v fgrep | sed 's/\\(.*\\):.*##/\\1:/' | column -t -s '|'\n\n.docker-build:\n\tmake build\n\n.env:\n\t@if [ ! -f .env ]; \\\n\tthen \\\n\techo \"Creating .env file...\"; \\\n\techo \"# USER_ID=\\n# GROUP_ID=\\n\" > .env; \\\n\tfi\n\n.PHONY: build\nbuild: .env  ## | Build docker image\n\tdocker-compose build --build-arg USER_ID=${USER_ID} --build-arg GROUP_ID=${GROUP_ID} app\n\ttouch .docker-build\n\n.PHONY: clean\nclean:  ## | Remove build and runtime artifacts\n\trm .docker-build\n\trm -rf fx_crash_sig.egg-info\n\trm -rf build dist\n\tfind . -name \"__pycache__\" -type d | xargs rm -rf \n\n.PHONY: lint\nlint: .docker-build  ## | Run linters\n\tdocker-compose run --rm app /app/bin/run_lint.sh\n\n.PHONY: reformat \nreformat:  ## | Reformat code\n\tdocker-compose run --rm app /app/bin/run_lint.sh --reformat\n\n.PHONY: test\ntest:  ## | Run tests in docker environment\n\tdocker-compose run --rm app /app/bin/run_tests.sh\n",
  "readme": "# fx-crash-sig\n\nGet crash signature from Firefox crash trace\n\nTake crash trace and:\n\n1. Use [tecken](https://github.com/mozilla-services/tecken) symbolication to symbolicate crash trace\n\n2. use [socorro-siggen](https://github.com/willkg/socorro-siggen) to get crash signature\n\n\n## Install (from [PyPI](https://pypi.org/project/fx-crash-sig/))\n\nTo install:\n\n```sh\npip install fx-crash-sig\n```\n\n(Optional) Install with google-cloud-bigquery library requirements:\n\n```sh\npip install fx-crash-sig[bq]\n```\n\n## Usage\n\n[Example script](/example.py):\n\n```py\nfrom fx_crash_sig import sample_traces\nfrom fx_crash_sig.crash_processor import CrashProcessor\n\ncrash_processor = CrashProcessor()\n\nsignature = crash_processor.get_signature(sample_traces.trace1)\n```\n\nCommand line (using [sample.json](/sample.json)):\n\n```sh\n$ cat sample.json | fx-crash-sig\nEMPTY: no crashing thread identified\n```\n\n```sh\n$ python example.py\n```\n\n\n## For develpment\n\nBuild:\n\n```sh\nmake build\n```\n\nLint:\n\n```sh\nmake lint\nmake reformat\n```\n\nTest docker environment:\n\n```sh\nmake test\n```\n\nIf you have problems with file permissions when using the Docker image, edit\nyour `.env` file and change the `USER_ID` and `GROUP_ID` values to match your\nuid and gid.\n\nTest against Python versions:\n\n```sh\ntox\n```\n\nNote: This requires you have Python environments set up and a virtual\nenvironment with tox installed.\n\n\n## Release process\n\n1. Create a `release_X_Y_Z` branch\n2. Update version and release date in `fx_crash_sig/__init__.py`\n3. Run tests\n4. Push branch to GitHub, create a PR, review it, and merge it\n5. Create a signed tag, push to GitHub:\n   ```sh\n   git tag -s vX.Y.Z\n   git push --tags REMOTE TAGNAME\n   ```\n6. Build package files:\n   ```sh\n   python setup.py sdist bdist_wheel\n   ```\n   Be sure to use Python 3 with a virtualenv with an updated `requirements.dev.txt` file.\n7. Upload to PyPI:\n   ```sh\n   twine upload dist/*\n   ```\n"
},
{
  "name": "ff-subbeta-beta-release-matching",
  "files": {
    "/": [
      "README.md",
      "perf_release_criteria",
      "poc"
    ]
  },
  "makefile": null,
  "readme": "# Finding Firefox Beta Subsets Resembling Release\n\nWork related to the application of [Statistical matching](https://en.wikipedia.org/wiki/Matching_(statistics)) methods to finding subsets of Firefox Beta users that are representative of Release. These subsets can be utilized to forecast Release behavior _before_ it is launched to the populace. \n\n# Project\nThe primary project details are given in the [PRD](https://docs.google.com/document/d/1Ygz6MkudYHZjnDnD9Z97kUyFrvV3KGWsjXyPjddhHq0/edit?usp=sharing). The results and deliverables of the project [milestones](https://docs.google.com/document/d/1Ygz6MkudYHZjnDnD9Z97kUyFrvV3KGWsjXyPjddhHq0/edit#heading=h.lvb9l8gw2nee) are contained in the `perf_release_criteria` directory. \n\n# Proof-of-concept\nInitial proof-of-concept work is contained in the `poc` directory. \nThe [MatchIt](https://cran.r-project.org/web/packages/MatchIt/vignettes/matchit.pdf) library in R was used for matching.\n\n* `data_prep`: contains pyspark scripts (converted from Databricks notebooks) that created datasets from Firefox telemetry pipeline.\n   - `Beta_Release_Matching_Perf_Metrics_POC.py`: data munger for the [initial analysis](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release.html#tl;dr).\n   - `Beta_Release_Matching_Perf_Metrics_Validation.py`: data munger for the [validating analysis](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release_validation.html)\n* `analysis`: contains R Markdown, scripts, and final rendered html reports. \n   - `modeling_poc.Rmd`: grunt exploratory work regarding initial statistical modeling efforts.\n   - `report_poc.Rmd`: final R Markdown report regarding [initial statistical modeling](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release.html#tl;dr).\n   - `report_validation.Rmd`: final R Markdown report regarding [method validation](https://metrics.mozilla.com/protected/cdowhygelund/beta_subset_release_validation.html).    \n\n\n"
},
{
  "name": "Relman-PostMortems",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Release Management  action items from Firefox retrospectives\n"
},
{
  "name": "rra3json",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "requirements.txt",
      "rra3json.py",
      "rra3json.yml",
      "rra_schema.json"
    ]
  },
  "makefile": null,
  "readme": "This program integrates with service-map: https://github.com/mozilla/service-map\nIt posts a JSON version of the Gdocs RRA documents to service-map, to be precise.\n\nSee also: \n- https://wiki.mozilla.org/Security/Risk_management/Rapid_Risk_Assessment\n- https://wiki.mozilla.org/Security/Data_Classification\n- https://infosec.mozilla.com\n\n# Get oauth2 credentials\n\nSee https://developers.google.com/api-client-library/python/start/get_started for a complete guide. This is the TLDR\nversion:\n- As your user, login to https://console.developers.google.com/project/ and create a new project.\n- Go to \"Credentials\".\n- Click \"Create credentials: Service account key\"\n\nYou'll get a JSON key back (JWT), that's your credentials. It should contain `\"type\": \"service_account\"`, a\n`client_email`, a `private_key` and a bunch of metadata.\n\nNOTE: Make sure you authorize your service email (`client_email` field) to all the document you'll want rra3json to have\naccess to! By default it has no accesses.\n\n# JSON Format\n\nRefer to the `rra_schema.json` file.\n\n# How to run\n\nMake sure you have a configuration file such as `rra3json.yml` with all information filed in and run:\n```\n    #Make a venv if you like and activate it\n    $ pip install -r requirements.txt\n    $ ./rra3json.py -c rra3json.yml -s google_credentials.json\n```\n"
},
{
  "name": "wiki.mozilla.org",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "LocalSettings.php",
      "README.md",
      "composer.json",
      "composer.lock",
      "contribute.json",
      "core",
      "dev-setup.sh",
      "developer-setup.sh",
      "dist",
      "env_secrets.php",
      "extensions",
      "htaccess",
      "index.html",
      "nubis",
      "patches",
      "secrets.php-dist",
      "skins",
      "tools",
      "update.sh"
    ]
  },
  "makefile": null,
  "readme": "wiki.mozilla.org\n================\n\nThis is the deployment repository for the [wiki.mozilla.org](https://wiki.mozilla.org) project. To find out more about this project please visit the [About](https://wiki.mozilla.org/MozillaWiki:About) page. To get involved with the project visit the [Team](https://wiki.mozilla.org/MozillaWiki:Team) page.\n\n## Install\nIf you wish to have a local install of wiki.m.o including posts and content you will need to get the following datasets:\n\n1. A copy of the production database*\n2. A copy of the images directory\n3. A copy of the extesnions/Bugzilla/charts directory**\n\n\\* There is talk of generating a sanitized (remove PII) database for convenience.\n\n** This should not be the case. This extension should store its data in the images folder or in the temp file location as appropriate.\n\nOnce you have a copy of the aforementioned datasets you need to:\n\n1. clone this repository\n2. create a secrets.php file (see below)\n3. run the install.sh script (found in the tools directroy)\n\n## The secrets.php file\nThis file is where information specific to a particular deployment is stored. In hindsight I must admit that this is a poorly chosen name. This file should be self-explanatory. You can crib off of the -dist file or set environment variables in which case they will be automatically consumed.\n\n## Updating core\nMediaWiki core is installed as a submodule. To update it, simply follow your normal submodule workflow:\n```bash\ncd core\ngit checkout TAG\ncd ../\ngit add core\ngit commit\ngit push\n```\n\n## Extensions\nWe are installing extensions in three separate ways.\n### The Subversion model\nA few (two) extensions are available through Subversion only. These extensions are included fully. To update them you need to navigate into the extension's folder and issue an `svn up`. Then simply follow your usual procedure for committing upstream.\n### The git submodule model\nThe majority of extensions are installed as git submodules. Simply follow normal submodule practice for this. In short:\n- navagate to path/to/submodule directory\n- `git checkout TAG`\n- navagate to top level\n- `git add path/to/submodule`\n\n### The Composer model\nExtensions installed with composer need to be updated using the `php tools/composer.phar` command. For information on usage of this command see the [Composer Documentation](https://getcomposer.org/doc/). There are several things to note about using Composer in conjunction with MediaWiki.\n- While Composer normally installs in a directory named `vendor`, they are also duplicated on install to the `extensions` directory. These should not be checked into git and as such need to be added to the `.gitignore` file.\n- You need to be sure to `git add composer.lock` file whenever you make changes to the `composer.json` manually or with the `composer.phar` command. This will avoid errors when setting up a fresh install.\n- Composer automatically handles dependency resolution. Therefore you should not add any dependent extensions to the extensions directory.\n- Extensions installed with Composer are automatically loaded through the `vendor/autoload.php` file and do not need to be included in the `LocalSettings.php` file.\n\n\n"
},
{
  "name": "wikimo-sandstone",
  "files": {
    "/": [
      "Sandstone.hooks.php",
      "Sandstone.php",
      "images",
      "modules"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "unicode-slugify",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "dev-requirements.txt",
      "requirements.txt",
      "setup.py",
      "slugify",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "# Unicode Slugify\n\nUnicode Slugify is a slugifier that generates unicode slugs.  It was originally\nused in the Firefox Add-ons web site to generate slugs for add-ons and add-on\ncollections. Many of these add-ons and collections had unicode characters and\nrequired more than simple transliteration.\n\n## Usage\n\n```python\n\nfrom slugify import slugify, SLUG_OK\n\n# Default usage : lower, spaces replaced with \"-\", only alphanum and \"-_~\" chars, keeps unicode\nslugify(u'B\u00e4n...g (bang)')\n# u'b\u00e4ng-bang'\n\n# Keep capital letters and spaces\nslugify(u'B\u00e4n...g (bang)', lower=False, spaces=True)\n# u'B\u00e4ng bang'\n\n# Replace non ascii chars with their \"best\" representation\nslugify(u'\u5317\u4eac (capital of China)', only_ascii=True)\n# u'bei-jing-capital-of-china'\n\n# Allow some extra chars\nslugify(u'\u5317\u4eac (capital of China)', ok=SLUG_OK+'()', only_ascii=True)\n# u'bei-jing-(capital-of-china)'\n\n# \"snake_case\" example\ndef snake_case(s):\n    # As \"-\" is not in allowed Chars, first one (`_`) is used for space replacement\n    return slugify(s, ok='_', only_ascii=True)\nsnake_case(u'\u5317\u4eac (capital of china)')\n# u'bei_jing_capital_of_china'\n\n# \"CamelCase\" example\ndef camel_case(s):\n    return slugify(s.title(), ok='', only_ascii=True, lower=False)\ncamel_case(u'\u5317\u4eac (capital of china)')\n# u'BeiJingCapitalOfChina'\n```\n\n## Thanks\n\nTomaz Solc, unidecode, https://pypi.python.org/pypi/Unidecode\n"
},
{
  "name": "icu4x_js_regexp",
  "files": {
    "/": [
      ".gitignore",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "build.rs",
      "data",
      "include",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# icu4x_js_regexp\n\nRegular expressions in JavaScript have various features to support internationalization. In some cases, these features are defined with reference to the [`Unicode Standard`]. For example, the [`Canonicalize`] algorithm used to define case-insensitive (`/i`) RegExps refers to the CaseFolding.txt file of the Unicode Character Database.\n\nThe [`ICU4X`] project provides components for internationalization written in Rust. This crate builds on ICU4X to provide access to the Unicode data necessary to implement a regular expression engine compatible with the [`ECMA262 standard`].\n\n[`Unicode Standard`]: https://unicode.org/standard/standard.html\n[`Canonicalize`]: https://tc39.es/ecma262/multipage/text-processing.html#sec-runtime-semantics-canonicalize-ch\n[`ICU4X`]: https://github.com/unicode-org/icu4x\n[`ECMA262 standard`]: https://tc39.es/ecma262/multipage/\n"
},
{
  "name": "ffi-support",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE-APACHE",
      "LICENSE-MIT",
      "README.md",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# FFI Support\n\n[![Docs](https://docs.rs/ffi-support/badge.svg)](https://docs.rs/ffi-support)\n\nThis crate implements a low-level support library to simplify implementing certain FFI patterns.\nIt was originally created for patterns in the [mozilla/application-services](https://github.com/mozilla/application-services)\nrepository, but that repo is working on replacing all uses of this crate with\nthe [mozilla/uniffi-rs](https://github.com/mozilla/uniffi-rs) project.\n\nIn other words, we consider this crate soft-deprecated and replaced by [UniFFI](https://github.com/mozilla/uniffi-rs).\n\nHowever, if this crate proves useful to others, it can assist with the following areas:\n\n1. Avoiding throwing panics over the FFI (which is undefined behavior)\n2. Translating rust errors (and panics) into errors that the caller on the other side of the FFI is able to handle.\n3. Converting strings to/from rust str.\n4. Passing non-string data (in a few ways, including exposing an opaque pointeer, marshalling data to JSON strings with serde, as well as arbitrary custom handling) back and forth between Rust and whatever the caller on the other side of the FFI is.\n\nAdditionally, it's documentation describes a number of the problems we've hit doing this to expose libraries to consumers on mobile platforms.\n\n## Usage\n\nAdd the following to your Cargo.toml\n\n```toml\nffi-support = \"0.4.4\"\n```\n\nFor further examples, the examples in the docs is the best starting point, followed by the usage code in the [mozilla/application-services](https://github.com/mozilla/application-services) repo (for example [here](https://github.com/mozilla/application-services/blob/main/components/places/ffi/src/lib.rs) or [here](https://github.com/mozilla/application-services/blob/main/components/places/src/ffi.rs)).\n\n## License\n\nDual licensed under the Apache License, Version 2.0 <LICENSE-APACHE> or\n<http://www.apache.org/licenses/LICENSE-2.0> or the MIT license <LICENSE-MIT> or\n<http://opensource.org/licenses/MIT>, at your option. All files in the project\ncarrying such notice may not be copied, modified, or distributed except\naccording to those terms.\n"
},
{
  "name": "gmp-api",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "gmp-entrypoints.h",
      "gmp-errors.h",
      "gmp-platform.h",
      "gmp-storage.h",
      "gmp-video-codec.h",
      "gmp-video-decode.h",
      "gmp-video-encode.h",
      "gmp-video-frame-encoded.h",
      "gmp-video-frame-i420.h",
      "gmp-video-frame.h",
      "gmp-video-host.h",
      "gmp-video-plane.h"
    ]
  },
  "makefile": null,
  "readme": "This repository contains the headers for developing Gecko Media Plugins. Gecko syncs with this repository.\n"
},
{
  "name": "cloudformation-cross-account-outputs",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "cloudformation",
      "process_cloudformation_sns_emission_lambda_function",
      "tests"
    ]
  },
  "makefile": "ROOT_DIR\t:= $(shell dirname $(realpath $(lastword $(MAKEFILE_LIST))))\nPARENTDIR       := $(realpath ../)\nAWS_DEFAULT_REGION = us-west-2\nS3_BUCKET_NAME  := public.us-west-2.infosec.mozilla.org\nS3_BUCKET_TEMPLATE_PATH\t:= cloudformation-cross-account-outputs/cf\nS3_BUCKET_TEMPLATE_URI\t:= s3://$(S3_BUCKET_NAME)/$(S3_BUCKET_TEMPLATE_PATH)\nHTTP_BUCKET_TEMPLATE_URI\t:= https://s3.amazonaws.com/$(S3_BUCKET_NAME)/$(S3_BUCKET_TEMPLATE_PATH)\n\nall:\n\t@echo 'Available make targets:'\n\t@grep '^[^#[:space:]].*:' Makefile\n\n.PHONY: cfn-lint test\ntest: cfn-lint\ncfn-lint: ## Verify the CloudFormation templates pass linting tests\n\t-cfn-lint cloudformation/*.yml\n\n.PHONY: upload-templates\nupload-templates:\n\tAWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) aws s3 sync cloudformation/ $(S3_BUCKET_TEMPLATE_URI) --exclude=\"*\" --include=\"*.yml\"\n\n.PHONY: create-stacks\ncreate-stacks: create-consumer-stacks create-role-stack create-dynamodb-stack\n\n.PHONY: create-consumer-stacks\ncreate-consumer-stacks:\n\tAWS_DEFAULT_REGION=us-west-2 aws cloudformation create-stack --stack-name cloudformation-sns-emission-consumer-us-west-2 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\tAWS_DEFAULT_REGION=us-east-1 aws cloudformation create-stack --stack-name cloudformation-sns-emission-consumer-us-east-1 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\tAWS_DEFAULT_REGION=us-west-1 aws cloudformation create-stack --stack-name cloudformation-sns-emission-consumer-us-west-1 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\tAWS_DEFAULT_REGION=eu-west-1 aws cloudformation create-stack --stack-name cloudformation-sns-emission-consumer-eu-west-1 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\n.PHONY: create-role-stack\ncreate-role-stack:\n\tAWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) aws cloudformation create-stack --stack-name cloudformation-sns-emission-consumer-role \\\n\t  --capabilities CAPABILITY_IAM \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer-role.yml\n\n.PHONY: create-dynamodb-stack\ncreate-dynamodb-stack:\n\tAWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) aws cloudformation create-stack --stack-name cloudformation-stack-emissions-dynamodb \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-stack-emissions-dynamodb.yml\n\n.PHONY: create-test-stack\ncreate-test-stack:\n\tAWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) aws cloudformation create-stack --stack-name test-stack-emission \\\n\t  --template-body file://tests/test_emission.yml\n\n.PHONY: update-stacks\nupdate-stacks: update-consumer-stacks update-role-stack update-dynamodb-stack\n\n.PHONY: update-consumer-stacks\nupdate-consumer-stacks:\n\tAWS_DEFAULT_REGION=us-west-2 aws cloudformation update-stack --stack-name cloudformation-sns-emission-consumer-us-west-2 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\tAWS_DEFAULT_REGION=us-east-1 aws cloudformation update-stack --stack-name cloudformation-sns-emission-consumer-us-east-1 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\tAWS_DEFAULT_REGION=us-west-1 aws cloudformation update-stack --stack-name cloudformation-sns-emission-consumer-us-west-1 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\tAWS_DEFAULT_REGION=eu-west-1 aws cloudformation update-stack --stack-name cloudformation-sns-emission-consumer-eu-west-1 \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer.yml\n\n.PHONY: update-role-stack\nupdate-role-stack:\n\tAWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) aws cloudformation update-stack --stack-name cloudformation-sns-emission-consumer-role \\\n\t  --capabilities CAPABILITY_IAM \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-sns-emission-consumer-role.yml\n\n.PHONY: update-dynamodb-stack\nupdate-dynamodb-stack:\n\tAWS_DEFAULT_REGION=$(AWS_DEFAULT_REGION) aws cloudformation update-stack --stack-name cloudformation-stack-emissions-dynamodb \\\n\t  --template-url $(HTTP_BUCKET_TEMPLATE_URI)/cloudformation-stack-emissions-dynamodb.yml\n",
  "readme": "# cloudformation-cross-account-outputs\n\n## Deploy the infrastructure\n\nIn the AWS account that you want other accounts to emit CloudFormation outputs\nto\n\n1. Create a DynamoDB table called `cloudformation-stack-emissions`\n   \n   * This can be done by deploying the \n     [`cloudformation-stack-emissions-dynamodb.yml`](https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-stack-emissions-dynamodb.yml)\n     CloudFormation template in the web console with this button\n     [![Launch CloudFormation Stack Emission DynamoDB](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=cloudformation-stack-emissions-dynamodb&templateURL=https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-stack-emissions-dynamodb.yml),\n     or by creating the table in the web console or on the command line. Only one DynamoDB stack in one region need be deployed in the AWS account.\n2. Deploy the CloudFormation template [`cloudformation-sns-emission-consumer-role.yml`](https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-sns-emission-consumer-role.yml)\n   with this button\n   [![Launch CloudFormation SNS Emission Consumer Role](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=cloudformation-sns-emission-consumer-role&templateURL=https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-sns-emission-consumer-role.yml)\n   in a single region which will create an IAM Role used by the Lambda function. Only one IAM Role stack in one region need be deployed in the AWS account.\n3. Deploy the CloudFormation template [`cloudformation-sns-emission-consumer.yml`](https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-sns-emission-consumer.yml)\n   with these buttons once in every region that you need to receive CloudFormation \n   outputs in. (You're welcome to deploy the template in regions other than \n   those listed below).\n   \n   * `us-west-2` : [![Launch CloudFormation SNS Emission Consumer](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=cloudformation-sns-emission-consumer&templateURL=https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-sns-emission-consumer.yml)\n   * `us-east-1` : [![Launch CloudFormation SNS Emission Consumer](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-east-1#/stacks/new?stackName=cloudformation-sns-emission-consumer&templateURL=https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/cloudformation-cross-account-outputs/cf/cloudformation-sns-emission-consumer.yml)\n   \n   Since CloudFormation custom resources can only emit to SNS topics in the same\n   region, a separate SNS topic must be deployed in every region that you use.\n   This stack creates\n   \n   * An SNS Topic to which other accounts will emit events to and Topic Policy\n   * A Lambda function subscribed to that SNS Topic\n\n## Emit outputs from a CloudFormation template\n\nCreate a CloudFormation template containing\n* A [CloudFormation Custom Resource](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cfn-customresource.html)\n  with the following properties\n  * `ServiceToken` : The SNS ARN of the SNS topic you created when deploying the\n    infrastructure\n  * An optional `category` field. This field is indexed in DynamoDB and as a result\n    items can be queried by category. If no `category` is set in the resource,\n    it will be set to the value `general` in the DB record. In the example below\n    the `category` value is set to `My Category`\n  * An arbitrary number of additional optional key value pairs. In the example below\n    there's a single key value pair with a key of `exampleKey` and value of\n    `Example Value`\n\nNote : You may want to constrain users from deploying this template in regions\nwhere you've not deployed an [SNS topic](cloudformation/cloudformation-sns-emission-consumer-topic.yml)\nto receive stack emissions. One way to do this is [with a region Mapping](https://gist.github.com/gene1wood/ae2b77a424d220f2d0605cb8637baa33)\n\n### Example CloudFormation template\n\n```yaml\nAWSTemplateFormatVersion: 2010-09-09\nResources:\n  PublishExampleToSNS:\n    Type: Custom::PublishToSNS\n    Version: '1.0'\n    Properties:\n      ServiceToken: !Join [ ':', [ 'arn:aws:sns', !Ref 'AWS::Region', '012345678901', 'cloudformation-stack-emissions' ] ]\n      category: My Category\n      exampleKey: Example Value\n```\n\n## Fetch emitted outputs\n\nData is queryable by either the `aws-account-id` or the `category` fields.\n\nYou can fetch data from the DynamoDB table with the aws command line. To fetch\nthe `exampleKey` value for all emissions in account `012345678901` query like\nthis\n\n```\naws dynamodb query --table-name cloudformation-stack-emissions \\\n  --expression-attribute-names '{\"#a\": \"aws-account-id\"}' \\\n  --expression-attribute-values '{\":i\": {\"S\": \"012345678901\"}}' \\\n  --key-condition-expression \"#a = :i\" \\\n  --projection-expression exampleKey \\\n  --output text --query 'Items[].exampleKey.S'\n```\n\nor for all accounts given `category` value of `testing` query the secondary\nglobal index `category` like\n\n```\naws dynamodb query --table-name cloudformation-stack-emissions \\\n  --index-name category \\\n  --expression-attribute-names '{\"#c\": \"category\"}' \\\n  --expression-attribute-values '{\":v\": {\"S\": \"My Category\"}}' \\\n  --key-condition-expression \"#c = :v\" \\\n  --projection-expression exampleKey \\\n  --output text --query 'Items[].exampleKey.S'\n```\n\nOr to get exampleKey from a specific record based on an AWS account ID and category\n\n```\naws dynamodb query --table-name cloudformation-stack-emissions \\\n  --expression-attribute-names '{\"#a\": \"aws-account-id\", \"#c\": \"category\"}' \\\n  --expression-attribute-values '{\":i\": {\"S\": \"012345678901\"}, \":v\": {\"S\": \"My Category\"}}' \\\n  --key-condition-expression \"#a = :i\" \\\n  --filter-expression \"#c = :v\" \\\n  --projection-expression exampleKey \\\n  --output text --query 'Items[].exampleKey.S'\n```\n\n### Automatically added attributes\n\nThe following attributes are always set\n* `aws-account-id` : The AWS account ID in which the CloudFormation stack was\n  deployed\n* `id` : The GUID of the CloudFormation stack combined with the Logical Resource\n  name of the CloudFormation resource, separated by a `+` character. Example :\n  `6ff5f130-20d6-11e9-b98a-0a1528792fce+PublishTestToSNS`\n\nThe following attributes are set if they aren't passed in the `Properties` of\nthe CloudFormation stack\n* `category`: If it's not defined in the CloudFormation template, it's set to\n  `general`\n* `region` : The AWS region in which the CouldFormation stack was deployed\n* `stack-name` : The name of the CloudFormation stack\n* `last-updated` : The datetime that the record was last updated in UTC time\n\n## DynamoDB Schema\n\n* Table name : `cloudformation-stack-emissions`\n* Partition key : `aws-account-id` which contains the [AWS Account ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html)\n  of the AWS account that contains the CloudFormation stack which is emitting\n  data\n* Sort key : `id` which contains the the GUID of the CloudFormation stack \n  combined with the Logical Resource name of the CloudFormation resource, \n  separated by a `+` character.\n* Global Secondary Index : `category`\n  * Partition key : `category` which contains the category set in the \n    CloudFormation template or the default of `general`\n  * Sort key : `id`\n* Attributes : All additional Resource Properties of the CloudFormation Custom\n  Resource are inserted into a DynamoDB Item as attributes (key value pairs)\n\n"
},
{
  "name": "snakepit",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assets",
      "bin",
      "package.json",
      "scripts",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Snakepit\n\nSnakepit is a machine learning job scheduler with the following features:\n- Scheduling of concurrent machine learning jobs\n- Support for multi-machine and multi-GPU jobs\n- Job's tasks running in interconnected LXD containers with \"root\" access\n- Built-in user and group management\n- Jobs get access to training data according to user's access rights\n- Remote access through command line client over HTTP API\n- Remote data access through FUSE mounts (even during training)\n\n__The Snakepit service has not gone through an in-depth security-audit yet.\nTherefore you should not offer unknown/random users access to your service.__\n\n## Getting Started\n\nThis following instructions are intended for administrative Snakepit users\nwho want to configure and run an own Snakepit cluster.\n\nIf you are a Snakepit end-user and just want to know how to run jobs\non an existing Snakepit cluster,\nyou should follow the [snakepit-client user-guide](https://github.com/mozilla/snakepit-client/)\n\n### Big picture\n\n![Overview - three jobs on a Snakepit cluster](assets/Snakepit_overview.png)\n\n- The typical setup for a Snakepit machine learning cluster is to have a so called __head node__ machine and a bunch of __worker node__ machines. \n- The head node is typically hosting the __Snakepit service__ and has/provides access to the outer world. In our scenario it also contains all the (training) data.\n- The worker nodes are connected to the head node through a (high speed) local network and are typically equipped with __GPUs__. You can also run a one-machine setup (e.g. for testing or development) where head node and worker nodes are essentially on one machine.\n\n- A job is started on the user's computer through snakepit's command-line client from within a git repository checkout.\n- The client sends all relevant information of the checkout (address of repository, hash, diff, ...) through Snakepit's __HTTP API__ to the Snakepit service.\n- The snakepit service now starts a so called __pit__ (like a \"pod\" in Kubernetes).\n- A pit (and therefore a job) consists of __processes__ and associated data (checkout of job repository).\n- Each process is represented by its own __LXD container__ which is a para-virtualized environment using Linux-namespaces (think of a lightweight virtual machine).\n- For each job (pit) there is exactly one so called __daemon process__ which will run on the head-node. Its responsibility is to provide data to the other processes of the pit.\n- The so called __worker processes__ of a pit can access the provided data through __sshfs__ mounts to the daemon process.\n- Each worker process executes the same __.compute__ script of the job (which is typically taken from the job's repository checkout).\n- All worker processes are running on worker nodes and each of them has exclusive access to its allocated sub-set of resources on it (typically GPUs).\n\n### Prerequisites\n\n* At least one machine with at least one GPU (at the moment there is only support for Nvidia GPUs)\n* Latest Nvidia drivers for each GPU\n* [LXD](https://linuxcontainers.org/lxd/) (3.0+) installed on each machine\n* A front-end web-server of your choice on the main machine (optional but recommended)\n* git\n\n### Configuring LXD\n\nBefore Snakepit can get installed, LXD has to be configured on \nall involved machines (if not already done).\nSo on each machine of your cluster you have to call \n```\n$ sudo lxd init\n```\nDuring the following questionnaire you'll be asked, if you want to create a new storage pool.\nIt is highly recommended to create a copy-on-write one on base of `zfs` or `btrfs`.\nEach machine's storage pool should have at least 10 GB of space.\nOn the following question you should respond with `yes`:\n```\nWould you like LXD to be available over the network (yes/no) [default=no]? yes\n```\nYou'll be asked to set a password which will be required later during Snakepit's setup.\n\n__After Snakepit is configured and/or the machine got added, you should unset it again:__\n```\n$ lxc config unset core.trust_password\n```\n\n### Installing\n\nAll the following steps are only to be done on the head node.\nFirst you have do create a Snakepit user:\n```\n$ sudo adduser snakepit\n[...]\n```\n\nFirst clone the Snakepit project.\nFrom within Snakepit's project root, you can now call:\n```\n/path/to/snakepit/clone$ sudo bin/prepare-directories.sh snakepit /snakepit\n```\nThis will create the required data directory structure in `/snakepit` owned by user `snakepit`.\nThis directory is from now on called \"data-root\".\nYou could also pick a different path.\n\nNow it's time to prepare the snakepit service:\n```\n/path/to/snakepit/clone$ sudo bin/prepare-service.sh /snakepit /path/to/snakepit/clone\n```\nThis will create the `snakepit` LXD container and bind the data-root to its internal directory `/data`\nand `/path/to/snakepit/clone` to its internal directory `/code`. If you omit `/path/to/snakepit/clone`,\nthe script will clone the project another time within the container into `/code`.\nThe script is also automatically mapping the outer directory-owner of\nthe data-root (in our case user `snakepit`) to its inner `root` user.\n\nIf you get a line with \"Problem accessing https://...:8443\", you have to figure out the URL for\nthe local LXD service and run the provided command.\nThe `bin/prepare-service.sh` script looks for the `lxdbr0` bridge network adapter (this is a default one in LXD).\nIf not existing, it will create and attach it to the snakepit service container as `eth0`.\nThe following commands will help you figuring out the service address:\n* `sudo lxc exec snakepit -- ip addr` lists all interfaces of the snakepit service container and their IP addresses\n* `sudo lxc network list` shows all LXD networks\n* `sudo lxc network show <network-name>` shows details and addresses of one network\n* `sudo lxc exec snakepit -- curl -k https://<address>:8443/1.0` tests an address from inside the snakepit service container\n\nNext step is to create the worker and daemon LXD container images:\n```\n/path/to/snakepit/clone$ sudo bin/prepare-images.sh\n```\nThis is a highly automated process and should not require any interaction.\n\nAfter this you have the chance to install any required software into the worker image:\n```\n/path/to/snakepit/clone$ sudo lxc exec snakepit-worker -- bash\nroot@snakepit-worker:/root# apt install some-requirement\n[...]\nroot@snakepit-worker:/root# exit\n```\n\nBefore the images can be used, you have to publish them:\n```\n/path/to/snakepit/clone$ sudo bin/publish-images.sh\n```\n\n### Configuring NFS\n\nNFS is used for job data access. sshFS was used previously, but new workloads benefit from the faster disk access NFS allows.\n\nSteps below assume the following internal networking layout. Adjust accordingly if different.\n\n```bash\nhead node is at 192.168.1.1\nworker nodes are at 192.168.2.1, 192.168.3.1, etc\n```\n\n#### Configure NFS on the head node\n\nOn the head node, install the nfs-server package.\n\n```bash\n$ sudo apt install nfs-kernel-server\n```\n\nAs root, add the following line to the `/etc/exports` file.\n\n```bash\n/snakepit       192.168.0.0/16(rw,no_root_squash,no_subtree_check)\n```\n\nThen restart with `systemctl restart nfs-server`. Verify exports are working with `exportfs`.\n\n#### Configure NFS on the worker nodes\n\nThe steps below need to be done on each worker node.\n\nInstall the nfs client package.\n\n```bash\n$ sudo apt install nfs-common\n```\n\nDetermine the UID and GID of the snakepit user on the head node.\n\n```bash\n# on the head node\n\n# from the system\n$ id snakepit\nuid=1777(snakepit) gid=1777(snakepit) groups=1777(snakepit),27(sudo),110(lxd)\n\n# from snakepit config\n$ lxc exec snakepit -- cat /etc/snakepit/snakepit.conf | grep mountUid\nmountUid: \"1777\"\n```\n\nCreate a snakepit user with the same UID and GID as on the head node.\n\nNFS won't work if the UID is not the same.\n\n```bash\n$ sudo addgroup --gid 1777 snakepit\n$ sudo adduser --uid 1777 --gid 1777 --disabled-password --gecos '' snakepit\n```\n\nCreate the mount point.\n\n```bash\n$ sudo mkdir /mnt/snakepit\n```\n\nEdit /etc/fstab as root. Add the following line.\n\n```bash\n192.168.1.1:/snakepit   /mnt/snakepit   nfs   nosuid,hard,tcp,bg,noatime 0 0\n```\n\nMount and verify that it's working.\n\n```bash\n$ sudo mount /mnt/snakepit\n$ ls -la /mnt/snakepit\n# there should be files owned by snakepit:snakepit\n```\n\n### Access to Snakepit service\n\nThe snakepit service itself only provides unencrypted HTTP access. \nTherefore it is highly recommended to run snakepit behind a front-end web server with HTTPS configuration.\nThe front-end server has to forward requests to port 80 of the address of the `eth0` interface of\nthe snakepit service (`sudo lxc exec snakepit -- ip addr`).\nYou can check connectivity through\n```\n$ curl http://<snakepit-service-address>/hello\nHere I am\n```\n\nFor clients to be able to connect to the service, they have to have access to a so called `.pitconnect.txt` file.\nIts first line has to be the (outer) service URL without trailing slash.\nIf you have/want to go for a self-signed HTTPS certificate of your front-end server,\nyou can add the certificate content under that first line in the `.pitconnect.txt` file.\nThe `.pitconnect.txt` is considered public and in case of a self-signed certificate\nit is to be distributed to users on a separate channel (like email).\nThe snakepit client will only accept valid certificates or the one provided through the `.pitconnect.txt` file.\n\n### First time use\n\nFor the following steps you have to first [install the snakepit client](https://github.com/mozilla/snakepit-client/#installation).\n\nWithin a directory that contains the `.pitconnect.txt` file (from the last step),\nyou can now test your configuration end-to-end:\n```\n$ pit status\nNo user info found. Seems like a new user or first time login from this machine.\nPlease enter an existing or new username: tilman\nFound no user of that name.\nDo you want to register this usename (yes|no)? yes\nFull name: Tilman Kamp\nE-Mail address: ...\nNew password: ************\nReinput a same one to confirm it: ************\n   JOB   S SINCE        UC% UM% USER       TITLE                RESOURCE \n```\n\nAs you are the first user, Snakepit automatically granted you admin rights:\n```\n$ pit show me\nUsername:         tilman\nFull name:        Tilman Kamp\nE-Mail address:   ...\nIs administrator: yes\n```\n\n### Adding nodes\n\nBefore one can run jobs on a worker node, the node has to be added to the snakepit service:\n```\n$ pit add node:n0 endpoint=https://...:8443\nLXD endpoint password: **********\n```\nHere we gave the node the short-name \"n0\" and its LXD API URL as endpoint.\nThe password is the one that was specified during LXD configuration of the node.\nIf the node has been added successfully, this password should be unset (see LXD config section).\n\nIf the node had been added successfully, you should take a look at the node's GPUs (also called resources):\n```\n$ pit show node:n0\nNode name: n0\nState:     ONLINE\nResources: \n  0: \"GeForce GTX 1070\" (cuda 0)\n  1: \"GeForce GTX 1070\" (cuda 1)\n```\n\nTime to define a model name alias:\n```\n$ pit add alias:gtx1070 name=\"GeForce GTX 1070\"\n$ pit show node:n0\nNode name: n0\nState:     ONLINE\nResources: \n  0: \"GeForce GTX 1070\" aka \"gtx1070\" (cuda 0)\n  1: \"GeForce GTX 1070\" aka \"gtx1070\" (cuda 1)\n```\n\nTime to run a first test job:\n```\n$ pit run \"First light\" [2:gtx1070] -d 'cat /proc/driver/nvidia/gpus/**/*' -l\nJob number: 190\nRemote:     origin <https://github.com/...>\nHash:       ...\nDiff LoC:   0\nResources:  \"[2:gtx1070]\"\n\n[2018-12-14 17:04:58] [daemon] Pit daemon started\n[2018-12-14 17:05:01] [worker 0] Worker 0 started\n[2018-12-14 17:05:01] [worker 0] Model: \t\t GeForce GTX 1070\n[2018-12-14 17:05:01] [worker 0] IRQ:   \t\t 139\n[2018-12-14 17:05:01] [worker 0] GPU UUID: \t ...\n[2018-12-14 17:05:01] [worker 0] Video BIOS: \t 86.04.26.00.80\n[2018-12-14 17:05:01] [worker 0] Bus Type: \t PCIe\n[2018-12-14 17:05:01] [worker 0] DMA Size: \t 47 bits\n[2018-12-14 17:05:01] [worker 0] DMA Mask: \t 0x7fffffffffff\n[2018-12-14 17:05:01] [worker 0] Bus Location: \t 0000:01:00.0\n[2018-12-14 17:05:01] [worker 0] Device Minor: \t 0\n[2018-12-14 17:05:01] [worker 0] Blacklisted:\t No\n[2018-12-14 17:05:01] [worker 0] Binary: \"\"\n[2018-12-14 17:05:01] [worker 0] Model: \t\t GeForce GTX 1070\n[2018-12-14 17:05:01] [worker 0] IRQ:   \t\t 142\n[2018-12-14 17:05:01] [worker 0] GPU UUID: \t ...\n[2018-12-14 17:05:01] [worker 0] Video BIOS: \t 86.04.26.00.80\n[2018-12-14 17:05:01] [worker 0] Bus Type: \t PCIe\n[2018-12-14 17:05:01] [worker 0] DMA Size: \t 47 bits\n[2018-12-14 17:05:01] [worker 0] DMA Mask: \t 0x7fffffffffff\n[2018-12-14 17:05:01] [worker 0] Bus Location: \t 0000:02:00.0\n[2018-12-14 17:05:01] [worker 0] Device Minor: \t 1\n[2018-12-14 17:05:01] [worker 0] Blacklisted:\t No\n[2018-12-14 17:05:01] [worker 0] Binary: \"\"\n[2018-12-14 17:05:01] [worker 0] Worker 0 ended with exit code 0\n[2018-12-14 17:05:01] [daemon] Worker 0 requested stop. Stopping pit...\n```\n\n__Et voil\u00e0 - you got your first snakepit cluster.__\nFor further understanding of jobs and their runtime environment,\nrefer to the [snakepit-client user-guide](https://github.com/mozilla/snakepit-client/).\n\n## Configuration\n\nThe configuration of the snakepit service is read from a YAML file at\n`/etc/snakepit/snakepit.conf` inside the snakepit container.\nYou can edit it through vim:\n```\n$ sudo lxc exec snakepit -- vim /etc/snakepit/snakepit.conf\n$ sudo lxc exec snakepit -- systemctl restart snakepit\n```\n\nPossible configuration values are:\n- interface:         Interface(s) to bind the service to - default 0.0.0.0 (all)\n- port:              Port of the service - default 80\n- logLevel:          How verbose the service logs to system log (0=DEBUG, 1=INFO, 2=ERROR) - default 1\n- debugHttp:         Debug HTTP activity (true/false) - default false\n- debugJobFS:        Debug remote mount activity (true/false) - default false\n- tokenSecret:       Path to a file containing crypto-secret for access tokens\n- tokenTTL:          Lifetime of access tokens before users have to re-authenticate (time-value) - default 1d\n- hashRounds:        How many hash rounds access tokens are going through - default 10\n- endpoint:          Head node's LXD HTTPS API endpoint\n- clientKey:         Path to cryptographic key file for accessing head node's LXD endpoint\n- clientCert:        Path to cryptographic certificate file for accessing head node's LXD endpoint\n- lxdTimeout:        HTTP timeout in seconds for all LXD API access (time-value) - default 10s\n- lxdBridge:         Bridge name of the network bridge that each container should connect its first NIC with - default lxdbr0\n- lxdDomain:         Domain name for all containers - default lxd\n- containerTimeout:  Timeout for LXD container state change - default 30s\n- pollInterval:      Polling interval for checking LXD container states in ms - default 1000\n- maxParallelPrep:   Maximum number of parallel job preparations - default 2\n- maxPrepDuration:   Timeout for preparation phase (time-value) - default 1h\n- maxStartDuration:  Timeout for start phase (time-value) - default 5m\n- mountRoot:         Path to data-root on head-node - default /snakepit\n- queryLimit:        Maximum number of returned list entries per user query\n\n## Managing data\n\nThere are four different data domains in Snakepit.\nAll of them are represented by certain sub-directories within the data-root directory.\nJobs have the same read/write rights as their owning users.\n\n* Shared data: `<data-root>/shared/` - Files in this directory are read-only for everyone and considered public.\n    Only users with direct access to the head-node can change its contents.\n* Group data: `<data-root>/groups/<group-name>/` - Admins and all members of the given group have read/write access to all contents.\n* User data: `<data-root>/home/<user-name>/` - Admins and the given user have read-write access.\n* Job data: `<data-root>/pits/<job-number>/` - Admins, the owning user and group members of groups specified in the \"groups\" property of the job have read-access. Only the running job is allowed to write data.\n\n`<data-root>/cache/` contains all cached git clones.\n\n`<data-root>/db.json` is the database of the snakepit service.\n\n## Troubleshooting\n\nThe snakepit service is running as a regular systemd service (named \"snakepit\") inside the snakepit container.\nSo you can control it through `systemctl` and monitor it through `journalctl`.\n\nIn case of a tough problem you can also stop the systemd service and run snakepit manually:\n```\n$ sudo lxc exec snakepit -- bash\nroot@snakepit:~# systemctl stop snakepit\nroot@snakepit:~# cd /code\nroot@snakepit:/code# npm start\n\n> snakepit@0.0.1 start /code\n> node src/service.js\n\nget https://...:8443/1.0 \nstate head 1\nstate n0 1\nget https://...:8443/1.0/containers \npitReport []\n'Snakepit service running on 0.0.0.0:80'\n[...]\n```\nWith configuration `logLevel: 0` this should give you a good start for figuring out what's going on.\n\nTo get a better understanding of how a running job/pit looks like from LXD's perspective,\nyou could list the running containers:\n\n```\n$ sudo lxc list\n+---------------+---------+-----------------------+--------------------------+------------+-----------+\n|     NAME      |  STATE  |         IPV4          |           IPV6           |    TYPE    | SNAPSHOTS |\n+---------------+---------+-----------------------+--------------------------+------------+-----------+\n| snakepit      | RUNNING | 192.168.... (eth0)    | fd42:...          (eth0) | PERSISTENT |           |\n+---------------+---------+-----------------------+--------------------------+------------+-----------+\n| sp-head-191-d | RUNNING | 10.125.... (eth0)     | fd42:...          (eth0) | PERSISTENT |           |\n+---------------+---------+-----------------------+--------------------------+------------+-----------+\n| sp-n0-191-0   | RUNNING | 10.125.... (eth0)     | fd42:...          (eth0) | PERSISTENT |           |\n+---------------+---------+-----------------------+--------------------------+------------+-----------+\n```\n\nAs you can see, a Snakepit container name (with the exception of Snakepit's service container) consists of the following parts (in given order):\n* \"sp-\": The common prefix allows using LXD for other purposes than Snakepit as long as containers are prefixed differently.\n* \"&lt;node-name&gt;-\": The name of the node the container runs on. This is required in case of double-adding a node for single-node setups and demos (like in this case).\n* \"&lt;pit-number&gt;-\": The pit number which is also the job number.\n* \"&lt;process-specifier&gt;\": \"d\" in case of the pit's daemon and the process/worker index in case of a worker process.\n\n## Help\n\n1. [**IRC**](https://wiki.mozilla.org/IRC) - You can contact us on the `#machinelearning` channel on [Mozilla IRC](https://wiki.mozilla.org/IRC); people there can try to answer/help\n\n2. [**Issues**](https://github.com/mozilla/snakepit/issues) - If you think you ran into a serious problem, feel free to open an issue in our repo.\n"
},
{
  "name": "agithub",
  "files": {
    "/": [
      ".gitattributes",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "COPYING",
      "README.md",
      "SECURITY.md",
      "agithub",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# The Agnostic GitHub API\n\n> It doesn't know, and you don't care!\n\n`agithub` is a REST API client with transparent syntax which facilitates\nrapid prototyping &mdash; on *any* REST API!\n\nOriginally tailored to the GitHub REST API, AGitHub has grown up to\nsupport many other REST APIs:\n\n* DigitalOcean\n* Facebook\n* GitHub\n* OpenWeatherMap\n* SalesForce\n\nAdditionally, you can add *full support* for another REST API with very\nlittle new code!  To see how, check out the [Facebook client], which has\nabout 30 lines of code.\n\nThis works because AGithub knows everything it needs to about protocol\n(REST, HTTP, TCP), but assumes nothing about your upstream API.\n\n[Facebook client]: agithub/Facebook.py\n\n# Use\n\nThe most striking quality of AGitHub is how closely its syntax emulates\nHTTP. In fact, you might find it even more convenient than HTTP, and\nalmost as general (as far as REST APIs go, anyway). The examples below\ntend to use the GitHub API as a reference point, but it is no less easy to\nuse `agithub` with, say, the Facebook Graph.\n\n## Create a client\n\n```python\nfrom agithub.GitHub import GitHub\nclient = GitHub()\n```\n\n## GET\n\nHere's how to do a `GET` request, with properly-encoded url parameters:\n\n```python\nclient.issues.get(filter='subscribed')\n```\n\nThat is equivalent to the following:\n\n```http\nGET /issues/?filter=subscribed\n```\n\n## POST\n\nHere's how to send a request body along with your request:\n\n```python\nsome_object = {'foo': 'bar'}\nclient.video.upload.post(body=some_object, tags=\"social devcon\")\n```\n\nThis will send the following request, with `some_object` serialized as\nthe request body:<sup>*</sup>\n\n```http\nPOST /video/upload?tags=social+devcon\n\n{\"foo\": \"bar\"}\n```\n\nThe `body` parameter is reserved and is used to define the request body to be\nPOSTed. `tags` is an example query parameter, showing that you can pass both\nan object to send as the request body as well as query parameters.\n\n<sup>*</sup> For now, the request body is limited to JSON data; but\nwe plan to add support for other types as well\n\n## Parameters\n\n### `headers`\n\nPass custom http headers in your ruquest with the reserved parameter `headers`.\n\n```python\nfrom agithub.GitHub import GitHub\ng = GitHub()\nheaders = {'Accept': 'application/vnd.github.symmetra-preview+json'}\nstatus, data = g.search.labels.get(headers=headers, repository_id=401025, q='\u00af\\_(\u30c4)_/\u00af')\nprint(data['items'][0])\n```\n\n```text\n{u'default': False, u'name': u'\\xaf\\\\_(\\u30c4)_/\\xaf', u'url': u'https://api.github.com/repos/github/hub/labels/%C2%AF%5C_(%E3%83%84)_/%C2%AF', u'color': u'008672', u'node_id': u'MDU6TGFiZWwxMTcwNjYzNTM=', u'score': 43.937515, u'id': 117066353, u'description': u''}\n\n```\n\n### `body`\n\nIf you're using `POST`, `PUT`, or `PATCH` (`post()`, `put()`, or `patch()`), \nthen you should include the body as the `body=` argument. The body is \nserialized to JSON before sending it out on the wire.\n\n```python\nfrom agithub.GitHub import GitHub\ng = GitHub()\n# This Content-Type header is only required in this example due to a GitHub \n# requirement for this specific markdown.raw API endpoint\nheaders={'Content-Type': 'text/plain'}  \nbody = '# This should be my header'\nstatus, data = g.markdown.raw.post(body=body, headers=headers)\nprint(data)\n```\n\n```text\n<h1>\n<a id=\"user-content-this-should-be-my-header\" class=\"anchor\" href=\"#this-should-be-my-header\" aria-hidden=\"true\"><span aria-hidden=\"true\" class=\"octicon octicon-link\"></span></a>This should be my header</h1>\n\n```\n\n\n\n## Example App\n\n1. First, instantiate a `GitHub` object.\n\n   ```python\n   from agithub.GitHub import GitHub\n   g = GitHub()\n   ```\n\n2. When you make a request, the status and response body are passed back\n   as a tuple.\n\n   ```python\n   status, data = g.users.octocat.get()\n   print(data['name'])\n   print(status)\n   ```\n\n   ```text\n   The Octocat\n   200\n   ```\n\n3. If you forget the request method, `agithub` will complain that you\n   haven't provided enough information to complete the request.\n\n   ```python\n   g.users\n   ```\n   \n   ```text\n   <class 'agithub.github.IncompleteRequest'>: /users\n   ```\n\n4. Sometimes, it is inconvenient (or impossible) to refer to a URL as a\n   chain of attributes, so indexing syntax is provided as well. It\n   behaves exactly the same. In these examples we use indexing syntax because \n   you can't have a python function name\n\n   * starting with a digit : `1`\n   * containing a dash (`-`) character : `Spoon-Knife`\n\n   ```python\n   g.repos.github.hub.issues[1].get()\n   g.repos.octocat['Spoon-Knife'].branches.get()\n    ```\n\n    ```text\n   (200, { 'id': '#blah', ... })\n   (200, [ list, of, branches ])\n\n    ```\n\n5. You can also pass query parameter to the API as function parameters to the\n   method function (e.g. `get`).\n\n   ```python\n   status, data = g.repos.octocat['Spoon-Knife'].issues.get(\n       state='all', creator='octocat')\n   print(data[0].keys())\n   print(status)\n   ```\n\n   ```text\n   [u'labels', u'number', \u2026 , u'assignees']\n   200\n   ```\n\n   Notice the syntax here:\n   `<API-object>.<URL-path>.<request-method>(<query-parameters>)`\n   \n   * `API-object` : `g`\n   * `URL-path` : `repos.octocat['Spoon-Knife'].issues`\n   * `request-method` : `get`\n   * `query-parameters` : `state='all', creator='octocat'`\n\n6. As a weird quirk of the implementation, you may build a partial call\n   to the upstream API, and use it later.\n\n   ```python\n   def following(self, user):\n       return self.user.following[user].get\n   \n   myCall = following(g, 'octocat')\n   if 204 == myCall()[0]:\n       print 'You are following octocat'\n   ```\n   \n   ```text\n   You are following octocat\n   ```\n\n   You may find this useful &mdash; or not.\n\n7. Finally, `agithub` knows nothing at all about the GitHub API, and it\n   won't second-guess you.\n\n   ```python\n   g.funny.I.donna.remember.that.one.head()\n   ```\n   \n   ```text\n   (404, {'message': 'Not Found'})\n   ```\n\n   The error message you get is directly from GitHub's API. This gives\n   you all of the information you need to survey the situation.\n\n7. If you need more information, the response headers of the previous\n   request are available via the `getheaders()` method.\n\n   ```python\n   g.getheaders()\n   ```\n   \n   ```text\n   [('status', '404 Not Found'),\n    ('x-ratelimit-remaining', '54'),\n    \u2026\n    ('server', 'GitHub.com')]\n   ```\n   \n   Note that the headers are standardized to all lower case. So though, in this\n   example, GitHub returns a header of `X-RateLimit-Remaining` the header is\n   returned from `getheaders` as `x-ratelimit-remaining`\n\n## Error handling\nErrors are handled in the most transparent way possible: they are passed\non to you for further scrutiny. There are two kinds of errors that can\ncrop up:\n\n1. Networking Exceptions (from the `http` library). Catch these with\n   `try .. catch` blocks, as you otherwise would.\n\n2. GitHub API errors. These mean you're doing something wrong with the\n   API, and they are always evident in the response's status. The API\n   considerately returns a helpful error message in the JSON body.\n\n## Specific REST APIs\n\n`agithub` includes a handful of implementations for specific REST APIs. The\nexample above uses the GitHub API but only for demonstration purposes. It\ndoesn't include any GitHub specific functionality (for example, authentication).\n\nHere is a summary of additional functionality available for each distinct REST\nAPI with support included in `agithub`. Keep in mind, `agithub` is designed\nto be extended to any REST API and these are just an initial collection of APIs.\n\n### GitHub : [`agithub/GitHub.py`](agithub/GitHub.py)\n\n#### GitHub Authentication\n\nTo initiate an authenticated `GitHub` object, pass it your username and password\nor a [token](https://github.com/settings/tokens).\n\n```python\nfrom agithub.GitHub import GitHub\ng = GitHub('user', 'pass')\n```\n\n```python\nfrom agithub.GitHub import GitHub\ng = GitHub(token='token')\n```\n\n#### GitHub Pagination\n\nWhen calling the GitHub API with a query that returns many results, GitHub will\n[paginate](https://developer.github.com/v3/#pagination) the response, requiring\nyou to request each page of results with separate API calls. If you'd like to\nautomatically fetch all pages, you can enable pagination in the `GitHub` object\nby setting `paginate` to `True`.\n\n```python\nfrom agithub.GitHub import GitHub\ng = GitHub(paginate=True)\nstatus, data = g.repos.octocat['Spoon-Knife'].issues.get()\n\nstatus, data = g.users.octocat.repos.get(per_page=1)\nprint(len(data))\n```\n\n```text\n8\n```\n\n(added in v2.2.0)\n\n#### GitHub Rate Limiting\n\nBy default, if GitHub returns a response indicating that a request was refused\ndue to [rate limiting](https://developer.github.com/v3/#rate-limiting), agithub\nwill wait until the point in time when the rate limit is lifted and attempt\nthe call again.\n\nIf you'd like to disable this behavior and instead just return the error\nresponse from GitHub set `sleep_on_ratelimit` to `False`.\n\n```python\nfrom agithub.GitHub import GitHub\ng = GitHub(sleep_on_ratelimit=False)\nstatus, data = g.repos.octocat['Spoon-Knife'].issues.get()\nprint(status)\nprint(data['message'])\n```\n\n```text\n403\nAPI rate limit exceeded for 203.0.113.2. (But here's the good news: Authenticated requests get a higher rate limit. Check out the documentation for more details.)\n```\n\n(added in v2.2.0)\n\n#### GitHub Logging\n\nTo see log messages related to GitHub specific features like pagination and\nrate limiting, you can use a root logger from the Python logging module.\n\n```python\nimport logging\nlogging.basicConfig()\nlogger = logging.getLogger()  # The root logger\nlogger.setLevel(logging.DEBUG)\nfrom agithub.GitHub import GitHub\ng = GitHub(paginate=True)\nstatus, data = g.repos.octocat['Spoon-Knife'].issues.get()\n```\n\n```text\nDEBUG:agithub.GitHub:No GitHub ratelimit remaining. Sleeping for 676 seconds until 14:22:43 before trying API call again.\nDEBUG:agithub.GitHub:Fetching an additional paginated GitHub response page at https://api.github.com/repositories/1300192/issues?page=2\nDEBUG:agithub.GitHub:Fetching an additional paginated GitHub response page at https://api.github.com/repositories/1300192/issues?page=3\n\u2026\n```\n\n## Semantics\n\nHere's how `agithub` works, under the hood:\n\n1. It translates a sequence of attribute look-ups into a URL; The\n   Python method you call at the end of the chain determines the\n   HTTP method to use for the request.\n2. The Python method also receives `name=value` arguments, which it\n   interprets as follows:\n   * `headers=`\n     * You can include custom headers as a dictionary supplied to the\n     `headers=` argument. Some headers are provided by default (such as\n     User-Agent). If these occur in the supplied dictionary, the default\n     value will be overridden.\n \n       ```python\n       headers = {'Accept': 'application/vnd.github.loki-preview+json'}\n       ```\n   * `body=`\n     * If you're using `POST`, `PUT`, or `PATCH` (`post()`, `put()`, and\n     `patch()`), then you should include the body as the `body=` argument.\n     The body is serialized to JSON before sending it out on the wire.\n   * GET Parameters\n     * Any other arguments to the Python method become GET parameters, and are\n     tacked onto the end of the URL. They are, of course, url-encoded for\n     you.\n3. When the response is received, `agithub` looks at its content\n   type to determine how to handle it, possibly decoding it from the\n   given char-set to Python's Unicode representation, then converting to\n   an appropriate form, then passed to you along with the response\n   status code. (A JSON object is de-serialized into a Python object.)\n\n## Extensibility\n`agithub` has been written in an extensible way. You can easily:\n\n* Add new HTTP methods by extending the `Client` class with\n  new Python methods of the same name (and adding them to the\n  [`http_methods` list][1]).\n\n* Add new default headers to the [`_default_headers` dictionary][2].\n  Just make sure that the header names are lower case.\n\n* Add a new media-type (a.k.a. content-type a.k.a mime-type) by\n  inserting a new method into the [`ResponseBody` class][3], replacing\n  `'-'` and `'/'` with `'_'` in the method name. That method will then be\n  responsible for converting the response body to a usable\n  form &mdash; and for calling `decode_body` to do char-set\n  conversion, if required. For example to create a handler for the content-type\n  `application/xml` you'd extend `ResponseBody` and create a new method like this\n  \n  ```python\n  import xml.etree.ElementTree as ET\n\n  class CustomResponseBody(ResponseBody):\n      def __init__(self):\n          super(ChildB, self).__init__()\n      \n      def application_xml(self):\n          # Handles Content-Type of \"application/xml\"\n          return ET.fromstring(self.body)\n  ```\n\nAnd if all else fails, you can strap in, and take 15 minutes to read and\nbecome an expert on the code. From there, anything's possible.\n\n[1]: https://github.com/mozilla/agithub/blob/b47661df9e62224a69216a2f11dbe574990349d2/agithub/base.py#L103-L110\n[2]: https://github.com/mozilla/agithub/blob/b47661df9e62224a69216a2f11dbe574990349d2/agithub/base.py#L22-L28\n[3]: https://github.com/mozilla/agithub/blob/b47661df9e62224a69216a2f11dbe574990349d2/agithub/base.py#L309-L332\n\n## License\nCopyright 2012&ndash;2016 Jonathan Paugh and contributors\nSee [COPYING](COPYING) for license details\n"
},
{
  "name": "consumer-trust",
  "files": {
    "/": [
      ".github",
      "README.md",
      "advisory",
      "tools"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "# building trustworthy products\n\nAt Mozilla, we work across product, law, policy, and ethics to design and develop trustworthy products that help people succeed and uphold basic human rights in societies around the world.\n\nBeyond words, public statements, or corporate missions - we believe in demonstrating our commitment to trust and safety through a measurable and systematic approach in our products from start to finish. That's why we are creating a program that touches every product at Mozilla and provides a readable trust and safety summary. This summary will include examples of how we incorporate our main pillars - privacy, security, safety, inclusion, and sustainability - in our products, and opportunities for creators and consumers to learn from and provide input on those decisions.\n\nThe challenges that face internet technologies today are complex and intertwined, and no one expert or organization knows all the answers. So we decided to do what we do best - open source the work we've done in this program to understand the problems and ways to address them. We want to work together with YOU our brilliant community of creators, contributors, and consumers of our products, to build a better, safer, more trustworthy internet.\n\nCheck out our [framework](https://github.com/MozillaDPX/trust/blob/master/tools/trust_framework.md) and provide [feedback](https://github.com/MozillaDPX/integrity/issues/new?assignees=&labels=zenhub-prod-integrity&template=toolkit_request.md)!\n\n\n## Table of Contents\n1. [program](#program)\n2. [tools](#tools)\n3. [resources](#resources)\n\n## program\nThe consumer trust program at Mozilla consists of two main elements: \n\n1) a semi-automated process in which product researchers, designers, engineers, and managers are empowered to use our resources and process to run trust evaluations of their product using our set of self-assessment tools (toolkit, checklist, resources).\n\n2) For complex challenges, product decisions, and ongoing program resource updates, the consumer trust program is made up of a team of ethical tech experts, maintains active communities with interested consumers, creators, and community members, as well as regular consultations with the integrity advisory council, made up of subject matter experts in related areas.\n\n### self-assess\nUse and contribute to our set of easy access resources built to serve internal and external product creators from product owners to user researchers.\n\n1. [framework](https://github.com/mozilla/consumer-trust/blob/master/tools/trust_framework.md)\n<br>High level framework for shared terminology, standard best practices, and key resources for experience integrity across Mozilla products.\n\n2. [toolkit](https://github.com/mozilla/consumer-trust/blob/master/tools/trust_toolkit.md)\n<br>Baseline checklist for primary product stakeholders: product owners, designers, user researchers, and engineers.\n\n3. [resource repository](https://www.zotero.org/groups/2695011/experience_integrity_public/library)\n<br>Dynamic database of trust related resources including industry guidelines, government regulations/frameworks, and academic research.\n\n### consult\nFor internal teams at Mozilla, the experience consumer trust team is available to support in investigating needs that: \n* are not met through self-assessment process \n* product teams are not able to complete themselves \n* require multi-disciplinary and/or ethical expertise\n\nFor teams or product creators external to Mozilla and interested in the trust program, we encourage you to get in touch! We love collaborating with our friends and colleagues across the industry: placeholder_email@mozilla.com. \n\nIf you would like to be a participant in consumer trust at Mozilla, please get in touch! For those who are already Mozillians, become a part of our products for people advocacy group. For those external to the Mozilla community, consider tuning in or joining our advisory council of external experts.\n\n\n## tools\n* [toolkit](https://github.com/MozillaDPX/integrity/blob/master/integrity_toolkit.md)\n* self-assessment\n\n\n## resources\n\n* [Resource List](https://www.zotero.org/groups/2695011/experience_integrity_public/library)\n* [Glossary](https://docs.google.com/document/d/154UATW0EzRaA1U-26-6P-hvc_UsI1PDcDMpU0VIVAO4/edit)\n"
},
{
  "name": "bug-status",
  "files": {
    "/": [
      ".gitignore",
      ".vscode",
      "Cargo.lock",
      "Cargo.toml",
      "README.md",
      "src"
    ]
  },
  "makefile": null,
  "readme": "To cross-compile for Windows.\n```\nrustup target add x86_64-pc-windows-gnu\nrustup toolchain install nightly-x86_64-pc-windows-gnu\nbrew install mingw-w64\ncargo build --target x86_64-pc-windows-gnu --release\n```\n\nTo look at the help.\n`cargo run --bin proton -- -h`"
},
{
  "name": "addons-nginx",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "addons.conf"
    ]
  },
  "makefile": null,
  "readme": "nginx configuration for addons.mozilla.org, built through docker hub\n"
},
{
  "name": "observatory-cli",
  "files": {
    "/": [
      ".eslintrc.yml",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "docker_example.png",
      "index.js",
      "package.json",
      "report.png",
      "shell_functions"
    ]
  },
  "makefile": null,
  "readme": "# Observatory by Mozilla CLI Client\n\n## Score your site's HTTPS practices\n\nObservatory by Mozilla is a project designed to help developers, system administrators, and security professionals configure their sites safely and securely.\n\n## Observatory in action!\n\n- <https://observatory.mozilla.org/>\n- [FAQ](https://observatory.mozilla.org/faq.html)\n\n### Example site report, with additional options\n\n![Screenshot of ssllabs.com report, showing colors](report.png)\n\nThe [full report url](https://observatory.mozilla.org/analyze.html?host=ssllabs.com) has suggestions to **repair** each of these issues.\n\n\n## Install\n\n```\n$ npm install -g observatory-cli\n```\n\n(Optional `Docker` instructions below.)\n\n## Usage\n\n1.  **Scan a site** for `https` best practices.\n\n    ```\n    # json!\n    $ observatory some.site.name\n\n    # include 'zero' scores, display as a tabular report\n    $ observatory some.site.name --zero --format=report\n\n    # attempt to force a re-scan\n    $ observatory some.site.name --rescan\n\n    ```\n\n2.  **Test a site** as part of a Continuous Integration pipeline.\n\n    Script will FAIL unless the grade is AT LEAST `B+`\n\n    ```\n    $ observatory some.site.name --min-grade B+\n    ```\n\n    ...and the score is at least 50.\n\n    ```\n    $ observatory some.site.name --min-grade B+ --min-score 50\n    ```\n\n\n3.  **Print the URL** for the expanded online report.\n\n    ```\n    $ observatory some.site.name --format=url\n    ```\n\n4.  **nagios** monitoring plugin mode.\n\n    For `--nagios <failcode>`, `failcode` will be the exit code if the test fails.\n\n    `--min-score`, `--min-grade`, `--zero`, `--skip` affect the test.\n\n    ```\n    $ observatory  --nagios 2 --min-score 85 -z --skip cookies\n    CRITICAL [\"content-security-policy\",...,\"x-xss-protection\"]\n    ```\n\n    Any **negative scores fail the test**, unless `--min-score` or `--min-grade` is specified.\n\n    ```\n    # '2' maps to nagios 'critical.'  Exits '2'\n\n    $ observatory ssllabs.com --nagios 2\n    CRITICAL [\"redirection\"]\n    ```\n\n    We can `--skip` the failing rule, and affect the score.\n\n    ```\n    $ observatory ssllabs.com --nagios 2 --skip redirection\n    observatory [INFO] modfiying score, because of --skip.  was: 100, now: 105\n    OK\n    ```\n\n    Quiet output with `-q`.\n\n    ```\n    $ observatory ssllabs.com --nagios 2 --skip redirection -q\n    OK\n    ```\n\n## Help Ouput\n\n```\n$ observatory --help\n\n  Usage: observatory [options] <site>\n\n  cli for interacting with Mozilla HTTP Observatory\n\n  https://observatory.mozilla.org/\n\n  Options:\n\n    -h, --help               output usage information\n    -V, --version            output the version number\n    --format [format]        format for output.  choice:  (json|report|csv|url).  `json` is default\n    --min-grade <grade>      testing: this grade or better, or exit(1)\n    --min-score <score>      testing: this score or better, or exit(1)\n    --nagios [failcode]      nagios mode, exits with [failcode] on failure\n    --rescan                 initiate a rescan instead of showing recent scan results\n    -z, --zero               show test results that don't affect the final score\n    --attempts <n>           number of attempts to try before failing\n    --api-version [version]  api version:  defaults to 1\n    --skip <rule>            skip rules by name.  works with min-score only\n    --tls                    do tls checks instead\n    -q, --quiet              turns off all logging\n\n\n  Output Formats (--format)\n    - json    json of the report\n    - report  plain-text tabular format\n    - csv     alias for report\n    - url     url for online version\n\n\n  Nagios Mode (--nagios)\n    - if `--min-score` and/or `--min-grade`, use those.\n    - else *any* negative rules fail the check.\n    - exits with integer `failcode`.\n```\n\n## Example Report, Text Version\n\nReport, with options:\n\n* `-z` to show '0' rules (all rules)\n* `--skip` to skip a rule (affects SCORE, but not GRADE)\n\n```\n$ observatory some.site --format=report -z --skip redirection\n\nobservatory [INFO] modfiying score, because of --skip.  was: 60, now: 65\n\nHTTP Observatory Report: some.site\n\nScore Description\n\n  -20 content-security-policy        Content Security Policy (CSP) implemented, but allows 'unsafe-inline' inside script-src\n  -10 x-xss-protection               X-XSS-Protection header not implemented\n   -5 x-content-type-options         X-Content-Type-Options header not implemented\n    0 contribute                     Contribute.json implemented with the required contact information\n    0 cookies                        No cookies detected\n    0 cross-origin-resource-sharing  Content is not visible via cross-origin resource sharing (CORS) files or headers\n    0 public-key-pinning             HTTP Public Key Pinning (HPKP) header not implemented\n    0 strict-transport-security      HTTP Strict Transport Security (HSTS) header set to a minimum of six months (15768000)\n    0 subresource-integrity          Subresource Integrity (SRI) not implemented, but all scripts are loaded from a similar origin\n    0 x-frame-options                X-Frame-Options (XFO) header set to SAMEORIGIN or DENY\n\nScore: 65 (modified due to --skip)\nGrade: C+\n\nFull Report Url: https://observatory.mozilla.org/analyze/some.site\n\n```\n\n## Technical / Development\n\n### Debug observatory api urls\n\n```\nNODE_DEBUG=request observatory --format report --rescan --zero www.mozilla.org\n```\n\n### API Documentation\n\nhttps://github.com/mozilla/http-observatory/blob/master/httpobs/docs/api.md\n\n### Dockerized `observatory-cli`\n\nUse the provided [Dockerfile](./Dockerfile), to build and execute `observatory` in Docker container.  Useful for Continuous Integration/Continuous Deployment (CI/CD) pipelines capable of running containers but that otherwise don't need a lot of extra software.\n\n**To get started,**\n\n1. Build the container.  Tag it as `mozilla/observatory-cli`\n\n    ```\n    docker build -t mozilla/observatory-cli .\n    ```\n\n2. Add a section like this to your `profile` (varies depending on your operating system and shell. `bash` shown).\n\n    ```\n    ## $HOME/.bashrc\n    if [[ -d $HOME/.bash_functions ]]; then\n        for file in $HOME/.bash_functions/*; do\n            . $file\n        done\n    fi\n    ```\n\n3. Create the directory referenced in point 2 and copy the files in `shell_functions` (not `bash_completion`) into that directory:\n\n    ```\n    $ mkdir $HOME/.bash_functions\n    $ find shell_functions -maxdepth 1 -type f -executable | while read file; do cp $file $HOME/.bash_functions; done\n    ```\n\n4. *Optional*: Add Bash completion to your shell.  (varies depending on your host operating system)\n\n    ```\n    ## On Red Hat based distributions:\n    sudo cp shell_functions/bash_completion/observatory.bash /etc/bash_completion.d/\n    ```\n\n5. Start a new shell and execute `observatory`.  Now it's in a Docker container.  Bash completion is available if you've added it.\n\n![Screenshot showing use of containerized observatory-cli](docker_example.png)\n\n\n## Related projects\n\n- [HTTP Observatory](https://github.com/mozilla/http-observatory) by April King\n- [Python observatory-cli](https://github.com/mozilla/http-observatory-cli) by April King\n"
},
{
  "name": "overwatch",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# overwatch\nMonitoring system for analytics team\n"
},
{
  "name": "audio-mixer",
  "files": {
    "/": [
      ".circleci",
      ".github",
      ".gitignore",
      "Cargo.toml",
      "README.md",
      "benches",
      "install_rustfmt_clippy.sh",
      "run_sanitizers.sh",
      "run_tests.sh",
      "src"
    ],
    "/.github": [
      "workflows"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Audio Mixer\n\n[![CircleCI](https://circleci.com/gh/mozilla/audio-mixer.svg?style=svg)](https://circleci.com/gh/mozilla/audio-mixer)\n[![Build & Test](https://github.com/mozilla/audio-mixer/actions/workflows/test.yml/badge.svg)](https://github.com/mozilla/audio-mixer/actions/workflows/test.yml)\n\nMixing audio data from any input channel layout to any output channel layout,\nin a *matrix-multiplication* form.\n\n```\noutput channel #1 \u25b8 \u2502 Silence    \u2502   \u2502 0, 0, 0, 0 \u2502   \u2502 FrontRight   \u2502 \u25c2 input channel #1\noutput channel #2 \u25b8 \u2502 FrontRight \u2502 = \u2502 R, C, 0, F \u2502 x \u2502 FrontCenter  \u2502 \u25c2 input channel #2\noutput channel #3 \u25b8 \u2502 FrontLeft  \u2502   \u2502 0, C, L, F \u2502   \u2502 FrontLeft    \u2502 \u25c2 input channel #3\n                          \u25b4                 \u25b4         \u2502 LowFrequency \u2502 \u25c2 input channel #4\n                          \u250a                 \u250a                \u25b4\n                          \u250a                 \u250a                \u250a\n                      out_audio      mixing matrix m       in_audio\n```\n\nFor example, the above means there are 3 output channels and 4 input channels.\nThe order of output channels is _Silence, FrontRight, and FrontLeft_.\nThe order of input channels is  _FrontRight, FrontCenter, FrontLeft, LowFrequency_.\n\nSo the output data in the _channel #2_ will be:\n```\n\nOutput data of ch #2 (FrontRight) =\n    R x input channel #1 (FrontRight)   +\n    C x input channel #2 (FrontCenter)  +\n    0 x input channel #3 (FrontLeft)    +\n    F x input channel #4 (LowFrequency)\n```\n\nwhere the _C, F, L, R_ are mixing coefficients.\nThe _Silence_ channel is a unused channel in the output device,\nso its channel data will always be zero.\n\n## License\n\nMPL-2\n"
},
{
  "name": "configman",
  "files": {
    "/": [
      ".gitignore",
      "AUTHORS",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "configman",
      "contribute.json",
      "demo",
      "docs",
      "requirements.txt",
      "setup.py",
      "test-requirements.txt",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "gettingstarted.rst",
      "index.rst",
      "introduction.rst",
      "tutorial.rst",
      "typeconversion.rst"
    ]
  },
  "makefile": null,
  "readme": "configman\n=========\n\n[![Travis](https://travis-ci.org/mozilla/configman.png?branch=master)](https://travis-ci.org/mozilla/configman)\n\nCopyright Mozilla, 2013 - 2015\n\nGeneral tool for setting up configuration options per namespaces.\nSupports reading and writing configs generally from and into config\nfiles.\n\n\nRunning tests\n-------------\n\nWe use [nose](http://code.google.com/p/python-nose/) to run all the\nunit tests and [tox](http://tox.testrun.org/latest/) to test multiple\npython versions. To run the whole suite just run:\n\n    tox\n\n`tox` will pass arguments after `--` to `nosetests`. To run with test\ncoverage calculation, run `tox` like this:\n\n    tox -- --with-coverage --cover-html --cover-package=configman\n\nIf you want to run a specific test in a testcase class, though,\nyou might consider just using `nosetests`:\n\n    nosetests configman.tests.test_config_manager:TestCase.test_write_flat\n\n\nMaking a release\n----------------\n\nBecause our `.travis.yml` has all the necessary information to automatically\nmake a release, all you need to do is to push a commit onto master.\nMost likely you will only want to do this after you have\nedited the `configman/version.txt` file. Suppose you make some changes:\n\n    git add configman/configman.py\n    git commit -m \"fixed something\"\n\nYou might want to push that to your fork and make a pull request. Then,\nto update the version and make a release, first do this:\n\n    vim configman/version.txt\n    git add configman/version.txt\n    git commit -m \"bump to version x.y.z\"\n    git push origin master\n\nAfter that travis, upon a successful build will automatically make a new\ntarball and wheel and upload it to [PyPI](https://pypi.python.org/pypi/configman)\n"
},
{
  "name": "discourse.mozilla.org",
  "files": {
    "/": [
      ".github",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "discourse-dev.yml",
      "discourse-prod.yml",
      "discourse-stage.yml",
      "includes"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# discourse.mozilla.org\nConfiguration for discourse.mozilla.org\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n"
},
{
  "name": "vtt.js",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "AUTHORS",
      "CODE_OF_CONDUCT.md",
      "Gruntfile.js",
      "LICENSE",
      "README.md",
      "bin",
      "bower.json",
      "dist",
      "lib",
      "package.json",
      "tests",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "# vtt.js\n\n[![Build Status](https://travis-ci.org/mozilla/vtt.js.svg?branch=master)](https://travis-ci.org/mozilla/vtt.js) [![npm-version](http://img.shields.io/npm/v/vtt.js.svg)](https://www.npmjs.org/package/vtt.js) [![Dependency Status](https://david-dm.org/mozilla/vtt.js.svg?theme=shields.io)](https://david-dm.org/mozilla/vtt.js) [![devDependency Status](https://david-dm.org/mozilla/vtt.js/dev-status.svg?theme=shields.io)](https://david-dm.org/mozilla/vtt.js#info=devDependencies)\n\nImplementation of the [WebVTT](https://developer.mozilla.org/en-US/docs/HTML/WebVTT) spec in JavaScript. Can be used\nin NodeJS, on the browser, and many other places. Mozilla uses this implementation for parsing and processing WebVTT\nfiles in Firefox/Gecko.\n\n## Table of Contents\n\n- [Spec Compliance](#spec-compliance)\n- [API](#api)\n  - [WebVTT.Parser](#webvttparser)\n    - [parse(data)](#parsedata)\n    - [flush()](#flush)\n    - [onregion(region)](#onregionregion)\n    - [oncue(cue)](#oncuecue)\n    - [onflush()](#onflush)\n    - [onparsingerror(error)](#onparsingerrorerror)\n  - [WebVTT.convertCueToDOMTree(window, cuetext)](#webvttconvertcuetodomtreewindow-cuetext)\n  - [WebVTT.processCues(window, cues, overlay)](#webvttprocesscueswindow-cues-overlay)\n  - [ParsingError](#parsingerror)\n  - [VTTCue](#vttcue)\n    - [Extended API](#extended-api)\n      - [toJSON()](#tojson)\n      - [VTTCue.fromJSON(json)](#vttcuefromjsonjson)\n      - [VTTCue.create(options)](#vttcuecreateoptions)\n  - [VTTRegion](#vttregion)\n    - [Extended API](#extended-api-1)\n        - [VTTRegion.fromJSON(json)](#vttregionfromjsonjson)\n        - [VTTRegion.create(options)](#vttregioncreateoptions)\n- [Browser](#browser)\n  - [Building Yourself](#building-yourself)\n  - [Bower](#bower)\n  - [Usage](#usage)\n- [Node](#node)\n  - [vtt.js](#vttjs-1)\n  - [node-vtt](#node-vtt)\n- [Developing vtt.js](#developing-vttjs)\n  - [Tests](#tests)\n    - [Writing Tests](#writing-tests)\n  - [Grunt Run Task](#grunt-run-task)\n\n# Spec Compliance\n\n- [Parsing](http://dev.w3.org/html5/webvtt/#webvtt-file-format-parsing) (Completed)\n  - [File](http://dev.w3.org/html5/webvtt/#webvtt-file-parsing) (Completed)\n  - [Region](http://dev.w3.org/html5/webvtt/#webvtt-region-settings-parsing) (Completed)\n  - [Cue Timings and Settings](http://dev.w3.org/html5/webvtt/#webvtt-cue-timings-and-settings-parsing) (Completed)\n  - [WebVTT Cue Text](http://dev.w3.org/html5/webvtt/#dfn-webvtt-cue-text-parsing-rules) (Completed)\n  - [Cue DOM Construction](http://dev.w3.org/html5/webvtt/#webvtt-cue-text-dom-construction-rules) (Completed)\n- [Rendering](http://dev.w3.org/html5/webvtt/#rendering) (In Progress)\n  - [Processing Model](http://dev.w3.org/html5/webvtt/#processing-model) (In Progress) ***No VTTRegion or vertical text support***\n    - [Apply WebVTT Cue Settings](http://dev.w3.org/html5/webvtt/#dfn-apply-webvtt-cue-settings) (In Progress)\n      - Steps 1 - 11 (Completed)\n      - Step 12 (In progress)\n  - [Applying CSS Properties](http://dev.w3.org/html5/webvtt/#applying-css-properties-to-webvtt-node-objects) (Completed)\n  - [CSS Extensions](http://dev.w3.org/html5/webvtt/#css-extensions) **(Won't Implement)**\n- [WebVTT API Shim](http://dev.w3.org/html5/webvtt/#webvtt-api-for-browsers) (Completed)\n  - [VTTCue](http://dev.w3.org/html5/webvtt/#vttcue-interface) (Completed) ***Shims the TextTrackCue interface as well***\n  - [VTTRegion](http://dev.w3.org/html5/webvtt/#vttregion-interface) (Completed)\n\n## Notes\n\nOur processing model portion of the specification makes use of a custom property, `hasBeenReset`. It allows us to detect\nwhen a VTTCue is dirty, i.e. one of its properties that affects display has changed and so we need to recompute its display\nstate. This allows us to reuse a cue's display state if it has already been computed and nothing has changed to effect its\nposition.\n\n# API\n\n## WebVTT.Parser\n\nThe parser has a simple API:\n\n```javascript\nvar parser = new WebVTT.Parser(window, stringDecoder);\nparser.onregion = function(region) {};\nparser.oncue = function(cue) {};\nparser.onflush = function() {};\nparser.onparsingerror = function(e) {};\nparser.parse(moreData);\nparser.parse(moreData);\nparser.flush();\n```\n\nThe Parser constructor is passed a window object with which it will create new\nVTTCues and VTTRegions as well as an optional StringDecoder object which\nit will use to decode the data that the `parse()` function receives. For ease of\nuse, a StringDecoder is provided via `WebVTT.StringDecoder()`. If a custom\nStringDecoder object is passed in it must support the API specified by the\n[#whatwg string encoding](http://encoding.spec.whatwg.org/#api) spec.\n\n### parse(data)\n\nHands data in some format to the parser for parsing. The passed data format\nis expected to be decodable by the StringDecoder object that it has. The parser\ndecodes the data and reassembles partial data (streaming), even across line breaks.\n\n```javascript\nvar parser = new WebVTT.Parser(window, WebVTT.StringDecoder());\nparser.parse(\"WEBVTT\\n\\n\");\nparser.parse(\"00:32.500 --> 00:33.500 align:start size:50%\\n\");\nparser.parse(\"<v.loud Mary>That's awesome!\");\nparser.flush();\n```\n\n### flush()\n\nIndicates that no more data is expected and will force the parser to parse any\nunparsed data that it may have. Will also trigger [onflush](#onflush).\n\n### onregion(region)\n\nCallback that is invoked for every region that is correctly parsed. Returns a [VTTRegion](#http://dev.w3.org/html5/webvtt/#dfn-vttregion)\nobject.\n\n```js\nparser.onregion = function(region) {\n  console.log(region);\n};\n```\n\n### oncue(cue)\n\nCallback that is invoked for every cue that is fully parsed. In case of streaming parsing oncue is\ndelayed until the cue has been completely received. Returns a [VTTCue](#http://dev.w3.org/html5/webvtt/#vttcue-interface) object.\n\n```js\nparser.oncue = function(cue) {\n  console.log(cue);\n};\n```\n\n### onflush()\n\nIs invoked in response to `flush()` and after the content was parsed completely.\n\n```js\nparser.onflush = function() {\n  console.log(\"Flushed\");\n};\n```\n\n### onparsingerror(error)\n\nIs invoked when a parsing error has occured. This means that some part of the\nWebVTT file markup is badly formed. See [ParsingError](#parsingerror) for more\ninformation.\n\n```js\nparser.onparsingerror = function(e) {\n  console.log(e);\n};\n```\n\n## WebVTT.convertCueToDOMTree(window, cuetext)\n\nParses the cue text handed to it into a tree of DOM nodes that mirrors the internal WebVTT node structure of\nthe cue text. It uses the window object handed to it to construct new HTMLElements and returns a tree of DOM\nnodes attached to a top level div.\n\n```javascript\nvar div = WebVTT.convertCueToDOMTree(window, cuetext);\n```\n\n## WebVTT.processCues(window, cues, overlay)\n\nConverts the cuetext of the cues passed to it to DOM trees&mdash;by calling convertCueToDOMTree&mdash;and\nthen runs the processing model steps of the WebVTT specification on the divs. The processing model applies the necessary\nCSS styles to the cue divs to prepare them for display on the web page. During this process the cue divs get added\nto a block level element (overlay). The overlay should be a part of the live DOM as the algorithm will use the\ncomputed styles (only of the divs to do overlap avoidance.\n\n```javascript\nvar divs = WebVTT.processCues(window, cues, overlay);\n```\n\n## ParsingError\n\nA custom JS error object that is reported through the\n[onparsingerror](#onparsingerror) callback. It has a `name`, `code`, and\n`message` property, along with all the regular properties that come with a\nJavaScript error object.\n\n```json\n{\n  \"name\": \"ParsingError\",\n  \"code\": \"SomeCode\",\n  \"message\": \"SomeMessage\"\n}\n```\n\nThere are two error codes that can be reported back currently:\n\n- 0 BadSignature\n- 1 BadTimeStamp\n\n**Note:** Exceptions other then `ParsingError` will be thrown and not reported.\n\n## VTTCue\n\nA DOM shim for the VTTCue. See the [spec](http://dev.w3.org/html5/webvtt/#vttcue-interface)\nfor more information. Our VTTCue shim also includes properties of its abstract base class\n[TextTrackCue](http://www.whatwg.org/specs/web-apps/current-work/multipage/the-video-element.html#texttrackcue).\n\n```js\nvar cue = new window.VTTCue(0, 1, \"I'm a cue.\");\n```\n\n**Note:** Since this polfyill doesn't implement the track specification directly the `onenter`\nand `onexit` events will do nothing and do not exist on this shim.\n\n### Extended API\n\nThere is also an extended version of this shim that gives a few convenience methods\nfor converting back and forth between JSON and VTTCues. If you'd like to use these\nmethods then us `vttcue-extended.js` instead of `vttcue.js`. This isn't normally\nbuilt into the `vtt.js` distributable so you will have to build a custom distribution\ninstead of using bower.\n\n#### toJSON()\n\nConverts a cue to JSON.\n\n```js\nvar json = cue.toJSON();\n```\n\n#### VTTCue.fromJSON(json)\n\nCreate and initialize a VTTCue from JSON.\n\n```js\nvar cue = VTTCue.fromJSON(json);\n```\n\n#### VTTCue.create(options)\n\nInitializes a VTTCue from an options object where the properties in the option\nobjects are the same as the properties on the VTTCue.\n\n```js\nvar cue = VTTCue.create(options);\n```\n\n## VTTRegion\n\nA DOM shim for the VTTRegion. See the [spec](http://dev.w3.org/html5/webvtt/#vttregion-interface)\nfor more information.\n\n```js\nvar region = new window.VTTRegion(0, 1, \"I'm a region.\");\ncue.region = region;\n```\n\n### Extended API\n\nThere is also an extended version of this shim that gives a few convenience methods\nfor converting back and forth between JSON and VTTRegions. If you'd like to use these\nmethods then us `vttregion-extended.js` instead of `vttregion.js`. This isn't normally\nbuilt into the `vtt.js` distributable so you will have to build a custom distribution\ninstead of using bower.\n\n#### VTTRegion.fromJSON(json)\n\nCreates and initializes a VTTRegion from JSON.\n\n```js\nvar region = VTTRegion.fromJSON(json);\n```\n\n#### VTTRegion.create(options)\n\nCreates a VTTRegion from an options object where the properties on the options\nobject are the same as the properties on the VTTRegion.\n\n```js\nvar region = VTTRegion.create(options);\n```\n\n# Browser\n\nIn order to use the `vtt.js` in a browser, you need to get the built distribution of vtt.js. The distribution\ncontains polyfills for [TextDecoder](http://encoding.spec.whatwg.org/), [VTTCue](http://dev.w3.org/html5/webvtt/#vttcue-interface),\nand [VTTRegion](http://dev.w3.org/html5/webvtt/#vttregion-interface) since not all browsers currently\nsupport them.\n\n## Building Yourself\n\nBuilding a browser-ready version of the library is done using `grunt` (if you haven't installed\n`grunt` globally, you can run it from `./node_modules/.bin/grunt` after running `npm install`):\n\n```bash\n$ grunt build\n$ Running \"uglify:dist\" (uglify) task\n$ File \"dist/vtt.min.js\" created.\n\n$ Running \"concat:dist\" (concat) task\n$ File \"dist/vtt.js\" created.\n\n$ Done, without errors.\n```\n\nYour newly built `vtt.js` now lives in `dist/vtt.min.js`, or alternatively, `dist/vtt.js` for an\nunminified version.\n\n## Bower\n\nYou can also get the a prebuilt distribution from [Bower](http://bower.io/). Either run the shell\ncommand:\n\n```bash\n$ bower install vtt.js\n```\n\nOr include `vtt.js` as a dependency in your `bower.json` file. `vtt.js` should now\nlive in `bower_components/vtt.js/vtt.min.js`. There is also an unminified\nversion included with bower at `bower_components/vtt.js/vtt.js`.\n\n## Usage\n\nTo use `vtt.js` you can just include the script on an HTML page like so:\n\n```html\n<html>\n<head>\n  <meta charset=\"utf-8\">\n  <title>vtt.js in the browser</title>\n  <script src=\"bower_components/vtt.js/vtt.min.js\"></script>\n</head>\n<body>\n  <script>\n    var vtt = \"WEBVTT\\n\\nID\\n00:00.000 --> 00:02.000\\nText\",\n        parser = new WebVTT.Parser(window, WebVTT.StringDecoder()),\n        cues = [],\n        regions = [];\n    parser.oncue = function(cue) {\n      cues.push(cue);\n    };\n    parser.onregion = function(region) {\n      regions.push(region);\n    }\n    parser.parse(vtt);\n    parser.flush();\n\n    var div = WebVTT.convertCueToDOMTree(window, cues[0].text);\n    var divs = WebVTT.processCues(window, cues, document.getElementById(\"overlay\"));\n  </script>\n  <div id=\"overlay\" style=\"position: relative; width: 300px; height: 150px\"></div>\n</body>\n</html>\n```\n\n# Node\n\nYou have a couple of options if you'd like to run the library from Node.\n\n## vtt.js\n\n`vtt.js` is on npm. Just do:\n\n```\nnpm install vtt.js\n```\n\nRequire it and use it:\n\n```js\nvar vtt = require(\"vtt.js\"),\n    WebVTT = vtt.WebVTT,\n    VTTCue = vtt.VTTCue,\n    VTTRegion = vtt.VTTRegion;\n\nvar parser = new WebVTT.Parser(window);\nparser.parse();\n// etc\n\nvar elements = WebVTT.processCues(window, cues, overlay);\nvar element = WebVTT.convertCueToDOMTree(window, cuetext);\n\nvar cue = new VTTCue(0, 1, \"I'm a cue.\");\nvar region = new VTTRegion();\n```\n\nSee the [API](#api) for more information on how to use it.\n\n**Note:** If you use this method you will have to provide your own window object\nor a shim of one with the necessary functionality for either the parsing or processing\nportion of the spec. The only shims that are provided to you are `VTTCue` and `VTTRegion`\nwhich you can attach to your global that is passed into the various functions.\n\n## node-vtt\n\nUse [node-vtt](https://github.com/mozilla/node-vtt). Node-vtt runs `vtt.js` on a PhantomJS page\nfrom Node so it has access to a full DOM and CSS layout engine which means you can run any part\nof the library you want. See the [node-vtt](https://github.com/mozilla/node-vtt) repo for more\ninformation.\n\n# Developing vtt.js\n\nA few things to note:\n\n* When bumping the version remember to use the `grunt release` task as this will\nbump `package.json` + `bower.json` and build the `dist` files for `vtt.js` in one\ngo.\n* The [Grunt Run Task](#grunt-run-task) tool is handy for running the library without having\nto run the whole test suite or set of tests.\n\n## Tests\n\nTests are written and run using [Mocha](https://mochajs.org/) on node.js.\n\nTo run all the tests, do the following:\n\n```bash\n$ npm test\n```\n\nIf you want to run individual tests, you can install the [Mocha](https://mochajs.org/) command-line\ntool globally, and then run tests per-directory:\n\n```bash\n$ npm install -g mocha\n$ cd tests/some/sub/dir\n$ mocha --reporter spec --timeout 200000\n```\n\nSee the [usage docs](https://mochajs.org/#usage) for further usage info.\n\n### Writing Tests\n\nTests are done by comparing live parsed output to a last-known-good JSON file. The JSON files\ncan be easily generated using `vtt.js`, so you don't need to write these by hand\n(see details below about [Grunt Run Task](#grunt-run-task)).\n\n#### TestRunner\n\nThere's a prebuilt API in place for testing different parts of `vtt.js`. Simply\nrequire the [TestRunner](https://github.com/mozilla/vtt.js/blob/master/tests/test-runner.js)\nmodule in the `lib` directory and start writing tests using `mocha`. See an example of a test file\n[here](https://github.com/mozilla/vtt.js/blob/master/tests/cue-settings/align/test.js)\nwith its first test's WebVTT file [here](https://github.com/mozilla/vtt.js/blob/master/tests/cue-settings/align/bad-align.vtt)\nand its corresponding [parsing JSON file](https://github.com/mozilla/vtt.js/blob/master/tests/cue-settings/align/bad-align.json)\nand [processing JSON file](https://github.com/mozilla/vtt.js/blob/master/tests/cue-settings/align/bad-align-proc.json).\nYou can also check out the [tests](https://github.com/mozilla/vtt.js/tree/master/tests)\ndirectory for more examples on how to write tests.\n\n#### jsonEqual(vttFile, jsonRefFile, message, onDone)\n\nFirst parses the WebVTT file as UTF8 and compares it to the reference JSON file\nand then parses the WebVTT file as a string and compares it to the reference JSON\nfile.\n\n#### jsonEqualStreaming(vttFile, jsonRefFile, message, onDone)\n\nSimulates parsing the file while streaming by splitting the WebVTT file into\nchunks. Will simulate parsing like this `n` times for a single WebVTT file where\n`n` is the length in unicode characters of the file, so use this only on small\nfiles or else you will get a timeout failure on your test.\n\n#### jsonEqualParsing(vttFile, jsonRefFile, message, onDone)\n\nRuns `jsonEqual` and `jsonEqualStreaming` in one go.\n\n#### jsonEqualProcModel(vttFile, jsonRefFile, message, onDone)\n\nRuns the processing model over the `VTTCues` and `VTTRegions` that are returned\nfrom parsing the WebVTT file.\n\n#### jsonEqualAll(vttFile, jsonRefFile, message, onDone)\n\nRuns `jsonEqualParsing` and `jsonEqualProcModel`. Note that `jsonRefFile` should\ncontain JSON that is generated from parsing. The processing model test will compare\nits results to a JSON file located at `[vttFile]-proc.json`. Therefore, if you\nhave a WebVTT file named `basic.vtt` the JSON reference file for the processing\nmodel tests will be `basic-proc.json`.\n\n#### jsonEqualAllNoStream(vttFile, jsonRefFile, message, onDone)\n\nRuns `jsonEqual` and `jsonEqualProcModel` use this if you want to do parsing\nand processing tests, but do not want to simulate streaming because you\nhave too big of a WebVTT file.\n\n## Grunt Run Task\n\nYou can automatically generate a JSON file for a given `.vtt` file using the\n`run` Grunt task.\n\nTo get parsed JSON output from some WebVTT file do:\n\n```bash\n$ grunt run:my-vtt-file.vtt\n$ grunt run:my-vtt-file.vtt > my-json-file.json\n```\n\nTo get processed output from the WebVTT file do:\n\n```bash\n$ grunt run:my-vtt-file.vtt:p\n$ grunt run:my-vtt-file.vtt:p > my-json-file.json\n```\n\nBy passing the `c` flag you can automatically copy the output into a JSON file\nwith the same name as the WebVTT file:\n\n```bash\n$ grunt run:my-vtt-file.vtt:c\n$ grunt run:my-vtt-file.vtt:pc\n```\n\nThe parsed JSON output now lives in `my-vtt-file.json` and the processing JSON\noutput lives in `my-vtt-file-proc.json`.\n\nYou can also run it over a directory copying the output of parsing or\nprocessing each WebVTT file to a corresponding JSON file like so:\n\n```bash\n$ grunt run:my-vtt-file-directory\n$ grunt run:my-vtt-file-directory:p\n```\n\nThis is useful when you've modified how `vtt.js` works and each JSON file needs\na slight change.\n\nThe `run` task utilizes a script called `cue2json`, but\ndoes a few other things for you before each run like building a development\nbuild for `cue2json` to use. It's also a bit easier to type in the CL options\nfor the task. If you want to know more about `cue2json` you can run it directly\nlike so:\n\n```bash\n$ ./bin/cue2json.js \n$ Generate JSON test files from a reference VTT file.\n$ Usage: node ./bin/cue2json.js [options]\n$ \n$ Options:\n$   -v, --vtt      Path to VTT file.                                                                                     \n$   -d, --dir      Path to test directory. Will recursively find all JSON files with matching VTT files and rewrite them.\n$   -c, --copy     Copies the VTT file to a JSON file with the same name.                                                \n$   -p, --process  Generate a JSON file of the output returned from the processing model. \n```\n\n**Notes:** \n\n* `cue2json` utilizes the last development build done. This is why the Grunt `run` task is\ngood as you don't have to remember to build it yourself. If you don't build it yourself then you could\npotentially get incorrect results from it.\n* Since `cue2json` uses the actual parser to generate these JSON files there is the possibility that\nthe generated JSON will contain bugs. Therefore, always check the generated JSON files to check that the\nparser actually parsed according to spec.\n"
},
{
  "name": "tls-observatory",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "PULL_REQUEST_TEMPLATE.md",
      "README.md",
      "certificate",
      "cipherscan",
      "conf",
      "config",
      "connection",
      "constants",
      "database",
      "go.mod",
      "go.sum",
      "kubernetes",
      "logger",
      "metrics",
      "static",
      "tlsobs-api",
      "tlsobs-runner",
      "tlsobs-scanner",
      "tlsobs",
      "tools",
      "truststores",
      "vendor",
      "version.json",
      "worker"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this\n# file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nBUILDREF\t:= $(shell git log --pretty=format:'%h' -n 1)\nBUILDDATE\t:= $(shell date +%Y%m%d)\nBUILDENV\t:= dev\nBUILDREV\t:= $(BUILDDATE)+$(BUILDREF).$(BUILDENV)\n\n# Supported OSes: linux darwin windows\n# Supported ARCHes: 386 amd64\nifeq ($(OS),windows)\n\tOS := windows\nelse\n\tOS := $(shell uname -s | tr [:upper:] [:lower:])\nendif\nARCH := amd64\n\nifeq ($(OS),windows)\n\tBINSUFFIX   := \".exe\"\nelse\n\tBINSUFFIX\t:= \"\"\nendif\nGO \t\t\t:= GOOS=$(OS) GOARCH=$(ARCH) go\nGOGETTER\t:= GOPATH=$(shell pwd)/.tmpdeps go get -d\nGOLDFLAGS\t:= -ldflags \"-X main.version=$(BUILDREV)\"\n\nall: test tlsobs-scanner tlsobs-api tlsobs tlsobs-runner\n\ntlsobs-scanner:\n\techo building TLS Observatory Scanner for $(OS)/$(ARCH)\n\t$(GO) build $(GOOPTS) -o $(GOPATH)/bin/tlsobs-scanner$(BINSUFFIX) $(GOLDFLAGS) github.com/mozilla/tls-observatory/tlsobs-scanner\n\ntlsobs-api:\n\techo building tlsobs-api for $(OS)/$(ARCH)\n\t$(GO) build $(GOOPTS) -o $(GOPATH)/bin/tlsobs-api$(BINSUFFIX) $(GOLDFLAGS) github.com/mozilla/tls-observatory/tlsobs-api\n\ntlsobs:\n\techo building tlsobs client for $(OS)/$(ARCH)\n\t$(GO) build $(GOOPTS) -o $(GOPATH)/bin/tlsobs$(BINSUFFIX) $(GOLDFLAGS) github.com/mozilla/tls-observatory/tlsobs\n\ntlsobs-runner:\n\techo building tlsobs-runner for $(OS)/$(ARCH)\n\t$(GO) build $(GOOPTS) -o $(GOPATH)/bin/tlsobs-runner$(BINSUFFIX) $(GOLDFLAGS) github.com/mozilla/tls-observatory/tlsobs-runner\n\nvendor:\n\tgo mod tidy -v\n\tgo mod vendor -v\n\ntest:\n# Skip tools/ dir, it has multiple main method\n\t$(GO) test `go list ./... | grep -v tools`\n\ntruststores:\n\tgit submodule update --init --recursive\n\tcd truststores && git pull origin master && cd ..\n\tcat truststores/data/apple/snapshot/*.pem > conf/truststores/CA_apple_latest.crt\n\tcat truststores/data/java/snapshot/*.pem > conf/truststores/CA_java.crt\n\tcurl -o conf/truststores/CA_AOSP.crt https://pki.goog/roots.pem\n\t$(GO) run tools/retrieveTruststoreFromCADatabase.go mozilla > conf/truststores/CA_mozilla_nss.crt\n\t$(GO) run tools/retrieveTruststoreFromCADatabase.go microsoft > conf/truststores/CA_microsoft.crt\n\ncipherscan:\n\tcd cipherscan && git pull origin master && cd ..\n\nciscotop1m:\n\twget http://s3-us-west-1.amazonaws.com/umbrella-static/top-1m.csv.zip\n\tunzip top-1m.csv.zip\n\tmv top-1m.csv conf/cisco-top-1m.csv\n\trm top-1m.csv.zip\n\tdos2unix conf/cisco-top-1m.csv\n\nalexatop1m:\n\twget http://s3.amazonaws.com/alexa-static/top-1m.csv.zip\n\tunzip top-1m.csv.zip\n\tmv top-1m.csv conf/alexa-top-1m.csv\n\trm top-1m.csv.zip\n\tdos2unix conf/alexa-top-1m.csv\n\n.PHONY: all test clean tlsobs-scanner tlsobs-api tlsobs-runner tlsobs vendor truststores cipherscan\n",
  "readme": "# Mozilla TLS Observatory\n\n[![What's Deployed](https://img.shields.io/badge/whatsdeployed-stage,prod-green.svg)](https://whatsdeployed.io/s-LVL)\n[![CircleCI](https://circleci.com/gh/mozilla/tls-observatory/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla/tls-observatory/tree/master)\n\nThe Mozilla TLS Observatory is a suite of tools for analysis and inspection on Transport Layer Security (TLS) services. The components of TLS Observatory include:\n\n- [EV Checker](https://tls-observatory.services.mozilla.com/static/ev-checker.html) - Tool for Certificate Authorities (CAs) who request a root certificate enabled for Extended Validation (EV).\n- [Certificate Explainer](https://tls-observatory.services.mozilla.com/static/certsplainer.html) - Web UI that parses fields of X.509 certificates\n- `tlsobs` - CLI tool for issuing scans of a website\n- `tlsobs-api` - HTTP webserver receiving website scan requests and displaying results\n- `tlsobs-runner` - Service that schedules website scans\n- `tlsobs-scanner` - Service that performs scans and analysis of websites\n\nWant the WebUI? Check out [Mozilla's Observatory](https://observatory.mozilla.org) !\n\n* [Mozilla TLS Observatory](#mozilla-tls-observatory)\n  * [Getting started](#getting-started)\n    * [Using the tlsobs client from Docker](#using-the-tlsobs-client-from-docker)\n  * [Developing](#developing)\n    * [Create the database](#create-the-database)\n    * [Starting the API and Scanner](#starting-the-api-and-scanner)\n    * [Run a scan locally](#run-a-scan-locally)\n    * [Configuration](#configuration)\n      * [tlsobs-api](#tlsobs-api)\n      * [tlsobs-scanner](#tlsobs-scanner)\n      * [tlsobs-runner](#tlsobs-runner)\n  * [API Endpoints](#api-endpoints)\n    * [POST /api/v1/scan](#post-apiv1scan)\n    * [GET /api/v1/results](#get-apiv1results)\n    * [GET /api/v1/certificate](#get-apiv1certificate)\n    * [POST /api/v1/certificate](#post-apiv1certificate)\n    * [GET /api/v1/paths](#get-apiv1paths)\n    * [GET /api/v1/truststore](#get-apiv1truststore)\n    * [GET /api/v1/issuereecount](#get-apiv1issuereecount)\n    * [GET /api/v1/__heartbeat__](#get-apiv1heartbeat)\n    * [GET /api/v1/__stats__](#get-apiv1stats)\n  * [Database Queries](#database-queries)\n  * [Core contributors](#contributors)\n  * [License](#license)\n\n## Getting started\n\nYou can use the TLS Observatory to compare your site against the mozilla guidelines.\nIt requires Golang 1.15+ to be installed:\n\n```bash\n$ go version\ngo version go1.15 linux/amd64\n\n$ export GOPATH=\"$HOME/go\"\n$ mkdir $GOPATH\n\n$ export PATH=$GOPATH/bin:$PATH\n```\n\nThen get the binary:\n\n```bash\n$ go get github.com/mozilla/tls-observatory/tlsobs\n```\n\nAnd scan using our hosted service:\n\n```bash\n$ tlsobs tls-observatory.services.mozilla.com\nScanning tls-observatory.services.mozilla.com (id 13528951)\nRetrieving cached results from 20h33m1.379461888s ago. To run a new scan, use '-r'.\n\n--- Certificate ---\nSubject  C=US, O=Mozilla Corporation, CN=tls-observatory.services.mozilla.com\nSubjectAlternativeName\n- tls-observatory.services.mozilla.com\nValidity 2016-01-20T00:00:00Z to 2017-01-24T12:00:00Z\nSHA1     FECA3CA0F4B726D062A76F47635DD94A37985105\nSHA256   315A8212CBDC76FF87AEB2161EDAA86E322F7C18B27152B5CB9206297F3D3A5D\nSigAlg   ECDSAWithSHA256\nKey      ECDSA 384bits P-384\nID       1281826\n\n--- Trust ---\nMozilla Microsoft Apple Android\n   \u2713        \u2713       \u2713      \u2713\n\n--- Chain of trust ---\nC=US, O=Mozilla Corporation, CN=tls-observatory.services.mozilla.com (id=1281826)\n\u2514\u2500\u2500C=US, O=DigiCert Inc, CN=DigiCert ECC Secure Server CA (id=5922)\n   \u2514\u2500\u2500C=US, O=DigiCert Inc, OU=www.digicert.com, CN=DigiCert Global Root CA (id=41)\n\n\n\n--- Ciphers Evaluation ---\nprio cipher                        protocols pfs                curves\n1    ECDHE-ECDSA-AES128-GCM-SHA256 TLSv1.2   ECDH,P-256,256bits prime256v1\n2    ECDHE-ECDSA-AES256-GCM-SHA384 TLSv1.2   ECDH,P-256,256bits prime256v1\nOCSP Stapling        false\nServer Side Ordering true\nCurves Fallback      false\n\n--- Analyzers ---\n* Mozilla evaluation: modern\n  - for modern level: consider adding ciphers ECDHE-RSA-AES256-GCM-SHA384, ECDHE-ECDSA-CHACHA20-POLY1305, ECDHE-RSA-CHACHA20-POLY1305, ECDHE-RSA-AES128-GCM-SHA256, ECDHE-ECDSA-AES256-SHA384, ECDHE-RSA-AES256-SHA384, ECDHE-ECDSA-AES128-SHA256, ECDHE-RSA-AES128-SHA256\n  - for modern level: consider enabling OCSP stapling\n  - for modern level: increase priority of ECDHE-ECDSA-AES256-GCM-SHA384 over ECDHE-ECDSA-AES128-GCM-SHA256\n  - for modern level: fix ciphersuite ordering, use recommended modern ciphersuite\n  - oldest clients: Firefox 27, Chrome 30, IE 11 on Windows 7, Edge 1, Opera 17, Safari 9, Android 5.0, Java 8\n* Grade: A (93/100)\n```\n\nThe analysis at the end tell you what need to be changed to reach the old, intermediate or modern level. We recommend to target the intermediate level by default, and modern if you don't care about old clients.\n\n### Using the tlsobs client from Docker\n\nA docker container also exists that contains the CLI, API, Scanner and Runner.\nFetch is from `docker pull mozilla/tls-observatory`.\n\n```bash\n$ docker pull mozilla/tls-observatory\n$ docker run -it mozilla/tls-observatory tlsobs accounts.firefox.com\n```\n\n## Developing\n\nYou can use the Kubernetes configuration provided in https://github.com/mozilla/tls-observatory/tree/master/kubernetes , or alternatively, you can do the following:\n\nYou can use the `mozilla/tls-observatory` docker container for development:\n\n```bash\n$ docker pull mozilla/tls-observatory\n$ docker run -it mozilla/tls-observatory /bin/bash\nroot@05676e6789dd:~# cd $GOPATH/src/github.com/mozilla/tls-observatory\nroot@05676e6789dd:/go/src/github.com/mozilla/tls-observatory# make\n```\n\nHowever, even with the docker container, you will need to setup your own\npostgresql database. See below.\n\nTo build a development environment from scratch, you will need Go 1.15 or above.\nYou can set it up on your own machine or via the `golang:1.15` Docker\ncontainer.\n\nRetrieve a copy of the source code using `go get`, to place it directly\nunder `$GOPATH/src/github.com/mozilla/tls-observatory`, then use `make`\nto build all components.\n\n```bash\n$ docker run -it golang:1.15\n\nroot@c63f11b8852b:/go# go get github.com/mozilla/tls-observatory\npackage github.com/mozilla/tls-observatory: no buildable Go source files in /go/src/github.com/mozilla/tls-observatory\n\nroot@c63f11b8852b:/go# cd $GOPATH/src/github.com/mozilla/tls-observatory\n\nroot@c63f11b8852b:/go/src/github.com/mozilla/tls-observatory# make\n```\n\n`make` runs the tests and compiles the scanner, api, command line client\nand runner. The resulting binaries are placed under `$GOPATH/bin`.\n\n### Create the database\n\nTLS Observatory uses PostgreSQL > 9.4. To create a database, use the\nschema in `database/schema.sql`.\n\n```bash\npostgres=# create database observatory;\nCREATE DATABASE\n\npostgres=# \\c observatory\nYou are now connected to database \"observatory\" as user \"postgres\".\n\npostgres=# \\i /go/src/github.com/mozilla/tls-observatory/database/schema.sql\n```\n\nThis automatically creates all tables, indexes, users and grants to work\nwith the default configuration.\n\n### Starting the API and Scanner\n\nFirst symlink the configuration to /etc/observatory and the cipherscan\nexecutable to /opt/cipherscan, as follows:\n\n```bash\nroot@c63f11b8852b:/# ln -s $GOPATH/src/github.com/mozilla/tls-observatory/conf /etc/tls-observatory\nroot@c63f11b8852b:/# ln -s $GOPATH/src/github.com/mozilla/tls-observatory/cipherscan /opt/cipherscan\n```\n\nThen start `tlsobs-api` and `tlsobs-scanner`. The API will listen on port 8083,\non localhost (or 172.17.0.2 if you're running in Docker).\n\n### Run a scan locally\n\nTo run a scan using the local scanner, set the `-observatory` flag of the `tlsobs`\nclient to use the local API, as follows:\n\n```bash\n$ tlsobs -observatory http://172.17.0.2:8083 ulfr.io\n```\n\n### Configuration\n\n#### tlsobs-api\n\nCustomize the configuration file under `conf/api.cfg` and using the following\nenvironment variables:\n\n* `TLSOBS_API_ENABLE` set to `on` or `off` to enable or disable the API\n* `TLSOBS_POSTGRES` is the hostname or IP of the database server (eg. `mypostgresdb.example.net`)\n* `TLSOBS_POSTGRESDB` is the name of the database (eg. `observatory`)\n* `TLSOBS_POSTGRESUSER` is the database user (eg. `tlsobsapi`)\n* `TLSOBS_POSTGRESPASS` is the database user password (eg. `mysecretpassphrase`)\n\n#### tlsobs-scanner\n\nCustomize the configuration file under `conf/scanner.cfg` and using the\nfollowing environment variables:\n\n* `TLS_AWSCERTLINT_DIR` set where awslabs/certlint directory exists\n* `TLSOBS_SCANNER_ENABLE` set to `on` or `off` to enable or disable the scabber\n* `TLSOBS_POSTGRES` is the hostname or IP of the database server (eg. `mypostgresdb.example.net`)\n* `TLSOBS_POSTGRESDB` is the name of the database (eg. `observatory`)\n* `TLSOBS_POSTGRESUSER` is the database user (eg. `tlsobsscanner`)\n* `TLSOBS_POSTGRESPASS` is the database user password (eg. `mysecretpassphrase`)\n\n#### tlsobs-runner\n\nRuns regular tests against target sites and sends notifications.\n\nSee `conf/runner.yaml` for an example of configuration. Some configuration\nparameters can also be provided through environment variables:\n\n* `TLSOBS_RUNNER_SMTP_HOST` is the hostname of the smtp server (eg. `mypostfix.example.net`)\n* `TLSOBS_RUNNER_SMTP_PORT` is the port of the smtp server (eg. `587`)\n* `TLSOBS_RUNNER_SMTP_FROM` is the from address of email notifications sent by the runner (eg. `mynotification@tlsobservatory.example.net`)\n* `TLSOBS_RUNNER_SMTP_AUTH_USER` is the smtp authenticated username (eg `tlsobsrunner`)\n* `TLSOBS_RUNNER_SMTP_AUTH_PASS` is the smtp user password (eg. `mysecretpassphrase`)\n* `TLSOBS_RUNNER_SLACK_WEBHOOK` is the slack webhook (eg. `https://hooks.slack.com/services/not/a/realwebhook`)\n* `TLSOBS_RUNNER_SLACK_USERNAME` is the what the message sender's username will be (eg. `tlsbot`)\n* `TLSOBS_RUNNER_SLACK_ICONEMOJI` is the what the message sender's icon will be (eg. `:telescope:`)\n\n## API Endpoints\n\n### POST /api/v1/scan\n\nSchedule a scan of a given target.\n\n```bash\n$ curl -X POST 'https://tls-observatory.services.mozilla.com/api/v1/scan?target=ulfr.io&rescan=true'\n```\n\n**Parameters**:\n\n* `target` is the FQDN of the target site. eg. `google.com`. Do not use protocol handlers or query strings.\n* `rescan` asks for a rescan of the target when set to true.\n* `params` JSON object in which each key represents one of TLS Observatory's workers. The value under each key will be passed as the parameters to the corresponding worker. For example, `{\"ev-checker\": {\"oid\": \"foo\"}}` will pass `{\"oid\": \"foo\"}` to the ev-checker worker. The following workers accept parameters:\n  * ev-checker: Expects a JSON object with the following keys:\n    * oid: the oid of the EV policy to check\n    * rootCertificate: the root certificate to check against, in PEM format\n\nFor example, with curl:\n\n```\ncurl -X POST \"http://localhost:8083/api/v1/scan?target=mozilla.org&rescan=true&params=%7B%0A%20%20%22ev-checker%22%3A%20%7B%0A%20%20%22rootcertificate%22%3A%20%22-----BEGIN%20CERTIFICATE-----%5CnMIIDxTCCAq2gAwIBAgIQAqxcJmoLQJuPC3nyrkYldzANBgkqhkiG9w0BAQUFADBs%5CnMQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3%5Cnd3cuZGlnaWNlcnQuY29tMSswKQYDVQQDEyJEaWdpQ2VydCBIaWdoIEFzc3VyYW5j%5CnZSBFViBSb290IENBMB4XDTA2MTExMDAwMDAwMFoXDTMxMTExMDAwMDAwMFowbDEL%5CnMAkGA1UEBhMCVVMxFTATBgNVBAoTDERpZ2lDZXJ0IEluYzEZMBcGA1UECxMQd3d3%5CnLmRpZ2ljZXJ0LmNvbTErMCkGA1UEAxMiRGlnaUNlcnQgSGlnaCBBc3N1cmFuY2Ug%5CnRVYgUm9vdCBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMbM5XPm%5Cn%2B9S75S0tMqbf5YE%2Fyc0lSbZxKsPVlDRnogocsF9ppkCxxLeyj9CYpKlBWTrT3JTW%5CnPNt0OKRKzE0lgvdKpVMSOO7zSW1xkX5jtqumX8OkhPhPYlG%2B%2BMXs2ziS4wblCJEM%5CnxChBVfvLWokVfnHoNb9Ncgk9vjo4UFt3MRuNs8ckRZqnrG0AFFoEt7oT61EKmEFB%5CnIk5lYYeBQVCmeVyJ3hlKV9Uu5l0cUyx%2BmM0aBhakaHPQNAQTXKFx01p8VdteZOE3%5CnhzBWBOURtCmAEvF5OYiiAhF8J2a3iLd48soKqDirCmTCv2ZdlYTBoSUeh10aUAsg%5CnEsxBu24LUTi4S8sCAwEAAaNjMGEwDgYDVR0PAQH%2FBAQDAgGGMA8GA1UdEwEB%2FwQF%5CnMAMBAf8wHQYDVR0OBBYEFLE%2Bw2kD%2BL9HAdSYJhoIAu9jZCvDMB8GA1UdIwQYMBaA%5CnFLE%2Bw2kD%2BL9HAdSYJhoIAu9jZCvDMA0GCSqGSIb3DQEBBQUAA4IBAQAcGgaX3Nec%5CnnzyIZgYIVyHbIUf4KmeqvxgydkAQV8GK83rZEWWONfqe%2FEW1ntlMMUu4kehDLI6z%5CneM7b41N5cdblIZQB2lWHmiRk9opmzN6cN82oNLFpmyPInngiK3BD41VHMWEZ71jF%5CnhS9OMPagMRYjyOfiZRYzy78aG6A9%2BMpeizGLYAiJLQwGXFK3xPkKmNEVX58Svnw2%5CnYzi9RKR%2F5CYrCsSXaQ3pjOLAEFe4yHYSkVXySGnYvCoCWw9E1CAx2%2FS6cCZdkGCe%5CnvEsXCS%2B0yx5DaMkHJ8HSXPfqIbloEpw8nL%2Be%2FIBcm2PN7EeqJSdnoDfzAIJ9VNep%5Cn%2BOkuE6N36B9K%5Cn-----END%20CERTIFICATE-----%22%2C%0A%20%20%22oid%22%3A%20%222.16.840.1.114412.22.1%22%0A%7D%0A%7D\"\n```\n\n**Output**: a `json` document containing the Scan ID.\n\n**Caching**: When `rescan` is not `true`, if a scan of the target was done over the last 24 hours, the scan ID is returned. Use `rescan=true` to force a rescan within 24 hours of the previous scan.\n\n**Rate Limits**: Each target can only be scanned every 3 minutes with `rescan=true`.\n\n### GET /api/v1/results\n\nRetrieve scan results by its ID.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/results?id=12302333\n```\n\n**Parameters**:\n\n* `id` is the Scan ID\n\n**Output**: a `json` document containing the scan results and the ID of the end-entity certificate.\n\n### GET /api/v1/certificate\n\nRetrieve a certificate by its ID.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/certificate?id=1\n```\n\n**Parameters**:\n\n* `id` is the Certificate ID\n* `sha256` the hexadecimal checksum of the DER certificate (only if `id` is not\n  provided)\n\n**Output**: a `json` document containing the parsed certificate and its raw X509 version encoded with base64.\n\n### POST /api/v1/certificate\n\nPublish a certificate.\n\n```bash\ncurl -X POST -F certificate=@example.pem https://tls-observatory.services.mozilla.com/api/v1/certificate\n```\n\n**Parameters**:\n\n* `certificate` is a POST multipart/form-data parameter that contains the PEM encoded certificate.\n\n**Output**: a `json` document containing the parsed certificate and its raw X509 version encoded with base64.\n\n**Caching**: Certificates are only stored once. The database uses the SHA256 hash of the DER (binary) certificate to identify duplicates. Posting a certificate already stored in database returns the stored version.\n\n### GET /api/v1/paths\n\nRetrieve the paths from a certificate to one of multiple roots.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/paths?id=1\n```\n\n**Parameters**:\n\n* `id` is the ID of the certificate to start the path at.\n* `sha256` the hexadecimal checksum of the DER certificate (only if `id` is not\n  provided)\n\n**Output**: a `json` document containing the paths document. Each entry in the path contains the current certificate and an array of parents, if any exist.\n\n### GET /api/v1/truststore\n\nRetrieve all the certificates in a given truststore.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/truststore?store=mozilla&format=pem\n```\n\n**Parameters**:\n\n* `store` is the store to retrieve certificates from. \"mozilla\", \"android\", \"apple\", \"microsoft\" and \"ubuntu\" are allowed.\n* `format`, either \"pem\" or \"json\".\n\n**Output**: if `format` is pem, a series of PEM-format certificates. If `format` is json, a json array of certificate objects, each with the same format of `/api/v1/certificate`.\n\n### GET /api/v1/issuereecount\n\nRetrieve the count of end-entity certificates that chain to the specified certificate. This is used to evaluate weight of a given issuer in the web pki.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/issuereecount?id=1\n```\n\n**Parameters**:\n\n* `id` is the ID of the certificate to start the path at.\n* `sha256` the hexadecimal checksum of the DER certificate (only if `id` is not\n  provided)\n\n**Output**: a `json` document containing the certificate itself under `issuer` and the count of end-entity certs under `eecount`.\n\n\n### GET /api/v1/__heartbeat__\n\nReturns a 200 OK.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/__heartbeat__\nI iz alive.\n```\n\n### GET /api/v1/__stats__\n\nReturns usage statistics in json (default) or text format.\n\nBy default, this endpoint returns stale data, refreshed the last time the\nendpoint was called, so it's possible to not have the latest available\nstatistics. Use the query parameter `details=full` to get the real-time stats,\nbut be aware that this is expensive and often times out.\n\n```bash\ncurl https://tls-observatory.services.mozilla.com/api/v1/__stats__?format=text&details=full\n\npending scans: 7\n\nlast 24 hours\n-------------\n- distinct targets: 21873\n- certs seen:       16459\n- certs added:      7886\n\nhourly scans\n------------\n2017-02-08T15:00:00Z    5\n2017-02-08T14:00:00Z    64\n2017-02-08T13:00:00Z    928\n2017-02-08T12:00:00Z    1969\n2017-02-08T11:00:00Z    1957\n2017-02-08T10:00:00Z    1982\n2017-02-08T09:00:00Z    2013\n2017-02-08T08:00:00Z    2031\n2017-02-08T07:00:00Z    2153\n2017-02-08T06:00:00Z    1860\n2017-02-08T05:00:00Z    1869\n2017-02-08T04:00:00Z    1944\n2017-02-08T03:00:00Z    1959\n2017-02-08T02:00:00Z    907\n2017-02-08T01:00:00Z    32\n2017-02-08T00:00:00Z    55\n2017-02-07T23:00:00Z    41\n2017-02-07T22:00:00Z    46\n2017-02-07T21:00:00Z    60\n2017-02-07T20:00:00Z    76\n2017-02-07T19:00:00Z    66\n2017-02-07T18:00:00Z    67\n2017-02-07T17:00:00Z    56\n```\n\n## Database Queries\n\n### Find certificates signed by CAs identified by their SHA256 fingerprint\n\n```sql\nSELECT certificates.id, certificates.subject, certificates.issuer\nFROM certificates INNER JOIN trust ON (certificates.id=trust.cert_id)\nWHERE trust.issuer_id in (\n    SELECT id FROM certificates\n    WHERE sha256_fingerprint IN (\n        'E7685634EFACF69ACE939A6B255B7B4FABEF42935B50A265ACB5CB6027E44E70',\n        'A4B6B3996FC2F306B3FD8681BD63413D8C5009CC4FA329C2CCF0E2FA1B140305'\n    ))\nAND certificates.is_ca='false';\n```\n\n### List signature algorithms of trusted certs\n\n```sql\nSELECT signature_algo, count(*)\nFROM certificates INNER JOIN trust ON (certificates.id=trust.cert_id)\nWHERE is_ca='false'\nAND trust.trusted_mozilla='true'\nGROUP BY signature_algo\nORDER BY count(*) DESC;\n```\n\n### Show expiration dates of trusted SHA-1 certificates\n\n```sql\nSELECT  extract('year' FROM date_trunc('year', not_valid_after)) as expiration_year,\n        extract('month' FROM date_trunc('month', not_valid_after)) as expiration_month,\n        count(*)\nFROM    certificates\n    INNER JOIN trust ON (certificates.id=trust.cert_id)\nWHERE is_ca='false'\n    AND trust.trusted_mozilla='true'\n    AND signature_algo='SHA1WithRSA'\nGROUP BY date_trunc('year', not_valid_after),\n         date_trunc('month', not_valid_after)\nORDER BY date_trunc('year', not_valid_after) ASC,\n         date_trunc('month', not_valid_after) ASC;\n```\n\n### Count trusted SHA-1 certs seen over the last month on TOP1M sites\n\n```sql\nSELECT distinct(certificates.id) as \"id\", cisco_umbrella_rank, domains, not_valid_before, not_valid_after, last_seen, signature_algo\nFROM certificates\n    INNER JOIN trust ON (certificates.id=trust.cert_id)\nWHERE is_ca='false'\n    AND trust.trusted_mozilla='true'\n    AND signature_algo='SHA1WithRSA'\n    AND cisco_umbrella_rank < 1000000\n    AND last_seen > NOW() - INTERVAL '1 month'\n    AND not_valid_after > NOW()\nORDER BY cisco_umbrella_rank ASC;\n```\n\n### List issuer, subject and SAN of Mozilla|Firefox certs not issued by Digicert\n\n```sql\nSELECT certificates.id,\n       issuer->'o'->>0 AS Issuer,\n       subject->>'cn' AS Subject,\n       san AS SubjectAltName\nFROM certificates\n      INNER JOIN trust ON (trust.cert_id=certificates.id),\n     jsonb_array_elements_text(x509_subjectAltName) AS san\nWHERE jsonb_typeof(x509_subjectAltName) != 'null'\n      AND ( subject#>>'{cn}' ~ '\\.(firefox|mozilla)\\.'\n            OR\n            san ~ '\\.(firefox|mozilla)\\.'\n          )\n      AND trust.trusted_mozilla='true'\n      AND certificates.not_valid_after>now()\n      AND cast(issuer#>>'{o}' AS text) NOT LIKE '%DigiCert Inc%'\nGROUP BY certificates.id, san\nORDER BY certificates.id ASC;\n```\n\n### Find count of targets that support the SEED-SHA ciphersuite\n\n```sql\nSELECT COUNT(DISTINCT(target))\nFROM scans, jsonb_array_elements(conn_info->'ciphersuite') as ciphersuites\nWHERE jsonb_typeof(conn_info) != 'null'\nAND ciphersuites->>'cipher'='SEED-SHA';\n```\n\n### Find intermediate CA certs whose root is trusted by Mozilla\n\n```sql\nSELECT id, subject\nFROM certificates\nWHERE is_ca=True\n  AND subject!=issuer\n  AND issuer IN (\n      SELECT subject\n      FROM certificates\n      WHERE in_mozilla_root_store=True\n  )\nGROUP BY subject, sha256_fingerprint;\n```\n\n### Find CA certs treated as EV in Firefox\n\nThe list is CA Certs that get EV treatment in Firefox can be [found here](https://dxr.mozilla.org/mozilla-central/source/security/certverifier/ExtendedValidation.cpp).\n\n```sql\nSELECT id, subject\nFROM certificates,\n     jsonb_array_elements_text(x509_certificatePolicies) AS cpol\nWHERE jsonb_typeof(x509_certificatePolicies) != 'null'\n  AND cpol IN ('1.2.392.200091.100.721.1','1.2.616.1.113527.2.5.1.1','1.3.159.1.17.1',\n               '1.3.6.1.4.1.13177.10.1.3.10','1.3.6.1.4.1.13769.666.666.666.1.500.9.1',\n               '1.3.6.1.4.1.14370.1.6','1.3.6.1.4.1.14777.6.1.1','1.3.6.1.4.1.14777.6.1.2',\n               '1.3.6.1.4.1.17326.10.14.2.1.2','1.3.6.1.4.1.17326.10.8.12.1.2',\n               '1.3.6.1.4.1.22234.2.14.3.11','1.3.6.1.4.1.22234.2.5.2.3.1',\n               '1.3.6.1.4.1.22234.3.5.3.1','1.3.6.1.4.1.22234.3.5.3.2','1.3.6.1.4.1.23223.1.1.1',\n               '1.3.6.1.4.1.29836.1.10','1.3.6.1.4.1.34697.2.1','1.3.6.1.4.1.34697.2.2',\n               '1.3.6.1.4.1.34697.2.3','1.3.6.1.4.1.34697.2.4','1.3.6.1.4.1.36305.2',\n               '1.3.6.1.4.1.40869.1.1.22.3','1.3.6.1.4.1.4146.1.1','1.3.6.1.4.1.4788.2.202.1',\n               '1.3.6.1.4.1.6334.1.100.1','1.3.6.1.4.1.6449.1.2.1.5.1','1.3.6.1.4.1.782.1.2.1.8.1',\n               '1.3.6.1.4.1.7879.13.24.1','1.3.6.1.4.1.8024.0.2.100.1.2','2.16.156.112554.3',\n               '2.16.528.1.1003.1.2.7','2.16.578.1.26.1.3.3','2.16.756.1.83.21.0',\n               '2.16.756.1.89.1.2.1.1','2.16.756.5.14.7.4.8','2.16.792.3.0.3.1.1.5',\n               '2.16.792.3.0.4.1.1.4','2.16.840.1.113733.1.7.23.6','2.16.840.1.113733.1.7.48.1',\n               '2.16.840.1.114028.10.1.2','2.16.840.1.114404.1.1.2.4.1','2.16.840.1.114412.2.1',\n               '2.16.840.1.114413.1.7.23.3','2.16.840.1.114414.1.7.23.3')\n  AND is_ca='true';\n```\n\n### Evaluate the quality of TLS configurations of top sites\n\nThis query uses the top1m ranking analyzer to retrieve the Mozilla evaluation of top sites.\n\n```sql\nobservatory=> SELECT COUNT(DISTINCT(target)), output->>'level' AS \"Mozilla Configuration\"\nFROM scans\n  INNER JOIN analysis ON (scans.id=analysis.scan_id)\nWHERE has_tls=true\n  AND target IN ( SELECT target\n                  FROM scans\n                  INNER JOIN analysis ON (scans.id=analysis.scan_id)\n                  WHERE worker_name='top1m'\n                    AND CAST(output->'target'->>'rank' AS INTEGER) < 10000\n                    AND timestamp > NOW() - INTERVAL '1 month')\n  AND worker_name='mozillaEvaluationWorker'\n  AND timestamp > NOW() - INTERVAL '1 month'\nGROUP BY has_tls, output->>'level'\nORDER BY COUNT(DISTINCT(target)) DESC;\n\n count | Mozilla Configuration\n-------+-----------------------\n  3689 | intermediate\n  1906 | non compliant\n  1570 | bad\n    15 | old\n(4 rows)\n\n```\n\n### Count Top 1M sites that support RC4\n```sql\nSELECT COUNT(DISTINCT(target))\nFROM scans,\n     jsonb_array_elements(conn_info->'ciphersuite') as ciphersuites\nWHERE jsonb_typeof(conn_info) = 'object'\n  AND jsonb_typeof(conn_info->'ciphersuite') = 'array'\n  AND ciphersuites->>'cipher' LIKE 'RC4-%'\n  AND target IN ( SELECT target\n                  FROM scans\n                       INNER JOIN analysis ON (scans.id=analysis.scan_id)\n                  WHERE worker_name='top1m'\n                    AND CAST(output->'target'->>'rank' AS INTEGER) < 1000000\n                    AND timestamp > NOW() - INTERVAL '1 month')\n  AND timestamp > NOW() - INTERVAL '1 month';\n  ```\n\n### Count Top 1M sites that support TLSv1.2\n```sql\nSELECT ciphersuites->'protocols' @> '[\"TLSv1.2\"]'::jsonb AS \"Support TLS 1.2\", COUNT(DISTINCT(target))\nFROM scans,\n     jsonb_array_elements(conn_info->'ciphersuite') as ciphersuites\nWHERE jsonb_typeof(conn_info) = 'object'\n  AND jsonb_typeof(conn_info->'ciphersuite') = 'array'\n  AND target IN ( SELECT target\n                  FROM scans\n                       INNER JOIN analysis ON (scans.id=analysis.scan_id)\n                  WHERE worker_name='top1m'\n                    AND CAST(output->'target'->>'rank' AS INTEGER) < 1000000\n                    AND timestamp > NOW() - INTERVAL '1 month')\n  AND timestamp > NOW() - INTERVAL '1 month'\nGROUP BY ciphersuites->'protocols' @> '[\"TLSv1.2\"]'::jsonb;\n```\n\n### Count end-entity certificates by issuer organizations\n```sql\nSELECT COUNT(*), issuer#>'{o}'->>0\nFROM certificates\n  INNER JOIN trust ON (certificates.id=trust.cert_id)\nWHERE certificates.is_ca = false\n  AND trust.trusted_mozilla=true\n  AND trust.is_current = true\nGROUP BY issuer#>'{o}'->>0\nORDER BY count(*) DESC;\n```\n\n### Count sites in the top 10k that are impacted by the Symantec distrust in Firefox 60\nnote: in Firefox 63, the not_valid_before condition will be removed\n```sql\nSELECT COUNT(DISTINCT(target))\nFROM scans\n  INNER JOIN analysis ON (scans.id=analysis.scan_id)\n  INNER JOIN certificates ON (scans.cert_id=certificates.id)\nWHERE has_tls=true\n  AND target IN ( SELECT target\n                  FROM scans\n                  INNER JOIN analysis ON (scans.id=analysis.scan_id)\n                  WHERE worker_name='top1m'\n                    AND CAST(output->'target'->>'rank' AS INTEGER) < 10000\n                    AND timestamp > NOW() - INTERVAL '1 week')\n  AND worker_name='symantecDistrust'\n  AND timestamp > NOW() - INTERVAL '1 week'\n  AND not_valid_before < '2016-06-01'\nGROUP BY has_tls, output->>'isDistrusted'\nORDER BY COUNT(DISTINCT(target)) DESC;\n```\n\n## Contributing\n\nWe're always happy to help new contributors. You can find us in `#observatory` on `irc.mozilla.org` ([Mozilla Wiki](https://wiki.mozilla.org/IRC)).\n\n### Dependencies\n\nWe currently vendor dependencies in `vendor/`.\n\nUsing a golang version with [`go mod`](https://golang.org/ref/mod#mod-commands),run `make vendor` update vendored dependencies.\n\n## Contributors\n\n * Julien Vehent\n * Dimitris Bachtis (original dev)\n * Adrian Utrilla\n\n## License\n\n * Mozilla Public License Version 2.0\n"
},
{
  "name": "fxemoji",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE.md",
      "README.md",
      "bin",
      "dist",
      "gulpfile.js",
      "index.html",
      "package.json",
      "run.sh",
      "svgs",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "#FxEmojis\nhttp://mozilla.github.io/fxemoji/\n\nFormerly the Firefox OS emoji set, now just FxEmojis. \n\n##Requirements\n\n* bash\n* TTX/FontTools - please use https://github.com/behdad/fonttools/\n* python\n  - lxml (pip install lxml)\n\n## Installation\n```\n$ git clone https://github.com/mozilla/fxemoji.git\n$ cd fxemoji/\n$ npm install\n```\n\n## Quick start\nChange permissions of `run.sh` with `chmod +x run.sh`\n```\n$./run.sh\n```\n\n## Prepare SVG's\nWe need few SVG's per icon.\n  - SVG file for each color layer\n  - SVG that contains the complete glyph.\n\nThe files need to be named with uni code prefix.\n\n###First Icon :\n- u1F60A-smileeyes.svg (complete glyph)\n- u1F60A.layer1.svg (Layer 1)\n- u1F60A.layer2.svg (Layer 2)\n- etc.\n\n###Second Icon :\n- u1F60B-smiletongue.svg (complete glyph)\n- u1F60B.layer1.svg (Layer 1)\n- u1F60B.layer2.svg (Layer 2)\n- etc.\n\n##How to Contribute\n\nPlease contact Mike Hoye (mhoye@mozilla.com) for contribution information; more info TBD. \n\n"
},
{
  "name": "reflex",
  "files": {
    "/": [
      ".babelrc",
      ".flowconfig",
      ".gitignore",
      ".npmignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "History.md",
      "License.md",
      "Readme.md",
      "package.json",
      "src",
      "test",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "nestegg",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "AUTHORS",
      "INSTALL",
      "LICENSE",
      "Makefile.am",
      "README.md",
      "TODO",
      "configure.ac",
      "docs",
      "include",
      "m4",
      "nestegg-uninstalled.pc.in",
      "nestegg.pc.in",
      "src",
      "test"
    ],
    "/docs": [
      "Doxyfile.in",
      "Makefile.am"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://github.com/mozilla/nestegg/actions/workflows/build.yml/badge.svg)](https://github.com/mozilla/nestegg/actions/workflows/build.yml)\n\nSee INSTALL for build instructions.\n\nLicensed under an ISC-style license.  See LICENSE for details.\n"
},
{
  "name": "discourse-expose-emails-in",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assets",
      "config",
      "plugin.rb"
    ]
  },
  "makefile": null,
  "readme": "# expose-emails-in\n*Discourse plugin which exposes category emails-in in useful ways*\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Installation\n\nFollow the Discourse [Install a Plugin](https://meta.discourse.org/t/install-a-plugin/19157) guide.\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "mozilla-reports",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "addons",
      "bug1381516.kp",
      "e10s_analyses",
      "etl",
      "examples",
      "projects",
      "script",
      "static",
      "testpilot",
      "tutorials"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Reports\n\nThis repository contains public data analyses.\n\nYou can find the rendered reports at:\n\n- https://mozilla.report\n\n# Adding a new report\n\nFirst, fork this repository and clone the fork locally.\nBe sure to set the `upstream` remote:\n\n```sh\ngit remote add upstream git@github.com:mozilla/mozilla-reports.git\n```\n\n## Copy your report\nOnce you have a local copy of the repository,\ncopy your report to a new directory in this repository.\nFor example, if you have an analysis in `~/Documents/user_count/`:\n\n```sh\ncd mozilla-private-reports\ncp -r ~/Documents/user_count ./user_count\n```\nYour report is **copied to the rendered site without any modifications**.\nBe sure that your analysis directory\ncontains a fully rendered copy of your report\n\nThe entire directory is included in the rendered site,\nso your report can be comprised of multiple pages\n\n## Add metadata\n\nAdd a `report.json` file to your new directory.\nThis registers your index.html file as a report\nand provides metadata necessary for generating the index pages.\n\nThe required fields are documented in [`docere`'s documentation](docere):\nHere's a minimal example:\n\n```json\n{\n  \"title\": \"User Count Report\",\n  \"publish_date\": \"2018-03-01\",\n  \"author\": \"Wadley Hickam\"\n}\n```\n\nBy default, [docere] looks links to an `index.html` file in your report directory.\nYou can override this by specifying a \"file\" key to `report.json`\n\n## Preview your changes\n\nWe use [docere] to generate metadata pages for these reports.\n\nMake sure you have a python environment with docere installed:\n\n```bash\npython3 -m venv venv\nsource venv/bin/active\npip install docere\n```\n\nAnd then you can preview content by invoking:\n\n```bash\n./script/render.sh\nfirefox site/index.html\n```\n\n## Publish\n\nTo publish your report, create a pull request and get review.\nOnce your report is merged to main,\nit will automatically be pushed to [dev RTMO]\n\nFor example, using [GitHub's CLI, `hub`](https://github.com/github/hub):\n\n```sh\ngit add .\ngit checkout -b user_count_report\ngit commit -am \"Add user count report\"\ngit push -u origin user_count_report\nhub pull-request\n```\n\n[docere]: https://github.com/harterrt/docere\n"
},
{
  "name": "mozdownload",
  "files": {
    "/": [
      ".appveyor.yml",
      ".github",
      ".gitignore",
      ".pyup.yml",
      ".travis.yml",
      ".travis",
      "CODE_OF_CONDUCT.md",
      "History.md",
      "LICENSE.txt",
      "MANIFEST.in",
      "README.md",
      "mozdownload",
      "pytest.ini",
      "requirements.txt",
      "requirements",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/.github": [
      "CODEOWNERS",
      "CONTRIBUTING.md"
    ]
  },
  "makefile": null,
  "readme": "# mozdownload\n\n[mozdownload](https://github.com/mozilla/mozdownload)\nis a [python package](http://pypi.python.org/pypi/mozdownload)\nwhich handles downloading of Mozilla applications.\n\n[![Travis](https://travis-ci.org/mozilla/mozdownload.svg?branch=master)](https://travis-ci.org/mozilla/mozdownload)\n[![Coverage](https://coveralls.io/repos/github/mozilla/mozdownload/badge.svg)](https://coveralls.io/github/mozilla/mozdownload)\n[![Issues](https://img.shields.io/github/issues/mozilla/mozdownload.svg)](https://github.com/mozilla/mozdownload/issues)\n[![pyup.io](https://pyup.io/repos/github/mozilla/mozdownload/shield.svg)](https://pyup.io/repos/github/mozilla/mozdownload/)\n\n## Installation\n\nIf the tool should only be used for downloading applications we propose to\ninstall it via pip. The following command will install the latest release:\n```bash\npip install mozdownload\n```\n\nOtherwise follow the steps below to setup a development environment. It is\nrecommended that [virtualenv](http://virtualenv.readthedocs.org/en/latest/installation.html)\nand [virtualenvwrapper](http://virtualenvwrapper.readthedocs.org/en/latest/)\nbe used in conjunction with mozdownload. Start by installing these,\nand [creating a virtualenv for the project](https://docs.python-guide.org/dev/virtualenvs/#lower-level-virtualenv).\nThen fork our repository into your own github account, and run:\n```bash\ngit clone https://github.com/%your_account%/mozdownload.git\ncd mozdownload\npython setup.py develop\n```\n\n## Command Line Usage\n\nThe `mozdownload` command will download the application based on the provided\ncommand line options.\n\n### Examples\n\nDownload the latest official Firefox release for your platform:\n```bash\nmozdownload --version=latest\n```\n\nDownload the latest official Firefox beta release for your platform:\n```bash\nmozdownload --version=latest-beta\n```\n\nDownload the latest official Firefox esr release for your platform:\n```bash\nmozdownload --version=latest-esr\n```\n\nDownload the latest Firefox release candidate for your platform:\n```bash\nmozdownload --type candidate --version=latest\n```\n\nDownload the latest Firefox Aurora build for Windows (32bit):\n```bash\nmozdownload --type=daily --branch=mozilla-aurora --platform=win32\n```\n\nDownload the latest official Thunderbird release for your platform:\n```bash\nmozdownload --application=thunderbird --version=latest\n```\n\nDownload the latest Earlybird build for Linux (64bit):\n```bash\nmozdownload --application=thunderbird --type=daily --branch=comm-aurora --platform=linux64\n```\n\nDownload this README file:\n```bash\nmozdownload --url=https://raw.github.com/mozilla/mozdownload/master/README.md\n```\n\nDownload a file from a URL protected with basic authentication:\n```bash\nmozdownload --url=http://example.com/secrets.txt --username=admin --password=password\n```\n\nRun `mozdownload --help` for detailed information on the command line options.\n\n### Command Line Options\n\nTo see the full list of command line options, execute the command below and check the list\nof options for the build type to download:\n```bash\nmozdownload --help\n```\n\n## API\n\nBeside the CLI mozdownload also offers an API to be used. To create specific instances of scrapers\nthe FactoryScraper class can be used. Here some examples:\n```python\n# Create a release scraper for the German locale of Firefox 40.0.3\nfrom mozdownload import FactoryScraper\nscraper = FactoryScraper('release', version='40.0.3', locale='de')\n\n# Create a candidate scraper for Windows 32bit of Firefox 41.0b9\nfrom mozdownload import FactoryScraper\nscraper = FactoryScraper('candidate', version='41.0b9', platform='win32')\n\n# Create a daily scraper for the latest Dev Edition build on the current platform\nfrom mozdownload import FactoryScraper\nscraper = FactoryScraper('daily', branch='mozilla-aurora')\n```\n\nAll those scraper instances allow you to retrieve the url which is used to download the files, and the filename for the local destination:\n```python\nfrom mozdownload import FactoryScraper\nscraper = FactoryScraper('daily')\nprint scraper.url\nprint scraper.filename\n```\n\nTo actually download the remote file the download() method has to be called:\n```python\nfrom mozdownload import FactoryScraper\nscraper = FactoryScraper('daily')\nfilename = scraper.download()\n```\n\n## Testing\n\nTo run the entire test suite to check if your changes create any errors, run `tox`.\n\nIf you only run very specific tests, please specify it via `tox -- -k <keyword>`.\nFor example, if you are only interested in tests that look at tinderbox builds, run `tox -- -k tinderbox`.\nThe `-k <keyword>` works for folders, filenames and even names of test methods.\n"
},
{
  "name": "msix-packaging",
  "files": {
    "/": [
      ".gitattributes",
      ".github",
      ".gitignore",
      "CMakeLists.txt",
      "CMakeSettings.json",
      "CODEOWNERS",
      "LICENSE",
      "Microsoft.MSIX.Packaging.targets",
      "MsixCore",
      "Package.nuspec.cmakein",
      "ProjectIcon.png",
      "README.md",
      "SUPPORT.md",
      "THIRD PARTY CODE NOTICE",
      "cmake",
      "lib",
      "makeaosp.sh",
      "makeaosponwinx86.cmd",
      "makeios.sh",
      "makelinux.sh",
      "makemac.sh",
      "makewin.cmd",
      "manifest.cmakein",
      "pipelines",
      "release_master.cmd",
      "release_master.ps1",
      "resources",
      "sample",
      "src",
      "tdf-guidance.md",
      "tools"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "# MSIX SDK \r\n   Copyright (c) 2019 Microsoft Corp.  All rights reserved.\r\n\r\n## Description\r\n   The MSIX SDK project is an effort to enable developers on a variety of platforms to pack and unpack\r\n   packages for the purposes of distribution from either the Microsoft Store, or their own content distribution networks.\r\n    \r\n   The MSIX Packaging APIs that a client app would use to interact with .msix/.appx packages are a subset of those\r\n   documented [here](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446766(v=vs.85).aspx).\r\n\r\n## Overview\r\nThe MSIX SDK project includes cross platform API support for packing and unpacking of .msix/.appx packages\r\n\r\n|                                      |                                 |\r\n|--------------------------------------|---------------------------------|\r\n| **msix**      | A shared library (DLL on Win32, dylib on macOS, SO on Linux and Android) that exports a subset of the functionality contained within appxpackaging.dll on Windows. See [here](https://msdn.microsoft.com/en-us/library/windows/desktop/hh446766(v=vs.85).aspx) for additional details.<br />On all platforms instead of CoCreating IAppxFactory, a C-style export: CoCreateAppxFactory is provided. Similarly, the CoCreateAppxBundleFactory export is equivalent as CoCreating IAppxBundleFactory.<br /><br /> The 'UnpackPackage' and 'UnpackBundle' exports that provide a simplified unpackage implementation. Similarly, PackPackage provides a simplified package implementation. See the [samples directory](sample) for usage of the SDK.|\r\n| **makemsix**  | A command line wrapper over the MSIX library entrypoints. makemsix supports pack and unpack. Use the -? to get information about the options supported.|\r\n| **MSIX Core** | A client app that uses installs .msix/.appx packages on Windows 7 SP1 and later versions of Windows. Go to the [MSIX Core project](MsixCore/README.md) page, to get more details.|\r\n\r\nGuidance on how to package your app contents and construct your app manifest such that it can take advantage of the cross platform support of this SDK is [here](tdf-guidance.md).\r\n\r\n## Release Notes\r\nRelease notes on the latest features and performance improvements made to the SDK are listed [here](https://docs.microsoft.com/en-us/windows/msix/msix-sdk/release-notes/sdk-release-notes-1.7)\r\n\r\n## Setup Instructions\r\n1. Clone the repository:\r\n        ```git clone [URL]```\r\n\r\n## Issues\r\nIf you are using Visual Studio 2017 and you run into errors about not being able to find the v140 toolset:\r\n1. Install the Microsoft Build Tools (https://chocolatey.org/packages/microsoft-build-tools)\r\n2. Start -> visual studio installer -> Visual Studio Build Tools 2017 -> Modify the 2014 toolset -> individual components \r\n3. Make sure that VC++ 2015.3 v140 toolset for desktop is selected and then unselect VC++ 2017 141 toolset\r\n4. Close, then re-open the solution.\r\n\r\n## Dependencies\r\nDepending on the platform for which the MSIX shared library (MSIX.DLL | libmsix.dylib | libmsix.so) is compiled, one or \r\nmore of the following dependencies may be statically linked into the binary:\r\n\r\n* [ZLib Tag v1.2.11 Commit cacf7f1d4e3d44d871b605da3b647f07d718623f](https://github.com/madler/zlib/releases/tag/v1.2.11)\r\n* [Xerces-C Tag Xerces-C_3_2_0 Commit dffc3028df8ea44985c92f2df28115860e39e344](https://github.com/apache/xerces-c/releases/tag/Xerces-C_3_2_0)\r\n* [OpenSSL Tag OpenSSL_1_0_2q Commit 5707219a6aae8052cb98aa361d115be01b8fd894](https://github.com/openssl/openssl/releases/tag/OpenSSL_1_0_2q)\r\n* [Android NDK](https://developer.android.com/ndk)\r\n\r\nFor convinience, Zlib, Xerces-C and OpenSSL are git-subtrees that are mapped in under the lib folder of this project.  Edits on top of these subtrees for build related optimizations are tracked within this repository.\r\n\r\nThe Android NDK is only required for targeting the Android platform.\r\n\r\n## Prerequisites\r\nThe project uses git-lfs to store some large binary test files. You can find out more and install it from here:\r\n\r\nhttps://git-lfs.github.com/\r\n\r\nMake sure that you have CMAKE installed on your machine \r\n\r\n   * https://cmake.org/download/\r\n\r\nOne or more of the following prerequisites may also be required on your machine:\r\n\r\n##### Ninja-build:\r\nhttps://github.com/ninja-build/ninja/releases\r\n\r\n##### Android NDK:\r\nhttps://developer.android.com/ndk/downloads/index.html\r\n\r\n##### Clang/LLVM:\r\nhttp://releases.llvm.org/download.html\r\n    \r\n##### VS 2017 clients:\r\nOpen Visual Studio 2017\r\nFile->Open Folder->navigate to project root and select \"CMakeLists.txt\"\r\n\r\nSee [cmake-support-vs](https://blogs.msdn.microsoft.com/vcblog/2016/10/05/cmake-support-in-visual-studio/) for details regarding how to configure your environment.\r\n\r\n##### Xcode clients: \r\n\r\nopen terminal, from project root:\r\nmkdir build && cd build && cmake -DMACOS=on -G\"Xcode\" ..\r\nopen xcode\r\nFile->Open->navigate to project root/build and select \"Project.xcodeproj\"\r\n\r\nSee [cmake-Xcode-integration](https://www.johnlamp.net/cmake-tutorial-2-ide-integration.html#section-Xcode) for additional details\r\n\r\n## Build\r\n### On Windows using Visual Studio nmake:\r\n```\r\n   makewin.cmd <x86|x64> [options]\r\n```\r\n   This will start MSVC environment calling vcvarsall.bat <arch>, clean the output directory, call cmake and nmake. The latest Visual Studio version is obtained by calling vswhere.exe \r\n\r\n\r\n### On Mac using make:\r\n```\r\n   ./makemac [options]\r\n   ./makeios [options]\r\n```\r\n\r\n### On Linux using make:\r\n```\r\n   ./makelinux [options]\r\n   ./makeaosp [options]\r\n```\r\n\r\n### How to compile for Android on Windows:\r\n\r\n- Unpack the latest Android NDK to c:\\android-ndk\r\n- Unpack Ninja-build to c:\\ninja\r\n- Add c:\\ninja to the path environment variable\r\n- Create a folder under the root of the enlistment called \"android\", cd into that folder, then run the following command to create ninja build files:\r\n```\r\n    cmake -DCMAKE_ANDROID_NDK=c:/android-ndk ^\r\n        -DCMAKE_ANDROID_NDK_TOOLCHAIN_VERSION=clang ^\r\n        -DCMAKE_SYSTEM_NAME=Android ^\r\n        -DCMAKE_SYSTEM_VERSION=19 ^\r\n        -DCMAKE_ANDROID_ARCH_ABI=x86 ^\r\n        -DCMAKE_ANDROID_STL_TYPE=c++_shared ^\r\n        -DCMAKE_BUILD_TYPE=Release ^\r\n        -DAOSP=on ^\r\n        -G\"Ninja\" ..\r\n```\r\nTo compile, run the following command from the android folder:\r\n```\r\n    ninja\r\n```\r\n\r\n### Enable pack features\r\n\r\n   By default, pack is *NOT* turned on in the build scripts and is not supported for mobile devices. Use the --pack option in the build scripts or pass -DMSIX_PACK=on to the CMake command to enable it. You will have to set also -DUSE_VALIDATION_PARSE=on in the build script, otherwise the build operation will fail.\r\n  \r\n## Build Status\r\nThe following native platforms are in development now:\r\n\r\n### Windows\r\n||master|\r\n|---|---|\r\n**Debug x32**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20debug_32_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Debug x64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20debug_64_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x32**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_32_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_64_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x32 Without Bundle support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_32_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x64 Without Bundle support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_64_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x32 With Validation Parser**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_32_validation_parser)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x64 With Validation Parser**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_64_validation_parser)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Debug x32 With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20debug_32_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Debug x64 With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20debug_64_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x32 With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_32_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x64 With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_64_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x32 Xerces With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_32_xerces)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n**Release x64 Xerces With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Windows%20CI?branchName=master&jobName=Windows&configuration=Windows%20release_64_xerces)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=64&branchName=master)|\r\n\r\nBuilt in the Azure Pipelines windows-latest pool. See specifications [here](https://github.com/actions/virtual-environments/blob/main/images/win/Windows2019-Readme.md)\r\n\r\n### macOS\r\n||master|\r\n|---|---|\r\n**Debug**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20debug_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20release_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release Without Bundle support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20release_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Debug With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20debug_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20release_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Debug arm64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20debug_nopack_arm64)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release arm64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20release_nopack_arm64)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release Without Bundle support arm64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20release_nobundle_arm64)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Debug With Pack arm64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20debug_pack_arm64)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release With Pack arm64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS%20release_pack_arm64)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release Universal**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS_universal_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release Without Bundle support Universal**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macOS_universal_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n**Release With Pack Universal**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20macOS%20CI?branchName=master&jobName=macOS&configuration=macO_universal_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=69&branchName=master)|\r\n\r\nBuilt in the Azure Pipelines macOS pool. See specification [here](https://github.com/actions/virtual-environments/blob/main/images/macos/macos-10.15-Readme.md)\r\n\r\n### iOS\r\n||master|\r\n|---|---|\r\n**Debug Emulator**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20iOS%20CI?branchName=master&jobName=iOS&configuration=iOS%20debug_x86)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=74&branchName=master)|\r\n**Release Emulator**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20iOS%20CI?branchName=master&jobName=iOS&configuration=iOS%20release_x86)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=74&branchName=master)|\r\n**Release Emulator Without Bundle support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20iOS%20CI?branchName=master&jobName=iOS&configuration=iOS%20release_x86_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=74&branchName=master)\r\n**Release arm64**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20iOS%20CI?branchName=master&jobName=iOS&configuration=iOS%20release_arm64)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=74&branchName=master)\r\n\r\nBuilt in the Azure Pipelines macOS pool. See specification [here](https://github.com/Microsoft/azure-pipelines-image-generation/blob/master/images/macos/macos-10.14-Readme.md)\r\n\r\n### Android\r\n||master|\r\n|---|---|\r\n**Debug Emulator**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20aosp%20CI?branchName=master&jobName=AOSP&configuration=AOSP%20debug_emulator)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=76&branchName=master)|\r\n**Release Emulator**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20aosp%20CI?branchName=master&jobName=AOSP&configuration=AOSP%20release_emulator)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=76&branchName=master)|\r\n**Release Emulator Without Bundle support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20aosp%20CI?branchName=master&jobName=AOSP&configuration=AOSP%20release_emulator_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=76&branchName=master)|\r\n**Release arm**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20aosp%20CI?branchName=master&jobName=AOSP&configuration=AOSP%20release_arm)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=76&branchName=master)|\r\n**Release arm Without Bundle support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20aosp%20CI?branchName=master&jobName=AOSP&configuration=AOSP%20release_arm_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=76&branchName=master)|\r\n\r\nBuilt in the Azure Pipelines macOS pool. See specification [here](https://github.com/Microsoft/azure-pipelines-image-generation/blob/master/images/macos/macos-10.14-Readme.md)\r\n\r\n### Linux\r\n||master|\r\n|---|---|\r\n**Debug**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Linux%20CI?branchName=master&jobName=Linux&configuration=Linux%20debug_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=72&branchName=master)|\r\n**Release**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Linux%20CI?branchName=master&jobName=Linux&configuration=Linux%20release_nopack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=72&branchName=master)|\r\n**Release Without Bundle Support**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Linux%20CI?branchName=master&jobName=Linux&configuration=Linux%20release_nobundle)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=72&branchName=master)|\r\n**Release With Validation Parser**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Linux%20CI?branchName=master&jobName=Linux&configuration=Linux%20release_validation_parser)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=72&branchName=master)|\r\n**Debug With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Linux%20CI?branchName=master&jobName=Linux&configuration=Linux%20debug_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=72&branchName=master)|\r\n**Release With Pack**|[![Build Status](https://dev.azure.com/ms/msix-packaging/_apis/build/status/msix-packaging%20Linux%20CI?branchName=master&jobName=Linux&configuration=Linux%20release_pack)](https://dev.azure.com/ms/msix-packaging/_build/latest?definitionId=72&branchName=master)|\r\n\r\nBuilt in the Azure Pipelines Hosted Ubuntu 1604. See specification [here](https://github.com/Microsoft/azure-pipelines-image-generation/blob/master/images/linux/Ubuntu1604-README.md)\r\n\r\n## Windows 7 support\r\nThe MSIX SDK is fully supported and tested on Windows 7. However, an Application Manifest **_MUST_**  be included to any executable that is expected to run on Windows 7 and uses msix.dll. Specifically, the Application Manifest **_MUST_**  include the supportedOS flags for Windows 7. The manifest is not included on msix.dll because the compat manifest doesn't matter on DLLs.\r\nSee the [manifest](manifest.cmakein) that is used for makemsix and samples of this project as example. The Windows 7 machine might also require the [Microsoft Visual C++ Redistributable](https://www.visualstudio.com/downloads/) binaries installed to run properly. Alternatively, build msix.dll with makewin.cmd <x86|x64> -mt [options] to use static version of the runtime library and don't require the redistributables.\r\n\r\n## Android support\r\nThe MSIX SDK minimum supported for Android is API Level 19.\r\n\r\nWe also produce msix-jni.jar which acts as a helper to get the languages from the Android device. Because of it, we expect either a -DANDROID_SDK and -DANDROID_SDK_VERSION on the cmake command and, if not present, we default to $ANDROID_HOME and 24 respectively.\r\nThe default level for the SDK level is 24 because we use the [Configuration class](https://developer.android.com/reference/android/content/res/Configuration) and, depending on the version of the device, we either use the locale attribute (deprecated as of API level 24) or getLocales.\r\nWe recommend using the [makeaosp](makeaosp) script to build for Android on non-Windows devices.\r\n\r\n## Apple Silicon\r\nTo enable building the MSIX SDK to run on Apple Silicon do the following:\r\n1. Install Xcode beta 12 (https://developer.apple.com/download/)\r\n2. Change active developer directory `sudo xcode-select -switch /Applications/Xcode-beta.app/Contents/Developer`\r\n3. Build using makemac.sh `./makemac.sh -arch arm64 --skip-tests`\r\n\r\n## Testing\r\nmsixtest uses Catch2 as testing framework. msixtest is either an executable or a shared library, depending on the platform. It has a single entrypoint msixtest_main that takes argc and argv, as main, plus the path were the test packages are located. The shared library is used for our mobile test apps, while non-mobile just forwards the arguments to msixtest_main. It requires msix.dll to be build with \"Release\" or \"RelWithDebInfo\" CMake switch. \r\n\r\nFirst build the project, then:\r\n\r\n### Testing for non-mobile devices:\r\nGo to the build directory and run msixtes\\msixtest.exe. You can run an specific test by running msixtest [test name]. By default, the test will only output the failling tests, use -s to output successfull tests.\r\n\r\n### Testing on mobile devices:\r\n#### iOS\r\nFirst build the project for iOS, then launch xCode and load src/test/mobile/iOSBVT.xcworkspace, compile the test app, and then launch the iPhone simulator. You can also run \"testios.sh -p iOSBVT/iOSBVT.xcodeproj\" from src/test/mobile. \r\n\r\n#### Android:\r\nFrom within bash, navigate to src/test/mobile, and run \"./testaosponmac.sh\".\r\n\r\n## Releasing\r\nIf you are the current maintainer of this project:\r\n\r\n  1. Pull latest payload to release in master\r\n  2. Confirm that all platforms/architectures/flavors build and all BVTs pass\r\n  3. From a windows cmd prompt: release_master.cmd\r\n  4. Confirm that new branch called \"release_v1.xxx\" where \"xxx\" is the next incremental version is created\r\n  \r\n## Contributing\r\nThis project welcomes contributions and suggestions. Most contributions require you to\r\nagree to a Contributor License Agreement (CLA) declaring that you have the right to,\r\nand actually do, grant us the rights to use your contribution. For details, visit\r\nhttps://cla.microsoft.com.\r\n\r\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need\r\nto provide a CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the\r\ninstructions provided by the bot. You will only need to do this once across all repositories \r\nusing our CLA.\r\n\r\nIf you have any questions or comments, you can send them [our team](mailto:MSIXPackagingOSSCustomerQs@service.microsoft.com) directly! \r\n\r\nThis project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).\r\nFor more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)\r\nor contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional \r\nquestions or comments.\r\n\r\n## Report a Computer Security Vulnerability\r\nIf you are a security researcher and believe you have found a security vulnerability that meets\r\nthe [definition of a security vulnerability](https://technet.microsoft.com/library/cc751383.aspx) that is not resolved by the [10 Immutable Laws of Security](https://technet.microsoft.com/library/cc722487.aspx),\r\nplease send e-mail to us at secure@microsoft.com. To help us to better understand the nature and\r\nscope of the possible issue, please include as much of the below information as possible.\r\n\r\n  * Type of issue (buffer overflow, SQL injection, cross-site scripting, etc.)\r\n  * Product and version that contains the bug, or URL if for an online service\r\n  * Service packs, security updates, or other updates for the product you have installed\r\n  * Any special configuration required to reproduce the issue\r\n  * Step-by-step instructions to reproduce the issue on a fresh install\r\n  * Proof-of-concept or exploit code\r\n  * Impact of the issue, including how an attacker could exploit the issue\r\n\r\nMicrosoft follows [Coordinated Vulnerability Disclosure](https://technet.microsoft.com/security/dn467923.aspx) (CVD) and, to protect the ecosystem, we \r\nrequest that those reporting to us do the same.  To encrypt your message to our PGP key, please\r\ndownload it from the [Microsoft Security Response Center PGP Key](https://aka.ms/msrcpgp). You should receive a response\r\nwithin 24 hours. If for some reason you do not, please follow up with us to ensure we received\r\nyour original message. For further information, please visit the Microsoft Security Response \r\nPolicy and Practices page and read the [Acknowledgment Policy for Microsoft Security Bulletins](https://www.microsoft.com/technet/security/bulletin/policy.mspx).\r\n\r\nFor additional details, see [Report a Computer Security Vulnerability](https://technet.microsoft.com/en-us/security/ff852094.aspx) on Technet\r\n"
},
{
  "name": "feature_table",
  "files": {
    "/": [
      "query.sql"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "browserid-crypto",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "ChangeLog",
      "LICENSE",
      "README.md",
      "bin",
      "bundle-prelim.js",
      "bundle.js",
      "bundle.sh",
      "index.js",
      "lib",
      "libs",
      "package.json",
      "scripts",
      "test-timing.js",
      "test.html",
      "test",
      "test2.html"
    ]
  },
  "makefile": null,
  "readme": "JavaScript implementation of JSON Web Signatures and JSON Web Tokens as needed by BrowserID.\n\n[![Build Status](https://secure.travis-ci.org/mozilla/browserid-crypto.png)](http://travis-ci.org/mozilla/browserid-crypto)\n\n- libs contains third-party libraries that need to be included. See\nlibs/dependencies.txt and libs/package.txt\n\n- This is written as CommonJS modules for node and\n  such. Browserify is used to bundle it all up.\n\nNOTE: this is written as future documentation of v0.2 APIs, which will not\nbe backwards compatible with v0.1.\n\nOverview\n===\n\nJSON Web Tokens (JWTs) look like:\n\n    eyJ0eXAiOiJKV1QiLA0KICJhbGciOiJIUzI1NiJ9\n    .\n    eyJpc3MiOiJqb2UiLA0KICJleHAiOjEzMDA4MTkzODAsDQogImh0dHA6Ly9leGFt\n    cGxlLmNvbS9pc19yb290Ijp0cnVlfQ\n    .\n    dBjftJeZ4CVP-mB92K27uhbUJU1p1r_wW1gFWFOEjXk\n\n(line breaks are for readability)\n\nJWTs are made up of three components, each base64url-encoded, joined by a period character. A JWT can be either a JWS (JSON Web Signature) or a JWE (JSON Web Encryption). In this library, we only consider JWS. Because JWT is effectively the abstract superclass of both JWS and JWE, we don't expose JWT APIs directly (as of v0.2.0). We simply expose a JWS API.\n\nWe use JWK (JSON Web Keys) to specify keys:\nhttp://tools.ietf.org/html/draft-ietf-jose-json-web-key-00\n\nWe use JWA (JSON Web Algorithms) to specify algorithms:\nhttp://tools.ietf.org/html/draft-ietf-jose-json-web-algorithms-00\n(we add algorithm \"DSA\" to indicate DSA, with DS160 the standard DSA 1024/160.)\n\nUsage\n=====\n\n  1. for Node 4+ ensure that you are using g++ 4.8 (use `CXX=g++-4.8` to force that version)\n  2. npm install browserid-crypto\n  3. in javascript: `require('browserid-crypto')`\n\nBasic API\n=========\n```javascript\nvar jwcrypto = require(\"browserid-crypto\");\nrequire(\"browserid-crypto/lib/algs/ds\");\n\n// random number generation is taken care of automatically\n// with auto-seeding that is optimized for server or browser\n// setup\n\n// more entropy can be added as follows\n// this can be useful to incorporate server-provided entropy\n// on clients that don't have any good entropy of their own\n// entropy should be either a 32 bit int, an array of ints, or a string\njwcrypto.addEntropy('entropy');\n\n// generate a key\n// we use DSA, which is \"DS\" in JSON Web Algorithm parlance\n// we use keysize 160, which has a specific interpretation based\n// on the algorithm, in this case DSA 1024/160, standard DSA.\njwcrypto.generateKeypair({\n    algorithm: 'DSA',\n    keysize: 160\n}, function(err, keypair) {\n    // error in err?\n\n    // serialize the public key\n    console.log(keypair.publicKey.serialize());\n\n    // just the JSON object to embed in another structure\n    console.log(JSON.stringify({stuff: keypair.publicKey.toSimpleObject()}));\n\n    // replace this with the key to sign\n    var publicKeyToCertify = keypair.publicKey.serialize();\n\n    // create and sign a JWS\n    var payload = {principal: {email: 'some@dude.domain'},\n                    pubkey: jwcrypto.loadPublicKey(publicKeyToCertify)};\n\n    jwcrypto.sign(payload, keypair.secretKey, function(err, jws) {\n        // error in err?\n\n        // serialize it\n        console.log(jws.toString());\n\n        // replace with things to verify\n    var signedObject = jws;\n    var publicKey = keypair.publicKey;\n\n        // verify it\n        jwcrypto.verify(signedObject, publicKey, function(err, payload) {\n            // if verification fails, then err tells you why\n            // if verification succeeds, err is null, and payload is\n            // the signed JS object.\n        });\n    });\n\n    // replace this with the key to load\n    var storedSecretKey = keypair.secretKey.serialize();\n\n    // also, if loading a secret key from somewhere\n    var otherSecretKey = jwcrypto.loadSecretKey(storedSecretKey);\n});\n```\n\nAssertions\n====\n\nSometimes the JSON object to sign should be a standard assertion with pre-defined fields.\n\n```javascript\nvar assertion = require(\"browserid-crypto\").assertion;\n\n// payload of the assertion\nvar payload = {principal: {email: 'some@dude.domain'}};\n\n// add special fields which will be encoded properly\n// payload cannot contain reserved fields\nassertion.sign(payload, {issuer: \"foo.com\", expiresAt: new Date(new Date().valueOf() + 5000),\n                            issuedAt: new Date().valueOf(), audience: \"https://example.com\"},\n                    keypair.secretKey,\n                    function(err, signedAssertion) {\n    // a normal signedObject, much like above\n    // can be verified with jwcrypto.verify\n\n    // or verified specifically for jwt, with expiration verification\n    var now = new Date();\n    assertion.verify(signedObject, keypair.publicKey, now, function(err, payload, assertionParams) {\n        // payload is the original payload\n        // assertionParams contains issuedAt, expiresAt as dates\n        // and issuer and audience as strings.\n    });\n});\n```\n\nNote that timestamps (for `issuedAt` and `expiresAt`) are integers containing the standard JS milliseconds-since-epoch, or objects with methods named `.valueOf()` which will return such an integer. The assertion format currently serializes these integers verbatim; a future version may serialize them as seconds (instead of milliseconds) to conform with the JWT specifications.\n\nCerts\n=======\n\nSometimes the JSON objects to sign are certificates\n\n```javascript\nvar cert = require(\"browserid-crypto\").cert;\n\nvar keyToCertify = keypairToCertify.publicKey;\nvar principal = {email: \"someone@example.com\"};\n\nvar assertionParams = {issuer: \"foo.com\", issuedAt: new Date(),\n                        expiresAt: new Date()};\n\n// cert params, kid is optional, others are required\nvar certParams = {kid: \"key-2012-08-11\",\n                    publicKey: keyToCertify,\n                    principal: principal};\n\nvar additionalPayload = {};\n\n// payload cannot contain reserved fields\ncert.sign(certParams,\n            assertionParams, additionalPayload,\n            keypair.secretKey,\n            function(err, signedObject) {\n    // normal signedObject\n    // can be verified with jwcrypto.verify\n\n    // or verified specifically for certification\n    // include a date that is considered the \"now\"\n    cert.verify(signedObject, keypair.publicKey, now, function(err, payload, assertionParams, certParams) {\n        // the extra payload\n        // the assertionParams specifics\n        // the certParams include publicKey being certified, and principal bound to it.\n    });\n});\n\n// bundle a cert chain and an assertion\nvar bundle = cert.bundle([certs], assertion);\n\nfunction getPK(issuer, next) {\n    // function to get a public key for an issuer\n}\n\nvar now = new Date();\n\n// verify just the chain of certs\ncert.verifyChain([certs], now, getPK, function(err, certParamsArray) {\n    // err is an error or null\n    // if no error:\n    // certParamsArray is the array of individual cert params from each verification\n    // including specifically the publicKey and principal parameters\n});\n\n// verify a chain of certs and assertion\ncert.verifyBundle(bundle, now, getPK, function(err, certParamsArray, payload, assertionParams) {\n    // err is an error or null\n    // if no error:\n    // certParamsArray is the array of individual cert params from each verification\n    // payload is the assertion payload, and assertionParams is the assertion params.\n});\n```\n"
},
{
  "name": "sre-adrs",
  "files": {
    "/": [
      ".gitignore",
      "Makefile",
      "README.md",
      "TOC.md",
      "decisions",
      "generate_toc.sh"
    ]
  },
  "makefile": "gentoc:\n\t./generate_toc.sh > TOC.md\n\ndefault: gentoc\n",
  "readme": "# Web SRE Architectural Decision Records (ADRs) Repository\n\nThis repository contains some discussion, the approval process, and approved architectural decision records (ADRs) for the Web SRE Team. The original proposal document for this Web SRE ADR repository can be found [here](https://docs.google.com/document/d/1pZlYCyXcZbmQq1O-g4BNJD1uZTjluYKkk7BSu2BwOGU/edit#).\n\n## About\n\nThe Web SRE Team set up this ADR repository in order to help consolidate, clarify, and communicate our team's decisions. \n\nIf you're new to ADRs, see a good definition & overview of ADRs here: https://github.com/joelparkerhenderson/architecture_decision_record. Within Mozilla, a number of teams and projects already use ADRs, like the following:\n* [Marketing ADRs & Common Infrastructure](https://github.com/mozmeao/infra/)\n* [Mozilla FxA ADRs](https://github.com/mozilla/fxa/blob/main/docs/adr)\n* [Refractr (application) codebase architectural docs (not ADRs, but similar in thought)](https://github.com/mozilla-it/refractr/blob/main/docs/refractr-architecture.md)\n\n## Scope\n\nThe scope of \"architectural decisions\" here is left intentionally vague, as it can be refined as we try out this ADR approach. \n\nIdeally, however, the following information is not stored here:\n* implementation steps or docs (that should go into Mana with links to/from the ADR here as needed)\n* history of systems or extended documentation \n* runbooks themselves (though there could possibly be an ADR on the expectations and templates of a runbook)\n* system FAQs & how-tos\n\n## Process\n\nAll ADRs are essentially Markdown documents put into the `decisions` directory in this repository, further refined via subdirectories for specific teams (e.g. `websre`) and a `department` directory for decisions made at a SRE department-wide scale. This setup is to easily take expansions if other teams also want to use this repository in the future; for now, the primary ADRs are created and headed for `decisions/websre`.\n\nAll ADRs go through a review and approval process that mirrors gitflow, e.g.:\n\n1. Create feature branch off main & add your ADR - as a new markdown document in the correct `decisions` directory & following our ADR template\n2. Commit & push your work up to this repository & create a PR against main. Tag the folks whose approval you need for this particular ADR.\n    1. For the Web SRE Team, you can use the IT-SE GitHub team as the reviewer.\n    2. Web SRE requires unananimous approval for the ADR to pass;\n    3. The PR should be noted in the correct team channels or meetings to make sure people review. \n3. Review happens against the PR. If any blockers to moving the proposed ADR come up, they're addressed in those same channels / team meeetings and handed off for further refinement before revisiting the PR.\n4. Once there is approval, the PR is merged into main and the ADR is considered approved.\n5. After the approval, the author should ensure any work tickets related to implementation of this decision exist in the relevant backlogs.\n\nAny new requests for topics to be written up in an ADR can be made as a ticket under this epic: https://jira.mozilla.com/browse/SE-1670\n\n## Template\n\nNew ADRs should largely follow the template stored in [template.md](decisions/global/templates/template.md).\n\n## Automation\n\nIf desired, you can use [the adr cli](https://github.com/npryce/adr-tools) and the [adr-viewer pip package](https://github.com/mrwilson/adr-viewer) to work with proposals in this repository. \n\nNB: given our multi-team setup (for possible growth) and [limitations of adr-tools](https://github.com/npryce/adr-tools/issues/48), this repository is setup currently for `adr` to workonly within a specific sub directory.  To use for a team or subdirectory, first cd to it, and then use the adr cli tool to interact with adrs.\n\nInstallation:\n```\n$ brew install adr-tools # see other install methods here: https://github.com/npryce/adr-tools/blob/master/INSTALL.md\n$ adr\n  usage: adr help COMMAND [ARG] ...\n  COMMAND is one of:\n  help\n  new\n  link\n  list\n  init\n  config\n  generate\n  upgrade-repository\n  Run 'adr help COMMAND' for help on a specific command.\n$ pip install adr-viewer # used with python 3.9 here\n$ adr-viewer --help\n  Usage: adr-viewer [OPTIONS]\n  \n  Options:\n    --adr-path TEXT  Directory containing ADR files.  [default: doc/adr/]\n    --output TEXT    File to write output to.  [default: index.html]\n    --serve          Serve content at http://localhost:8000/\n    --help           Show this message and exit.\n```\n\nUsing adr-tools cli (adr):\n\nTo handle multiple teams we've made the table of contents generator work just on filenames.\nAs such you can generally treat each adr directory as independent from the point of view of the ADR CLI tool.\n\n### create new adr proposal for your team:\nThis assumes someone has initialized a team or group folder for you previously.\nIf not, the info is below.\n\n```\n$ cd decisions/$team\n$ adr new My Awesome Web SRE Team ADR\n  ./0002-my-awesome-global-adr.md\n# then add your info & go\n$ vi 0002-my-awesome-global-adr.md\n```\n\n### initialize a new team- or group-specific decisions directory:\n\nYou'll need a new folder for your team, feel free to add as needed.\nYou will need to force add the adr-dir, as we want to ignore it at the root of repo,\nbut it's nice to have, to make the init/new commands work well.\n\n```\n$ mkdir -p decisions/$team\n$ cd decisions/$team\n$ echo \".\" > .adr-dir\n$ vi decisions/my-new-team/0001-record-architecture-decisions.md # then add your info & go\n$ cp -r decisions/websre/templates decisions/my-new-team # set up custom templates if you like\n$ vi decisions/my-new-team/templates/template.md # edit that custom team template\n```\n\n### generate whole repository Table of Contents:\n```\n$ make gentoc\n# this will run the custom script generate_toc.sh, and generate the table of contents into the TOC.md file\n# commit this to git if you'd like.\n```\n\nFor a cleaner presentation, adr-viewer (a python package) is being considered as part of the CI to generate static files for a website from this repository.\n\n### serve localhost:8000 view of Web SRE ADRs\n```\n$ adr-viewer --serve --adr-path decisions/websre\n  Starting server at http://localhost:8000/\n  {server output; cntl + c to exit}\n```\n\nThis could be used one day to generate HTML to serve via GitHub pages, but currently is just here for exploration.\n"
},
{
  "name": "extension-finder",
  "files": {
    "/": [
      ".config",
      ".glitch-assets",
      "README.md",
      "index.html",
      "lunr.js",
      "lunr.min.js",
      "script.js",
      "styles.css"
    ]
  },
  "makefile": null,
  "readme": "# My cool website\n\nThis file describes your project to the community. What's your cool website about? What makes it special?\n\n\u30fd(\u0e4f\u2200\u0e4f )\uff89\n\n## \u2190 index.html\n\nWhere you'll write the content of your website. \n\n## \u2190 styles.css\n\nCSS files add styling rules to your content\n\n## \u2190 script.js\n\nIf you're feeling fancy you can add interactivity to your site with Javascript\n\n"
},
{
  "name": "partybal",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "conda-linux-64.lock",
      "conda-osx-64.lock",
      "entrypoint.sh",
      "environment.yml",
      "index.html.jinja2",
      "invoke.py",
      "jan_erichsen.jpg",
      "mypy.ini",
      "plot_functions.R",
      "pyproject.toml",
      "template.Rmd.jinja2"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# party balloon\n\nit's like a pilot balloon, but unserious.\n\n![](jan_erichsen.jpg)\n\n# invocation\n\n`python invoke.py invoke --output ./output -j 4` will run all experiments that have changed since the last run, using 4 R invocations at once. If `j` is not specified, partybal will run in parallel by default, starting as many threads as you have processors. The resulting HTML will be stored in `./output/`.\n\n# what _on earth_ is going on here\n\nPartybal creates a static website that serves as a proof-of-existence for statistics from [Jetstream](https://github.com/mozilla/jetstream).\n\nBasically what happens, what happens is:\n\n* invoke.py downloads the experiment results from GCS\n* invoke.py uses the contents of the results and `template.Rmd.jinja2` to generate an RMarkdown document for each experiment that's changed since its last run (or since `--updated-seconds-ago`).\n* invoke.py shells out to R to build each experiment's RMarkdown notebook to HTML\n\n`plot_functions.R` has a function named `plot_X` for each Jetstream statistic `X`, which accepts the arguments `df`, `metric`, `comparison`, `period`, and `segment`.\nPartybal will probably break if you add a Statistic to Jetstream and don't add a corresponding `plot_` function here. That would be sad!\n\nThe plot_ functions can return anything that knitr knows how to render, but a ggplot grob or a data.frame are good choices.\n\n# environments\n\nSpin up an environment with `conda create -n partybal --file conda-linux-64.lock` (or `-osx` as appropriate)\n\nUpdate dependencies with `conda update --all`.\n\nSave a new lock file from environment.yml by `pip install`ing conda-lock and running `conda-lock`.\n\n# state\n\npartybal tries not to do more work than it has to.\n\nPartybal will generate a complete TOC after every run but will generate results pages only for the experiments that changed, so if you are not persisting the output path between runs, you should not remove files from GCS that are not present in the local output path when you synchronize it to GCS.\n\nIt decides which experiments to run one of two ways:\n\n## memory, turn your face to the moonlight\n\nBy default, it remembers when it last ran, and only regenerates pages for experiments that changed since that run.\n\nThe path it uses to remember this state is chosen with `appdirs.user_cache_dir(\"partybal\", \"Mozilla\")` ([docs](https://github.com/ActiveState/appdirs)).\n\nIf this path is not persisted between runs, Partybal will always do lots of work.\n\n## since \\<seconds\\> ago\n\nAlternatively, if you pass `--updated-seconds-ago N`, it will only rebuild experiments that have changed in the last N seconds.\n"
},
{
  "name": "disk_key_encrypter",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "__init__.py",
      "apache",
      "apps",
      "bin",
      "manage.py",
      "middleware",
      "oidc",
      "openresty",
      "requirements.txt",
      "settings.py-dist",
      "settings_public_or_non_public.py-dist",
      "static",
      "urls.py",
      "wsgi.py"
    ]
  },
  "makefile": null,
  "readme": "disk_key_encrypter\n==================\n\ndjango webapp for gpg encryption of disk keys\n\nLicense\n==================\nThis Source Code Form is subject to the terms of the Mozilla Public\nLicense, v. 2.0. If a copy of the MPL was not distributed with this\nfile, You can obtain one at http://mozilla.org/MPL/2.0/.\n\nInstallation\n==================\npip install requirements.txt\n\nAdditional Credit\n==================\nThis software uses the following open source code/projects:\n\ndjango gpg encryption script\nhttps://gist.github.com/1327072\n\ndjango-bootstrap\nhttps://github.com/earle/django-bootstrap\n"
},
{
  "name": "nss-tools",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "ci",
      "cmd",
      "nss-code-review-checklist.yaml",
      "nss-code-review.py",
      "nss-land-commit.py",
      "nss-release-review.py",
      "nss-run.sh",
      "nss-ssl-gtests.sh",
      "nss-try-patch.sh",
      "nss-try.sh",
      "nss-uplift-unified.sh",
      "pytest.ini",
      "requirements.txt",
      "utils"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# nss-tools\nTools for interacting with NSS\n\nSee individual paths under cmd/ for tool names and descriptions\n"
},
{
  "name": "contribute.json",
  "files": {
    "/": [
      ".buildpacks",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Gruntfile.js",
      "LICENSE",
      "Procfile",
      "README.md",
      "app",
      "contribute.json",
      "knownurls.txt",
      "legacyredirect",
      "package.json",
      "requirements.txt",
      "runtime.txt",
      "schema.json",
      "setup.cfg",
      "stackato.yml"
    ]
  },
  "makefile": null,
  "readme": "contribute.json\n===============\n\nA JSON schema for open-source project contribution data.\n\nThis is currently a proposal and is not yet stable. Suggestions and pull-requests welcome.\n\nThis is the current draft. I'm presenting the schema as an example instance using `mozilla/bedrock`\nas the subject. Previous discussion can be found in the comments of [the original gist](https://gist.github.com/pmclanahan/a162224376ca110b4a40).\n\n```json\n{\n    // required\n    \"name\": \"Bedrock\",\n    \"description\": \"The app powering (most of) www.mozilla.org.\",\n    \"repository\": {\n        \"url\": \"https://github.com/mozilla/bedrock\",\n        \"license\": \"MPL2\",\n        // optional\n        \"type\": \"git\",\n        \"tests\": \"https://ci.mozilla.org/job/bedrock/\",\n        \"clone\": \"https://github.com/mozilla/bedrock.git\"\n    },\n\n    // optional\n    \"participate\": {\n        \"home\": \"https://wiki.mozilla.org/Mozilla.org\",\n        \"docs\": \"http://bedrock.readthedocs.org/\",\n        // optional\n        \"mailing-list\": \"https://www.mozilla.org/about/forums/#dev-mozilla-org\",\n        \"irc\": \"irc://irc.mozilla.org/#www\",\n        \"irc-contacts\": [\n            \"pmac\",\n            \"jgmize\",\n            \"malexis\",\n            \"cmore\"\n        ],\n        \"chat\": {\n            \"url\": \"irc://irc.mozilla.org/#www\",\n            \"contacts\": [\n                \"pmac\",\n                \"jgmize\",\n                \"malexis\",\n                \"cmore\"\n            ]\n        }\n    },\n    \"bugs\": {\n        \"list\": \"https://bugzilla.mozilla.org/buglist.cgi?query_format=advanced&bug_status=UNCONFIRMED&bug_status=NEW&product=www.mozilla.org\",\n        \"report\": \"https://bugzilla.mozilla.org/enter_bug.cgi?product=www.mozilla.org&component=Bedrock\",\n        \"mentored\": \"https://bugzilla.mozilla.org/buglist.cgi?f1=bug_mentor&o1=isnotempty&query_format=advanced&bug_status=NEW&product=www.mozilla.org&list_id=10866041\"\n    },\n    \"urls\": {\n        \"prod\": \"https://www.mozilla.org\",\n        \"stage\": \"https://www.allizom.org\",\n        \"dev\": \"https://www-dev.allizom.org\",\n        \"demo1\": \"https://www-demo1.allizom.org\"\n    },\n    \"keywords\": [\n        \"python\",\n        \"less-css\",\n        \"django\",\n        \"html5\",\n        \"jquery\"\n    ]\n}\n```\n\nValidation\n----------\n\nWe're currently using the [JSON Schema](http://json-schema.org/)\nstandard and we publish our schema at\n[schema.json](https://github.com/mozilla/contribute.json/blob/master/schema.json).\n\nYou can, for example, use the [json-schema-validator](https://json-schema-validator.herokuapp.com/)\nto validate your own `contribute.json` against this schema.\n\n\nFlask app\n---------\n\nThere's a server-side app that is currently available on\nhttps://www.contributejson.org\n\n## Running the Flask app locally\n\n```bash\n# clone from the main repo\ngit clone https://github.com/mozilla/contribute.json.git\n\n# go into the directory\ncd contribute.json\n\n# using virtualenv wrapper, create a new virtual environment for the project.\nmkvirtualenv contribute.json\n\n# intall the requirements\npip install -r requirements.txt\n\n# install the npm dependencies\nnpm install\n\n# generate the CSS files (use `grunt watch` to work on the LESS files)\ngrunt less\n\n# You can run the app with\nDEBUG=true python app\n\nopen http://localhost:5000/\n```\n\nNB! Most of the functionality is built as an [AngularJS](https://angularjs.org/) app.\n\nSentry\n------\n\nTo use Sentry you just need to create an environment variable called\n`SENTRY_DSN` and it will be used.\n"
},
{
  "name": "webdev-blog",
  "files": {
    "/": [
      "README.md",
      "beer_and_tell",
      "extravaganza"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Webdev Blog\nThis repository contains an archive of meeting-summary blog posts from the\n[Mozilla Webdev Blog][] in Markdown format. It's used primarily to facilitate\nreview of these summary posts before they get posted.\n\n[Mozilla Webdev Blog]: https://blog.mozilla.org/webdev/\n\n## License\nContent in this repository is licensed under the\n[Creative Commons Attribution Share-Alike License v3.0][CC_BY-SA_3.0] or any\nlater version.\n\n[CC_BY-SA_3.0]: http://creativecommons.org/licenses/by-sa/3.0/\n"
},
{
  "name": "wagtail-footnotes",
  "files": {
    "/": [
      ".gitignore",
      "CHANGELOG.md",
      "LICENCE.txt",
      "MANIFEST.in",
      "README.md",
      "setup.cfg",
      "setup.py",
      "wagtail_footnotes"
    ]
  },
  "makefile": null,
  "readme": "# Wagtail Footnotes\n\nAdd footnotes functionality to your Wagtail project.\n\n## Usage\n - Add the app to `INSTALLED_APPS`:\n   ```python\n   INSTALLED_APPS = [\n       ...\n       \"wagtail_footnotes\",\n       ...\n   ]\n   ```\n - Add the footnotes `urls.py` to your project's `urls.py`:\n   ```python\n   from wagtail_footnotes import urls as footnotes_urls\n   urlpatterns = [\n       ...\n       path(\"footnotes/\", include(footnotes_urls)),\n       ...\n   ]\n   ```\n   Note: The URL has to be defined as above as it is currently hardcoded in the Javascript.\n - Update your page models to show the footnotes field:\n   ```python\n   class InformationPage(BasePage):\n        ...\n        content_panels = [\n            ...\n            InlinePanel(\"footnotes\", label=\"Footnotes\"),\n        ]\n   ```\n - Update your `RichTextBlock`s \n    - Add `\"footnotes\"` to the `features` arg for each `RichTextBlock` that you want to have this functionality\n    - You will also need to change any `RichTextBlock`s to `wagtail_footnotes.blocks.RichTextBlockWithFootnotes`\n    - You can add the footnotes to `RichTextBlock`s across the project by updating `WAGTAILADMIN_RICH_TEXT_EDITORS[\"default\"][\"OPTIONS\"][\"features\"]`:\n      ```python\n      WAGTAILADMIN_RICH_TEXT_EDITORS = {\n          \"default\": {\n              \"WIDGET\": \"wagtail.admin.rich_text.DraftailRichTextArea\",\n              \"OPTIONS\": {\"features\": [\"bold\", \"italic\", \"h3\", \"h4\", \"ol\", \"ul\", \"link\", \"footnotes\"]},\n          }\n      }\n      ```\n - Update your page templates to include `{% include \"wagtail_footnotes/includes/footnotes.html\" %}`\n - Make and run migrations:\n   ```\n   ./manage.py makemigrations\n   ./manage.py migrate\n   ```\n\n## Settings\n\n - `WAGTAIL_FOOTNOTES_TEXT_FEATURES`\n   - Default: `[\"bold\", \"italic\", \"link\"]`\n   - Use this to update a list of Rich Text features allowed in the footnote text.\n\n## Common issues\n - I click on the `Fn` button in the editor and it stops working\n    - This is likely because the URL in the JS does not match the URL of the footnotes view. Check the URL in `wagtail_footnotes/static/footnotes/js/footnotes.js` matches the URL you set.\n - `NoneType` error when rendering page.\n    - Make sure you are rendering the field in the template using `{% include_block page.field_name %}`\n"
},
{
  "name": "stmoab",
  "files": {
    "/": [
      ".coveragerc",
      ".flake8",
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "Makefile",
      "README.rst",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "stmoab",
      "test_requirements.txt"
    ]
  },
  "makefile": "lint:\n\tflake8 stmoab/utils.py\n\tflake8 stmoab/dashboards/SummaryDashboard.py\n\tflake8 stmoab/dashboards/StatistcalDashboard.py\n\tflake8 stmoab/dashboards/ExperimentDashboard.py\n\tflake8 stmoab/tests/base.py\n\tflake8 stmoab/tests/test_summary_dashboard.py\n\tflake8 stmoab/test/test_utils.py\n\tflake8 stmoab/tests/test_experiment_dashboard.py\n\tflake8 stmoab/tests/test_statistical_dashboard.py\n\ntest: lint\n\tnosetests --with-coverage --cover-package=stmoab",
  "readme": ".. image:: https://travis-ci.org/mozilla/stmoab.svg?branch=master\n  :target: https://travis-ci.org/mozilla/stmoab\n\n.. image:: https://coveralls.io/repos/github/mozilla/stmoab/badge.svg?branch=master\n  :target: https://coveralls.io/github/mozilla/stmoab?branch=master\n\n====================\nSt. Moab\n====================\n\nA tool for automating a/b testing analysis using stmo dashboards (https://sql.telemetry.mozilla.org)\n"
},
{
  "name": "redash_client",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.rst",
      "lambda.sh",
      "redash_client",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "test_requirements.txt"
    ]
  },
  "makefile": "build:\n\t./lambda.sh\n\nlint:\n\tflake8 redash_client/constants.py\n\tflake8 redash_client/client.py\n\tflake8 redash_client/tests/test_redash.py\n\ntest: lint\n\tnosetests --with-coverage --cover-package=redash_client\n",
  "readme": ".. image:: https://travis-ci.org/mozilla/redash_client.svg?branch=master\n  :target: https://travis-ci.org/mozilla/redash_client\n\n.. image:: https://coveralls.io/repos/github/mozilla/redash_client/badge.svg?branch=master\n  :target: https://coveralls.io/github/mozilla/redash_client?branch=master\n\n====================\nRedash Python Client\n====================\n\nA client for the Redash API for stmo (https://sql.telemetry.mozilla.org)\n\n=======\nInstall\n=======\n\n.. code-block:: bash\n\n  pip install redash_client\n\n=====\nUsage\n=====\n\nBefore using :code:`RedashClient`, set the :code:`REDASH_API_KEY` environment variable to your Redash API key:\n\n:code:`export REDASH_API_KEY=<your_api_key>`\n\nTo import and use :code:`RedashClient`:\n\n.. code:: python\n\n  import os\n  from redash_client.client import RedashClient\n\n  api_key = os.environ[\"REDASH_API_KEY\"]\n  redash_client = RedashClient(api_key)\n\n  # Make a Redash API call:\n  redash_client.search_queries(\"AS Template:\")\n\n\n===============\nPackage for Pip\n===============\n\nFirst, you must update the :code:`version` field in :code:`setup.py`.\nThen run this commands:\n\n.. code-block:: bash\n\n  python setup.py sdist bdist_wheel\n  twine upload dist/*\n\nMake sure you have ``wheel`` and ``twine`` installed.\n"
},
{
  "name": "tracking-protection-issues",
  "files": {
    "/": [
      ".circleci",
      ".env",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "Procfile",
      "README.md",
      "app",
      "run.py",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Tracking Protection Issues\n\n[![CircleCI](https://circleci.com/gh/mozilla/tracking-protection-issues/tree/master.svg?style=shield)](https://circleci.com/gh/mozilla/tracking-protection-issues/tree/master)\n\nRepo used to host the code for filing private tracking protection issues. \n\n## Local installation\n\nWe're using [pipenv](https://docs.pipenv.org/) for this project. Be sure to [install it first](https://docs.pipenv.org/#install-pipenv-today).\n\n0. clone the repo\n1. `pipenv sync --dev`\n2. `pipenv run flask run`\n\nNote: If you're trying to use this in production, it will expect a lot of environment variables. Check out config.py.\n\n\ud83d\udea8 Please don't ever add any to the `.env` file and check it in. \ud83d\udea8\n\n## How it works\n\nThis server exposes a `/new` endpoint that expects the following `multipart/form-data` payload via `POST`, represented here in some kind of pseudo-schema:\n\n```\n{\n  \"body\": the issue body (required)\n  \"title\": the issue title (required),\n  \"labels\": 'one, two, three' (optional)\n}\n```\n\nA GitHub issue will be created, assuming all the credentials are correct. \n\nFor privacy and security reasons, the GitHub repo and its issues are private and locked down to a small team of Mozilla employees within the @mozilla GitHub org. If you think you need access, ping miket@mozilla.com.\n"
},
{
  "name": "snakepit-client",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Snakepit Client\n\nCommand-line client for the [snakepit machine learning job scheduler](https://github.com/mozilla/snakepit)\n\n## Installation\n\nThis is a preliminary installation guide, as the client is not mature enough for being hosted as NPM package yet.\n\n### Prerequisites\n\n* git\n* Node.js (8.00+ is tested)\n\n### Installing\n\nFollow these steps to install the client:\n```\n$ git clone https://github.com/mozilla/snakepit-client.git\n[...]\n$ cd snakepit\nsnakepit$ npm install\n[...]\nsnakepit$ sudo npm link\n[...]\nsnakepit$ pit --help\nUsage: pit [options] [command]\n\nOptions:\n  -V, --version                               output the version number\n  -h, --help                                  output usage information\n\nCommands:\n  add <entity> [properties...]                adds an entity to the system\n  remove|rm <entity>                          removes an entity from the system\n  set <entity> <assignments...>               sets properties of an entity\n  get <entity> <property>                     gets a property of an entity\n  show <entity>                               shows info about an entity\n  add-group <entity> <group>                  adds the entity to the access group\n  remove-group <entity> <group>               removes the entity from the access group\n  stop <jobNumber>                            stops a running job\n  run|put [options] <title> [clusterRequest]  enqueues current directory as new job\n  log [options] <jobNumber>                   show job's log\n  download <jobNumber>                        downloads job directory as .tar.gz archive\n  ls <jobNumber> [path]                       lists contents within a job directory\n  cp <jobNumber> <jobPath> <fsPath>           copies contents within from job directory to local file system\n  mount [options] <entity> [mountpoint]       mounts the data directory of an entity to a local mountpoint\n  status                                      prints a job status report\n  *\n```\n\n### First time use\n\nThe administrators of the Snakepit cluster should've provided you a so called `.pitconnect.txt` file.\nThis file is to be placed either in your home directory or inside a project root (overruling the one in your home directory).\n\nTo test your setup, change into that directory and run the following:\n```\n$ pit status\nNo user info found. Seems like a new user or first time login from this machine.\nPlease enter an existing or new username: tilman\nFound no user of that name.\nDo you want to register this usename (yes|no)? yes\nFull name: Tilman Kamp\nE-Mail address: ...\nNew password: ************\nReinput a same one to confirm it: ************\n   JOB   S SINCE        UC% UM% USER       TITLE                RESOURCE \n```\nIf your username had been known already, the client would've asked you for the password \nand registered a token for this (additional) machine.\n\nIf all went well, the following command shows your account status:\n```\n$ pit show me\nUsername:         tilman\nFull name:        Tilman Kamp\nE-Mail address:   ...\n```\n\n## Running jobs\n\nRunning a job is done through the `run` command:\n```\n$ pit run --help\nUsage: run|put [options] <title> [clusterRequest]\n\nenqueues current directory as new job\n\nOptions:\n  -p, --private               prevents automatic sharing of this job\n  -c, --continue <jobNumber>  continues job with provided number by copying its \"keep\" directory over to the new job\n  -d, --direct <commands>     directly executes provided commands through bash instead of loading .compute file\n  -l, --log                   waits for and prints job's log output\n  -h, --help                  output usage information\n\n    Examples:\n\n    $ pit run \"My task\" 2:[8:gtx1070]\n    $ pit run \"My command\" [] -d 'hostname; env'\n\n  \"title\" is a short text that will later help identifying the job and its purpose.\n  \"clusterRequest\" is an expression to specify resources this job requires from the cluster.\n  It's a comma separated list of \"process requests\".\n  Each \"process request\" specifies the number of process instances and (divided by colon and in braces) which resources to allocate for one process instances (on one node).\n  The first example will allocate 2 process instances. For each process, 8 \"gtx1070\" resources will get allocated.\n  You can also provide a \".pitrequest.txt\" file with the same content in your project root as default value.\n```\n\n### Finding and allocating resources (GPUs)\n\nAs you can see, we have to specify a so-called cluster-request to allocate a set of GPUs.\nBut for being able to do so, we first need to know, which resources/GPUs are available in the cluster:\n```\n$ pit show nodes\nn0\nn1\n$ pit show node:n0\nNode name: n0\nState:     ONLINE\nResources: \n  0: \"GeForce GTX 1070\" aka \"gtx1070\" (cuda 0)\n  1: \"GeForce GTX 1070\" aka \"gtx1070\" (cuda 1)\n```\n\nWe found at least one node with 2 \"GeForce GTX 1070\" GPUs.\nSo let's allocate both and run a test job on them:\n```\n$ pit run \"First light\" [2:gtx1070] -d 'cat /proc/driver/nvidia/gpus/**/*' -l\nJob number: 190\nRemote:     origin <https://github.com/...>\nHash:       ...\nDiff LoC:   0\nResources:  \"[2:gtx1070]\"\n\n[2018-12-14 17:04:58] [daemon] Pit daemon started\n[2018-12-14 17:05:01] [worker 0] Worker 0 started\n[2018-12-14 17:05:01] [worker 0] Model: \t\t GeForce GTX 1070\n[2018-12-14 17:05:01] [worker 0] IRQ:   \t\t 139\n[2018-12-14 17:05:01] [worker 0] GPU UUID: \t ...\n[2018-12-14 17:05:01] [worker 0] Video BIOS: \t 86.04.26.00.80\n[2018-12-14 17:05:01] [worker 0] Bus Type: \t PCIe\n[2018-12-14 17:05:01] [worker 0] DMA Size: \t 47 bits\n[2018-12-14 17:05:01] [worker 0] DMA Mask: \t 0x7fffffffffff\n[2018-12-14 17:05:01] [worker 0] Bus Location: \t 0000:01:00.0\n[2018-12-14 17:05:01] [worker 0] Device Minor: \t 0\n[2018-12-14 17:05:01] [worker 0] Blacklisted:\t No\n[2018-12-14 17:05:01] [worker 0] Binary: \"\"\n[2018-12-14 17:05:01] [worker 0] Model: \t\t GeForce GTX 1070\n[2018-12-14 17:05:01] [worker 0] IRQ:   \t\t 142\n[2018-12-14 17:05:01] [worker 0] GPU UUID: \t ...\n[2018-12-14 17:05:01] [worker 0] Video BIOS: \t 86.04.26.00.80\n[2018-12-14 17:05:01] [worker 0] Bus Type: \t PCIe\n[2018-12-14 17:05:01] [worker 0] DMA Size: \t 47 bits\n[2018-12-14 17:05:01] [worker 0] DMA Mask: \t 0x7fffffffffff\n[2018-12-14 17:05:01] [worker 0] Bus Location: \t 0000:02:00.0\n[2018-12-14 17:05:01] [worker 0] Device Minor: \t 1\n[2018-12-14 17:05:01] [worker 0] Blacklisted:\t No\n[2018-12-14 17:05:01] [worker 0] Binary: \"\"\n[2018-12-14 17:05:01] [worker 0] Worker 0 ended with exit code 0\n[2018-12-14 17:05:01] [daemon] Worker 0 requested stop. Stopping pit...\n```\nBoth GPUs were allocated for one process.\n\nBut what if we want to have two processes allocating one GPU each?\nLet's try:\n```\n$ pit run \"Second light\" 2:[gtx1070] -d 'cat /proc/driver/nvidia/gpus/**/*' -l\nJob number: 191\nRemote:     origin <https://github.com/...>\nHash:       ...\nDiff LoC:   0\nResources:  \"2:[gtx1070]\"\n\n[2018-12-14 22:58:27] [daemon] Pit daemon started\n[2018-12-14 22:58:28] [worker 0] Worker 0 started\n[2018-12-14 22:58:28] [worker 0] Model: \t\t GeForce GTX 1070\n[2018-12-14 22:58:28] [worker 0] IRQ:   \t\t 139\n[2018-12-14 22:58:28] [worker 0] GPU UUID: \t GPU-9009fe9c-0cca-ea59-631c-14d419efc397\n[2018-12-14 22:58:28] [worker 0] Video BIOS: \t 86.04.26.00.80\n[2018-12-14 22:58:28] [worker 0] Bus Type: \t PCIe\n[2018-12-14 22:58:28] [worker 0] DMA Size: \t 47 bits\n[2018-12-14 22:58:28] [worker 0] DMA Mask: \t 0x7fffffffffff\n[2018-12-14 22:58:28] [worker 0] Bus Location: \t 0000:01:00.0\n[2018-12-14 22:58:28] [worker 0] Device Minor: \t 0\n[2018-12-14 22:58:28] [worker 0] Blacklisted:\t No\n[2018-12-14 22:58:28] [worker 0] Binary: \"\"\n[2018-12-14 22:58:28] [worker 0] Worker 0 ended with exit code 0\n[2018-12-14 22:58:28] [worker 1] Worker 1 started\n[2018-12-14 22:58:28] [worker 1] Model: \t\t GeForce GTX 1070\n[2018-12-14 22:58:28] [worker 1] IRQ:   \t\t 142\n[2018-12-14 22:58:28] [worker 1] GPU UUID: \t GPU-f5ee1d0f-392c-5999-a708-00eedb04a761\n[2018-12-14 22:58:28] [worker 1] Video BIOS: \t 86.04.26.00.80\n[2018-12-14 22:58:28] [worker 1] Bus Type: \t PCIe\n[2018-12-14 22:58:28] [worker 1] DMA Size: \t 47 bits\n[2018-12-14 22:58:28] [worker 1] DMA Mask: \t 0x7fffffffffff\n[2018-12-14 22:58:28] [worker 1] Bus Location: \t 0000:02:00.0\n[2018-12-14 22:58:28] [worker 1] Device Minor: \t 1\n[2018-12-14 22:58:28] [worker 1] Blacklisted:\t No\n[2018-12-14 22:58:28] [worker 1] Binary: \"\"\n[2018-12-14 22:58:28] [worker 1] Worker 1 ended with exit code 0\n[2018-12-14 22:58:28] [daemon] Worker 0 requested stop. Stopping pit...\n[2018-12-14 22:58:28] [daemon] Worker 1 requested stop. Stopping pit...\n```\n\nAs you can see, the difference makes the resource allocation format:\nWhile\n```\n[2:gtx1070]\n```\nallocates __1__ process with __2__ GPUs, \n```\n2:[gtx1070]\n```\nallocates __2__ processes with __1__ GPU each.\nThe square brackets represent a process and `n:` prefixes are quantifiers. No quantifier means \"1:\".\n\nIt's also possible to allocate processes without GPUs and processes with multiple - comma-separated - GPU types:\n```\n$ pit run \"Strange job\" 2:[],4:[gtx1070,2:gtx1060] -d 'echo \"Strange!\"'\n```\nThis example allocates 2 processes without GPUs and 4 processes with 1 gtx1070 and 2 gtx1060 each.\n\nIf you specify multiple processes, they can also get allocated on different machines.\nIt's important to keep in mind that one process cannot be split in half and scheduled to more than one machine. \n\n### Communicating with other processes\n\nOnce you allocated multiple processes for a job, the instances have to be able to communicate with each other.\nThis can be achieved through a set of environment variables that is provided to each process/script-instance:\n\n* `$NUM_GROUPS`: Number of (comma separated) \"process-groups\". E.g. allocation \"2:[],[gtx1060]\" represents two process-groups.\n* `$NUM_PROCESSES_GROUP<i>`: Number of processes in process-group with index i. E.g. in \"2:[],[gtx1060]\" the value of `$NUM_PROCESSES_GROUP0` is 2.\n* `$HOST_GROUP<i>_PROCESS<j>`: Hostname of process j in process-group i.\n* `$GROUP_INDEX`: Group-index of current process.\n* `$PROCESS_INDEX`: Process-index of current process within its process-group.\n\nLet's imagine a job with allocation \"2:[]\".\nTo let the two processes ping each other, the first process (0) has to execute\n```\nping $HOST_GROUP0_PROCESS1\n```\nand the other (1) has to execute\n```\nping $HOST_GROUP0_PROCESS0\n```\n\n### Accessing data\n\nThere are four different data domains in Snakepit.\nJobs have the same read/write rights as their owning users.\nWithin your `.compute` script or a direct command you can use the following environment variable to access data:\n* Shared data: `$SHARED_DIR` - Files in this directory are read-only for everyone and considered public.\n    Only users with direct access to the head-node can change its contents.\n* Group data: `$<GROUP-NAME>_GROUP_DIR` - Admins and all members of the given group have read/write access to all contents.\n* User data: `$USER_DIR` - Admins and the user itself have read-write access.\n* Job data: `$JOB_DIR` and `$SRC_DIR` (where the `.compute` script is running) - Admins, the owning user and group members of groups specified in the \"groups\" property of the job have read-access. Only the running job is allowed to write data.\n\n## Known limitations\n\n- No integrated support for Git LFS. Work-around: Commit/Push LFS binaries to your remote/origin repository before scheduling a job.\n- Problems with binaries. Work-around: Commit/Push binaries to your remote/origin repository before scheduling a job.\n- File diffs are only done on tracked files. Work-around: `git add <filename>` before scheduling a job (and removing it afterwards if not to be pushed to repo).\n\n## Help\n\n1. [**IRC**](https://wiki.mozilla.org/IRC) - You can contact us on the `#machinelearning` channel on [Mozilla IRC](https://wiki.mozilla.org/IRC); people there can try to answer/help\n\n2. [**Issues**](https://github.com/mozilla/snakepit-client/issues) - If you think you ran into a serious problem, feel free to open an issue in our repo.\n"
},
{
  "name": "planet-content",
  "files": {
    "/": [
      "branches"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "stmocli",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "setup.py",
      "stmocli",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "# Introduction\n\nSt. Mocli allows you to keep SQL queries in github repositories\nand easily deploy to\n[re:dash](https://redash.io/).\n\n[Re:dash](https://redash.io/)\nis great for quickly creating and sharing simple analyses.\nHowever, sometimes we want to **treat our queries like code**.\nIn re:dash, it's difficult to get review, track revisions, or collaborate on queries.\n\n# Workflow\n\nSt. Mocli is still in early alpha,\nso this workflow is going to change.\n\n## Preliminaries\n\nYou should have a Redash API key to perform most operations.\nYou can get one from your\n[Redash user settings page](https://sql.telemetry.mozilla.org/users/me).\n\nThen, add something like:\n\n```bash\nexport REDASH_API_KEY=\"Tua1aith1ay9roh5thuGhoh6sa3raene\"\n```\n\nto your ~/.bash_profile, or pass the key to stmocli on the command line like:\n\n```bash\nstmocli --redash-api-key Tua1aith1ay9roh5thuGhoh6sa3raene view query.sql\n```\n\nNote that `--redash-api-key` has to come before the verb on the command line.\n\n## `init` a directory\n\n**Implemented**!\n\n`stmocli init`\n\nCreates an empty `.stmocli.conf` file in the current directory.\n\n## `track` an existing query\n\n**Implemented**!\n\n`stmocli track <redash_id> <filename>`\n\nThis command downloads the SQL statements associated with the given redash_id\nand saves it in a file with the given name.\nThe necessary metadata is then added to the config file.\n\nFor example, calling\n`stmocli track 49741 poc.sql`\nwould create a file in the current directory called `poc.sql`,\nwith the following content:\n\n```sql\nSELECT\n    normalized_channel\nFROM longitudinal\nLIMIT 10\n```\n\nAssuming this is the first query being tracked, `.stmocli.conf` would look like this:\n\n```json\n{\n  \"poc.sql\": {\n    \"query_id\": 49741,\n    \"data_source_id\": <data source>,\n    \"name\": <query name>,\n    \"description\": <query description>,\n    \"schedule\": <schedule interval in seconds>,\n    \"options\": <query options>\n  }\n}\n```\n\n## `pull` a linked query\n\n**Implemented!**\n\n`stmocli pull [<file_name>...]`\n\nPulls the current SQL statements and metadata from re:dash for the given query files.\nIf no file name is specified, pull data for all queries.\nThis will **overwrite local data**.\nBe sure to use version control.\n\n`<file_name>` must be a key in the dictionary stored in `.stmocli.conf`\n\n## `push` a query\n\n**Implemented!**\n\n`stmocli push [<filename>...]`\n\nPushes the current SQL statements and metadata to re:dash for the given query file.\n\nYou can specify one or more query files to be pushed, these must be keys in the\ndictionary stored in `.stmocli.conf`. If no file names are specified, all SQL\nstatements are pushed.\n\n# Roadmap\n\n## Push-only and Automatic deploys\n\nThis tool assumes no edits happen in re:dash, which is a bad assumption.\nEdits made in re:dash get overwritten if you `push` without `pull`ing first.\n\nIdeally, there would be a Mocli-user in re:dash that owns all Mocli queries.\nThis would ensure all queries controlled by Mocli cannot be edited in re:dash.\nWe could then remove the `pull` command, and this tool becomes `push`-only.\n\nFrom there we can have a scheduled job (hourly?) that pushes master to STMO.\nMaybe we add a git-hook that pushes master on commit. Seamless.\n\n## `preview` a query\n\nUsers will need to upload queries to a temporary re:dash query to preview the results.\nThis should be easy to do with a `preview` command.\nIt may also be useful to execute queries against presto directly.\n\n## `start` a new query\n\nCurrently, St. Mocli can only track existing queries.\nWe should add a `start` command that will make it easy to start queries from the cli.\n"
},
{
  "name": "foxfooding-campaign-MR1",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Foxfooding MR1 Campaign\n\nThis is a repo that includes all the bugs that were reported by the community in the Foxfooding campaign.\n\nSpam issues or isssues that violated Mozilla's CPG have been deleted.\n\nIssues that have been verified, have been transfered to bugzilla.mozilla.org\n"
},
{
  "name": "PyFxA",
  "files": {
    "/": [
      ".circleci",
      ".coveragerc",
      ".gitignore",
      "CHANGES.txt",
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "MANIFEST.in",
      "README.rst",
      "dev-requirements.txt",
      "fxa",
      "pytest.ini",
      "setup.cfg",
      "setup.py"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "===========================================================\nPyFxA: Python library for interacting with Firefox Accounts\n===========================================================\n\n.. image:: https://travis-ci.org/mozilla/PyFxA.svg?branch=master\n    :target: https://travis-ci.org/mozilla/PyFxA\n\nThis is python library for interacting with the Firefox Accounts ecosystem.\n\nEventually, it is planned to provide easy support for the following features:\n\n* being a direct firefox accounts authentication client\n* being an FxA OAuth Service Provider\n* accessing attached services\n* helps interactions with Firefox Account servers wiht requests Authentication plugins.\n\nBut none of that is ready yet; caveat emptor.\n\n\nFirefox Accounts\n================\n\nCurrently, basic auth-server operations should work like so:\n\n.. code-block:: python\n\n    from fxa.core import Client\n\n    client = Client(\"https://api.accounts.firefox.com\")\n    client.create_account(\"test@example.com\", \"MySecretPassword\")\n\n    session = client.login(\"test@example.com\", \"MySecretPassword\")\n    cert = session.sign_certificate(myPublicKey)\n    session.change_password(\"MySecretPassword\", \"ThisIsEvenMoreSecret\")\n\n\nFxA OAuth Relier\n================\n\nTrade the authentication code against a longer lived OAuth token:\n\n.. code-block:: python\n\n    from fxa.oauth import Client\n\n    client = Client()\n    token = client.trade_code(\"client-id\", \"client-secret\", \"code-1234\")\n\n\nVerify an OAuth token:\n\n.. code-block:: python\n\n    from fxa.oauth import Client\n    from fxa.errors import ClientError\n\n    client = Client()\n\n    try:\n        profile = client.verify_token(\"123456...\")\n    except ClientError:\n        print \"Invalid token\"\n\n    print(\"User id\", profile[\"user\"])\n\n\nTesting email addresses\n=======================\n\nThere's also very basic integration with restmail.net, to allow for\ntesting with live email addresses.  It works like this:\n\n.. code-block:: python\n\n    from fxa.core import Client\n    from fxa.tests.utils import TestEmailAccount\n\n    # Create a testing account using an @restmail.net address.\n    acct = TestEmailAccount()\n    client = Client(\"https://api.accounts.firefox.com\")\n    session = client.create_account(acct.email, \"MySecretPassword\")\n\n    # Verify the account using the code from email.\n    acct.fetch()\n    for m in acct.messages:\n        if \"x-verify-code\" in m[\"headers\"]:\n            session.verify_email_code(m[\"headers\"][\"x-verify-code\"])\n\n    ...\n\n    # Destroy the account once you're done with it.\n    acct.clear()\n    client.destroy_account(acct.email, \"MySecretPassword\")\n\n\nPassing tokens and assertions to other applications\n===================================================\n\nPyFxA provides a ``fxa-client`` that you can use to export Bearer\nTokens and Browser ID assertions.\n\n\nGet a Bearer Token for an existing account\n------------------------------------------\n\n.. code-block:: bash\n\n    fxa-client --bearer --auth you@domain.tld \\\n        --account-server https://api.accounts.firefox.com/v1 \\\n        --oauth-server https://oauth.accounts.firefox.com/v1\n\n    Please enter a password for you@domain.tld: \n\n    # ---- BEARER TOKEN INFO ----\n    # User: you@domain.tld\n    # Scopes: profile\n    # Account: https://api.accounts.firefox.com/v1\n    # Oauth: https://oauth.accounts.firefox.com/v1\n    # ---------------------------\n    export OAUTH_BEARER_TOKEN=\"3f5106b203c...b728ef93fe29203aad44ee816a45b2f2ff57a6aed7a3\"\n\n\nCreate a new account Bearer Token on stage\n------------------------------------------\n\n.. code-block:: bash\n\n    fxa-client --bearer --create --prefix hello\n\n    # ---- BEARER TOKEN INFO ----\n    # User: hello-89331eba46e970dc1686ba2dc4583fc9@restmail.net\n    # Scopes: profile\n    # Account: https://api-accounts.stage.mozaws.net/v1\n    # Oauth: https://oauth.stage.mozaws.net/v1\n    # ---------------------------\n    export OAUTH_BEARER_TOKEN=\"ecb5285d59b28e6768fe60d76e6994877ffb16d3232c...72bdee05ea8a5\"\n\n\nCreate a new account BrowserID assertion on stage\n-------------------------------------------------\n\n.. code-block:: bash\n\n    fxa-client --browserid --create --audience https://token.stage.mozaws.net/ --prefix syncto\n    # ---- BROWSER ID ASSERTION INFO ----\n    # User: syncto-5bcf63598bf6026a6833035821742d3e@restmail.net\n    # Audience: https://token.stage.mozaws.net/\n    # Account: https://api-accounts.stage.mozaws.net/v1\n    # ------------------------------------\n    export FXA_BROWSERID_ASSERTION=\"eyJhbGciOiJSUzI1NiJ9.eyJw......VNKcPu6Uc9Y4pCuGcdM0UwaA\"\n    export FXA_CLIENT_STATE=\"abaa31cc3b16aaf6759f2cba164a54be\"\n\n\nWith Requests\n=============\n\nUsing Firefox Account BrowserID with Requests\n---------------------------------------------\n\nYou can use the ``FxABrowserIDAuth`` to build the BrowserID assertion:\n\n.. code-block:: python\n\n    from fxa.core import Client\n    from fxa.plugins.requests import FxABrowserIDAuth\n\n    email = acct.email\n    password = \"MySecretPassword\"\n\n    raw_resp = requests.get('https://token.services.mozilla.com/1.0/sync/1.5',\n                            auth=FxABrowserIDAuth(email, password,\n                                                  with_client_state=True))\n\n    raw_resp.raise_for_status()\n    resp = raw_resp.json()\n    user_id = resp['uid']\n\n\nUsing Firefox Account Bearer Token with Requests\n------------------------------------------------\n\nYou can use the ``FxABearerTokenAuth`` to build the Bearer Token:\n\n.. code-block:: python\n\n    from fxa.core import Client\n    from fxa.plugins.requests import FxABearerTokenAuth\n\n    email = acct.email\n    password = \"MySecretPassword\"\n\n    raw_resp = requests.get('https://profile.accounts.firefox.com/v1/profile',\n                            auth=FxABearerTokenAuth(email, password,\n                                                    ['profile'], client_id))\n\n    raw_resp.raise_for_status()\n    resp = raw_resp.json()\n    user_id = resp['uid']\n\n\nWith HTTPie\n===========\n\nUsing Firefox Account BrowserID with HTTPie\n-------------------------------------------\n\nYou can use the httpie plugin provided with PyFxA to build the BrowserID request:\n\n.. code-block:: http\n\n    BID_WITH_CLIENT_STATE=True \\\n        http GET https://token.services.mozilla.com/1.0/sync/1.5 \\\n        --auth-type=fxa-browserid --auth \"email:password\" -v\n\n    GET /1.0/sync/1.5 HTTP/1.1\n    Accept: */*\n    Accept-Encoding: gzip, deflate\n    Authorization: BrowserID eyJhbG..._EqaQ\n    Connection: keep-alive\n    Host: token.services.mozilla.com\n    User-Agent: HTTPie/0.9.2\n    X-Client-State: 97b945...920fac4d4d5f0dc6...2992\n\n    HTTP/1.1 200 OK\n    Access-Control-Allow-Credentials: true\n    Access-Control-Allow-Headers: DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization,X-Conditions-Accepted\n    Access-Control-Allow-Methods: GET, POST, OPTIONS\n    Access-Control-Max-Age: 1728000\n    Connection: keep-alive\n    Content-Length: 414\n    Content-Type: application/json; charset=UTF-8\n    Date: Tue, 21 Jul 2015 10:48:42 GMT\n    X-Timestamp: 1437475722\n\n    {\n        \"api_endpoint\": \"https://sync-230-us-west-2.sync.services.mozilla.com/1.5/99283757\",\n        \"duration\": 3600,\n        \"hashalg\": \"sha256\",\n        \"id\": \"eyJub2RlI....FlYzdiMCIsICJ1aWQiOiAyMDIzODc3NX2Bvj5zv..7S2jRaw__-....eh3hiSVWA==\",\n        \"key\": \"lSw-MvgK....ebu9JsX-yXS70NkiXu....6wWgVzU0Q=\",\n        \"uid\": 99283757\n    }\n\n.. note::\n\n    You can configure the audience by settings the ``BID_AUDIENCE``\n    environment variable.\n\n\tYou can also compute the Token Server client state using the\n\t``BID_WITH_CLIENT_STATE`` environment variable.\n\n\nUsing Firefox Account Bearer Tokens with HTTPie\n-----------------------------------------------\n\nYou can use the httpie plugin provided with PyFxA to build the Bearer\ntoken request:\n\n.. code-block:: http\n\n    $ http GET https://profile.accounts.firefox.com/v1/profile \\\n        --auth-type fxa-bearer --auth \"email:password\" -v\n\n    GET /v1/profile HTTP/1.1\n    Accept: */*\n    Accept-Encoding: gzip, deflate\n    Authorization: Bearer 98e05e12ba...0d61231e88daf91\n    Connection: keep-alive\n    Host: profile.accounts.firefox.com\n    User-Agent: HTTPie/0.9.2\n\n    HTTP/1.1 200 OK\n    Connection: keep-alive\n    Content-Length: 92\n    Content-Type: application/json; charset=utf-8\n    Date: Tue, 21 Jul 2015 14:47:32 GMT\n    Server: nginx\n    access-control-allow-headers: Authorization, Content-Type, If-None-Match\n    access-control-allow-methods: GET, HEAD, POST, PUT, PATCH, DELETE, OPTIONS\n    access-control-allow-origin: *\n    access-control-expose-headers: WWW-Authenticate, Server-Authorization\n    access-control-max-age: 86400\n    cache-control: no-cache\n    content-encoding: gzip\n    etag: \"d1cf22901b3e3be527c06e27689be705bb22a172\"\n    strict-transport-security: max-age=15552000; includeSubdomains\n    vary: accept-encoding\n\n    {\n        \"email\": \"email@address.com\",\n        \"uid\": \"63b91ca4ec19ad79f320eaf5815d75e9\"\n    }\n\n.. note::\n\n    You can configure the following:\n\n      - FXA_CLIENT_ID: To choose the CLIENT_ID (default to Firefox Dev id)\n      - FXA_SCOPES: To choose the list of scopes\n      - FXA_ACCOUNT_SERVER_URL: To select the account server url\n        (default to: https://api.accounts.firefox.com/v1)\n      - FXA_OAUTH_SERVER_URL: To select the oauth server url\n        (default to: https://oauth.accounts.firefox.com/v1)\n"
},
{
  "name": "foundation-icons",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "assets",
      "favicon.png",
      "index.html",
      "js",
      "source-files",
      "style.css",
      "svgs.zip",
      "svgs"
    ]
  },
  "makefile": null,
  "readme": "#Mozilla Foundation Icons & Icon Font\n\nHello and welcome, check out the [Demo Page](http://mozilla.github.io/foundation-icons/) for a list of icons & usage instructions.\n"
},
{
  "name": "deepspeech-playbook",
  "files": {
    "/": [
      ".gitignore",
      "ALPHABET.md",
      "AM_vs_LM.md",
      "CONTINUOUS_INTEGRATION.md",
      "DATA_FORMATTING.md",
      "DEEPSPEECH.md",
      "DEPLOYMENT.md",
      "ENVIRONMENT.md",
      "EXAMPLES.md",
      "INTRO.md",
      "LICENSE.md",
      "README.md",
      "SCORER.md",
      "TESTING.md",
      "TRAINING.md",
      "images"
    ]
  },
  "makefile": null,
  "readme": "# DeepSpeech Playbook\n\nA crash course on training speech recognition models using DeepSpeech.\n\n## Quick links\n\n* [DeepSpeech on GitHub](https://github.com/mozilla/DeepSpeech)\n* [DeepSpeech documentation on ReadTheDocs](https://deepspeech.readthedocs.io/en/latest/)\n* [DeepSpeech discussions on Mozilla's Discourse forum](https://discourse.mozilla.org/c/deepspeech/247)\n* [Common Voice Datasets](https://commonvoice.mozilla.org/en/datasets)\n* [How to install Docker](https://docs.docker.com/engine/install/)\n\n## [Introduction](INTRO.md)\n\nStart here. This section will set your expectations for what you can achieve with the DeepSpeech Playbook, and the prerequisites you'll need to start to train your own speech recognition models.\n\n## [About DeepSpeech](DEEPSPEECH.md)\n\nOnce you know what you can achieve with the DeepSpeech Playbook, this section provides an overview of DeepSpeech itself, its component parts, and how it differs from other speech recognition engines you may have used in the past.\n\n## [Formatting your training data](DATA_FORMATTING.md)\n\nBefore you can train a model, you will need to collect and format your _corpus_ of data. This section provides an overview of the data format required for DeepSpeech, and walks through an example in prepping a dataset from Common Voice.\n\n## [The alphabet.txt file](ALPHABET.md)\n\nIf you are training a model that uses a different alphabet to English, for example a language with diacritical marks, then you will need to modify the `alphabet.txt` file.\n\n## [Building your own scorer](SCORER.md)\n\nLearn what the scorer does, and how you can go about building your own.\n\n## [Acoustic model and language model](AM_vs_LM.md)\n\nLearn about the differences between DeepSpeech's _acoustic_ model and _language_ model and how they combine to provide end to end speech recognition.\n\n## [Setting up your training environment](ENVIRONMENT.md)\n\nThis section walks you through building a Docker image, and spawning DeepSpeech in a Docker container with persistent storage. This approach avoids the complexities of dependencies such as `tensorflow`.\n\n## [Training a model](TRAINING.md)\n\nOnce you have your training data formatted, and your training environment established, this section will show you how to train a model, and provide guidance for overcoming common pitfalls.\n\n## [Testing a model](TESTING.md)\n\nOnce you've trained a model, you will need to validate that it works for the context it's been designed for. This section walks you through this process.\n\n## [Deploying your model](DEPLOYMENT.md)\n\nOnce trained and tested, your model is deployed. This section provides an overview of how you can deploy your model.\n\n## [Applying DeepSpeech to real world problems](EXAMPLES.md)\n\nThis section covers specific use cases where DeepSpeech can be applied to real world problems, such as transcription, keyword searching and voice controlled applications.\n\n## [Setting up Continuous Integration](CONTINUOUS_INTEGRATION.md)\n\nLearn how to set up Continuous Integration (CI) for your own fork of DeepSpeech. Intended for developers who are utilising DeepSpeech for their own specific use cases. \n\n---\n\n# Introductory courses on machine learning\n\nProviding an introduction to machine learning is beyond the scope of this PlayBook, howevever having an understanding of machine learning and deep learning concepts will aid your efforts in training speech recognition models with DeepSpeech.\n\nHere, we've linked to several resources that you may find helpful; they're listed in the order we recommend reading them in.\n\n* [Digital Ocean's introductory machine learning tutorial](https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning) provides an overview of different types of machine learning. The diagrams in this tutorial are a great way of explaining key concepts.\n\n* [Google's machine learning crash course](https://developers.google.com/machine-learning/crash-course/ml-intro) provides a gentle introduction to the main concepts of machine learning, including _gradient descent_, _learning rate_, _training, test and validation sets_ and _overfitting_.\n\n* If machine learning is something that sparks your interest, then you may enjoy [the MIT Open Learning Library's Introduction to Machine Learning course](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/course/), a 13-week college-level course covering perceptrons, neural networks, support vector machines and convolutional neural networks.\n\n---\n\n# How you can help provide feedback on the DeepSpeech PlayBook\n\nYou can help to make the DeepSpeech PlayBook even better by providing [via a GitHub Issue](https://github.com/mozilla/deepspeech-playbook/issues)\n\n* Please _try these instructions_, particularly for building a Docker image and running a Docker container, on multiple distributions of Linux so that we can identify corner cases.\n\n* Please _contribute your tacit knowledge_ - such as:\n  - common errors encountered in data formatting, environment setup, training and validation\n  - techniques or approaches for improving the scorer, alphabet file or the accuracy of Word Error Rate (WER) and Character Error Rate (CER).\n  - case studies of the work you or your organisation have been doing, showing your approaches to data validation, training or evaluation.\n\n* Please identify errors in text - with many eyes, bugs are shallow :-)\n"
},
{
  "name": "games.mozilla.org",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENCE",
      "Procfile",
      "README.md",
      "app.json",
      "package-lock.json",
      "package.json",
      "public_html",
      "server.js"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Games\n\n[Mozilla Games site](https://games.mozilla.org/)\n\n\n## Installation\n\nTo install the Node dependencies:\n\n    npm install\n\n\n## Development\n\nAll of the web content is static (see the [https://github.com/mozilla/moz-games/tree/master/public](public/) directory).\n\nTo serve the site from the simple server:\n\n    npm run dev\n\nThen launch the site from your favourite browser:\n\n[__http://localhost:8080/__](http://localhost:8080/)\n\nIf you wish to serve the site from a different port:\n\n    PORT=8000 npm run dev\n\n\n## Deployment\n\nIn production, the server is run like so:\n\n    npm start\n\nAlternatively:\n\n    npm run prod\n\nTo run the server through Heroku's [foreman](https://devcenter.heroku.com/articles/procfile):\n\n    foreman start web\n\n\n## Localisation\n\nAll webpage content is localised using [webL10n](https://github.com/fabi1cazenave/webL10n), a client-side library for internationalisation (i18n) / localisation (l10n).\n\nIf you would like to submit new translations:\n\n1. Ensure a section exists for the locale in [`l10n/locales.ini`](https://github.com/mozilla/moz-games/blob/master/public/gdc/l10n/locales.ini), followed by an `import` rule. For example:\n\n    ```properties\n    [fr]\n    @import url(data.fr.properties)\n    ```\n\n2. Open the corresponding [`.properties`](https://github.com/mozilla/moz-games/blob/master/public/gdc/l10n/data.fr.properties) file, and fill in all the translations.\n3. Open a pull request.\n\n\n## Contributing\n\n[Contributions are very welcome!](CONTRIBUTING.md)\n"
},
{
  "name": "mozlog",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# mozlog\n\n[![NPM version](https://badge.fury.io/js/mozlog.svg)](http://badge.fury.io/js/mozlog)\n[![Build Status](https://travis-ci.org/mozilla/mozlog.svg?branch=master)](https://travis-ci.org/mozilla/mozlog)\n\nA logger that outputs JSON that adheres to Heka's expected schema.\n\n```\nnpm install --save mozlog\n```\n\n## Usage\n\nYou must create a  `mozlog` instance before using it's loggers. This is\nessentially setting the `app` name, the `level`, and the `fmt`.\n\nFor the brave (or those who know `intel`'s configuration options), you\ncan pass a `config` property to have fine-grained control.\n\n```js\n// create your mozlog instance\nconst mozlog = require('mozlog')({\n  app: 'fxa-oauth-server',\n  level: 'verbose', //default is INFO\n  fmt: 'pretty', //default is 'heka'\n  uncaught: 'exit', // default is 'log', also available as 'ignore'\n  debug: true, //default is false\n  stream: process.stderr //default is process.stdout\n});\n```\n\nYou may want the level set down to `verbose` or `debug` when developing.\nLikewise, you may want the line to be readable by humans when\ndeveloping, so the `pretty` formatter will help.\n\nIn production, the defaults will serve you well: `info` and `heka`.\n\n```js\nvar log = mozlog('routes.client.register');\n\nlog.info(op, { some: details });\n// such as\nlog.debug('newClient', { id: client.id, name: client.name });\n```\n\nFirst parameter is a string \"op\". It should be unique within the file.\nSecond parameter is some optional object, with keys and values that\nwould make sense when looking at the logs again.\n\nThe `debug` option (not level) in the config will add in some asserts\nthat your usage adheres to the above: that there's only ever at most 2\narguments to a log function, the first is a string without spaces.\n"
},
{
  "name": "http-observatory-cli",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "httpobscli",
      "requirements.txt",
      "setup.cfg",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "Mozilla HTTP Observatory :: Command Line Utility\n================================================\n\nPlease note that this version of the Observatory CLI has been deprecated, and replaced with a `considerably more powerful version <https://github.com/mozilla/observatory-cli>`_.\n\nGetting started with the HTTP Observatory (docker)\n-----------------------------------------\n\n\n.. code:: bash\n\n    $ docker run --rm fgribreau/httpobs-cli www.mozilla.org\n    Score: 30 [E]\n    Modifiers:\n        [  -5] Initial redirection from http to https is to a different host, preventing HSTS\n        [  -5] Subresource Integrity (SRI) not implemented, but all external scripts are loaded over https\n        [  -5] X-Content-Type-Options header not implemented\n        [ -10] X-XSS-Protection header not implemented\n        [ -20] HTTP Strict Transport Security (HSTS) header not implemented\n        [ -25] Content Security Policy (CSP) header not implemented\n\nGetting started with the HTTP Observatory (python)\n-----------------------------------------\n\nFirst install the client:\n\npip install httpobs-cli\n\n\n.. code:: bash\n\n    $ pip install httpobs-cli\n\nAnd then scan websites to your heart's content, using our hosted\nservice:\n\n::\n\n    $ httpobs www.mozilla.org\n    Score: 30 [E]\n    Modifiers:\n        [  -5] Initial redirection from http to https is to a different host, preventing HSTS\n        [  -5] Subresource Integrity (SRI) not implemented, but all external scripts are loaded over https\n        [  -5] X-Content-Type-Options header not implemented\n        [ -10] X-XSS-Protection header not implemented\n        [ -20] HTTP Strict Transport Security (HSTS) header not implemented\n        [ -25] Content Security Policy (CSP) header not implemented\n\n    $ httpobs www.google.com\n    Score: 35 [D-]\n    Modifiers:\n        [  +5] Preloaded via the HTTP Public Key Pinning (HPKP) preloading process\n        [  -5] X-Content-Type-Options header not implemented\n        [ -20] Cookies set without using the Secure flag or set over http\n        [ -20] HTTP Strict Transport Security (HSTS) header not implemented\n        [ -25] Content Security Policy (CSP) header not implemented\n\n    $ httpobs --zero github.com\n    Score: 120 [A+]\n    Modifiers:\n        [  +5] HTTP Public Key Pinning (HPKP) header set to a minimum of 15 days (1296000)\n        [  +5] Preloaded via the HTTP Strict Transport Security (HSTS) preloading process\n        [  +5] Subresource Integrity (SRI) is implemented and all scripts are loaded from a similar origin\n        [  +5] X-Frame-Options (XFO) implemented via the CSP frame-ancestors directive\n        [   0] All cookies use the Secure flag and all session cookies use the HttpOnly flag\n        [   0] Content Security Policy (CSP) implemented with 'unsafe-inline' inside style-src\n        [   0] Content is not visible via cross-origin resource sharing (CORS) files or headers\n        [   0] Contribute.json isn't required on websites that don't belong to Mozilla\n        [   0] Initial redirection is to https on same host, final destination is https\n        [   0] X-Content-Type-Options header set to \"nosniff\"\n        [   0] X-XSS-Protection header set to \"1; mode=block\"\n\nIf you want additional options, such as to see the raw scan output, use\n``httpobs --help``:\n\n::\n\n    $ httpobs --help\n    usage: httpobs [options] host\n\n    positional arguments:\n      host           hostname of the website to scan\n\n    optional arguments:\n      -h, --help     show this help message and exit\n      -d, --debug    output only raw JSON from scan and tests\n      -r, --rescan   initiate a rescan instead of showing recent scan results\n      -v, --verbose  display progress indicator\n      -x, --hidden   don't list scan in the recent scan results\n      -z, --zero     show test results that don't affect the final score\n\nAuthors\n-------\n\n-  April King\n\nLicense\n-------\n\n-  Mozilla Public License Version 2.0\n"
},
{
  "name": "foresight",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "landscape"
    ]
  },
  "makefile": null,
  "readme": "# Emerging Technologies - Strategic Foresight & Explorations\n\nThe strategic foresight and explorations team sits in Emerging Technologies, Mozilla's R&D group. We gather weak signals of change to inform our exploration and product efforts in a wider future context. We focus on the future edges of our current research areas, new explorations, and inclusion. \n\nAll important links can be [found on our wiki here](https://wiki.mozilla.org/Strategic_Foresight_and_Explorations).\n"
},
{
  "name": "m-response-api",
  "files": {
    "/": [
      "Dockerfile",
      "Pipfile",
      "Pipfile.lock",
      "Procfile",
      "api.py"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "marketing-project-template",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "credentials.yaml",
      "mozwebqa.cfg",
      "page.py",
      "pages",
      "requirements.txt",
      "templateREADME.md",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "Mozilla WebQA Test Templates for the Marketing Products\n====================\n\nMozilla WebQA Test Templates for the Marketing Products.\n\n## tests/test_file.py\nThis contains a template to create a test file to hold test cases\n\n## pages/page_object.py\nThis contains a template to create a Page Objects for a project\n\nLicense\n-------\nThis software is licensed under the [MPL] 2.0:\n\n    This Source Code Form is subject to the terms of the Mozilla Public\n    License, v. 2.0. If a copy of the MPL was not distributed with this\n    file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n[MPL]: http://www.mozilla.org/MPL/2.0/"
},
{
  "name": "vautomator-serverless",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.rst",
      "docs",
      "handler.py",
      "kms_policy.json",
      "lib",
      "package.json",
      "requirements.txt",
      "scanners",
      "serverless.yml",
      "setup.cfg",
      "tests",
      "vendor"
    ],
    "/docs": [
      "Makefile",
      "conf.py",
      "index.rst",
      "make.bat",
      "setup.rst",
      "troubleshoot.rst",
      "usage.rst"
    ]
  },
  "makefile": "ROOT_DIR\t:= $(shell dirname $(realpath $(lastword $(MAKEFILE_LIST))))\nAWS_REGION\t:=\nAWS_PROFILE :=\nTENABLE_IO := N\nKMS_POLICY_FILE :=\nKMS_KEYID :=\nCUSTOM_DOMAIN :=\n\nifeq ($(TENABLE_IO), Y)\n  $(info Tenable.io support enabled, make sure you passed API keys as variables!)\n  TENABLE_ACCESS_KEY = ${TIOA}\n  TENABLE_SECRET_KEY = ${TIOS}\n  ifeq ($(KMS_POLICY_FILE), )\n    ifeq ($(KSM_KEYID), )\n      DEFAULT_KMS = Y\n\tendif\n  else\n    KMS_DESCRIPTION := \"KMS key for vautomator-serverless\"\n    KMS_KEYID := $(shell aws --profile '$(AWS_PROFILE)' kms create-key --policy \\\n\tfile://./'$(KMS_POLICY_FILE)' --description '$(KMS_DESCRIPTION)' --query 'KeyMetadata.KeyId')\n  endif\nelse\n  $(info Tenable.io support disabled.)\nendif\n\nall:\n\t@echo 'Available make targets:'\n\t@grep '^[^#[:space:]^\\.PHONY.*].*:' Makefile\n\n.SILENT: setup\n.PHONY: setup\nifeq ($(CUSTOM_DOMAIN), Y)\n  $(info Configuring custom domain, make sure you set it up in serverless.yml!)\n  setup:\n\texport AWS_SDK_LOAD_CONFIG=true && \\\n\tnpm install serverless-domain-manager --save-dev && \\\n\tsls create_domain --aws-profile $(AWS_PROFILE)\nendif\nifdef DEFAULT_KMS\n  setup:\n\texport AWS_SDK_LOAD_CONFIG=true && \\\n\taws --profile $(AWS_PROFILE) ssm put-parameter --name \"TENABLEIO_ACCESS_KEY\" \\\n\t--value $(TENABLE_ACCESS_KEY) --type SecureString --overwrite && \\\n\taws --profile $(AWS_PROFILE) ssm put-parameter --name \"TENABLEIO_SECRET_KEY\" \\\n\t--value $(TENABLE_SECRET_KEY) --type SecureString --overwrite\nelse\n  ifdef KMS_KEYID\n  setup:\n\texport AWS_SDK_LOAD_CONFIG=true && \\\n\taws --profile $(AWS_PROFILE) ssm put-parameter --name \"TENABLEIO_ACCESS_KEY\" \\\n\t--value $(TENABLE_ACCESS_KEY) --type SecureString --key-id $(KMS_KEYID) --overwrite && \\\n\taws --profile $(AWS_PROFILE) ssm put-parameter --name \"TENABLEIO_SECRET_KEY\" \\\n\t--value $(TENABLE_SECRET_KEY) --type SecureString --key-id $(KMS_KEYID) --overwrite\n  endif\nendif\n\n.PHONY: validate\nvalidate: export AWS_SDK_LOAD_CONFIG=true\nvalidate:\n\tsls deploy --noDeploy --region $(AWS_REGION) --aws-profile $(AWS_PROFILE)\n\n.PHONY: deploy\ndeploy: export AWS_SDK_LOAD_CONFIG=true\ndeploy:\n\tnpm install serverless-python-requirements --save-dev && \\\n\tnpm install serverless-pseudo-parameters --save-dev && \\\n\tnpm install serverless-apigw-binary --save-dev && \\\n\tnpm install serverless-step-functions --save-dev && \\\n\tnpm install serverless-prune-plugin --save-dev && \\\n\tnpm install serverless-domain-manager --save-dev\n\tsls deploy --region $(AWS_REGION) --aws-profile $(AWS_PROFILE)\n\n.PHONY: test\ntest:\n\tpython -m pytest tests/\n\n.PHONY: flake8\nflake8:\n\tflake8 ./*py\n\tflake8 lib/*py\n\tflake8 scanners/*py\n\tflake8 tests/*py\n\n.PHONY: clean\nclean:\n\tfind . -name .pytest_cache -type d -exec rm -rf {}\\;\n\tfind . -name __pycache__ -type d -exec rm -rf {}\\;",
  "readme": "**************************\nvautomator-serverless\n**************************\n\nThis project uses serverless framework and attempts to create a\nserverless environment that could be used to automate vulnerability\nassessment tasks from multiple ingestion points, such as on-demand\nsubmission of a host via a REST API, regular scanning of a known list of hosts, and opportunistically scanning of hosts appearing in Certificate Transparency logs.\n\nThis is under development with more features being added as different\nbranches. The tool currently supports:\n\n*   A single API endpoint (``/scan``) which performs all scans on a given host, and emails the results to desired email address(es).\n*   Addition of a target to the scan queue for port scan by an API endpoint (``/ondemand/portscan``). \n*   Addition of a target to the scan queue for HTTP Observatory scan by an API endpoint (``/ondemand/httpobservatory``) \n*   Addition of a target to the scan queue for TLS Observatory scan by an API endpoint (``/ondemand/tlsobservatory``) \n*   Addition of a target to the scan queue for SSH Observatory scan by an API endpoint (``/ondemand/sshobservatory``)\n*   Addition of a target to the scan queue for a directory enumeration scan (currently with ``dirb``) by an API endpoint (``/ondemand/direnum``)\n*   Addition of a target to the scan queue for a Google web search by an API endpoint (``/ondemand/websearch``)\n*   [OPTIONAL] Addition of a target to the scan queue for a Tenable.io scan by an API endpoint (``/ondemand/tenablescan``)\n*   Performing requested scan type (port, HTTP Observatory, TLS Observatory or SSH Observatory) on hosts in the queue\n*   Scheduled port scans from a hard-coded list of hosts (disabled by default)\n*   Scheduled directory enumeration scans (via ``dirb``) from a hard-coded list of hosts (disabled by default)\n*   Scheduled HTTP Observatory scans from a hard-coded list of hosts (disabled by default)\n*   Scheduled TLS Observatory scans from a hard-coded list of hosts (disabled by default)\n*   Scheduled SSH Observatory scans from a hard-coded list of hosts (disabled by default)\n*   An endpoint to retrieve the scan results for a given host (``/results``)\n*   Manually add a host to the scan queue (for PoC purposes).\n\nAll API endpoints are currently protected by an API key. Ideally this should be replaced with SSO integration.\n\nResults from all scans are placed in an S3 bucket specified in\n``serverless.yml``.\n\nPort scans are performed using a `statically compiled nmap\nbinary <https://github.com/ernw/static-toolbox/releases/download/1.0.2/nmap-7.70SVN-b5bd185-x86_64-portable.zip>`_,\n`packaged within the serverless\napplication <https://github.com/mozilla/vautomator-serverless/blob/master/serverless.yml#L64-L66>`_.\n\nDirectory enumeration scans are performed via ``dirb``, compiled\nspecifically for Amazon Linux and the binary and all supporting files\npackaged within the serverless application, similar to the ``nmap``\nbinary.\n\n.. note:: UDP port scans are not supported as Lamdba functions can not run as root/privileged users.\n\nSetup\n===========\n\nPlease refer to the setup steps `here <https://vautomator-serverless.rtfd.io/en/latest/setup.html>`_.\n\nOn-demand Scan REST APIs\n=========================\n\nPlease refer to REST API documentation `here <https://vautomator-serverless.rtfd.io/en/latest/usage.html>`_.\n"
},
{
  "name": "mattermost-heroku",
  "files": {
    "/": [
      "LICENSE",
      "Procfile",
      "README.md",
      "app.json",
      "bin",
      "default-config.json",
      "start.sh"
    ]
  },
  "makefile": null,
  "readme": "# Deploy Mattermost Team or Enterprise Edition to Heroku\n\n[![Deploy](https://www.herokucdn.com/deploy/button.svg)](https://heroku.com/deploy)\n\nThis buildpack is an [inline buildpack](https://github.com/kr/heroku-buildpack-inline/) (tldr: this repo deploys to Heroku and uses itself as a buildpack) for deploying [Mattermost](https://mattermost.org) to [Heroku](https://heroku.com).\nIt must be used in tandem with [This customized Nginx Buildpack](https://github.com/cadecairos/nginx-buildpack) that allow mattermost to communicate with Nginx using a TCP port instead of a socket.\n\n\n### Known to work with Mattermost 4.5.0 Team and Enterprise editions\n\n## Configuration options\n\n### Buildpack Specific Variables\n\nSet `MATTERMOST_VERSION` to the release version you'd like to install. i.e. '4.5.0'\nSet `MATTERMOST_TYPE` to either 'team' or 'enterprise'\n\n### Mattermost Configuration\n\nMattermost-Heroku supports every configuration option available in Mattermost (although some make no sense to use with Mattermost+Nginx on Heroku, i.e. \"Forward80To443\"). To set an environment variable, prefix it with \"MM_\" followed by the setting type, and then the setting key, in all upper case. [You can read up about how this works in the Mattermost configuration documentation](https://docs.mattermost.com/administration/config-settings.html#configuration-settings)\n\n## Rebuilding\n\nIf you maintain a fork of this repo, you can link the Heroku app to your fork, which will enable you to build from the Heroku dashboard's deploy page.\n\nAn alternative to this would be to use the [Heroku Build API to create a new build](https://devcenter.heroku.com/articles/build-and-release-using-the-api#creating-builds)\n\nFor example, a curl request like this would do the trick (substitute the right variables):\n\n```bash\ncurl -n -X POST https://api.heroku.com/apps/$YOUR_APP/builds \\\n-d '{\"source_blob\": {\"url\":\"https://github.com/mozilla/mattermost-heroku/archive/{$LATEST_BUILDPACK_VERSION}.tar.gz\", \"version\": \"${COMMIT_HASH}\"}}' \\\n-H 'Accept: application/vnd.heroku+json; version=3' \\\n-H \"Content-Type: application/json\"\n```\n\nThe \"version\" parameter is optional in the example above.\n\nThis curl request can be made manually from a developer's machine, or it can be set up as a job in something like [Jenkins]()https://jenkins.io/. Keep in mind that Authorization headers will need to be included in the request.\n\nAlso, the example above assumes that the machine it's being run on has heroku.com credentials saved in your `~/.netrc` file.\n\n## Warnings\n\n1. Don't make configuration changes in the Mattermost admin console.\n   Any configuration changes in the Mattermost admin console will be lost on dyno restart (which may be every 24 hours) and will not be distributed across multiple dynos.\n2. Not using s3 means any uploads will be lost on dyno restart or application reconfiguration or redeploy and won't be consistent across multiple dynos.\n   Without s3 backing this is not anything more than a one time demo.\n\n## Enterprise Edition\nActivate it as instructed in the docs https://docs.mattermost.com/install/ee-install.html\n"
},
{
  "name": "video-bg-play",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "_locales",
      "icon.svg",
      "manifest.json",
      "video-bg-play-content.js"
    ]
  },
  "makefile": "FILES := manifest.json \\\n         video-bg-play-content.js \\\n         $(wildcard _locales/*/messages.json) \\\n         icon.svg \\\n         README.md\n\nvideo-bg-play.zip: $(FILES) Makefile\n\trm -f video-bg-play.xpi\n\tzip video-bg-play.xpi $(FILES)\n",
  "readme": "# Video Background Play Fix  ![logo](/icon.svg)\n\nFirefox for Android can continue playing video even if you switch to another tab or app.\nHowever, sites can detect these user actions with the [Page Visibility API](https://developer.mozilla.org/en-US/docs/Web/API/Page_Visibility_API) and the [Fullscreen API](https://developer.mozilla.org/en-US/docs/Web/API/Fullscreen_API).\nThis add-on is designed to block events and properties exposed by the APIs.\n\n## License\n\nAdd-on code: MIT.\n\n\"[Glasses](https://thenounproject.com/term/glasses/1473422)\" icon used in the logo by rahmatmasiv from [the Noun Project](https://thenounproject.com/)  under the [CC BY 3.0 US](https://creativecommons.org/licenses/by/3.0/us/).\n\n## Technical detail\n\nThe add-on injects a content script to replace the properties exposed, and stops events from propagating when applicable.\n\n### Page Visibility API\n\nThe add-on blocks `visibilitychange` event, and set `document.hidden` to be always `false` and `document.visibilityState` to be forever `visible`.\n\n### Fullscreen API\n\nThe add-on doesn't generally override the Fullscreen API because at the moment this is not required and the original implementation caused some broken UI after existing fullscreen.\nAs a site-specific workaround, we do however block `fullscreenchange` events on Vimeo to prevent playback from stopping when exiting fullscreen.\n\n### User activity tracking\n\nSome pages stop playback if they don't detect any user activity for a certain amount of time. To avoid this, the add-on ensures that the time of the last user activity is regularly updated.\n\n## Sites\n\nAs a demonstration, the content script currently injects itself to the following sites:\n\n* youtube.com and youtube-nocookie.com\n* vimeo.com\n"
},
{
  "name": "commonware",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "commonware",
      "contribute.json",
      "examples",
      "fabfile.py",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "==========\nCommonware\n==========\n\n.. image:: https://img.shields.io/travis/mozilla/commonware/master.svg\n    :target: https://travis-ci.org/mozilla/commonware\n.. image:: https://img.shields.io/pypi/v/commonware.svg\n    :target: https://pypi.python.org/pypi/commonware\n\nCommonware is a collection of small but useful tools for Django.\n\nThey seemed too small to be worth their own packages, but we also wanted\nto share them. So here they are.\n\nNote\n====\n\nAs of version 0.6.0 the minimum supported Python version is 3.6 and the minimum Django is 2.2.\nIf you need support for older versions you should use version 0.5.0.\n"
},
{
  "name": "builders",
  "files": {
    "/": [
      ".gitignore",
      "CNAME",
      "LICENSE",
      "README.md",
      "alumni.html",
      "assets",
      "connect",
      "faqs.html",
      "favicon.ico",
      "index.html",
      "programs.html",
      "rules.html",
      "springlab",
      "who-we-fund.html"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla\u2019s Fix-The-Internet Spring MVP Lab Annoucement Website\n\n\n**Visit [http://www.mozilla.org/builders](http://www.mozilla.org/builders) for additional details and information on how to apply by the April 6, 2020 deadline.**\n\nThe internet's amazing, weird, sometimes a little icky. Let\u2019s face it: it could use some fixing. Are you a student, coder, designer, creator who wants to help us build new services and platforms that foster collaboration during this crisis, or put privacy first, or combat filter bubbles and polarization, or AI platforms that work for the end user, the citizen? Mozilla\u2019s Fix-The-Internet MVP Lab is an 8 week-long incubator-style program this Spring to mobilize & fund around products and technologies that enable everyone to connect and build a better society.\n\n## About\n\nMozilla\u2019s Fix-The-Internet Incubator was started by a leading computer scientist & product leader (currently on faculty at Harvard), the former CTO of Joyent (node.js) and SVP Firefox, and two seasoned entrepreneurs (with a combined 10 exits and 25 #1 app store hits). Join us and a world class group of your peers in learning together, and building the next billion-user application that makes the internet live up to its full potential.\n\n## Contact\n\n[fixspringlab@mozilla.com](mailto:fixspringlab@mozilla.com)\n"
},
{
  "name": "imo-placeholder",
  "files": {
    "/": [
      "LICENSE",
      "assets",
      "index.html"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "action-tmate",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "LICENSE",
      "README.md",
      "_config.yml",
      "action.yml",
      "babel.config.js",
      "docs",
      "jest.config.js",
      "lib",
      "package-lock.json",
      "package.json",
      "src"
    ],
    "/docs": [
      "checks-tab.png",
      "contributors.md"
    ],
    "/.github": [
      "CODEOWNERS",
      "FUNDING.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Debug your [GitHub Actions](https://github.com/features/actions) by using [tmate](https://tmate.io)\n\n[![GitHub Actions](https://github.com/mxschmitt/action-tmate/workflows/Node.js%20CI/badge.svg)](https://github.com/mxschmitt/action-tmate/actions)\n[![GitHub Marketplace](https://img.shields.io/badge/GitHub-Marketplace-green)](https://github.com/marketplace/actions/debugging-with-tmate)\n\nThis GitHub Action offers you a direct way to interact with the host system on which the actual scripts (Actions) will run.\n\n## Features\n\n- Debug your GitHub Actions by using SSH or Web shell\n- Continue your Workflows afterwards\n\n## Supported Operating Systems\n\n- Linux\n- macOS\n- Windows\n\n## Getting Started\n\nBy using this minimal example a [tmate](https://tmate.io) session will be created.\n\n```yaml\nname: CI\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Setup tmate session\n      uses: mxschmitt/action-tmate@v3\n```\n\nTo get the connection string, just open the `Checks` tab in your Pull Request and scroll to the bottom. There you can connect either directly per SSH or via a web based terminal.\n\n![GitHub Checks tab](./docs/checks-tab.png \"GitHub Checks tab\")\n\n## Without sudo\n\nBy default we run the commands using sudo. If you get `sudo: not found` you can use the parameter below to execute the commands directly.\n\n```yaml\nname: CI\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Setup tmate session\n      uses: mxschmitt/action-tmate@v3\n      with:\n        sudo: false\n```\n\n## Timeout\n\nBy default the tmate session will remain open until the workflow times out. You can [specify your own timeout](https://docs.github.com/en/free-pro-team@latest/actions/reference/workflow-syntax-for-github-actions#jobsjob_idstepstimeout-minutes) in minutes if you wish to reduce GitHub Actions usage.\n\n```yaml\nname: CI\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Setup tmate session\n      uses: mxschmitt/action-tmate@v3\n      timeout-minutes: 15\n```\n\n## Only on failure\nBy default a failed step will cause all following steps to be skipped. You can specify that the tmate session only starts if a previous step [failed](https://docs.github.com/en/actions/reference/context-and-expression-syntax-for-github-actions#failure).\n\n```yaml\nname: CI\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Setup tmate session\n      if: ${{ failure() }}\n      uses: mxschmitt/action-tmate@v3\n```\n\n## Use registered public SSH key(s)\n\nBy default anybody can connect to the tmate session. You can opt-in to install the public SSH keys [that you have registered with your GitHub profile](https://docs.github.com/en/github/.authenticating-to-github/adding-a-new-ssh-key-to-your-github-account).\n\n```yaml\nname: CI\non: [push]\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v2\n    - name: Setup tmate session\n      uses: mxschmitt/action-tmate@v3\n      with:\n        limit-access-to-actor: true\n```\n\nIf the registered public SSH key is not your default private SSH key, you will need to specify the path manually, like so: `ssh -i <path-to-key> <tmate-connection-string>`.\n\n## Continue a workflow\n\nIf you want to continue a workflow and you are inside a tmate session, just create a empty file with the name `continue` either in the root directory or in the project directory by running `touch continue` or `sudo touch /continue`.\n\n## Connection string / URL is not visible\n\nThe connection string will be written in the logs every 5 seconds. For more information checkout issue [#1](https://github.com/mxschmitt/action-tmate/issues/1).\n"
},
{
  "name": "openwpm-utils",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "openwpm_utils",
      "requirements.txt",
      "setup.cfg",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "\n# `openwpm_utils`\n\nA collection of utilities for working with OpenWPM datasets\n\nThe domain\\_utils are available as a standalone package [domain_utils](github.com/mozilla/domain_utils)\n\n\n## Installation\n\n    $ pip install openwpm-utils\n\nOr\n\n    $ python setup.py install\n"
},
{
  "name": "browserid-local-verify",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "ChangeLog",
      "LICENSE",
      "README.md",
      "bin",
      "lib",
      "package.json",
      "testing.js",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/mozilla/browserid-local-verify.png?branch=master)](https://travis-ci.org/mozilla/browserid-local-verify)\n\n# A node.js BrowserID verification library\n\nThis repository contains a node.js library for local verification of BrowserID assertions. It is used by [this standalone verifier](https://github.com/mozilla/browserid-verifier).\n\nThe library has the following scope and features:\n\n  * **authoritative domain lookup** - given a domain, follow the browserid standard to resolve it (following authority delagation) into the ultimatly responsible domain and its public key.\n  * **.well-known document parsing**\n  * **(multiple) secondary IdP support** - The library can be initialized with a set of trusted \"fallback IdPs\" that are considered authoritative when lookup fails (no support document can be found).\n  * **external HTTP implementation** - You can use node.js's http implementation, or override it and support your own (useful for HTTP proxied environments)\n  * **command line tools** - all of these features are exposed via command line tools for manual inspection of domain's persona configuration.\n  * **assertion verification** - the features above all fuel a simple yet flexible API for local verification of assertions.\n\n# Alternatives\n\nThis library is targeted at robust local verification, to subsume all of the features required by mozilla's implementation of assertion verification in [Persona][].  If you're looking for a clean and simple library appropriate for website use (using the verifier hosted by persona), see [browserid-verify][].\n\n[Persona]: https://persona.org\n[browserid-verify]: https://npmjs.org/browserid-verify\n\n# USAGE\n\n    npm install browserid-local-verify\n\n## (simple) verifying an assertion\n\n    var browserid = require('browserid-local-verify');\n\n    browserid.verify({\n      assertion: assertion,\n      audience:  \"http://example.com\"\n    }, function(err, details) {\n      console.log(details);\n    });\n\n## looking up an authority for a domain\n\n    var browserid = require('browserid-local-verify');\n\n    browserid.lookup({\n      domain: \"mozilla.org\"\n    }, function(err, details) {\n      // check err\n      console.log(details.authority);\n      console.log(details.pubKey);\n      console.log(details.delegationChain);\n    });\n\n## configuration\n\nAll functions accept configuration parameters documented below.\n\n    browserid.lookup({\n      httpTimeout: 5.0,\n      domain: \"mozilla.org\"\n    }, function(err, details) {\n      ...\n    });\n\nOr you can allocate a library instance.  This allows you to specify configuration once at instantiation time.  Any configuration parameters or function arguments may be specified a instantiation time and become the default for subsequently invoked functions:\n\n    var BrowserID = require('browserid-local-verify');\n\n    var b = new BrowserID({ httpTimeout: 20.0 });\n    b.lookup({ domain: \"mozilla.org\" }, function(err, details) {\n      // ...\n    });\n\n## Configuration and Arguments\n\n### Common Arguments\n\n* **httpRequest**: A function that allows the client to control how http requests are performed.\n  * input arguments: (domain, path, callback)\n  * callback argbuments: (err, statusCode, headers, body)\n* **httpTimeout**: How long in seconds we should wait for a response when looking up a well-known document over HTTP. (default: 10)\n* **maxDelegations**: How many times authority may be delegated.\n* **insecureSSL**: When true, invalid SSL certificates are ignored (NEVER use this in production).\n* **fallback**: A domain that is authoritative when support document lookup fails for the prinicpal email address's domain.\n\n### lookup specific\n\n* **domain**: the domain for which we should lookup the support document\n* **principalDomain**: the domain of the email address for which we should discover the support document of the authority\n\n### verification specific\n\n* **now**: override the current time for purposes of assertion verification. (useful for testing)\n* **assertion**: the assertion to verify\n* **audience**: the expected assertion audience\n* **trustedIssuers**: An array of domains that will be trusted to vouch for any identity, regardless of the authority as determined from the email addresses domain.\n\n## debug output and metrics\n\nThe BrowserID class emits events:\n\n    var b = new BrowserID();\n\n    b.on('debug', function(msg) {\n      console.log('debug output:', msg);\n    });\n\n    b.on('metric', function(metric, value) {\n      console.log(metric + \":\", value);\n    });\n\n    b.lookup(\"mozilla.org\", function(err, details) {\n      // ...\n    });\n"
},
{
  "name": "outreachy-datascience-2021",
  "files": {
    "/": [
      "2nd_contribution",
      "README.md",
      "initial_contribution"
    ]
  },
  "makefile": null,
  "readme": "# outreachy-datascience-2021\n\n[![Gitter](https://badges.gitter.im/mozilla-datascience-outreach/community.svg)](https://gitter.im/mozilla-datascience-outreach/community?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\nThis is the main repo used for prospective 2021 Outreachy interns to contribute Mozilla Data team projects.\n\nIt covers the project:\n\n* [Identifying Usage Personalities in Firefox](https://www.outreachy.org/outreachy-may-2021-internship-round/communities/firefox-data-team/#identifying-usage-personalities-in-firefox)\n\n# Contribution\nThe initial contribution process steps are the following:\n1. Clone this repo\n   - This will download the relevant dataset in [initial_contribution](https://github.com/mozilla/outreachy-datascience-2021/tree/master/initial_contribution)\n2. Read through the [initial_contribution/README.md](https://github.com/mozilla/outreachy-datascience-2021/blob/master/initial_contribution/README.md) file.\n   - This contains the instructions on how to contribute.  \n3. Submit your work via email to one of the mentors. \n\nThe subsequent contribution step is described in `2nd_contribution`.\n\n# Submission\n  * Deadline for `initial_contribution` submissions are April 15th, 2021. \n  * Deadline for `2nd_contribution` submissions are April 22nd, 2021.\n  * Please submit your final notebook/script by email to one of the mentors.\n\nPlease feel free to ask any questions you have on [Gitter](https://gitter.im/mozilla-datascience-outreach/community?utm_source=share-link&utm_medium=link&utm_campaign=share-link). \n"
},
{
  "name": "doozer",
  "files": {
    "/": [
      ".gitignore",
      "README.rst",
      "__init__.py",
      "apps",
      "docs",
      "log_settings.py",
      "manage.py",
      "media",
      "migrations",
      "requirements",
      "settings.py",
      "templates",
      "urls.py",
      "wsgi"
    ],
    "/docs": [
      "quickstart.rst"
    ]
  },
  "makefile": null,
  "readme": "=================================================\nDoozer - JavaScript killed the ActionScript star.\n=================================================\n\nDoozer is a Django-powered platform for fun!\n"
},
{
  "name": "schema-dictionary",
  "files": {
    "/": [
      ".circleci",
      ".dependabot",
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.js",
      ".storybook",
      "docs",
      "package.json",
      "postcss.config.js",
      "public",
      "rollup.config.js",
      "scripts",
      "src",
      "stories",
      "style.css.in",
      "tailwind.config.js"
    ],
    "/docs": [
      "development.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "setup-msys2",
  "files": {
    "/": [
      ".eslintrc.json",
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "HACKING.md",
      "LICENSE",
      "README.md",
      "action.yml",
      "main.js",
      "package-lock.json",
      "package.json",
      "release.sh",
      "test.sh"
    ],
    "/.github": [
      "dependabot.yml",
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "<p align=\"center\">\n  <a title=\"msys2.github.io\" href=\"https://msys2.github.io\"><img src=\"https://img.shields.io/website.svg?label=msys2.github.io&longCache=true&style=flat-square&url=http%3A%2F%2Fmsys2.github.io%2Findex.html&logo=github\"></a><!--\n  -->\n  <a title=\"Join the chat at https://gitter.im/msys2/msys2\" href=\"https://gitter.im/msys2/msys2\"><img src=\"https://img.shields.io/badge/chat-on%20gitter-4db797.svg?longCache=true&style=flat-square&logo=gitter&logoColor=e8ecef\"></a><!--\n  -->\n  <a title=\"'action' workflow Status\" href=\"https://github.com/msys2/setup-msys2/actions?query=workflow%3Aaction\"><img alt=\"'action' workflow Status\" src=\"https://img.shields.io/github/workflow/status/msys2/setup-msys2/action?longCache=true&style=flat-square&label=action&logo=github\"></a><!--\n  -->\n  <a title=\"Dependency Status\" href=\"https://david-dm.org/msys2/setup-msys2\"><img src=\"https://img.shields.io/david/msys2/setup-msys2.svg?longCache=true&style=flat-square&label=deps&logo=npm\"></a>\n</p>\n\n# Setup MSYS2\n\n**setup-msys2** is a JavaScript GitHub Action (GHA) to setup an [MSYS2](https://www.msys2.org/) environment (i.e. MSYS, MINGW32 and/or MINGW64 shells) using the GHA [toolkit](https://github.com/actions/toolkit) for automatic caching.\n\n## Context\n\n[MSYS2](https://www.msys2.org/) is available by default in [windows-latest](https://github.com/actions/virtual-environments/blob/master/images/win/Windows2019-Readme.md#msys2) [virtual environment](https://github.com/actions/virtual-environments) for GitHub Actions, located at `C:\\msys64`. Moreover, there is work in progress for making `bash` default to MSYS2 (see [actions/virtual-environments#1525](https://github.com/actions/virtual-environments/issues/1525)). However, the default installation has some caveats at the moment (see [actions/virtual-environments#1572](https://github.com/actions/virtual-environments/issues/1572)):\n\n- It is updated every ~10 days.\n- It includes a non-negligible set of pre-installed packages. As a result, update time can be up to 10 min.\n- Caching of installation packages is not supported.\n- MSYS2/MINGW are neither added to the PATH nor available as a custom `shell` option.\n\n**setup-msys2** works around those constraints:\n\n- Using option `release: false`, the default installation is used, but automatic caching is supported and a custom entrypoint is provided.\n- By default (`release: true`), **setup-msys2** downloads and extracts the latest tarball available at [repo.msys2.org/distrib/x86_64](http://repo.msys2.org/distrib/x86_64/), a clean and up-to-date environment is set up in a temporary location, and a custom entrypoint (`msys2`) is provided. Hence, the overhead of updating pre-installed but unnecessary packages is avoided.\n\nTherefore, usage of this Action is recommended to all MSYS2 users of GitHub Actions, since caching and the custom entrypoint are provided regardless of option `release`.\n\n## Usage\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n```\n\nThen, for scripts:\n\n```yaml\n  - shell: msys2 {0}\n    run: |\n      uname -a\n```\n\nIt is also possible to execute specific commands from cmd/powershell scripts/snippets. In order to do so, `-c` is required:\n\n```yaml\n  - shell: powershell\n    run: msys2 -c 'uname -a'\n```\n\n```yaml\n  - shell: cmd\n    run: msys2 -c 'uname -a'\n```\n\n### Default shell\n\nIn order to reduce verbosity, it is possible to set `msys2` as the default shell. For example:\n\n```yaml\n  defaults:\n    run:\n      shell: msys2 {0}\n  steps:\n  - uses: msys2/setup-msys2@v2\n    with:\n      update: true\n      install: >-\n        base-devel\n        git\n  #- run: git config --global core.autocrlf input\n  #  shell: bash\n  - uses: actions/checkout@v2\n  - run: git describe --dirty\n```\n\nNote that setting `autocrlf` is required in specific use cases only. See [actions/checkout#250](https://github.com/actions/checkout/issues/250).\n\n### Build matrix\n\nIt is common to test some package/tool on MINGW32 (32 bit) and MINGW64 (64 bit), which typically requires installing different sets of packages through option `install`. GitHub Actions' `strategy` and `matrix` fields allow to do so, as explained in [docs.github.com: Configuring a build matrix](https://docs.github.com/en/actions/configuring-and-managing-workflows/configuring-a-workflow#configuring-a-build-matrix) and [docs.github.com: `jobs.<job_id>.strategy.matrix`](https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions#jobsjob_idstrategymatrix). See, for example:\n\n- [msys2/MINGW-packages: .github/workflows/main.yml](https://github.com/msys2/MINGW-packages/blob/master/.github/workflows/main.yml).\n- [ghdl/ghdl: .github/workflows/push.yml](https://github.com/ghdl/ghdl/blob/99b542c849311c92e87e2c70d283de133c9d4093/.github/workflows/push.yml#L56-L102).\n\nFind further details at [#40](https://github.com/msys2/setup-msys2/issues/40) and [#102](https://github.com/msys2/setup-msys2/issues/102).\n\n### Options\n\n#### msystem\n\nBy default, `MSYSTEM` is set to `MINGW64`. However, an optional parameter named `msystem` is supported, which expects `MSYS`, `MINGW64` or `MINGW32`. For example:\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n    with:\n      msystem: MSYS\n```\n\nFurthermore, the environment variable can be overridden. This is useful when multiple commands need to be executed in different contexts. For example, in order to build a PKGBUILD file and then test the installed artifact:\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n    with:\n      msystem: MSYS\n  - shell: msys2 {0}\n    run: |\n      makepkg-mingw -sCLfc --noconfirm --noprogressbar\n      pacman --noconfirm -U mingw-w64-*-any.pkg.tar.xz\n  - run: |\n      set MSYSTEM=MINGW64\n      msys2 -c '<command to test the package>'\n```\n\n#### path-type\n\nDefines which parts of the Windows `$env:PATH` environment variable leak into the MSYS2 environment. Allowed values:\n\n- `strict`: do not inherit anything from `$env:PATH`.\n- `minimal` *(default)*: only inherit the default Windows paths from `$env:PATH` (so that `cmd.exe` and `powershell.exe` are available for example).\n- `inherit`: inherit everything; warning: this can lead to interference with other tools installed on the system.\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n    with:\n      path-type: minimal\n```\n\nThis option corresponds to the `MSYS2_PATH_TYPE` setting in MSYS2; hence it can be set per step through `env`. See [msys2/MSYS2-packages: filesystem/profile](https://github.com/msys2/MSYS2-packages/blob/915946a637e1f2b7e26e32782f3af322009293db/filesystem/profile#L28-L45) for further details about the configuration of each option.\n\n#### release\n\nBy default (`true`), retrieve and extract base installation from upstream GitHub Releases. If set to `false`, the installation available in the virtual environment is used:\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n    with:\n      release: false\n```\n\n#### update\n\nBy default, the installation is not updated; hence package versions are those of the installation tarball. By setting option `update` to `true`, the action will try to update the runtime and packages cleanly:\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n    with:\n      update: true\n```\n\n#### install\n\nInstalling additional packages after updating the system is supported through option `install`. The package or list of packages are installed through `pacman --noconfirm -S --needed`.\n\n```yaml\n  - uses: msys2/setup-msys2@v2\n    with:\n      update: true\n      install: >-\n        git\n        base-devel\n```\n"
},
{
  "name": "fxa-dev",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "aws",
      "roles"
    ]
  },
  "makefile": null,
  "readme": "# AWS Ansible-based docker development environment for Firefox Accounts\n\n* [Prerequisites](#prerequisites)\n* [Usage](#usage)\n  * [SSH](#ssh)\n  * [Custom Docker tags](#custom-docker-tags)\n  * [Docker Stopped|Started](#docker-stoppedstarted)\n  * [Custom fxa-dev branch](#custom-fxa-dev-branch)\n* [Layout Notes](#layout-notes)\n* [Example urls](#example-urls)\n\n## Prerequisites\n\n- [ansible](http://docs.ansible.com/intro_installation.html) >=2.2\n- [boto](https://github.com/boto/boto#installation) >=2.6\n\n### macOS\n\nUse: `brew install ansible && pip install boto3`\n\n## Usage\n\nTo run on AWS change directory to `aws`\n```sh\ncd aws\n```\n\n1. Set the `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` environment variables\n3. create a `environments/foo.yml` file ('foo' can be anything)\n  a) see `environments/EXAMPLE.yml` for a base reference\n  b) it is recommended that you set values for `owner` and `reaper_spare_me`\n4. run `make foo`\n\nAfter the cloudformation stacks has been created, `cloud-init` will run an\ninitial `ansible` playbook to set up the box. A cronjob run every 10 minutes\nwill pick up changes as needed. The logs for the initial playbook run are in\n`/var/log/cloud-init-output.log`. If the cloudformation was created OK, but\nthe services do not come up, check that log output for why.\n\n### SSH\n\nYou can ssh into the EC2 instance with `ssh ec2-user@meta-{{ whatever you configured in foo.yml }}`. \n\n### Custom Docker tags\n\nBy default, the `latest` tag will be used. This can be adjusted\nto use other image tags by setting any of `{auth_docker_tag,\nauthdb_docker_tag, content_docker_tag,\ncustoms_docker_tag, oauth_docker_tag, profile_docker_tag, rp_docker_tag}` in\nyour environments/foo.yml configuration file. \n\n> NOTE: you must commit and push changes to that file to affect an existing EC2 instance.\n\n### Docker stopped|started:\n\nBy default, all docker containers are 'started'. If\nyou want to selectively keep a service 'stopped', you can set any of\n`{auth_docker_state, authdb_docker_state, basket_docker_state, content_docker_state,\ncustoms_docker_state, oauth_docker_state, profile_docker_state, rp_docker_state}` in\nyour environments/foo.yml configuration file. \n\n> NOTE: you must commit and push changes to that file to affect an existing EC2 instance.\n\n### Custom fxa-dev branch\n\nYou can control the branch of fxa-dev for each environment by changing the `{fxadev_git_version}` value in the environment configuration file.\n\n## Layout Notes\n\n- fxa sources are in `/data/fxa-dev`.\n- node processes are run by docker\n  - config is setup by ansible `docker_container` module (e.g., roles/auth/tasks/main.yml)\n  - run `docker ps; docker images` for info\n- ansible will do a docker pull, and restart the container if the image, or configuration, has changed.\n- nginx is the web frontend\n  - config in `/etc/nginx/conf.d`\n- node process logs are available with, e.g., `docker logs auth-server`.\n\n## Example urls\n\n- content server: https://latest.dev.lcip.org\n- auth server: https://latest.dev.lcip.org/auth/\n- oauth server: https://oauth-latest.dev.lcip.org\n- profile server: https://latest.dev.lcip.org/profile\n- demo oauth site: https://123done-latest.dev.lcip.org\n- ssh access: ec2-user@meta-latest.dev.lcip.org\n\n## About using docker_container and quoting of environment values\n\n`docker_container` (>=2.8) now insists that environment values be quoted. However, when evaluating `\"{{ foo }}\"`, those quotes are removed. So use the `to_json` jinja2 filter to ensure that the value is quoted. Note: I use `to_json` instead of `quote` because `quote` will not quote Boolean values `true` and `false`.\n\nIf not quoted, the error will look like `\"Non-string value found for env option. Ambiguous env options must be wrapped in quotes to avoid them being interpreted. Key: ENV_VAR_NAME\"`. If you see this error, add a `to_json` in your templates and try again.\n\n\n\n"
},
{
  "name": "vautomator-client",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "config.yml",
      "pytest.ini",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "src",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# vautomator-client\nClient to use the vautomator-serverless back-end.\n\n## Install\n\n1. Clone this repository: `git clone https://github.com/mozilla/vautomator-client.git && cd vautomator-client`\n2. Create a virtual env (I use `pipenv`): `pipenv --python 3.x`\n3. Install as egg: `python setup.py install`\n\n## Usage\n\n_**NOTE:** This client is only intended to work with the vautomator API (see https://github.com/mozilla/vautomator-serverless)_\n\nThe client supports 3 modes: `run` to run a vulnerability scan, `download` to download scan results (manually, if you have to), and `monitor` to monitor CT logs for new subdomains under \"mozilla.com\", \"mozilla.org\" and \"firefox.com\".\n\nIt is highly recommended to use the great `maws` tool (https://pypi.org/project/mozilla-aws-cli-mozilla/), before running a scan with this client. Otherwise, the client will prompt for an API key, which you will have to obtain from `infosec-dev` AWS account.\n\n### Pre-requisites\n\n- In your virtual environment, install `maws`: `pip install mozilla-aws-cli-mozilla`\n- Sign in to AWS via SSO: `eval $(maws -w)`. When prompted in the browser, select `infosec-dev-MAWS-Admin` role. If everything goes well you now should have AWS credentials set as your environment variables.\n\n### Run it!\n1. To run a scan on a target host: `va_ondemand run www.mozilla.org`\n    - If everything goes well, you should, in an hour or so, have results sent to an [SNS Topic](https://github.com/mozilla/vautomator-serverless/blob/31908da57a769dd097ee73977a7863d3af3143ca/serverless.yml#L419-L432) which in turn has a Google Group, [vautomator-results](https://groups.google.com/a/mozilla.com/forum/#!forum/vautomator-results) subscribed to it. If you join this Google Group, you will get results emailed to you.\n2. To (manually) download results for a scan: `va_ondemand download www.mozilla.org`.\n    - This should create a `tar.gz` file under a folder called `results` in the current working directory, containing output from tooling.\n3. To monitor CT logs and automatically kick off a scan for specific subdomains: `va_ondemand monitor`.\n    - Note that this mode is blocking, as it will listen for events in certificate transparency logs, until you end the program.\n"
},
{
  "name": "taskcluster-github-decision",
  "files": {
    "/": [
      "README.md",
      "requirements.txt",
      "tc-decision.py"
    ]
  },
  "makefile": null,
  "readme": "# A TaskCluster-GitHub decision tooling\n\nBasic tooling to create a decision task for TaskCluster GitHub.\n"
},
{
  "name": "action-automatic-releases",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "action.yml",
      "dist"
    ]
  },
  "makefile": null,
  "readme": "# GitHub Automatic Releases\n\nThis action simplifies the GitHub release process by automatically uploading assets, generating changelogs, handling pre-releases, and so on.\n\n## Contents\n\n1. [Usage Examples](#usage-examples)\n1. [Supported Parameters](#supported-parameters)\n1. [Event Triggers](#event-triggers)\n1. [Versioning](#versioning)\n1. [How to get help](#how-to-get-help)\n1. [License](#license)\n\n> **NOTE**: The `marvinpinto/action-automatic-releases` repository is an automatically generated mirror of the [marvinpinto/actions](https://github.com/marvinpinto/actions) monorepo containing this and other actions. Please file issues and pull requests over there.\n\n## Usage Examples\n\n### Automatically generate a pre-release when changes land on master\n\nThis example workflow will kick in as soon as changes land on `master`. After running the steps to build and test your project:\n\n1. It will create (or replace) a git tag called `latest`.\n1. Generate a changelog from all the commits between this, and the previous `latest` tag.\n1. Generate a new release associated with the `latest` tag (removing any previous associated releases).\n1. Update this new release with the specified title (e.g. `Development Build`).\n1. Upload `LICENSE.txt` and any `jar` files as release assets.\n1. Mark this release as a `pre-release`.\n\nYou can see a working example of this workflow over at [marvinpinto/actions](https://github.com/marvinpinto/actions/releases/tag/latest).\n\n```yaml\n---\nname: \"pre-release\"\n\non:\n  push:\n    branches:\n      - \"master\"\n\njobs:\n  pre-release:\n    name: \"Pre Release\"\n    runs-on: \"ubuntu-latest\"\n\n    steps:\n      # ...\n      - name: \"Build & test\"\n        run: |\n          echo \"done!\"\n\n      - uses: \"marvinpinto/action-automatic-releases@latest\"\n        with:\n          repo_token: \"${{ secrets.GITHUB_TOKEN }}\"\n          automatic_release_tag: \"latest\"\n          prerelease: true\n          title: \"Development Build\"\n          files: |\n            LICENSE.txt\n            *.jar\n```\n\n### Create a new GitHub release when tags are pushed to the repository\n\nSimilar to the previous example, this workflow will kick in as soon as new tags are pushed to GitHub. After building & testing your project:\n\n1. Generate a changelog from all the commits between this and the previous [semver-looking](https://semver.org/) tag.\n1. Generate a new release and associate it with this tag.\n1. Upload `LICENSE.txt` and any `jar` files as release assets.\n\nOnce again there's an example of this over at [marvinpinto/actions](https://github.com/marvinpinto/actions/releases/latest).\n\n```yaml\n---\nname: \"tagged-release\"\n\non:\n  push:\n    tags:\n      - \"v*\"\n\njobs:\n  tagged-release:\n    name: \"Tagged Release\"\n    runs-on: \"ubuntu-latest\"\n\n    steps:\n      # ...\n      - name: \"Build & test\"\n        run: |\n          echo \"done!\"\n\n      - uses: \"marvinpinto/action-automatic-releases@latest\"\n        with:\n          repo_token: \"${{ secrets.GITHUB_TOKEN }}\"\n          prerelease: false\n          files: |\n            LICENSE.txt\n            *.jar\n```\n\n## Supported Parameters\n\n| Parameter               | Description                                                | Default  |\n| ----------------------- | ---------------------------------------------------------- | -------- |\n| `repo_token`\\*\\*        | GitHub Action token, e.g. `\"${{ secrets.GITHUB_TOKEN }}\"`. | `null`   |\n| `draft`                 | Mark this release as a draft?                              | `false`  |\n| `prerelease`            | Mark this release as a pre-release?                        | `true`   |\n| `automatic_release_tag` | Tag name to use for automatic releases, e.g `latest`.      | `null`   |\n| `title`                 | Release title; defaults to the tag name if none specified. | Tag Name |\n| `files`                 | Files to upload as part of the release assets.             | `null`   |\n\n### Notes:\n\n- Parameters denoted with `**` are required.\n- The `files` parameter supports multi-line [glob](https://github.com/isaacs/node-glob) patterns, see repository examples.\n\n## Event Triggers\n\nThe GitHub Actions framework allows you to trigger this (and other) actions on _many combinations_ of events. For example, you could create specific pre-releases for release candidate tags (e.g `*-rc*`), generate releases as changes land on master (example above), nightly releases, and much more. Read through [Workflow syntax for GitHub Actions](https://help.github.com/en/articles/workflow-syntax-for-github-actions) for ideas and advanced examples.\n\n## Versioning\n\nEvery commit that lands on master for this project triggers an automatic build as well as a tagged release called `latest`. If you don't wish to live on the bleeding edge you may use a stable release instead. See [releases](../../releases/latest) for the available versions.\n\n```yaml\n- uses: \"marvinpinto/action-automatic-releases@<VERSION>\"\n```\n\n## How to get help\n\nThe main [README](https://github.com/marvinpinto/actions/blob/master/README.md) for this project has a bunch of information related to debugging & submitting issues. If you're still stuck, try and get a hold of me on [keybase](https://keybase.io/marvinpinto) and I will do my best to help you out.\n\n## License\n\nThe source code for this project is released under the [MIT License](/LICENSE). This project is not associated with GitHub.\n"
},
{
  "name": "cpg",
  "files": {
    "/": [
      "CHANGELOG.md",
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# README\nThis repository currently holds relevant information related to Mozilla's Community Participation Guidelines.\n\n# Requesting an Update\nTo request an update, or change to Mozilla's Community Participation Guidelines, please open an issue in this repository.\n"
},
{
  "name": "msvc-dev-cmd",
  "files": {
    "/": [
      ".eslintrc.json",
      ".github",
      ".gitignore",
      "LICENSE",
      "README.md",
      "action.yml",
      "hello.c",
      "index.js",
      "node_modules",
      "package-lock.json",
      "package.json"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "<a href=\"https://github.com/ilammy/msvc-dev-cmd\"><img alt=\"GitHub Actions status\" src=\"https://github.com/ilammy/msvc-dev-cmd/workflows/msvc-dev-cmd/badge.svg\"></a>\n\n# msvc-dev-cmd\n\n[GitHub Action](https://github.com/features/actions) for configuring Developer Command Prompt for Microsoft Visual C++.\n\nThis sets up the environment for compiling C/C++ code from command line.\n\nSupports Windows. Does nothing on Linux and macOS.\n\n## Inputs\n\n- `arch` \u2013 target architecture\n  - native compilation:\n    - `x64` (default) or its synonyms: `amd64`, `win64`\n    - `x86` or its synonyms: `win32`\n  - cross-compilation: `x86_amd64`, `x86_arm`, `x86_arm64`,\n  \t`amd64_x86`, `amd64_arm`, `amd64_arm64`\n- `sdk` \u2013 Windows SDK to use\n  - do not specify to use the default SDK\n  - or specify full Windows 10 SDK number (e.g, `10.0.10240.0`)\n  - or write `8.1` to use Windows 8.1 SDK\n- `toolset` \u2013 select VC++ compiler toolset version\n  - do not specify to use the default toolset\n  - `14.0` for VC++ 2015 Compiler Toolset\n  - `14.XX` for the latest 14.XX toolset installed (e.g, `14.11`)\n  - `14.XX.YYYYY` for a specific full version number (e.g, `14.11.25503`)\n- `uwp` \u2013 set `true` to build for Universal Windows Platform (i.e., for Windows Store)\n- `spectre` \u2013 set `true` to use Visual Studio libraries with [Spectre](https://meltdownattack.com) mitigations\n\n## Example usage\n\n```yaml\njobs:\n  test:\n    - uses: actions/checkout@v1\n    - uses: ilammy/msvc-dev-cmd@v1\n    - name: Build something requiring CL.EXE\n      run: |\n        cmake -G \"NMake Makefiles\" .\n        nmake\n    # ...\n```\n\n## Caveats\n\n### Name conflicts with `shell: bash`\n\nUsing `shell: bash` in Actions may shadow some of the paths added by MSVC.\nIn particular, `link.exe` (Microsoft C linker) is prone to be shadowed by `/usr/bin/link` (GNU filesystem link tool).\n\nUnfortunately, this happens because GitHub Actions unconditionally *prepend* GNU paths when `shell: bash` is used,\non top of any paths set by `msvc-dev-cmd`, every time at the start of each new step.\nHence, there aren't many non-destructive options here.\n\nIf you experience compilation errors where `link` complains about unreasonable command-line arguments,\n\u201cextra operand *something-something*\u201d \u2013 that's probably it.\nRecommended workaround is to remove `/usr/bin/link` if that interferes with your builds.\nIf this is not acceptable, please file an issue, then we'll figure out something better.\n\n## License\n\nMIT, see [LICENSE](LICENSE).\n"
},
{
  "name": "riskHeatMap",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "MANIFEST.in",
      "Pipfile",
      "Pipfile.lock",
      "README.rst",
      "app.py",
      "attach_policy.json",
      "auth.py",
      "config.dev.yml",
      "config.prod.yml",
      "config.py",
      "css",
      "decorators.py",
      "env.example",
      "heatmap",
      "index.html",
      "observatory",
      "package.json",
      "serverless.yml",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "Overview\n========\n\nA dashboard site for the risk heatmap and the observatory dashboard.\nHosted in AWS, lambda/serverless via the serverless framework.\n\nThe risk heatmap and observatory dashboard are just plain html/js files. The AWS/Serverless bits are just for scaling and authentication.\n\nData Model Overview\n===================\nThe risk heatmap is made up of several elements:\n\n - services\n - asset groups\n - assets\n - indicators\n\nAll of these combine to determine how the services display risk.\n\nServices are the focal point for rolling up the other elements into a risk-based heatmap view. Services are simply things you run in your environment (your website, your HR system, your order processing, etc).\nServices with little information (no assets, etc) are more opaque (i.e. see through). Services with a full compliment of asset groups/assets/indicators are solid.\nEach service is given three scores that determine its heatmap display:\n\n - Risk score\n - Visibility score\n - Impact score\n\nThe risk score is the summary of the underlying asset groups, their assets and the risk score of the indicators we have visibility to.\nThe visibility score is a rating of how much data/visibility we have about a service.\nThe impact score is a rating of how impactful this service is in our environment to our overall risk.\n\nThe display orients itself as follows:\n - The higher the risk score, the taller the service.\n - The higher the visibility score the less opague, more solid the service.\n - The higher the impact, the darker the color\n\n From these display elements you can determine which services in your environment need attention.\n\nService Data Model\n==================\nAt the very least, a service needs to have the following JSON fields:\n - name: The name of the service\n\nIdeally it also has:\n - highest_risk_impact: The impact rating (one of unknown, none, low, medium, high, maximum)\n\nIf impact is not provided, it is considered 'unknown'.\nAny other key: value pairs associated with the service will be displayed in the details panel when a service is selected. URLs will automatically be turned into HTML links.\n\nAsset Group Data Model\n=======================\nAsset groups are simply groups of asssets. For example if you have a 'Service' that is your company.com website, you may have an asset group for website-production and another one for website-development.\n\nAn Asset group needs to have the following JSON fields:\n - name: the name of the group (website-production for example)\nIt can also contain:\n - description: a text sentence about the group\n\nAsset Data Model\n================\nAn asset is a technical component of a service. A server, a database, an AWS account, a web site, etc can all be examples of an asset.\n\nAssets need the following JSON fields:\n - asset_identifier: the unique identifier for the asset (server fqdn, website domain name, etc)\n\nAny other key/value fields accompanying the asset will be used in displaying the detail\n\nIndicator Data Model\n====================\nAn indicator is meant to be a flexible way of relating something we know about an asset that may 'indicate' risk. For example, a summary of vulnerabilities associated with an asset, a score from the web observatory, a summary of a web application security scan, etc.\nIndicators are the most complex data model but follow a similar pattern:\n\nFields:\n - description: A sentence describing the indicator (\"Mozilla Observatory scan\" for example)\n - details: a sub object containing the details for each type of indicator\n\nObservatory Indicator\n=====================\nMozilla Observatory scores are summarized into indicators as follows:\n\n\n    \"description\": \"Mozilla Observatory scan\",\n    \"details\": {\n        \"grade\": \"A+\",\n        \"tests\": [\n            {\n                \"name\": \"content-security-policy\",\n                \"pass\": true\n            },\n            {\n                \"name\": \"contribute\",\n                \"pass\": true\n            },\n        ]\n    },\n    \"event_source_name\": \"Mozilla Observatory\",\n    \"id\": \"c996310e-9dda-4b93-aae5-6f9680e35fd9\",\n    \"likelihood_indicator\": \"medium\",\n    \"timestamp_utc\": \"2018-09-28T19:36:46.200307+00:00\"\n\nAs you can see the grade along with the individual tests, noting pass/fail are presented.\n\nVulnerability Indicator\n=======================\n\nContacts\n--------\nJeff Bryner <jeff@jeffbryner.com>, twitter @0x7eff\nApril King  <april@mozilla.org>\n"
},
{
  "name": "frost",
  "files": {
    "/": [
      ".coveragerc",
      ".dockerignore",
      ".github",
      ".gitignore",
      ".pre-commit-config.yaml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.md",
      "aws",
      "cache.py",
      "config.yaml.example",
      "conftest.py",
      "custom_config.py",
      "docs",
      "example_cache",
      "exemptions.py",
      "frost",
      "gcp",
      "gsuite",
      "helpers.py",
      "meta_test_cache.py",
      "renovate.json",
      "requirements.txt",
      "service_report_generator.py",
      "setup.py",
      "severity.py"
    ],
    "/docs": [
      ".nojekyll",
      "Architecture.rst",
      "CodingConventions.rst",
      "ContributingDocumentation.rst",
      "FAQ.rst",
      "Makefile",
      "MozillaDeployment.rst",
      "NewServices.rst",
      "Source.rst",
      "UseCases.rst",
      "conf.py",
      "frost-snowman-logo.png",
      "index.rst",
      "readme-include.md",
      "requirements.txt"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": "\nTODAY := $(shell date '+%Y-%m-%d')\n\n.DEFAULT_GOAL := all\n\nAWS_PROFILE := default\n\nPYTEST_OPTS := ''\n\nAUTOBUILD_OPTS ?= --open-browser --port=0 --delay 5\n\nall: check_venv\n\tfrost test\n\nawsci: check_venv\n\tfrost test --continue-on-collection-errors -m aws aws/**/*.py \\\n\t\t-k \"not test_ec2_security_group_in_use\" \\\n\t\t--json=results-$(AWS_PROFILE)-$(TODAY).json $(PYTEST_OPTS)\n\ncheck_venv:\nifeq ($(VIRTUAL_ENV),)\n\t$(error \"Run frost from a virtualenv (try 'make install && source venv/bin/activate')\")\nendif\n\ncheck_conftest_imports:\n\t# refs: https://github.com/mozilla/frost/issues/119\n\tgrep --recursive --exclude-dir '*venv' --include '*.py' '^import\\s+conftest|^from\\s+conftest\\s+import\\s+pytest' ./ ; [ $$? -eq 1 ]\n\nclean: clean-cache clean-python\n\trm -rf venv\n\t# remember to deactivate your active virtual env\n\nclean-cache: check_venv\n\t@# do as little work as possible to clear the cache, and guarantee success\n\tfrost test  --cache-clear --continue-on-collection-errors \\\n\t\t--collect-only -m \"no_such_marker\" \\\n\t\t--noconftest --tb=no --disable-warnings --quiet \\\n\t    || true\n\nclean-python:\n\tfind . -type d -name venv -prune -o -type d -name __pycache__ -print0 | xargs -0 rm -rf\n\ndoc-build: check_venv\n\ttype sphinx-build || { echo \"please run `make install-docs` to build docs\"; false; }\n\t@# we regen the api docs every time -- they are not checked in.\n\trm -rf docs/source\n\tsphinx-apidoc --no-toc -o docs/source .\n\t@# TODO: Add new service modules below also in docs/Source.rst\n\tfor module in frost aws gcp gsuite; do \\\n\t\tsphinx-apidoc -f -o docs/source/$$module $$module ; \\\n\tdone\n\tmake -C docs clean html\n\ndoc-preview: check_venv\n\t@#sphinx-autobuild \"$(SOURCEDIR)\" \"$(BUILDDIR)\" $(SPHINXOPTS) $(O)\n\tsphinx-autobuild $(AUTOBUILD_OPTS) \"docs/\" \"docs/_build/html/\" $(SPHINXOPTS) $(O)\n\ndoctest: check_venv\n\tfrost test -vv --doctest-modules --doctest-glob='*.py' -s --offline --debug-calls $(shell find . -type f -name '*.py' | grep -v venv | grep -v .pyenv | grep -v setup.py) \\\n\t\t--doctest-modules -s --offline --debug-calls\n\ncoverage: check_venv\n\tfrost test --cov-config .coveragerc --cov=. \\\n\t\t--aws-profiles example-account \\\n\t\t-o python_files=meta_test*.py \\\n\t\t-o cache_dir=./example_cache/ \\\n\t\t--offline \\\n\t\t$(shell find . -type f -name '*.py' | grep -v venv | grep -v .pyenv | grep -v setup.py)\n\tcoverage report -m\n\tcoverage html\n\nflake8: check_venv\n\tflake8 --max-line-length 120 $(shell git ls-files | grep \\.py$$)\n\nblack: check_venv\n\tpre-commit run black --all-files\n\ninstall: venv\n\t( . venv/bin/activate && pip install -U pip && pip install -r requirements.txt && python setup.py develop && pre-commit install )\n\ninstall-docs: venv\n\t( . venv/bin/activate && pip install -r docs/requirements.txt )\n\nsetup_gsuite: check_venv\n\tpython -m bin.auth.setup_gsuite\n\nmetatest:\n\tfrost test --aws-profiles example-account \\\n\t\t-o python_files=meta_test*.py \\\n\t\t-o cache_dir=./example_cache/\n\nvenv:\n\tpython3 -m venv venv\n\nbuild-image:\n\tdocker build -t localhost/frost:latest .\n\n.PHONY: \\\n\tall \\\n\tawsci \\\n\tblack \\\n\tbuild-image \\\n\tcheck_conftest_imports \\\n\tcheck_venv \\\n\tclean \\\n\tclean-cache \\\n\tclean-python \\\n\tcoverage \\\n\tdoc-build \\\n\tdoc-preview \\\n\tdoctest \\\n\tflake8 \\\n\tinstall \\\n\tinstall-docs \\\n\tmetatest \\\n\tsetup_gsuite \\\n\tvenv\n",
  "readme": "# Frost\n\n[![PyPI version](https://badge.fury.io/py/frost.svg)](https://badge.fury.io/py/frost)\n[![Documentation](https://img.shields.io/badge/Docs-gh--pages-yellowgreen.svg)](https://mozilla.github.com/frost/)\n\n![frost snowman logo](docs/frost-snowman-logo.png)\n\nHTTP clients and a wrapper around\n[pytest](https://docs.pytest.org/en/latest/index.html) tests to verify\nthat third party services are configured correctly. For example:\n\n* Are our AWS DB snapshots publicly accessible?\n* Are there dangling DNS entries in Route53?\n\n## Usage\n\n### Installing\n\n1. Install [Python 3.8](https://www.python.org/downloads/)\n1. Run `git clone git@github.com:mozilla/frost.git; cd frost; make install`\n\n### Usage\n\n```console\n$ frost --help\nUsage: frost [OPTIONS] COMMAND [ARGS]...\n\n  FiRefox Operations Security Testing API clients and tests\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\n  Commands:\n    list  Lists available test filenames packaged with frost.\n    test  Run pytest tests passing all trailing args to pytest.\n\n$ frost test --help\nUsage: frost test [OPTIONS] [PYTEST_ARGS]...\n\n  Run pytest tests passing all trailing args to pytest.\n\n  Adds the pytest args:\n\n  -s to disable capturing stdout\n  https://docs.pytest.org/en/latest/capture.html\n\n  and frost specific arg:\n\n  --debug-calls to print AWS API calls\n\nOptions:\n  --help  Show this message and exit.\n```\n\n### Running\n\nTo fetch RDS resources from the cache or AWS API and check that\nbackups are enabled for DB instances for [the configured aws\nprofile](https://docs.aws.amazon.com/cli/latest/userguide/cli-config-files.html)\nnamed `default` in the `us-west-2` region\n\n\n1. find the test file path:\n\n```console\n$ frost list | grep rds\n./aws/rds/test_rds_db_instance_backup_enabled.py\n./aws/rds/test_rds_db_snapshot_encrypted.py\n./aws/rds/test_rds_db_instance_is_postgres_with_invalid_certificate.py\n./aws/rds/test_rds_db_instance_encrypted.py\n./aws/rds/test_rds_db_security_group_does_not_grant_public_access.py\n./aws/rds/test_rds_db_instance_not_publicly_accessible_by_vpc_sg.py\n./aws/rds/test_rds_db_instance_minor_version_updates_enabled.py\n./aws/rds/test_rds_db_instance_is_multiaz.py\n./aws/rds/test_rds_db_snapshot_not_publicly_accessible.py\n```\n\nNote: **packaged frost tests are relative to the frost install**\n\n1. run the test:\n\n```console\nfrost test aws/rds/test_rds_db_instance_backup_enabled.py --aws-profiles default\n```\n\nFrost adds the options:\n\n* `--aws-profiles` for selecting one or more AWS profiles to fetch resources for or the AWS default profile / `AWS_PROFILE` environment variable\n* `--aws-regions` for selecting one or more AWS regions to test as a CSV e.g. `us-east-1,us-west-2`. **defaults to all regions**\n* `--gcp-project-id` for selecting the GCP project to test. **Required for GCP tests**\n* `--offline` a flag to tell HTTP clients to not make requests and return empty params\n* [`--config`](#custom-test-config) path to test custom config file\n\nand produces output showing calls to the AWS API and failing for a DB\ninstance with backups disabled:\n\n```console\n============================================================ test session starts ============================================================\nplatform linux -- Python 3.8.2, pytest-6.0.2, py-1.9.0, pluggy-0.13.1\nrootdir: /home/gguthe/frost\nplugins: json-0.4.0, cov-2.10.0, html-1.20.0, metadata-1.10.0\ncollecting ... calling AWSAPICall(profile='default, region='ap-northeast-1', service='rds', method='describe_db_instances', args=[], kwargs={})\ncalling AWSAPICall(profile='default, region='ap-northeast-2', service='rds', method='describe_db_instances', args=[], kwargs={})\ncalling AWSAPICall(profile='default, region='ap-south-1', service='rds', method='describe_db_instances', args=[], kwargs={})\ncalling AWSAPICall(profile='default, region='ap-southeast-1', service='rds', method='describe_db_instances', args=[], kwargs={})\ncalling AWSAPICall(profile='default, region='ap-southeast-2', service='rds', method='describe_db_instances', args=[], kwargs={})\n...\ncalling AWSAPICall(profile='default, region='us-west-2', service='rds', method='list_tags_for_resource', args=[], kwargs={'ResourceName': 'arn:aws:rds:us-west-2:redacted:db:test-db-ro-dev1'})\ncollected 21 items\n\naws/rds/test_rds_db_instance_backup_enabled.py F....................\n\n================================================================= FAILURES ==================================================================\n____________________________________ test_rds_db_instance_backup_enabled[test-db-ro-dev1] ___________________________________________________\n\nrds_db_instance = {'AllocatedStorage': 250, 'AutoMinorVersionUpgrade': True, 'AvailabilityZone': 'us-east-1a', 'BackupRetentionPeriod': 0, ..\n.}\n\n    @pytest.mark.rds\n    @pytest.mark.parametrize(\n        \"rds_db_instance\", rds_db_instances_with_tags(), ids=get_db_instance_id,\n    )\n    def test_rds_db_instance_backup_enabled(rds_db_instance):\n>       assert (\n            rds_db_instance[\"BackupRetentionPeriod\"] > 0\n        ), \"Backups disabled for {}\".format(rds_db_instance[\"DBInstanceIdentifier\"])\nE       AssertionError: Backups disabled for test-db-ro-dev1\nE       assert 0 > 0\n\naws/rds/test_rds_db_instance_backup_enabled.py:12: AssertionError\n========================================================== short test summary info ==========================================================\nFAILED aws/rds/test_rds_db_instance_backup_enabled.py::test_rds_db_instance_backup_enabled[test-db-ro-dev1] - AssertionError...\n======================================================= 2 failed, 21 passed in 14.32s =======================================================\n```\n\n#### IAM Policy for frost\n\nThe below policy will allow you to run all AWS tests in frost against all resources in your account.\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"PytestServicesReadOnly\",\n      \"Action\": [\n        \"autoscaling:DescribeLaunchConfigurations\",\n        \"cloudtrail:DescribeTrails\",\n        \"ec2:DescribeFlowLogs\",\n        \"ec2:DescribeInstances\",\n        \"ec2:DescribeSecurityGroups\",\n        \"ec2:DescribeSnapshotAttribute\",\n        \"ec2:DescribeSnapshots\",\n        \"ec2:DescribeVolumes\",\n        \"ec2:DescribeVpcs\",\n        \"elasticache:DescribeCacheClusters\",\n        \"elasticloadbalancing:DescribeLoadBalancers\",\n        \"es:DescribeElasticsearchDomains\",\n        \"es:ListDomainNames\",\n        \"iam:GenerateCredentialReport\",\n        \"iam:GetCredentialReport\",\n        \"iam:GetLoginProfile\",\n        \"iam:ListAccessKeys\",\n        \"iam:ListAttachedGroupPolicies\",\n        \"iam:ListAttachedRolePolicies\",\n        \"iam:ListAttachedUserPolicies\",\n        \"iam:ListGroupPolicies\",\n        \"iam:ListGroupsForUser\",\n        \"iam:ListMFADevices\",\n        \"iam:ListRolePolicies\",\n        \"iam:ListRoles\",\n        \"iam:ListUserPolicies\",\n        \"iam:ListUsers\",\n        \"rds:DescribeDbInstances\",\n        \"rds:DescribeDbSecurityGroups\",\n        \"rds:DescribeDbSnapshotAttributes\",\n        \"rds:DescribeDbSnapshots\",\n        \"rds:ListTagsForResource\",\n        \"redshift:DescribeClusterSecurityGroups\",\n        \"redshift:DescribeClusters\",\n        \"s3:GetBucketAcl\",\n        \"s3:GetBucketCORS\",\n        \"s3:GetBucketLogging\",\n        \"s3:GetBucketPolicy\",\n        \"s3:GetBucketVersioning\",\n        \"s3:GetBucketWebsite\",\n        \"s3:ListAllMyBuckets\",\n        \"s3:ListBucket\"\n      ],\n      \"Effect\": \"Allow\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n#### Setting up GCP tests\n\n##### Enabling required API's for your project\n\n```\ngcloud [--project <project name>] services enable bigquery-json.googleapis.com\ngcloud [--project <project name>] services enable cloudresourcemanager.googleapis.com\ngcloud [--project <project name>] services enable compute.googleapis.com\ngcloud [--project <project name>] services enable sqladmin.googleapis.com\n```\n\n#### Setting up GSuite tests\n\nMake sure to have an OAuth2 app created and have the `client_secret.json` file in `~/.credentials` and then run:\n\n```\nmake setup_gsuite\n```\n\n### Caching\n\nThe AWS client will use AWS API JSON responses when available and save them using AWS profile, region, service name, service method, [botocore](http://botocore.readthedocs.io/) args and kwargs in the cache key to filenames with the format `.cache/v/pytest_aws:<aws profile>:<aws region>:<aws service>:<service method>:<args>:<kwargs>.json` e.g.\n\n```\nhead .cache/v/pytest_aws:cloudservices-aws-stage:us-west-2:rds:describe_db_instances::.json\n{\n    \"DBInstances\": [\n        {\n            \"AllocatedStorage\": 5,\n            \"AutoMinorVersionUpgrade\": true,\n            \"AvailabilityZone\": \"us-west-2c\",\n            \"BackupRetentionPeriod\": 1,\n            \"CACertificateIdentifier\": \"rds-ca-2015\",\n            \"CopyTagsToSnapshot\": false,\n            \"DBInstanceArn\": \"arn:aws:rds:us-west-2:123456678901:db:test-db\",\n```\n\nThese files can be removed individually or all at once with [the pytest --cache-clear](https://docs.pytest.org/en/latest/cache.html#usage) option.\nThe cache can be disabled entirely with [the pytest -p no:cacheprovider](https://stackoverflow.com/questions/47744076/preventing-pytest-from-creating-cache-directories-in-pycharm).\n\n## Custom Test Config\n\nfrost adds a `--config` cli option for passing in a custom config file specific to tests within frost.\n\nThe example config in repo (`config.yaml.example`):\n```\nexemptions:\n  - test_name: test_ec2_instance_has_required_tags\n    test_param_id: i-0123456789f014c162\n    expiration_day: 2019-01-01\n    reason: ec2 instance has no owner\n  - test_name: test_ec2_security_group_opens_specific_ports_to_all\n    test_param_id: '*HoneyPot'\n    expiration_day: 2020-01-01\n    reason: purposefully insecure security group\nseverities:\n  - test_name: test_ec2_instance_has_required_tags\n    severity: INFO\n  - test_name: '*'\n    severity: ERROR\nregressions:\n  - test_name: test_ec2_security_group_opens_all_ports_to_all\n    test_param_id: '*mycustomgroup'\n    comment: this was remediated by ops team\naws:\n  admin_groups:\n    - \"Administrators\"\n  admin_policies:\n    - \"AWSAdminRequireMFA\"\n  user_is_inactive:\n    no_activity_since:\n      years: 1\n      months: 0\n    created_after:\n      weeks: 1\n  access_key_expires_after:\n    years: 1\n    months: 0\n  required_tags:\n    - Name\n    - Type\n    - App\n    - Env\n  required_amis:\n    - ami-00000000000000000\n    - ami-55555555555555555\n  # Allowed ports for the test_ec2_security_group_opens_specific_ports_to_all\n  # test for all instances\n  allowed_ports_global:\n    - 25\n  # Allowed ports for the test_ec2_security_group_opens_specific_ports_to_all\n  # test for specific instances. In this example, we are allowing ports 22\n  # and 2222 for all security groups that include the word 'bastion' in them.\n  allowed_ports:\n    - test_param_id: '*bastion'\n      ports:\n        - 22\n        - 2222\ngcp:\n  allowed_org_domains:\n    - mygsuiteorg.com\n  allowed_gke_versions:\n    - 1.15.12-gke.20\n    - 1.16.13-gke.401\n    - 1.17.9-gke.1504\n    - 1.18.6-gke.3504\n  # Allowed ports for the test_firewall_opens_any_ports_to_all\n  # test for all firewalls\n  allowed_ports_global:\n    - 25\n  # Allowed ports for the test_firewall_opens_any_ports_to_all\n  # test for specific firewalls. In this example, we are allowing ports 22\n  # and 2222 for all firewalls that include the word 'bastion' in them.\n  allowed_ports:\n    - test_param_id: '*bastion'\n      ports:\n        - 22\n        - 2222\ngsuite:\n  domain: 'example.com'\n  user_is_inactive:\n    no_activity_since:\n      years: 1\n      months: 0\n```\n\n### Test Exemptions\n\nfrost custom config format adds support for\nmarking test and test resource IDs as expected failures.\n\nThe keys for each exemption rule is:\n* test_name - Name of the test\n* test_param_id - test ID (usually an AWS resource ID) (prefix with `*` to turn into a regex matcher)\n* expiration_day - exception expiration day (as YYYY-MM-DD)\n* reason - exception reason\n\nThe config looks like:\n```\n...\nexemptions:\n  - test_name: test_ec2_instance_has_required_tags\n    test_param_id: i-0123456789f014c162\n    expiration_day: 2019-01-01\n    reason: ec2 instance has no owner\n  - test_name: test_ec2_security_group_opens_specific_ports_to_all\n    test_param_id: '*HoneyPot'\n    expiration_day: 2020-01-01\n    reason: purposefully insecure security group\n...\n```\n\n#### Enabling regex for test ID\n\nYou can prefix the test ID with a `*` to enable regex matching for the test ID. The `*` prefix will be stripped\noff, and the rest will be used as a regex.\n\nFor example:\n - `*foobar` becomes `foobar`\n - `*foo\\w+` becomes `foo\\w+`\n\nFor more information on Python's regex syntax see: [Regular Expression HOWTO](https://docs.python.org/3.4/howto/regex.html#regex-howto).\n\n**Note:** All regex rules are applied first. As well, the ordering of both regex and non-regex rules is top to bottom and the first one wins.\n\n\nWhen a json report is generated, the exemptions will show up in the\njson metadata as serialized markers:\n\n```json\npython -m json.tool report.json | grep -C 20 xfail\n...\n                        \"markers\": {\n                            \"ec2\": {\n                                \"name\": \"ec2\",\n                                \"args\": [],\n                                \"kwargs\": {}\n                            },\n                            \"parametrize\": {\n                                \"name\": \"parametrize\",\n                                \"args\": [\n                                    \"...skipped...\"\n                                ],\n                                \"kwargs\": [\n                                    \"...skipped...\"\n                                ]\n                            },\n                            \"xfail\": {\n                                \"name\": \"xfail\",\n                                \"args\": [],\n                                \"kwargs\": {\n                                    \"reason\": \"ec2 instance has no owner\",\n                                    \"strict\": true,\n                                    \"expiration\": \"2019-01-01\"\n                                }\n                            }\n                        },\n...\n```\n\n\n#### Test Severity\n\nfrost custom config format adds support for marking the severity of a certain test. A severity can be `INFO`, `WARN`, or `ERROR`.\n\nThese do not modify pytest results (pass, fail, xfail, skip, etc.).\n\nThe config looks like:\n\n```\n...\nseverities:\n  - test_name: test_ec2_instance_has_required_tags\n    severity: INFO\n  - test_name: '*'\n    severity: ERROR\n...\n```\n\nAnd results in a severity and severity marker being included in the\njson metadata:\n\n```console\nfrost test -s --aws-profiles stage --aws-require-tags Name Type App Stack -k test_ec2_instance_has_required_tags --config config.yaml.example --json=report.json\n...\n```\n\n```json\npython -m json.tool report.json\n{\n    \"report\": {\n        \"environment\": {\n            \"Python\": \"3.6.2\",\n            \"Platform\": \"Darwin-15.6.0-x86_64-i386-64bit\"\n        },\n        \"tests\": [\n            {\n...\n                \"metadata\": [\n                    {\n...\n                    \"markers\": {\n...\n                            \"severity\": {\n                                \"name\": \"severity\",\n                                \"args\": [\n                                    \"INFO\"\n                                ],\n                                \"kwargs\": {}\n                            }\n                        },\n...\n                        \"severity\": \"INFO\",\n                        \"unparametrized_name\": \"test_ec2_instance_has_required_tags\"\n                    }\n...\n```\n\n### AWS Config\n\nfrost has a suite of AWS tests. This section of the custom config includes configuration options specific\nto these tests.\n\nThe config looks like:\n```\n...\naws:\n  # Relative time delta for test_iam_user_is_inactive. no_activity_since will be used as the failure marker,\n  # so in this example any user that hasn't had any activity for a year will be marked as a \"failure\". created_after\n  # is used as a grace period, so in this case any user that was created within the last week will be automatically\n  # pass this test.\n  user_is_inactive:\n    no_activity_since:\n      years: 1\n      months: 0\n    created_after:\n      weeks: 1\n  # Required tags used within the test_ec2_instance_has_required_tags test\n  required_tags:\n    - Name\n    - Type\n    - App\n    - Env\n  # Allowed ports for the test_ec2_security_group_opens_specific_ports_to_all\n  # test for all instances\n  allowed_ports_global:\n    - 25\n  # Allowed ports for the test_ec2_security_group_opens_specific_ports_to_all\n  # test for specific instances. In this example, we are allowing ports 22\n  # and 2222 for all security groups that include the word 'bastion' in them.\n  allowed_ports:\n    - test_param_id: '*bastion'\n      ports:\n        - 22\n        - 2222\n...\n```\n\n### GSuite Config\n\nfrost has a suite of GSuite tests. This section of the\ncustom config includes configuration options specific to these tests.\n\n**Make sure to [setup GSuite](#setting-up-gsuite-tests) before running GSuite tests**\n\nThe config looks like:\n```\ngsuite:\n  # The specific GSuite domain to test.\n  domain: 'example.com'\n  # Relative time delta for test_admin_user_is_inactive. no_activity_since will be used as the failure marker,\n  # so in this example any user that hasn't had any activity for a year will be marked as a \"failure\".\n  user_is_inactive:\n    no_activity_since:\n      years: 1\n      months: 0\n```\n\n### Test Accuracy\n\nThere are two important things to note about `frost` tests that may be different from your expectations.\n\nFirst, the focus is on \"actionable results\". This plays out as an attempt to reduce false\npositives by trying to filter out unused resources. An example of this can be seen by looking at\nany of the security group tests, where we are skipping any security groups that are not attached to a resource.\n\nSecond, there are some tests that make naive assumptions instead of trying to capture the complexities\nof the system. The current best example of this is all IAM tests that relate to \"admin\" users. How we\nare determining what an user or role is an admin is based simply off substring matching on the policies\nattached. This obviously has a high chance of false negatives.\n\n## Development\n\n### Goals\n\n1. replace one-off scripts for each check\n1. share checks with other organizations\n1. consolidate bugs in one place (i.e. one thing to update)\n1. in pytest use a known existing framework for writing checks\n1. be vendor agnostic e.g. support checks across cloud providers or in hybrid environments or competing services\n1. cache and share responses to reduce third party API usage (i.e. lots of tests check AWS security groups so fetch them once)\n1. provide a way to run a single test or subset of tests\n1. focus on actionable results (see [test accuracy](#test-accuracy) for more information)\n\n### Non-Goals\n\n1. Invent a new DSL for writing expectations (use pytest conventions)\n1. Verify how third party services or their client libraries work\n   (e.g. don't answer \"Does GET / on the CRUD1 API return 400 when\n   query param `q` is `$bad_value`?\")\n\n### Design\n\nCurrently this is a monolithic pytest package, but should eventually\n[be extracted into a pytest plugin](https://github.com/mozilla/frost/issues/3) and with [separate dependent\npytest plugins for each service](https://github.com/mozilla/frost/issues/4).\n\nAPI responses should fit on disk and in memory (i.e. don't use this\nfor log processing or checking binaries for malware), and be safe to\ncache for minutes, hours, or days (i.e. probably don't use this for\nmonitoring a streaming API) (NB: [bug for specifying data\nfreshness](https://github.com/mozilla/frost/issues/5)).\n\nAdditionally we want:\n\n* data fetching functions in a `resources.py`\n* data checking and test helpers in a `helpers.py`\n* prefix test files with `test_`\n* doctests for non test files (e.g. `helpers.py`, `resources.py`, `client.py`)\n  * tests that depend on external IO or the runtime environment (env vars, file system, HTTP) to use the prefix `meta_test_` (and probably `mock` or `pytest.monkeypatch`)\n    * JSON fixtures for anonymized cached http call in `example_cache/v/`\n* tests to have pytest markers for any services they depend on for data\n* HTTP clients should be read only and use read only credentials\n* running a test should not modify services\n\n#### File Layout\n\n```console\nfrost\n...\n\u251c\u2500\u2500 example_cache\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 v\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 cache\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 lastfailed\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 pytest_aws:example-account:us-east-1:ec2:describe_instances::.json\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 pytest_aws:example-account:us-east-1:ec2:describe_security_groups::.json\n...\n\u251c\u2500\u2500 <third party service A>\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 client.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 conftest.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 meta_test_client.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 <subservice A (optional)>\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 helpers.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 resources.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_ec2_security_group_all_ports.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 <subservice b (optional)>\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 resources.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ...\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500 test_s3_bucket_web_hosting_disabled.py\n\u2514\u2500\u2500 <third party service B>\n \u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n \u00a0\u00a0 \u251c\u2500\u2500 conftest.py\n \u00a0\u00a0 \u251c\u2500\u2500 helpers.py\n \u00a0\u00a0 \u251c\u2500\u2500 resources.py\n \u00a0\u00a0 \u2514\u2500\u2500 test_user_has_escalation_policy.py\n```\n\n### Adding an example test\n\nLet's write a test to check that http://httpbin.org/ip returns an AWS IP:\n\n1. create a file `httpbin/test_httpbin_ip.py` with the contents:\n\n```python\nimport itertools\nimport ipaddress\nimport pytest\nimport json\nimport urllib.request\n\n\ndef get_httpbin_ips():\n    # IPs we always want to test\n    ips = [\n        '127.0.0.1',\n        '13.58.0.0',\n    ]\n\n    req = urllib.request.Request('http://httpbin.org/ip')\n\n    with urllib.request.urlopen(req) as response:\n        body = response.read().decode('utf-8')\n        ips.append(json.loads(body).get('origin', None))\n\n    return ips\n\n\ndef get_aws_ips():\n    req = urllib.request.Request('https://ip-ranges.amazonaws.com/ip-ranges.json')\n\n    with urllib.request.urlopen(req) as response:\n        body = response.read().decode('utf-8')\n        return json.loads(body)['prefixes']\n\n\n@pytest.mark.httpbin\n@pytest.mark.aws_ip_ranges\n@pytest.mark.parametrize(\n    ['ip', 'aws_ip_ranges'],\n    zip(get_httpbin_ips(), itertools.repeat(get_aws_ips())))\ndef test_httpbin_ip_in_aws(ip, aws_ip_ranges):\n    for aws_ip_range in aws_ip_ranges:\n        assert ipaddress.IPv4Address(ip) not in ipaddress.ip_network(aws_ip_range['ip_prefix']), \\\n          \"{0} is in AWS range {1[ip_prefix]} region {1[region]} service {1[service]}\".format(ip, aws_ip_range)\n```\n\nNotes:\n\n* we add two data fetching functions that return lists that we can zip into tuples for [the pytest parametrize decorator](https://docs.pytest.org/en/latest/parametrize.html#pytest-mark-parametrize-parametrizing-test-functions)\n* we add markers for the services we're fetching data from\n\n\n1. Running `frost test` with the test file explicitly included we see that one of the IPs is an AWS IP:\n\n```console\nfrost test httpbin/test_httpbin_ip_in_aws.py\nplatform darwin -- Python 3.6.2, pytest-3.3.2, py-1.5.2, pluggy-0.6.0\nmetadata: {'Python': '3.6.2', 'Platform': 'Darwin-15.6.0-x86_64-i386-64bit', 'Packages': {'pytest': '3.3.2', 'py': '1.5.2', 'pluggy': '0.6.0'}, 'Plugins': {'metadata': '1.5.1', 'json': '0.4.0', 'html': '1.16.1'}}\nrootdir: /Users/gguthe/frost, inifile:\nplugins: metadata-1.5.1, json-0.4.0, html-1.16.1\ncollected 3 items\n\nhttpbin/test_httpbin_ip_in_aws.py .F.                                                                                               [100%]\n\n================================================================ FAILURES =================================================================\n____________________________________________ test_httpbin_ip_in_aws[13.58.0.0-aws_ip_ranges1] _____________________________________________\n\nip = '13.58.0.0'\naws_ip_ranges = [{'ip_prefix': '13.32.0.0/15', 'region': 'GLOBAL', 'service': 'AMAZON'}, {'ip_prefix': '13.35.0.0/16', 'region': 'GLOB...on': 'us-west-1', 'service': 'AMAZON'}, {'ip_prefix': '13.57.0.0/16', 'region': 'us-west-1', 'service': 'AMAZON'}, ...]\n\n    @pytest.mark.httpbin\n    @pytest.mark.aws_ip_ranges\n    @pytest.mark.parametrize(\n        ['ip', 'aws_ip_ranges'],\n        zip(get_httpbin_ips(), itertools.repeat(get_aws_ips())),\n        # ids=lambda ip: ip\n        )\n    def test_httpbin_ip_in_aws(ip, aws_ip_ranges):\n        for aws_ip_range in aws_ip_ranges:\n>           assert ipaddress.IPv4Address(ip) not in ipaddress.ip_network(aws_ip_range['ip_prefix']), \\\n              \"{0} is in AWS range {1[ip_prefix]} region {1[region]} service {1[service]}\".format(ip, aws_ip_range)\nE           AssertionError: 13.58.0.0 is in AWS range 13.58.0.0/15 region us-east-2 service AMAZON\nE           assert IPv4Address('13.58.0.0') not in IPv4Network('13.58.0.0/15')\nE            +  where IPv4Address('13.58.0.0') = <class 'ipaddress.IPv4Address'>('13.58.0.0')\nE            +    where <class 'ipaddress.IPv4Address'> = ipaddress.IPv4Address\nE            +  and   IPv4Network('13.58.0.0/15') = <function ip_network at 0x107cf66a8>('13.58.0.0/15')\nE            +    where <function ip_network at 0x107cf66a8> = ipaddress.ip_network\n\nhttpbin/test_httpbin_ip_in_aws.py:43: AssertionError\n=================================================== 1 failed, 2 passed in 15.69 seconds ===================================================\n```\n\nNote: marking tests as expected failures with `@pytest.mark.xfail` can hide data fetching errors\n\nTo improve this we could:\n\n1. Add parametrize ids so it's clearer which parametrize caused test failures\n1. Add directions about why it's an issue and how to fix it or what the associated risks are\n\nAs we add more tests we can:\n\n1. Move the JSON fetching functions to `<service name>/resources.py` files and import them into the test\n1. Move the fetching logic to a shared library `<service name>/client.py` and save to the pytest cache\n1. Add a `<service name>/conftest.py` and register the service's marks in a `pytest_configure` to resolve some warnings\n"
},
{
  "name": "for-firefox",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "assets",
      "badges.js",
      "index.html",
      "media",
      "package-lock.json",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "Firefox is only as strong as its passionate users. Because we're independent, people need to make a conscious choice to use a non-default browser on their system. We're most successful when happy users tell others about an alternative worth trying.\n\nIf you're a Firefox user and want to show your support, we've made a collection of badges you can add to your website to tell users, &ldquo;I use Firefox, and you should too!&rdquo; [Check them out here.](https://mozilla.github.io/for-firefox/)\n"
},
{
  "name": "tensorflow",
  "files": {
    "/": [
      ".bazelrc",
      ".github",
      ".gitignore",
      ".taskcluster.yml",
      "ACKNOWLEDGMENTS",
      "ADOPTERS.md",
      "AUTHORS",
      "BUILD",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUES.md",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "README.md",
      "RELEASE.md",
      "SECURITY.md",
      "WORKSPACE",
      "arm_compiler.BUILD",
      "compilers",
      "configure",
      "configure.py",
      "models.BUILD",
      "package.sh",
      "taskcluster",
      "tc-apt.sh",
      "tc-brew.sh",
      "tc-build.sh",
      "tc-package.sh",
      "tc-pacman.sh",
      "tc-schedule.sh",
      "tc-setup.sh",
      "tc-vars.sh",
      "tensorflow",
      "third_party",
      "tools"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "<div align=\"center\">\n  <img src=\"https://www.tensorflow.org/images/tf_logo_transp.png\"><br><br>\n</div>\n\n-----------------\n\n\n| **`Documentation`** |\n|-----------------|\n| [![Documentation](https://img.shields.io/badge/api-reference-blue.svg)](https://www.tensorflow.org/api_docs/) |\n\n**TensorFlow** is an open source software library for numerical computation\nusing data flow graphs. The graph nodes represent mathematical operations, while\nthe graph edges represent the multidimensional data arrays (tensors) that flow\nbetween them. This flexible architecture enables you to deploy computation to\none or more CPUs or GPUs in a desktop, server, or mobile device without\nrewriting code. TensorFlow also includes\n[TensorBoard](https://github.com/tensorflow/tensorboard), a data visualization\ntoolkit.\n\nTensorFlow was originally developed by researchers and engineers\nworking on the Google Brain team within Google's Machine Intelligence Research\norganization for the purposes of conducting machine learning and deep neural\nnetworks research.  The system is general enough to be applicable in a wide\nvariety of other domains, as well.\n\nTensorFlow provides stable Python and C APIs as well as non-guaranteed backwards\ncompatible API's for C++, Go, Java, JavaScript and Swift.\n\nKeep up to date with release announcements and security updates by\nsubscribing to\n[announce@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/announce).\n\n## Installation\n\nTo install the current release for CPU-only:\n\n```\npip install tensorflow\n```\n\nUse the GPU package for CUDA-enabled GPU cards:\n\n```\npip install tensorflow-gpu\n```\n\n*See [Installing TensorFlow](https://www.tensorflow.org/install) for detailed\ninstructions, and how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n**Nightly pip packages**\n* We are pleased to announce that TensorFlow now offers nightly pip packages\nunder the [tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-gpu](https://pypi.python.org/pypi/tf-nightly-gpu) project on pypi.\nSimply run `pip install tf-nightly` or `pip install tf-nightly-gpu` in a clean\nenvironment to install the nightly TensorFlow build. We support CPU and GPU\npackages on Linux, Mac, and Windows.\n\n#### *Try your first TensorFlow program*\n\n```shell\n$ python\n```\n\n```python\n>>> import tensorflow as tf\n>>> tf.enable_eager_execution()\n>>> tf.add(1, 2).numpy()\n3\n>>> hello = tf.constant('Hello, TensorFlow!')\n>>> hello.numpy()\n'Hello, TensorFlow!'\n```\n\nLearn more examples about how to do specific tasks in TensorFlow at the\n[tutorials page of tensorflow.org](https://www.tensorflow.org/tutorials/).\n\n## Contribution guidelines\n\n**If you want to contribute to TensorFlow, be sure to review the [contribution\nguidelines](CONTRIBUTING.md). This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.**\n\n**We use [GitHub issues](https://github.com/tensorflow/tensorflow/issues) for\ntracking requests and bugs, so please see\n[TensorFlow Discuss](https://groups.google.com/a/tensorflow.org/forum/#!forum/discuss)\nfor general questions and discussion, and please direct specific questions to\n[Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow).**\n\nThe TensorFlow project strives to abide by generally accepted best practices in open-source software development:\n\n[![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/1486/badge)](https://bestpractices.coreinfrastructure.org/projects/1486)\n\n\n## Continuous build status\n\n### Official Builds\n\n| Build Type      | Status | Artifacts |\n| ---             | ---    | ---       |\n| **Linux CPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-cc.html) | [pypi](https://pypi.org/project/tf-nightly/) |\n| **Linux GPU**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-gpu-py3.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/) |\n| **Linux XLA**   | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/ubuntu-xla.html) | TBA |\n| **MacOS**       | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/macos-py2-cc.html) | [pypi](https://pypi.org/project/tf-nightly/) |\n| **Windows CPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-cpu.html) | [pypi](https://pypi.org/project/tf-nightly/) |\n| **Windows GPU** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/windows-gpu.html) | [pypi](https://pypi.org/project/tf-nightly-gpu/) |\n| **Android**     | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/android.html) | [![Download](https://api.bintray.com/packages/google/tensorflow/tensorflow/images/download.svg)](https://bintray.com/google/tensorflow/tensorflow/_latestVersion) |\n| **Raspberry Pi 0 and 1** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi01-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv6l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv6l.whl) |\n| **Raspberry Pi 2 and 3** | [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py2.html) [![Status](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.svg)](https://storage.googleapis.com/tensorflow-kokoro-build-badges/rpi23-py3.html) | [Py2](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp27-none-linux_armv7l.whl) [Py3](https://storage.googleapis.com/tensorflow-nightly/tensorflow-1.10.0-cp34-none-linux_armv7l.whl) |\n\n\n### Community Supported Builds\n\nBuild Type                                                                                                                                                                                      | Status                                                                                                                                                                                   | Artifacts\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------\n**IBM s390x**                                                                                                                                                                                   | [![Build Status](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/badge/icon)](http://ibmz-ci.osuosl.org/job/TensorFlow_IBMZ_CI/)                                                        | TBA\n**Linux ppc64le CPU** Nightly                                                                                                                                                                   | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Build/)                                  | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Nightly_Artifact/)\n**Linux ppc64le CPU** Stable Release                                                                                                                                                            | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)                  | [Release](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_CPU_Release_Build/)\n**Linux ppc64le GPU** Nightly                                                                                                                                                                   | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Build/)                                  | [Nightly](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Nightly_Artifact/)\n**Linux ppc64le GPU** Stable Release                                                                                                                                                            | [![Build Status](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/badge/icon)](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)                  | [Release](https://powerci.osuosl.org/job/TensorFlow_PPC64LE_GPU_Release_Build/)\n**Linux CPU with Intel\u00ae MKL-DNN** Nightly                                                                                                                                                       | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-linux-cpu/)                                | [Nightly](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-whl-nightly/)\n**Linux CPU with Intel\u00ae MKL-DNN** Python 2.7<br> **Linux CPU with Intel\u00ae MKL-DNN** Python 3.4<br> **Linux CPU with Intel\u00ae MKL-DNN** Python 3.5<br> **Linux CPU with Intel\u00ae MKL-DNN** Python 3.6 | [![Build Status](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/badge/icon)](https://tensorflow-ci.intel.com/job/tensorflow-mkl-build-release-whl/lastStableBuild) | [1.12.0 py2.7](https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.12.0-cp27-cp27mu-linux_x86_64.whl)<br>[1.12.0 py3.4](https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.12.0-cp34-cp34m-linux_x86_64.whl)<br>[1.12.0 py3.5](https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.12.0-cp35-cp35m-linux_x86_64.whl)<br>[1.12.0 py3.6](https://storage.googleapis.com/intel-optimized-tensorflow/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl)\n\n## For more information\n\n*   [TensorFlow Website](https://www.tensorflow.org)\n*   [TensorFlow Tutorials](https://www.tensorflow.org/tutorials/)\n*   [TensorFlow Model Zoo](https://github.com/tensorflow/models)\n*   [TensorFlow Twitter](https://twitter.com/tensorflow)\n*   [TensorFlow Blog](https://medium.com/tensorflow)\n*   [TensorFlow Course at Stanford](https://web.stanford.edu/class/cs20si)\n*   [TensorFlow Roadmap](https://www.tensorflow.org/community/roadmap)\n*   [TensorFlow White Papers](https://www.tensorflow.org/about/bib)\n*   [TensorFlow YouTube Channel](https://www.youtube.com/channel/UC0rqucBdTuFTjJiefW5t-IQ)\n*   [TensorFlow Visualization Toolkit](https://github.com/tensorflow/tensorboard)\n\nLearn more about the TensorFlow community at the [community page of tensorflow.org](https://www.tensorflow.org/community) for a few ways to participate.\n\n## License\n\n[Apache License 2.0](LICENSE)\n"
},
{
  "name": "webxr-polyfill",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CODING.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "dist-footer.js",
      "dist-header.js",
      "examples",
      "index.html",
      "package-lock.json",
      "package.json",
      "polyfill",
      "screenshots",
      "tests",
      "viewer.html",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# (deprecated, experimental) WebXR polyfill with examples\n\nThe API for \"WebXR\" implemented in this repository is based on a [proposed draft proposal for WebXR](https://github.com/mozilla/webxr-api) we created as a starting point for discussing WebXR in the fall of 2017, to explore what it might mean to expand WebVR to include AR/MR capabilities.\n\nWe initially created this polyfill when the community group was calling the specification \"WebVR\", so using \"WebXR\" was not confusing. Now that the community group is working towards changing the name of the spec, this repo may be confusing to newcomers. \n\nWe're working to bring this repo's master branch in line with the community group's draft spec.  But that work is not yet complete.\n\nThe WebVR community has shifted WebVR in this direction.  The group is now called the [Immersive Web Community Group](https://github.com/immersive-web/) and the WebVR specification has now become the [WebXR Device API](https://github.com/immersive-web/webxr). You should consider that spec as ground-truth for WebXR, and it is what you will likely see appearing in browsers through the rest of 2018 and into 2019.\n\nWe will continue to experiment with extensions to, and new ideas for, WebXR in this library.  Soon, we expect it to be integrated directly in our [WebXR Viewer iOS app](https://github.com/mozilla-mobile/webxr-ios) and no longer be included directly in any web pages.\n\n## WebXR library with examples\n\nThis repository holds an implementation of a non-compliant version of WebXR, along with sample code demonstrating how to use the API.\n\n## WARNING\n\nTHIS SOFTWARE IS NON-STANDARD AND PRERELEASE, IS *NOT* READY FOR PRODUCTION USE, AND *WILL* SOON HAVE BREAKING CHANGES.\n\nNOTHING IN THIS REPO COMES WITH ANY WARRENTY WHATSOEVER. DO NOT USE IT FOR ANYTHING EXCEPT EXPERIMENTS.\n\nThere may be pieces of the library that are stubbed out and throw 'Not implemented' when called.\n\n## Running the examples\n\nThe master branch of this repo is automatically built and hosted at https://examples.webxrexperiments.com\n \nThe develop branch is hosted at https://develop.examples.webxrexperiments.com\n\n## Building and Running the examples\n\nClone this repo and then change directories into webxr-polyfill/\n\n<a href=\"https://docs.npmjs.com/getting-started/installing-node\">Install npm</a> and then run the following:\n\n\tnpm install   # downloads webpack and an http server\n\tnpm start     # builds the polyfill in dist/webxr-polyfill.js and start the http server in the current directory\n\nUsing one of the supported browsers listed below, go to http://YOUR_HOST_NAME:8080/\n\n## Portable builds\n\nTo build the WebXR polyfill into a single file that you can use in a different codebase: \n\n\tnpm run build\n\nThe resulting file will be in dist/webxr-polyfill.js\n\n## Writing your own XR apps\n\nThe WebXR polyfill is not dependent on any external libraries, but examples/common.js has a handy base class, XRExampleBase, that wraps all of the boilerplate of starting a WebXR session and rendering into a WebGL layer using Three.js.\n\nLook in [examples/ar_simplest/index.html](https://github.com/mozilla/webxr-polyfill/blob/master/examples/ar_simplest/index.html) for an example of how to extend [XRExampleBase](https://github.com/mozilla/webxr-polyfill/blob/master/examples/common.js) and how to start up an app.\n\nIf you run these apps on Mozilla's [ARKit based iOS app](https://github.com/mozilla-mobile/webxr-ios) then they will use the class in [polyfill/platform/ARKitWrapper.js](https://github.com/mozilla/webxr-polyfill/blob/master/polyfill/platform/ARKitWrapper.js) to get pose and anchor data out of ARKit.\n\nIf you run these apps on Google's old ARCore backed experimental browser then they will use the class in [polyfill/platform/ARCoreCameraRenderer.js](https://github.com/mozilla/webxr-polyfill/blob/master/polyfill/platform/ARCoreCameraRenderer.js) to use data out of ARCore.\n\nIf you run these apps on desktop Firefox or Chrome with a WebVR 1.1 supported VR headset, the headset will be exposed as a WebXR XRDisplay.\n\nIf you run these apps on a device with no VR or AR tracking, the apps will use the 3dof orientation provided by Javascript orientation events.\n \n## Supported Displays\n\n- Flat Display (AR only, needs VR)\n- WebVR 1.1 HMD (VR only, needs AR)\n- Cardboard (NOT YET)\n- Hololens (NOT YET)\n\n## Supported Realities\n\n- Camera Reality (ARKit on Mozilla iOS Test App, WebARonARCore on Android, WebARonARKit on iOS, WebRTC video stream (PARTIAL))\n- Virtual Reality (Desktop Firefox with Vive and Rift, Daydream (NOT TESTED), GearVR (Not Tested), Edge with MS MR headsets (NOT TESTED))\n- Passthrough Reality (NOT YET)\n\n## Supported Browsers\n\n- Mozilla [WebXR Playground](https://github.com/mozilla/webxr-ios) iOS App using ARKit\n- Google [ARCore Test Chrome on Android](https://github.com/google-ar/WebARonARCore)\n- Google [ARKit Test Chrome on iOS](https://github.com/google-ar/WebARonARKit)\n- Desktop Firefox with WebVR 1.1 HMDs\n- Mobile Safari, Chrome, and Firefox (PARTIAL, Daydream NOT TESTED)\n- GearVR Internet (NOT TESTED)\n"
},
{
  "name": "bugzilla-dashboard-backend",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      ".isort.cfg",
      ".pre-commit-config.yaml",
      ".taskcluster.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "bugzilla_dashboard",
      "mozdata.ini",
      "queries",
      "requirements-dev.txt",
      "requirements.txt",
      "run.sh",
      "setup.py",
      "taskcluster-hook.json",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "Bugzilla dashboard backend\n===================\nBackend to fetch details from bugzilla API for bugzilla dashboard\n\nSetup\n---------------\nThis is a Python 3 application, so it should be easy to bootstrap on your computer:\nCreate Virtual environment:\n```\nmkvirtualenv -p /usr/bin/python3 bugzilla-dashboard-backend\n```\nInstall dependencies:\n```\npip install -r requirements.txt -r requirements-dev.txt\n```\nTo run the linting tests:\n```\npre-commit run -a\n```\nTo run tests:\n```\npytest\n```\n\nDeployment\n----------\n\nFor deployment, you will need to update the ci configuration with the git hash of this\nproject.\nSee https://bugzilla.mozilla.org/show_bug.cgi?id=1691378 as example.\n"
},
{
  "name": "sheriffing-tools",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "benchmarks"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "krakenbenchmark.mozilla.org",
  "files": {
    "/": [
      "analyze-results.js",
      "changelog.txt",
      "compare-results.js",
      "explanations",
      "index.html",
      "json2.js",
      "kraken-1.0",
      "kraken-1.1",
      "kraken.css",
      "make-hosted.py",
      "resources",
      "sunspider",
      "sunspider-0.9.1",
      "sunspider-compare-results",
      "tests"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "looker-poc",
  "files": {
    "/": [
      "dashboards",
      "models",
      "views"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "wg-tracker",
  "files": {
    "/": [
      ".gitignore",
      "Cargo.lock",
      "Cargo.toml",
      "README.md",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# wg-tracker\n\nThis is a tool to help Gecko developers keep track of resolutions made in\nthe CSS Working Group.  Whenever a resolution is made and recorded in\n[an issue](https://github.com/w3c/csswg-drafts/issues/), a tracking issue\nwill be filed in the\n[wg-decisions](https://github.com/mozilla/wg-decisions/) repository.\nFrom there, Gecko developers can triage the resolutions and decide whether\nto file a bug in [Bugzilla](https://bugzilla.mozilla.org/) or to close them as\nas no action required.\n\nThis tool is run on a server maintained by [@heycam](https://github.com/heycam)\nand performs changes under the\n[@mozilla-apprentice](https://github.com/mozilla-apprentice) user.\n"
},
{
  "name": "MozStumbler",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "LICENSE.osmdroid",
      "Makefile",
      "README.md",
      "android",
      "android_studio",
      "build.gradle",
      "docs",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "libraries",
      "release_check.py",
      "rename_release.sh",
      "settings.gradle",
      "test.sh"
    ],
    "/docs": [
      "screencaps",
      "testing.md"
    ]
  },
  "makefile": "test: libtest unittest\n\t./gradlew copyTestResources\n\t./gradlew testGithubUnittest --info\n\nlibtest:\n\tcd libraries/stumbler; ./gradlew test\n\nunittest:\n\t./gradlew assembleGithubUnittest\n\ndebug:\n\t./gradlew assembleGithubDebug\n\ngithub:\n\t./release_check.py github\n\t./gradlew assembleGithubRelease\n\tsh rename_release.sh github-release\n\nplaystore:\n\t./release_check.py playstore\n\t./gradlew assemblePlaystoreRelease\n\tsh rename_release.sh playstore-release\n\nfdroid:\n\t./gradlew assembleFdroidRelease\n\tsh rename_release.sh fdroid-release\n\nclean:\n\trm -rf outputs\n\trm -rf libraries/stumbler/build\n\t./gradlew clean\n\ninstall_debug:\n\t./gradlew installGithubDebug\n",
  "readme": "MozStumbler [![Build Status](https://travis-ci.org/mozilla/MozStumbler.png?branch=dev)](https://travis-ci.org/mozilla/MozStumbler)\n===========\n\n**Note:**\n*Mozilla Stumbler was\n[retired on February 8, 2021](https://discourse.mozilla.org/t/retiring-mozilla-stumbler/75206).\nThis code works on Android 9, but not Android 10 or later.*\n\nPlease refer to the [wiki](https://github.com/mozilla/MozStumbler/wiki) for detailed documentation.\n\n# Building a debug version from command line #\n\nThe build system is smart enough to automatically download and install\nall the parts of the Android SDK for you.  If you cannot build, you\ncan either try to fix your Android dev enviroment to fit the\nandroid/build.gradle requirements - or you can simply remove\nANDROID_HOME, and all traces of your Android SDK from your PATH.\n\n```\nmake\n```\n\n# Building a debug version from Android Studio #\n\n[![Edit run configuration](https://raw.githubusercontent.com/mozilla/MozStumbler/dev/docs/screencaps/edit_configuration.png)](https://raw.githubusercontent.com/mozilla/MozStumbler/dev/docs/screencaps/edit_configuration.png)\n\n\n[![Add new run configuration](https://raw.githubusercontent.com/mozilla/MozStumbler/dev/docs/screencaps/add_new_config.png)](https://raw.githubusercontent.com/mozilla/MozStumbler/dev/docs/screencaps/add_new_config.png)\n\nSetup the Android Application to use two gradle aware make targets.\nYou must set 'Installation Options' to \"Deploy Nothing\".\n\nThe tricky part is to set the build tasks.  You will need two tasks of\ntype 'Gradle-Aware Make'.  Android Studio will autocomplete the names\nbelow when you start typing them in.\n\n1.  :android:assembleGithubDebug\n2.  :android:installGithubDebug\n\n[![Setup new run configuration](https://raw.githubusercontent.com/mozilla/MozStumbler/dev/docs/screencaps/setup_android_config.png)](https://raw.githubusercontent.com/mozilla/MozStumbler/dev/docs/screencaps/setup_android_config.png)\n"
},
{
  "name": "probe_knowledge_repo",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "README.md",
      "functions.py",
      "metrics",
      "metrics_schema.yaml",
      "requirements.txt"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "# Probe Knowledge Repo (Prototype)\n\nThis is intended to be a proof of concept repository meant to house knowledge and resources about our telemetry probes.\n\nEach probe for Firefox Desktop and Fenix has its own yaml file found in the `./metrics/fenix` and `./metrics/firefox_desktop` directories.\n\nFiles are split into two sections - the first section contains basic information reproduced from the probe dictionaries. The second section contains user-editable fields meant to be easily updated with links, tribal knowledge, gotchas etc about how the probe can be used analyzed. These yaml files can be edited directly in github.\n\n`functions.py` contains some (pretty hacky) python functions for pulling probe data down from the [probe info service](https://mozilla.github.io/probe-scraper/) and writing/updating the probe yaml files. Specifically:\n\n* `update_file_list` will pull down the most recent probe data from the probe info service and update existing yaml files where needed, as well as write new files if it finds new probes. This function will NOT overwrite what's been added to the user-editable sections of the existing yaml files.\n\n* `build_files` will create the per-probe yaml files from scratch. This function WILL overwrite any previous changes to the user-editable sections.\n\nI modified this repo from Jeff's [here](https://github.com/jklukas/yamlfun/), but I'm really using what he has there (yet).\n\n## Things to do\n\n1. Are these the right editable fields for the yaml files?\n2. Add tests (validate the format of the yaml files like jeff was doing).\n3. Does the formatting of the additional fields I added make them technically not yaml files anymore?\n4. Setup CI to periodically run the `update_file_list` function to get new probes and update probe metadata.\n5. Clean up the code in `functions.py` to conform with all the standards and such. Export the functions so they can be run from the CLI\n6. Define the process for those wanting to edit the yaml files - do we need reviews (I am leaning towards no).\n\n```bash\npython3 -m venv venv\npip install -r requirements.txt\n```\n"
},
{
  "name": "maintainer-cohort",
  "files": {
    "/": [
      ".DS_Store",
      "CODE_OF_CONDUCT.MD",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gemfile",
      "Gemfile.lock",
      "LICENSE",
      "README.md",
      "_articles",
      "_config.yml",
      "_includes",
      "_layouts",
      "_site",
      "contents.html",
      "css",
      "img",
      "index.md"
    ]
  },
  "makefile": null,
  "readme": "# README\n\nThis content is currently under development.\n"
},
{
  "name": "open-leadership-framework",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gemfile",
      "Gemfile.lock",
      "LICENSE.md",
      "README.md",
      "_articles",
      "_config.yml",
      "_includes",
      "_layouts",
      "acknowldgements.md",
      "canvas.md",
      "community-calls",
      "contents.html",
      "css",
      "decision-log.md",
      "global-sprint-coaching",
      "il8n",
      "images",
      "index.md",
      "personas.md",
      "roadmap.md",
      "use-cases",
      "vision.md"
    ]
  },
  "makefile": null,
  "readme": "# The Open Leadership Framework README\n\nProject lead and contact: [Chad Sansing](mailto:chad@mozillafoundation.org)\n\n**Note:** We are busy updating our Open Leadership Map white paper language into the Open Leadership Framework. Please excuse us as we make incremental changes in the open. We will have an \"official\" launch later in H1, 2018, once our edits and new content are set.  \n\n## Background\n\nWhat is open leadership? What makes it different from leadership, alone? How can we help make it a renewable resource that inspires virtuous loops of community participation and contribution to problem-solving worldwide? How can we help open leaders make their efforts to promote Internet health \nas inclusive, inviting, and empowering as possible?\n\nIn pursuit of answers to questions like those, we\u2019ve been developing an Open Leadership Framework. The aim of the framework is to establish an adaptable set of open leadership principles, practices, and skills that people can use for personal and professional development as part of an open community or project. More practical resources built from this framework -  like guides, curriculum, programming, and events - will come later. \n\nAt Mozilla, we\u2019re passionate about working open. This framework is our latest entry into ongoing conversations about what it means to work and lead open in today\u2019s world.\n\nWe believe that open leaders design, build, and empower for understanding, sharing, and participation and inclusion.\n\n![A table showing the skills of open leadership grouped by principle and practice](images/olf-framework-bw.png)\n\nIn the short-term, we hope the framework contributes to conversations about how to work open and enact the principles, practices, and skills of open leadership. In the long-term, we hope the map helps increase public demand for an open Web and society through projects and products that fulfill users\u2019 needs for agency, authenticity, and ownership.\n\nThis framework is the first step of a larger, on-going project to develop curriculum, events, trainings, and online resouces like an open leadership map that will help people locate themselves and then move ahead in their open leadership journeys.\n\n## Get involved!\n\nYou can visit the framework to check it out for yourself and think about how it relates to your understanding and practice of open leadership.\n\nYou should feel free to adapt, criticize, question, remix, and repurpose the framework for your own work. It\u2019s a living document and pushback is welcome and necessary to improve it.\n\nYou can also review these personas and use-cases to see how the framework might apply to your work, projects, and communities.\n\nIf you\u2019d like to discuss the framework further, you can send questions and suggestions directly to project lead [Chad Sansing](mailto:chad@mozillafoundation.org).\n\nFinally, if you\u2019d like to see examples of the Open Leadership Framework at work across Mozilla\u2019s network and the movement for Internet Health, check out these opportunities to get involved with open leadership programming:\n\n- [Fellowships](https://foundation.mozilla.org/opportunity/fellowships/)\n- [Global Sprint](https://mozilla.github.io/global-sprint/)\n- [Internet Health Report](https://internethealthreport.org/)\n- [MozFest](https://mozillafestival.org/)\n- [Open Leaders](https://mozilla.github.io/leadership-training/)\n- [Open Leadership 101](https://mozilla.teachable.com/p/open-leadership-101)\n- [Open Leadership Training Series](https://mzl.la/open-leadership)\n\n## Office hours\n\nOffice hours are on pause right now.\n\n## Community calls\n\nCommunity calls are on pause right now.\n\n## What kinds of skills do I need to contribute?\n\nYou don't need any special technical skills to contribute to this project. We will work with you to find a way to gather your feedback on this public alpha of the Open Leadership Framework. There are many ways to contrbiute regardless of your experience level with GitHub or working open.\n\n## Are you new to open leadership or working open?\n\nThis blog post, [\"How to Work Open\"](https://openmatt.org/2011/04/06/how-to-work-open/), by Matt Thompson, is an excellent primer.\n\n## How to contribute\n\nYou can contribute to this white paper in any way that makes sense to you.\n\nYou can follow this series of [weeknotes](https://medium.com/@chadsansing/open-leadership-map-weeknote-1-4f0c4b1b7798), or weekly updates about the project.\n\nYou can [stress test prototype use cases](https://medium.com/@chadsansing/testing-open-leadership-map-use-cases-dd8f41ccc8b0) built from the map's framework for personal and professional growth.\n\nEveryone is welcome to join the OLM office hours and community calls.\n\n***If you have a lot of experience with GitHub and working open,*** you might clone or fork the paper, send us pull requests to incorporate suggested edits, and/or even create a new version of the paper for your own use.\n\n***If you have some experience with GitHub and working open,*** you might share your feedback with us using the Issue Tracker feature as explained below.\n\n***If you have little experience with GitHub and working open,*** you can comment on [this Google Doc](https://docs.google.com/document/d/1CxQeaZW4fckRqmPeHn9SGSnY6f2cJX5bONEJYbhVTk0/edit#heading=h.daj7ikbuxrpq) or send your questions and feedback to curriculum manager [Chad Sansing](mailto:chad@mozillafoundation.org), the contact person for this project.\n\n\n## How to file an issue on GitHub\n\nGitHub can be challenging for new users. If you experience difficulty with it, you are not alone. We want to help you overcome those challenges or find a way to contribute that works for you.\n\nFor this project, we can use a feature called the \"Issue Tracker\" in GitHub to communicate with one another. It's kind of like a shared message board combined with a to-do list. To address an issue to a specific person, you add their name to the issue the same way you would to a tweet, like this for example: @chadsansing would address your message to Chad, the contact person for this project.\n\nIf the Issue Tracker is too difficult to use, contact Chad. He will work with you and help you find a way to contribute no matter what.\n\nHere are some steps you can take to get started on GitHub if you'd like to try it.\n\n1. First, create an account on [GitHub](https://github.com).\n\n2. Then visit [our repo](https://github.com/mozilla/open-leadership-framework) to contribute to the white paper.\n\n3. Next, click on the \"Issues\" tab near the top of the page.\n\n4. Finally, click on the green \"New Issue\" button to the left of the page. You can then title your issue and add content. Specificity helps.\n\nBefore you submit your issue, label it. You can choose one from the \"Labels\" dropdown menu to the right of your issue.\n\nWe have labels that will let you suggest:\n\n- *Case studies.* Help us curate stories that illustrate the best of Open Leadership and Working Open.\n- *Challenges.* Let us know when we get something wrong and suggest a way to fix it.\n- *Copy edits.* Help us find the mistakes and typos we've missed.\n- *Questions.* Let us know what you wonder about as you review the white paper.\n- *Other types of issues.* Feel free to share whatever is on your mind regarding the work.\n\nIf you'd like to learn even more about GitHub, check out the [*GitHub for Collaboration* section](https://mozilla.github.io/open-leadership-training-series/articles/github-for-collaboration/) of Mozilla's [Open Leadership Training Series](https://mozilla.github.io/open-leadership-training-series/articles/github-for-collaboration/).\n\n## What's next?\n\nLook for our \"official\" relase of the Open Leadership Framework later in H2, 2018.\n\n## Thank you! \n\nWe are so grateful for your time, attention, and leadership! Let us know how we might best move forward together.\n\n*Last updated 6/1/18*\n"
},
{
  "name": "leanplum-data-export",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      "Dockerfile",
      "Makefile",
      "README.md",
      "bin",
      "docker-compose.yml",
      "leanplum_data_export",
      "requirements",
      "setup.py",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: help clean clean-pyc clean-build list test coverage release\n\nhelp:\n\t@echo \"  clean-build -          Remove build artifacts\"\n\t@echo \"  clean-pyc -            Remove Python file artifacts\"\n\t@echo \"  lint -                 Check style with flake8\"\n\t@echo \"  test -                 Run tests quickly with the default Python\"\n\t@echo \"  install-requirements - install the requirements for development\"\n\t@echo \"  build                  Builds the docker images for the docker-compose setup\"\n\t@echo \"  docker-rm              Stops and removes all docker containers\"\n\t@echo \"  run                    Run a command. Can run scripts, e.g. make run COMMAND=\\\"./scripts/schema_generator.sh\\\"\"\n\t@echo \"  shell                  Opens a Bash shell\"\n\nclean: clean-build clean-pyc docker-rm\n\nclean-build:\n\trm -fr build/\n\trm -fr dist/\n\trm -fr *.egg-info\n\nclean-pyc:\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\nlint:\n\tflake8 .\n\ntest:\n\tdocker-compose run app test\n\ninstall-requirements:\n\tpip install -r requirements/requirements.txt\n\tpip install -r requirements/test_requirements.txt\n\nbuild:\n\tdocker-compose build\n\ndocker-rm: stop\n\tdocker-compose rm -f\n\nshell:\n\tdocker-compose run app bash\n\nrun:\n\tdocker-compose run -e GCLOUD_SERVICE_KEY app $(COMMAND)\n\nstop:\n\tdocker-compose down\n\tdocker-compose stop\n",
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/leanplum-data-export.svg?style=svg)](https://circleci.com/gh/mozilla/leanplum-data-export)\n\n# Leanplum Data Export\nThis repository is the job to export Mozilla data from Leanplum and into BQ.\n\nIt's dockerized to run on GKE. To run locally:\n\n```\npip install .\nleanplum-data-export export-leanplum \\\n  --app-id $LEANPLUM_APP_ID \\\n  --client-key $LEANPLUM_CLIENT_KEY \\\n  --date 20190101 \\\n  --bucket gcs-leanplum-export \\\n  --table-prefix leanplum \\\n  --bq-dataset dev_external \\\n  --prefix dev\n```\n\nDoing it this way will, by default, use your local GCP credentials.\nGCP only allows you to do this a few times\n\nAlternatively, run in Docker.\n\nFirst, create a service account with access to GCS and BigQuery.\nDownload a JSON key file and make it available in your\nenvironment as `GCLOUD_SERVICE_KEY`. Then run:\n\n```\nbq mk leanplum\nmake run COMMAND=\"leanplum-data-export export-leanplum \\\n  --app-id $LEANPLUM_APP_ID \\\n  --client-key $LEANPLUM_CLIENT_KEY \\\n  --date 20190101 \\\n  --bucket gcs-leanplum-export \\\n  --table-prefix leanplum \\\n  --bq-dataset leanplum \\\n  --prefix dev\"\n```\n\nThat will create the dataset in BQ, download the files, and make\nthem available in BQ in that dataset as external tables.\n\n## Development and Testing\n\nWhile iterating on development, we recommend using virtualenv\nto run the tests locally.\n\n### Run tests locally\n\nInstall requirements locally:\n```\npython3 -m virtualenv venv\nsource venv/bin/activate\nmake install-requirements\n```\n\nRun tests locally:\n```\npytest tests/\n```\n\n### Run tests in docker\n\nYou can run the tests just as CI does by building the container\nand running the tests.\n\n```\nmake clean && make build\nmake test\n```\n\n### Deployment\n\nThis project deploys automatically to GCR. The latest release is used to run the job.\n"
},
{
  "name": "dpx",
  "files": {
    "/": [
      ".DS_Store",
      ".github",
      "README.md"
    ],
    "/.github": [
      ".DS_Store",
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "# DPX\n\nPlease use this repo to report things you're working on for the Design and People Experiences team.\n\nIn order to request design input or support on something you're working on, you can either:\n\n1. Create an issue in your own repo and apply the relevant label (`zenhub-hubs` for Hubs, `zenhub-vpn` for VPN, `zenhub-prod-integrity` for product integrity)\n1. Create an issue in this repo using one of the [issue templates](https://github.com/mozilla/dpx-design-process/issues/new/choose)\n\nIf you create an issue in your own repo, make sure that it's a repo that the DPX team has at minimum read access to, so that they can be assigned to those issues for easier tracking. When in doubt, ping [@malqinneh](https://github.com/malqinneh).\n\nIssues from these sources will be linked to the DPX team's Zenhub workflow boards, where you can also see what else they're working on: \n\n* [DPX Hubs](https://app.zenhub.com/workspaces/dpx-hubs-5fb2cede17eb69001d393a6a/board?labels=zenhub-hubs&repos=104933644)\n* [DPX Privacy & Security - VPN](https://app.zenhub.com/workspaces/dpx-security--privacy-5faedaa62b4e9a000e3418ab/board?repos=293940617,287695634)\n* [DPX Product Integrity](https://app.zenhub.com/workspaces/dpx-product-integrity-team-5f626abe18405f000fd9d8ae/board?labels=zenhub-prod-integrity)\n* [DPX Future Products](https://app.zenhub.com/workspaces/dpx-future-products-601321fc13db9c0011a9d789/board?repos=293940617)\n* [DPX Research](https://app.zenhub.com/workspaces/dpx-research-5fd7c12d7260fd0015aaff59/board?repos=293940617)\n"
},
{
  "name": "redash-stmo",
  "files": {
    "/": [
      ".circleci",
      ".coverage",
      ".coveragerc",
      ".dockerignore",
      ".editorconfig",
      ".gitignore",
      ".isort.cfg",
      ".prettierrc.js",
      "AUTHORS.rst",
      "CHANGELOG.rst",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.rst",
      "bin",
      "docker-compose.yml",
      "pytest.ini",
      "setup.cfg",
      "setup.py",
      "src",
      "tests"
    ],
    "/.circleci": [
      "config.yml",
      "docker-compose.circle.yml"
    ]
  },
  "makefile": ".PHONY: bash build clean database devserver node_modules test_database test up\n\nbash:\n\tdocker-compose run --rm server bash\n\nbuild:\n\tdocker-compose build --pull\n\nclean:\n\trm -rf build/ dist/\n\nup:\n\tdocker-compose up\n\nnode_modules:\n\tdocker-compose run --rm server npm install\n\ndatabase:\n\tdocker-compose run --rm server create_tables\n\ndevserver:\n\tdocker-compose run --publish 8080:8080 server webpack_devserver\n\ntest_database:\n\tdocker-compose up --no-start\n\tdocker-compose start postgres\n\tdocker-compose run --rm server /extension/bin/wait-for-it.sh postgres:5432 -- echo \"Postgres started\"\n\tdocker-compose run --rm postgres psql -U postgres -h postgres -c \"create database tests;\" || echo \"Error while creating tests database\"\n\tdocker-compose run --rm server create_test_tables\n\ntest: build test_database\n\tdocker-compose run --rm server tests\n\tdocker-compose stop\n",
  "readme": "Redash-STMO\n===========\n\n`Redash <https://redash.io>`_ extensions for\n`sql.telemetry.mozilla.org <https://sql.telemetry.mozilla.org/>`_.\n\nOr as it should have been called: *St. Moredash* ;)\n\n.. image:: https://circleci.com/gh/mozilla/redash-stmo.svg?style=svg\n    :target: https://circleci.com/gh/mozilla/redash-stmo\n\n.. image:: https://codecov.io/gh/mozilla/redash-stmo/branch/main/graph/badge.svg\n    :target: https://codecov.io/gh/mozilla/redash-stmo\n\n.. image:: https://img.shields.io/badge/calver-YYYY.M.PATCH-22bfda.svg\n   :target: https://calver.org/\n   :alt: CalVer - Timely Software Versioning\n\nOverview\n--------\n\nInherits Redash's Docker setup\n  redash-stmo is using Redash's own Docker image for development to implement a\n  close development/production parity and extends it in various ways, e.g.\n  an own docker-compose configuration, an own docker-entrypoint script.\n\n  Specifically it uses Mozilla's \"rc\" tagged version of the Redash Docker\n  image, which includes (at the time of writing this, 2019-06-13) many\n  customizations from Mozilla's pseudo-temporary and regularly updated Redash\n  fork. The \"rc\" tagged Docker image is updated every time a \"rebase\" from\n  upstream Redash happens and is put to testing in the \"release\" Redash\n  environment on Mozilla's server.\n\n  Please review the `Redash Docker installation guidelines <https://redash.io/help/open-source/dev-guide/docker>`_ before continuing. It's important to know\n  those basics since many decisions for redash-stmo were derived from it.\n  Thank you.\n\nIs mounted under /extension\n  The current working directory (the directory with this ``README.rst``) is\n  mounted under the path ``/extension`` by docker-compose inside the Docker\n  container.\n\nRuns with Redash in /app\n  Since it reuses the Redash Docker image, you can find all the Redash setup\n  under the ``/app`` directory inside the Docker container.\n\nUses Redash's \"entrypoints\" for discovery\n  The way Redash finds new extensions is by using the so called \"entrypoints\"\n  of Python packages, metadata that is specified and distributed in\n  Python packages, that is read out by Redash at runtime to find the filesystem\n  locations for Redash extensions.\n\n  That's true for three kinds of entrypoints:\n\n  ``redash.extensions``\n    Python callables to be used to extend the Redash Flask app, e.g.\n    ``redash_stmo.data_sources.health:extension``.\n\n  ``redash.bundles``\n    Python packages that contain additional front-end files for the\n    webpack build process, e.g. ``redash_stmo.data_sources.link``.\n\n  ``redash.scheduled_jobs``\n    Python callables that return parameters for scheduled RQ jobs,\n    e.g. ``redash_stmo.data_sources.health:scheduled_job``.\n\nHooks into Webpack\n  Since Redash extensions like redash-stmo can also provide additional Webpack\n  bundles, the development setup runs Redash's `bundle-extension script <https://github.com/getredash/redash/blob/master/bin/bundle-extensions>`_ periodically\n  to copy the files from redash-stmo to the right place for webpack to pick\n  them up (``/app/client/app/extensions``).\n\n  See the section about the webpack development server below for more\n  information.\n\nDevelopment workflow\n--------------------\n\nWe provide some convenience Make tasks to be run from your host machine\n(not inside the Docker container) to ease this non-trivial application setup:\n\nBuild and update the local Docker image\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMany of the helpers below will implicitely run docker-compose to start the\nRedash containers and in effect automatically build the local Docker image\nas well if it doesn't exist.\n\nIf you'd like to build the local Docker image separately or if you'd like to\nfetch the latest version of the base Redash Docker image (or its child\nimage of the Mozilla Redash fork), e.g. in the event of a new rebase by\nMozilla staff, please run the following::\n\n    make build\n\nBehind the scenes\n   This will run ``docker-compose build --pull`` which will pull updates to\n   the Docker images used by the docker-compose setup, including the Redash,\n   Redis and Postgres images.\n\nCreate the database\n~~~~~~~~~~~~~~~~~~~\n\nOn you command line run this **ONCE** to create the database for\nRedash/redash-stmo setup:\n\n::\n\n    make database\n\nThis uses Redash's own ability and redash-stmo is just set up to reuse it.\n\nBehind the scenes\n  This will run docker-compose to create the server container that is\n  running the Redash Python server and in effect the Redis and Postgres\n  containers, too.\n\n  It will then initialize the Postgres tables needed for Redash.\n\nInstall npm modules\n~~~~~~~~~~~~~~~~~~~\n\nThen we'll install the Redash npm modules inside the server container::\n\n    make node_modules\n\nBehind the scenes\n  This will run ``npm install`` inside the server in the ``/app`` directory,\n  which is the directory with Redash's code from the Redash Docker base\n  image.\n\n  NOTE, the redash-stmo development setup mounts the ``/app/node_modules``\n  directory as a separate Docker volume, that will be maintained by Docker\n  and won't show up in or transfer to the host machine where Docker is\n  running.\n\nStart the containers\n~~~~~~~~~~~~~~~~~~~~\n\nTo start the whole set of Docker containers for a working environment\n(Redash server, RQ workers, Redis, Postgres) all you need to run is this::\n\n    make up\n\nBehind the scenes\n  This is pretty simply running ``docker-compose up``, to launch all\n  containers of the redash-stmo Docker setup.\n\n  NOTE: This **requires** first installing npm modules inside the container\n  above and creating the database as well!\n\nRun webpack devserver\n~~~~~~~~~~~~~~~~~~~~~\n\nIf you're developing a Redash extension that includes an additional webpack\nbundle (which will need to be included in Redash's webpack build process\nto be shipped in the client application bundle) you'll want to use the webpack\ndevelopment server.\n\nIt automatically compiles the Redash client application bundle on files\nchanges and proxies requests for the Redash server via a proxy running\non port 8080 (instead of the usual Redash port of 5000).\n\nAfter starting the containers using the description in the above step,\nopen a second terminal and **additionally run**::\n\n    make devserver\n\nBehind the scenes\n  This will run the webpack devserver in another instance of the server\n  container (not the same as when running ``make up``) and runs a script\n  that listens for files changes to ``.js`` and ``.jsx`` files in the\n  ``/extension`` directory.\n\n  When changes are detected, it'll automatically run Redash's\n  ``bundle-extensions`` script that does the heavy lifting of copying\n  the changed extension files into the ``/app/client/app/extensions``\n  directory, which triggers the webpack devserver to recompile the\n  client application bundle.\n\n  NOTE: This **requires** opening the Redash instance via\n  http://localhost:8080/ instead of http://localhost:5000/ to go through\n  the webpack devserver.\n\n\nStart shell\n~~~~~~~~~~~\n\nIn case you need to do any debugging or file system checks inside the\nserver container, you can create a bash shell by running::\n\n    make bash\n\nBehind the scenes\n  Any changes you make here outside the ``/extension`` directory\n  (which is mounted as a Docker volume with the current working directory on\n  the Docker host machine) and the following directores are not persisted.\n\n  List of directories inside the container that are mounted as Docker volumes:\n\n  ``/extension``\n    Maps the current working directory (where this README.rst is located)\n    on the host machine for developing the extension.\n\n  ``/home/redash/.cache``\n    Used by pip and other scripts,\n\n  ``/app/client/dist``\n    Directory to retain webpack build results, so webpack builds don't take\n    as long on consecutive runs.\n\n  ``/home/redash/.local``\n    Directory for \"user-installed\" Python packages. If you'd like you can\n    easily install additonal Python packages with the Docker container user\n    Redash using ``pip install --user <package>``. Installed scripts from\n    those packages will be found under ``/home/redash/.local/bin`` but\n    are also automatically added to ``PATH``.\n\n  ``/app/node_modules``\n    Directory for npm modules, that are installed when running ``npm install``\n    inside of ``/app`` in the container. Retained to make use of native npm\n    caching between consecutive runs.\n\nRun tests\n~~~~~~~~~\n\nRunning the Python based tests requires first creating a separate database\n(implemented by the ``test_database`` Make task) and then running the test\nrunner inside the container. The test database is not the same as the\ndatabse in use for regular development (e.g. to not overwrite development\ndata).\n\nFrontend or integration tests are currently not supported.\n\nTo run the tests (from the host machine) run::\n\n    make test\n\nThis will automatically run the ``test_database`` Make task before running\nthe tests.\n\nBehind the scenes\n  When launching the tests runner it'll the regular server container,\n  but also set the ``REDASH_DATABASE_URL`` environment variable to the\n  test database to prevent overwriting any data that you added to the\n  database the regular Redash interface (e.g. data sources, queries etc).\n\n  By default it uses `pytest <https://docs.pytest.org/>`_ to run\n  the Python tests in ``/extension``, with a number of parameters as\n  defined in the ``pytest.ini``.\n\n  If you'd like to add additional parameters to pytest simply appened the\n  command line arguments in ``pytest.ini``.\n\n  Alternatively, e.g. if you'd like to use `pdb <https://docs.python.org/3/library/pdb.html>`_ to debug a test, do this:\n\n  create the test database from the host machine\n    ``make test_database``\n\n  start a Bash shell in the container\n    ``make bash``\n\n  set the ``REDASH_DATABASE_URL`` env var in the container\n    ``export REDASH_DATABASE_URL=\"postgresql://postgres@postgres/tests\"``\n\n  change direcotry to extensio code\n    ``cd /extension``\n\n  run the tests with whatever parameter\n    ``pytest -vvv --pdb``\n\nIssues & questions\n------------------\n\nSee the `issue tracker on GitHub <https://github.com/mozilla/redash-stmo/issues>`_\nto open tickets if you have issues or questions about Redash-STMO.\n"
},
{
  "name": "form-fill-examples",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "autocomplete-all.html",
      "autocomplete-us-separate-fields.html",
      "base.js",
      "basic.html",
      "basic_cc.html",
      "basic_cc_2.html",
      "billing_shipping.html",
      "css",
      "heuristics",
      "index.html",
      "layout.html",
      "layout_cc.html",
      "multiple_sections",
      "name_utils.html",
      "name_utils.js",
      "password_manager",
      "test_css_filter.html",
      "textarea_select.html",
      "top_sites",
      "two_forms.html"
    ]
  },
  "makefile": null,
  "readme": "Test pages for address, credit card and login form filling.\n\nhttps://mozilla.github.io/form-fill-examples/\n"
},
{
  "name": "docere",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "docere",
      "setup.py",
      "tests",
      "tox.ini"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Introduction\n\nDocere is a workflow and set of tools for publishing data analyses.\n\nDocere sounds like \"dose air\"\n\n# Installation\n\nDocere is hosted on PyPI, so is installable via `pip`:\n\n    pip install docere\n\nOnce installed, see usage details via:\n\n    docere --help\n    docere render --help\n\n# Design Principles\n\n* Analysts should have total control of their report presentation\n* Analysts should be able to get their work reviewed\n* Tools should be simple and do one thing well\n* Changes to reports should be tracked,\n  but reproducibility is the responsibility of the analyst (not the tool)\n\n# High Level Workflow\n\n## Generate a report\n\nDocere starts with a `report` representing an analysis or a unit of knowledge.\nAn analyst can generate their report using any tools they like.\nThe only requirement is that their analysis result in a static HTML document,\nor has a URL.\n\n## Submit report to a knowledge-repo\n\nAll reports are stored in a central git repository, called the `knowledge repository`.\nTo submit a new report,\nopen a pull request against the knowledge repository.\n\nYou can add your report in either of two ways.\n\nIf you need to **use Docere to host HTML**, you should\ncreate a directory containing a *metadata file* named `report.json` or `report.toml` file.\n([TOML] is an INI-like configuration format, which is more flexible than JSON.)\nThe rendered HTML should go in the same directory.\n\nIf you want to link to **a report that's hosted somewhere else**, like Google Docs,\nthen add a .toml or .json file with any name to the external reports directory\nconfigured for your repository. It's probably named `external`.\n\nAt a minimum, your metadata file should include the following fields:\n\n* `title`: The title of the report\n* `publish_date`: YYYY-MM-DD format\n* `author`: The author's name, or `authors`: an array of author names\n\nFields that may be optional are:\n\n* `link`: a URL for an external report\n* `abstract`: an abstract to be rendered in the TOC\n\n\nThe following fields are the set of tags associated with a report. They're designed to be high-level in order to allow for discovery of similar knowledge. Find the set of tags [here](https://docs.google.com/spreadsheets/d/1RAz-0zyVSC-gM8nbxVfdCs0x8Bpz5ydWIR3ZZ6MAr9M/edit?usp=sharing) that can be used for the __product__, __area__ and __artifact__ attributes. The __project__ attribute is a free form tag that allows for work to be grouped under larger, longer term projects. For example, there could be multiple artifacts produced as the result of a major release, such as our recent 81 release named Shirley.\n\nMultiple tags should be represented in list format (see example).\n\n* `product`: the product that the work in the report is associated with\n* `area`: a more descriptive focus area of the product\n* `artifact`: the type of knowledge artifact the report is\n* `project`: if the report falls under an umbrella project, specify here. For now this is a free-form field.\n\nAn identical example in each format:\n\n<table>\n<thead><tr><th>\n\n`report.toml`\n\n</th><th>\n\n`report.json`\n\n</th></tr></thead>\n<tbody><tr>\n<td>\n\n```toml\n# TOML can have comments.\ntitle = \"My Cool Report\"\npublish_date = \"2020-01-30\"\nauthor = \"Mo Zilla\"\n# TOML supports multiline strings:\nabstract = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit,\nsed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"\"\"\nproduct = \"desktop\"\narea = [\"bookmarks\", \"accounts\"]\nartifact = \"experiment\"\nproject = \"project name\"\n```\n\n</td>\n<td>\n\n```json\n{\n  \"title\": \"My Cool Report\",\n  \"publish_date\": \"2020-01-30\",\n  \"author\": \"Mo Zilla\",\n  \"abstract\": \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\",\n  \"product\": \"desktop\",\n  \"area\": [\"bookmarks\", \"accounts\"],\n  \"artifact\": \"experiment\",\n  \"project\": \"project name\"\n}\n```\n\n</td>\n</tr></tbody></table>\n\nIf desired, this is an opportunity to get review for your analysis.\n\n[TOML]: https://toml.io/en/\n\n## Render content\n\nYou should configure CI to trigger docere when PRs are merged to master.\nDocere will then:\n\n* Copy the knowledge-repo to a new directory named `output`\n* Gather metadata for all known reports\n* Pass the metadata to the `metadata generators` to create the necessary `metadata pages`\n\n`Metadata pages` are auto-generated documents produced to make reports more discoverable.\nFor example, docere will generate a homepage that lists all reports in anti-chronological order.\nOther `metadata pages` could include: RSS feeds, topic pages, or reports by a specific contributor.\n\nI intend most `metadata generators` be implemented as plugins to this system.\nFor now, I'm including some very simple `metadata generators` by default.\n\n## Upload content\n\nDocere does not handle uploading the rendered site to a server.\nWe recommend configuring this through your CI provider.\nWe've included an example `.travis.yml` in this repository.\nYou can view the rendered documentation\n[here](http://docere-test.s3-website-us-east-1.amazonaws.com/).\n\n# Advantages and Weaknesses\n\n## HTML is difficult to review in GitHub\n\nA docere `knowledge repository` stores raw HTML files.\nThis gives the analyst complete control over the format of the report,\nbut comes with some notable disadvantages.\n\nHTML diffs are often cluttered with boilerplate.\nEven worse, GitHub doesn't allow you to review the rendered HTML page in your browser.\nIt would be much nicer if we could store markdown documents in the `knowledge repository`\nand render these to HTML when generating the static site.\nIn fact, this is what AirBnB's [knowledge-repo] does.\n\nWe decided against storing markdown because it takes control away from the analyst.\nPresenting data in a meaningful and compelling format is a difficult task.\nDifferent reports need different formats.\nIt is **not this tool's job to be opinionated**.\n\n## `reports` aren't inherently reproducible\n\nThis tool does not save any of the code used to generate a report.\nInstead, **the analyst is responsible for making their results reproducible**.\nThis can be done by linking to a commit in a GitHub repository\nor by including the code itself in the `report's` directory in the `knowledge repository`.\n\n## Requires interacting with Git\n\nUsing Git to store `reports` makes it easy to get review and track changes.\nHowever, some users will not be comfortable interacting with Git.\nFor now, these users are out of luck.\n\nWe may eventually explore a simpler front end,\nbut this is not current on our roadmap.\nDocere is build to be composable,\nso it should be easy to roll your own interface if you so desire!\n\n\n# Appendix\n\n## FAQ\n\n### Another static site generator?\n\n_Why_ are you building _another_ static site generator, Ryan?\nWhy!\n\nI just couldn't find any other static site generator\nthat let's the analyst have total control over the report.\nDocere, on the other hand, just aggregates reports.\nIt **doesn't render them**.\n\nFor example, checkout\n[this prototype using pelican](https://github.com/harterrt/dpel).\nWe're storing HTML files,\nbut everything is squeezed into the default pelican theme.\nI decided it would take more work to build a minimal template for pelican\nthan to just start over.\n\n### Why use `report.json` config files?\n\nMany static site generators prefer using front-matter to store metadata.\nIn my experience adding front-matter to a report is an unnecessary pain.\n\nIf you already have a working toolchain for creating reports,\nmodifying your templates to include front-matter is frustrating.\nIt's much easier to compose these toolchains if you use an external config file.\n\nFor example, you could create a simple bash script that\nstarts a branch in the knowledge-repo,\ncopies your report to the knowledge-repo,\nand copies a boilerplate `report.json` config to the right directory.\nNow imagine if that config needed to reside inside the HTML file.\n\n\n[knowledge-repo]: https://github.com/airbnb/knowledge-repo\n"
},
{
  "name": "cipherscan",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "OpenSSL-LICENSE",
      "README.md",
      "analyze.py",
      "ca-bundle.crt",
      "cipherscan",
      "cscan.py",
      "cscan.sh",
      "cscan",
      "cscan_tests",
      "openssl",
      "openssl-darwin64",
      "openssl.cnf",
      "server-side-tls-conf.json",
      "top1m"
    ]
  },
  "makefile": null,
  "readme": "CipherScan\n==========\n\n[![Build Status](https://travis-ci.org/mozilla/cipherscan.svg?branch=master)](https://travis-ci.org/mozilla/cipherscan)\n\n![cipherscan](https://pbs.twimg.com/media/CPbjvCFW8AAnUK3.png:large)\n\nCipherscan tests the ordering of the SSL/TLS ciphers on a given target, for all major versions of SSL and TLS. It also extracts some certificates informations, TLS options, OCSP stapling and more. Cipherscan is a wrapper above the `openssl s_client` command line.\n\nCipherscan is meant to run on all flavors of unix. It ships with its own built of OpenSSL for Linux/64 and Darwin/64. On other platform, it will use the openssl version provided by the operating system (which may have limited ciphers support), or your own version provided in the `-o` command line flag.\n\nExamples\n--------\n\nBasic test:\n```bash\n$ ./cipherscan google.com\n...................\nTarget: google.com:443\n\nprio  ciphersuite                  protocols                    pfs                 curves\n1     ECDHE-RSA-CHACHA20-POLY1305  TLSv1.2                      ECDH,P-256,256bits  prime256v1\n2     ECDHE-RSA-AES128-GCM-SHA256  TLSv1.2                      ECDH,P-256,256bits  prime256v1\n3     ECDHE-RSA-AES128-SHA         TLSv1.1,TLSv1.2              ECDH,P-256,256bits  prime256v1\n4     ECDHE-RSA-RC4-SHA            SSLv3,TLSv1,TLSv1.1,TLSv1.2  ECDH,P-256,256bits  prime256v1\n5     AES128-GCM-SHA256            TLSv1.2                      None                None\n6     AES128-SHA256                TLSv1.2                      None                None\n7     AES128-SHA                   TLSv1.1,TLSv1.2              None                None\n8     RC4-SHA                      SSLv3,TLSv1,TLSv1.1,TLSv1.2  None                None\n9     RC4-MD5                      SSLv3,TLSv1,TLSv1.1,TLSv1.2  None                None\n10    ECDHE-RSA-AES256-GCM-SHA384  TLSv1.2                      ECDH,P-256,256bits  prime256v1\n11    ECDHE-RSA-AES256-SHA384      TLSv1.2                      ECDH,P-256,256bits  prime256v1\n12    ECDHE-RSA-AES256-SHA         SSLv3,TLSv1,TLSv1.1,TLSv1.2  ECDH,P-256,256bits  prime256v1\n13    AES256-GCM-SHA384            TLSv1.2                      None                None\n14    AES256-SHA256                TLSv1.2                      None                None\n15    AES256-SHA                   SSLv3,TLSv1,TLSv1.1,TLSv1.2  None                None\n16    ECDHE-RSA-AES128-SHA256      TLSv1.2                      ECDH,P-256,256bits  prime256v1\n17    ECDHE-RSA-DES-CBC3-SHA       SSLv3,TLSv1,TLSv1.1,TLSv1.2  ECDH,P-256,256bits  prime256v1\n18    DES-CBC3-SHA                 SSLv3,TLSv1,TLSv1.1,TLSv1.2  None                None\n\nCertificate: trusted, 2048 bit, sha1WithRSAEncryption signature\nTLS ticket lifetime hint: 100800\nOCSP stapling: not supported\nCipher ordering: server\n```\n\nTesting STARTTLS:\n```\ndarwin$ $ ./cipherscan --curves -starttls xmpp jabber.ccc.de:5222\n................................\nTarget: jabber.ccc.de:5222\n\nprio  ciphersuite                  protocols              pfs                 curves\n1     ECDHE-RSA-AES256-GCM-SHA384  TLSv1.2                ECDH,P-256,256bits  prime256v1\n2     ECDHE-RSA-AES256-SHA384      TLSv1.2                ECDH,P-256,256bits  prime256v1\n3     ECDHE-RSA-AES256-SHA         TLSv1,TLSv1.1,TLSv1.2  ECDH,P-256,256bits  prime256v1\n4     DHE-RSA-AES256-GCM-SHA384    TLSv1.2                DH,1024bits         None\n5     DHE-RSA-AES256-SHA256        TLSv1.2                DH,1024bits         None\n6     DHE-RSA-AES256-SHA           TLSv1,TLSv1.1,TLSv1.2  DH,1024bits         None\n7     DHE-RSA-CAMELLIA256-SHA      TLSv1,TLSv1.1,TLSv1.2  DH,1024bits         None\n8     AES256-GCM-SHA384            TLSv1.2                None                None\n9     AES256-SHA256                TLSv1.2                None                None\n10    AES256-SHA                   TLSv1,TLSv1.1,TLSv1.2  None                None\n11    CAMELLIA256-SHA              TLSv1,TLSv1.1,TLSv1.2  None                None\n12    ECDHE-RSA-AES128-GCM-SHA256  TLSv1.2                ECDH,P-256,256bits  prime256v1\n13    ECDHE-RSA-AES128-SHA256      TLSv1.2                ECDH,P-256,256bits  prime256v1\n14    ECDHE-RSA-AES128-SHA         TLSv1,TLSv1.1,TLSv1.2  ECDH,P-256,256bits  prime256v1\n15    DHE-RSA-AES128-GCM-SHA256    TLSv1.2                DH,1024bits         None\n16    DHE-RSA-AES128-SHA256        TLSv1.2                DH,1024bits         None\n17    DHE-RSA-AES128-SHA           TLSv1,TLSv1.1,TLSv1.2  DH,1024bits         None\n18    DHE-RSA-SEED-SHA             TLSv1,TLSv1.1,TLSv1.2  DH,1024bits         None\n19    DHE-RSA-CAMELLIA128-SHA      TLSv1,TLSv1.1,TLSv1.2  DH,1024bits         None\n20    AES128-GCM-SHA256            TLSv1.2                None                None\n21    AES128-SHA256                TLSv1.2                None                None\n22    AES128-SHA                   TLSv1,TLSv1.1,TLSv1.2  None                None\n23    SEED-SHA                     TLSv1,TLSv1.1,TLSv1.2  None                None\n24    CAMELLIA128-SHA              TLSv1,TLSv1.1,TLSv1.2  None                None\n\nCertificate: UNTRUSTED, 2048 bit, sha1WithRSAEncryption signature\nTLS ticket lifetime hint: None\nOCSP stapling: not supported\nCipher ordering: client\nCurves ordering: server\nCurves fallback: False\n```\n\nExporting to JSON with the `-j` command line option:\n```javascript\n$ ./cipherscan --curves -j www.ebay.com | j\n{\n    \"curves_fallback\": \"False\",\n    \"serverside\": \"True\",\n    \"target\": \"www.ebay.com:443\",\n    \"utctimestamp\": \"2015-04-03T14:54:31.0Z\",\n    \"ciphersuite\": [\n        {\n            \"cipher\": \"AES256-SHA\",\n            \"ocsp_stapling\": \"False\",\n            \"pfs\": \"None\",\n            \"protocols\": [\n                \"TLSv1\",\n                \"TLSv1.1\",\n                \"TLSv1.2\"\n            ],\n            \"pubkey\": [\n                \"2048\"\n            ],\n            \"sigalg\": [\n                \"sha1WithRSAEncryption\"\n            ],\n            \"ticket_hint\": \"None\",\n            \"trusted\": \"True\"\n        },\n        {\n            \"cipher\": \"ECDHE-RSA-DES-CBC3-SHA\",\n            \"curves\": [\n                \"prime256v1\",\n                \"secp384r1\",\n                \"secp224r1\",\n                \"secp521r1\"\n            ],\n            \"curves_ordering\": \"server\",\n            \"ocsp_stapling\": \"False\",\n            \"pfs\": \"ECDH,P-256,256bits\",\n            \"protocols\": [\n                \"TLSv1\",\n                \"TLSv1.1\",\n                \"TLSv1.2\"\n            ],\n            \"pubkey\": [\n                \"2048\"\n            ],\n            \"sigalg\": [\n                \"sha1WithRSAEncryption\"\n            ],\n            \"ticket_hint\": \"None\",\n            \"trusted\": \"True\"\n        }\n    ]\n}\n```\n\nAnalyzing configurations\n------------------------\nThe motivation behind cipherscan is to help operators configure good TLS on their\nendpoints. To help this further, the script `analyze.py` compares the results of\na cipherscan with the TLS guidelines from https://wiki.mozilla.org/Security/Server_Side_TLS\nand output a level and recommendations.\n\n```bash\n$ ./analyze.py -t jve.linuxwall.info\njve.linuxwall.info:443 has intermediate tls\n\nChanges needed to match the old level:\n* consider enabling SSLv3\n* add cipher DES-CBC3-SHA\n* use a certificate with sha1WithRSAEncryption signature\n* consider enabling OCSP Stapling\n\nChanges needed to match the intermediate level:\n* consider enabling OCSP Stapling\n\nChanges needed to match the modern level:\n* remove cipher AES128-GCM-SHA256\n* remove cipher AES256-GCM-SHA384\n* remove cipher AES128-SHA256\n* remove cipher AES128-SHA\n* remove cipher AES256-SHA256\n* remove cipher AES256-SHA\n* disable TLSv1\n* consider enabling OCSP Stapling\n```\n\nIn the output above, `analyze.py` indicates that the target `jve.linuxwall.info`\nmatches the intermediate configuration level. If the administrator of this site\nwants to reach the modern level, the items that failed under the modern tests\nshould be corrected.\n\n`analyze.py` does not make any assumption on what a good level should be. Sites\noperators should know what level they want to match against, based on the\ncompatibility level they want to support. Again, refer to\nhttps://wiki.mozilla.org/Security/Server_Side_TLS for more information.\n\nNote on Nagios mode:\n`analyse.py` can be ran as a nagios check with `--nagios`. The exit code will\nthen represent the state of the configuration:\n* 2 (critical) for bad tls\n* 1 (warning) if it doesn't match the desired level\n* 0 (ok) if it matches.\ncipherscan can take more than 10 seconds to complete. To alleviate any timeout\nissues, you may want to run it outside of nagios, passing data through some\ntemporary file.\n\nOpenSSL\n-------\n\nCipherscan uses a custom release of openssl for linux 64 bits and darwin 64\nbits. OpenSSL is build from a custom branch maintained by Peter Mosmans that\nincludes a number of patches not merged upstream. It can be found here:\nhttps://github.com/PeterMosmans/openssl\n\nYou can build it yourself using following commands:\n```\ngit clone https://github.com/PeterMosmans/openssl.git --depth 1 -b 1.0.2-chacha\ncd openssl\n./Configure zlib no-shared experimental-jpake enable-md2 enable-rc5 \\\nenable-rfc3779 enable-gost enable-static-engine linux-x86_64\nmake depend\nmake\nmake report\n```\n\nThe statically linked binary will be `apps/openssl`.\n\nContributors\n------------\n\n* Julien Vehent <julien@linuxwall.info> (original author)\n* Hubert Kario <hkario@redhat.com> (co-maintainer)\n* Pepi Zawodsky <git@maclemon.at>\n* Michael Zeltner <m@niij.org>\n* Peter Mosmans <support@go-forward.net>\n* Vincent Riquer <v.riquer@b2f-concept.com>\n* Christian Stadelmann <dev@genodeftest.de>\n* Simon Deziel <simon.deziel@gmail.com>\n* Aaron Zauner <azet@azet.org>\n* Mike <mikedawg@gmail.com>\n* Phil Cohen <phlipper@users.noreply.github.com>\n* Samuel Kleiner <sam@firstbanco.com>\n* Richard Soderberg <https://twitter.com/floatingatoll>\n* Adam Crosby <adamcrosby@users.noreply.github.com>\n"
},
{
  "name": "zilla-slab",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "requirements.txt",
      "sources"
    ]
  },
  "makefile": null,
  "readme": "# Zilla Slab\n\nA custom family for Mozilla by Typotheque\n\n## Download\n\nTo quickly grab the generated fonts you can [download the latest release](https://github.com/mozilla/zilla-slab/releases/latest). You can also find Zilla Slab hosted on [Google Fonts](https://fonts.google.com/specimen/Zilla+Slab) and [Mozilla's own CDN](https://code.cdn.mozilla.net/fonts/zilla-slab.css) to reference in your projects directly. If you would like to generate the fonts from their original source, read on.\n\n## Installation\n\n```\nvirtualenv env\nsource env/bin/activate\n\npip install -r requirements.txt\n```\n\n## Dependencies to generate webfonts\n* ttfautohint\n* sfnt2woff\n* [woff2](https://github.com/google/woff2)\n\n### Install on MacOS\n```\nbrew tap bramstein/webfonttools\nbrew update\nbrew install ttfautohint\nbrew install bramstein/webfonttools/sfnt2woff\nbrew install bramstein/webfonttools/woff2\n\n```\n### Install on Linux\nInstall the dependencies with the respective package manager for your distribution (eg. apt - Debian-based systems, dnf - Red Hat-based systems, etc)\n\nNote : Compiling [woff2](https://github.com/google/woff2) from source is fairly easy with no additional dependencies (other than the included submodules - [brotli](https://github.com/google/brotli), [terryfy](https://github.com/MacPython/terryfy), [esaxx](https://github.com/hillbig/esaxx)).\n#### Debian-based systems (aptitude | apt)\n`apt install ttfautohint woff-tools`\n\n## Generating fonts\n\nbuild whilst env virtualenv is active\n```\ncd sources\nsh build.sh\n```\n"
},
{
  "name": "devtools-adb-extension",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Makefile",
      "README.md",
      "RELEASE.md",
      "dist",
      "extension",
      "sign.sh",
      "template-update.json"
    ]
  },
  "makefile": "ARCHS=linux linux64 mac64 win32\n\nEXTENSION_NAME=adb-extension\nVERSION=0.0.6pre\nXPI_NAME=$(EXTENSION_NAME)-$(VERSION)\n\nROOT_PATH=pub/labs/devtools/$(EXTENSION_NAME)\nROOT_UPDATE_URL=https://ftp.mozilla.org/$(ROOT_PATH)\nS3_BASE_URL=s3://net-mozaws-prod-delivery-contrib/$(ROOT_PATH)\n\ndefine build-xpis\n\tpushd extension; \\\n\tfor arch in $(ARCHS); do \\\n\t\techo \"[release-$$arch] Create manifest.json\"; \\\n\t\tsed \\\n\t\t\t-e \"s#@@UPDATE_URL@@#$(ROOT_UPDATE_URL)/$$arch/update.json#\" \\\n\t\t\t-e \"s#@@VERSION@@#$(VERSION)#\" \\\n\t\t\ttemplate-manifest.json > manifest.json; \\\n\t\techo \"[release-$$arch] ZIP to $(XPI_NAME)-$$arch.xpi\"; \\\n\t\tzip ../dist/$(XPI_NAME)-$$arch.xpi -r $$arch adb.json manifest.json; \\\n\t\techo \"[release-$$arch] Delete temporary manifest.json\"; \\\n\t\trm manifest.json; \\\n\tdone; \\\n\tpopd\nendef\n\ndefine clean\n\techo \"Remove previous xpi files\"; \\\n\trm -f **/*.xpi\nendef\n\ndefine release\n\tpushd dist; \\\n\tfor arch in $(ARCHS); do \\\n\t\techo \"[release-$$arch] Sign .xpi\"; \\\n\t\t../sign.sh $(XPI_NAME)-$$arch.xpi; \\\n\t\techo \"[release-$$arch] Upload .xpi\"; \\\n\t\taws s3 cp \\\n\t\t\t$(XPI_NAME)-$$arch.xpi \\\n\t\t\t$(S3_BASE_URL)/$$arch/$(XPI_NAME)-$$arch.xpi; \\\n\t\techo \"[release-$$arch] Copy to 'latest' .xpi\"; \\\n\t\taws s3 cp \\\n\t\t\t$(S3_BASE_URL)/$$arch/$(XPI_NAME)-$$arch.xpi \\\n\t\t\t$(S3_BASE_URL)/$$arch/adb-extension-latest-$$arch.xpi; \\\n\t\techo \"[release-$$arch] Create update.json\"; \\\n\t\tsed \\\n\t\t\t-e \"s#@@UPDATE_LINK@@#$(ROOT_UPDATE_URL)/$$arch/$(XPI_NAME)-$$arch.xpi#\" \\\n\t\t\t-e \"s#@@VERSION@@#$(VERSION)#\" \\\n\t\t\t../template-update.json > update.json; \\\n\t\techo \"[release-$$arch] Upload update.json\"; \\\n\t\taws s3 cp --cache-control max-age=3600 \\\n\t\t\tupdate.json \\\n\t\t\t$(S3_BASE_URL)/$$arch/update.json; \\\n\t\techo \"[release-$$arch] Delete temporary update.json\"; \\\n\t\trm update.json; \\\n\tdone; \\\n\tpopd\nendef\n\npackage:\n\t@$(call clean)\n\t@$(call build-xpis)\n\nclean:\n\t@$(call clean)\n\nrelease:\n\t@$(call release)\n",
  "readme": "## ADB Extension\n\nThis is a Firefox extension that supports remote debugging in Firefox DevTools.\nIt provides ADB binaries used by DevTools to connect to Firefox/GeckoView products on Android devices via USB.\n\n### Releases\n\nFor documentation about releases check out [RELEASE.md](./RELEASE.md)\n\n### Discussion\n\nFor questions and issues specific to this extension, you can use the [GitHub issue tracker](https://github.com/mozilla/devtools-adb-extension/issues).\n\nFor more general questions about remote debugging in DevTools or DevTools in general, you can:\n- come chat with us on [Slack](https://devtools-html-slack.herokuapp.com/) or IRC (#devtools at irc.mozilla.org)\n- file bugs on [Bugzilla](https://bugzilla.mozilla.org/enter_bug.cgi?product=DevTools)\n"
},
{
  "name": "deepspeech-pkguploadworker",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Procfile",
      "README.md",
      "config.json",
      "package.json",
      "requirements.txt",
      "script.py",
      "scriptworker.yaml"
    ]
  },
  "makefile": null,
  "readme": "# pkguploadscript"
},
{
  "name": "Fira",
  "files": {
    "/": [
      "Fira_Version_Report.md",
      "LICENSE",
      "README.md",
      "bower.json",
      "eot",
      "fira-mono.less",
      "fira-mono.sass",
      "fira-mono.scss",
      "fira-mono.styl",
      "fira-sans.less",
      "fira-sans.sass",
      "fira-sans.scss",
      "fira-sans.styl",
      "fira.css",
      "fira.less",
      "fira.sass",
      "fira.scss",
      "fira.styl",
      "index.html",
      "otf",
      "package.json",
      "source",
      "technical reports",
      "ttf",
      "woff",
      "woff2"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla's Fira Type Family\nhttp://mozilla.github.io/Fira/\n\n## Download Fira\n<a href=\"https://github.com/mozilla/Fira/releases/latest\">Latest Release</a><br>\n<a href=\"https://github.com/mozilla/Fira/releases\">All Releases</a>\n\n## Fira Roadmap\nSee the  <a href=\"https://docs.google.com/document/d/1fLxzQsULTv43umIhpB9Gv3Gi7aOBONHbqEbwZIipmxw/edit\">Fira Road Map</a> for further information on upcoming releases. Please add your comments or questions within the document.\n\n\n## How to Contribute to Fira\nIf you're interested in contributing, see our  <a href=\"https://docs.google.com/document/d/1QfxweGktJEdBvbd94y-5hiyqu32U9-h_ICPVs76Niyw/edit\">Fira Contribution Documentation</a>. Please add your comments or questions within the document.\n\n\n## Usage\nUse this font on your website!\n\n```html\n<link rel=\"stylesheet\" href=\"https://code.cdn.mozilla.net/fonts/fira.css\">\n```\n\n## External Resources\nFurther information on the design and specifications of the Fira typeface can be found at <a href=\"https://carrois.com/typefaces/FiraSans/\">Carrois Studio</a>.<br>\nFira can also be found in these foundries:<br>\n<a href=\"http://www.1001fonts.com/fira-sans-font.html\">1001 Fonts<br>\n<a href=\"https://typekit.com/fonts/fira-sans\">Adobe Typekit<br>\n<a href=\"https://www.google.com/fonts/specimen/Fira+Sans\">Google Fonts<br>\n<a href=\"https://www.fontsquirrel.com/fonts/fira-sans\">Font Squirrel<br>\n"
},
{
  "name": "DSAlign",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "align",
      "bin",
      "data",
      "doc",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# DSAlign\nDeepSpeech based forced alignment tool\n\n## Installation\n\nIt is recommended to use this tool from within a virtual environment.\nAfter cloning and changing to the root of the project,\nthere is a script for creating one with all requirements in the git-ignored dir `venv`:\n\n```shell script\n$ bin/createenv.sh\n$ ls venv\nbin  include  lib  lib64  pyvenv.cfg  share\n```\n\n`bin/align.sh` will automatically use it.\n\nInternally DSAlign uses the [DeepSpeech](https://github.com/mozilla/DeepSpeech/) STT engine.\nFor it to be able to function, it requires a couple of files that are specific to \nthe language of the speech data you want to align.\nIf you want to align English, there is already a helper script that will download and prepare\nall required data:\n\n```shell script\n$ bin/getmodel.sh \n[...]\n$ ls models/en/\nalphabet.txt  lm.binary  output_graph.pb  output_graph.pbmm  output_graph.tflite  trie\n```\n\n## Overview and documentation\n\nA typical application of the aligner is done in three phases: \n\n 1. __Preparing__ the data. Albeit most of this has to be done individually,\n    there are some [tools for data preparation, statistics and maintenance](doc/tools.md).\n    All involved file formats are described [here](doc/files.md).\n 2. __Aligning__ the data using [the alignment tool and it algorithm](doc/algo.md).\n 3. __Exporting__ aligned data using [the data-set exporter](doc/export.md).\n\n## Quickstart example\n\n### Example data\n\nThere is a script for downloading and preparing some public domain speech and transcript data.\nIt requires `ffmpeg` for some sample conversion.\n\n```shell script\n$ bin/gettestdata.sh\n$ ls data\ntest1  test2\n```\n\n### Alignment using example data\n\nNow the aligner can be called either \"manually\" (specifying all involved files directly):\n\n```shell script\n$ bin/align.sh --audio data/test1/audio.wav --script data/test1/transcript.txt --aligned data/test1/aligned.json --tlog data/test1/transcript.log\n```\n\nOr \"automatically\" by specifying a so-called catalog file that bundles all involved paths:\n\n```shell script\n$ bin/align.sh --catalog data/test1.catalog\n```\n"
},
{
  "name": "dscontrib",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "describe_revision.py",
      "setup.cfg",
      "setup.py",
      "src"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# dscontrib\n\nA Python library for Mozilla Data Science code snippets.\n\n## Installation in colab\nInstall this like any other python package:\n```\n!pip install dscontrib\n```\n\n## Installation of the bleeding edge on databricks clusters\nDatabricks-wide installations can't be updated without restarting the entire cluster. If you're using newish code then you'll want to install `dscontrib` locally to your notebook:\n```lang=py\ndbutils.library.installPyPI(\"dscontrib\")\n...\nimport dscontrib\n```\n\nEach commit to `master` triggers the release of a new version on PyPI.\n"
},
{
  "name": "vouched-mozillians",
  "files": {
    "/": [
      ".editorconfig",
      ".gitignore",
      ".gitmodules",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "bin",
      "docker-compose.yml",
      "docker",
      "docs",
      "fixtures-dev",
      "heroku.Dockerfile",
      "heroku.yml",
      "lib",
      "manage.py",
      "media",
      "mozillians",
      "requirements",
      "scripts",
      "setup.cfg",
      "wsgi"
    ],
    "/docs": [
      "Makefile",
      "api",
      "conf.py",
      "contribute.rst",
      "fake_funfactory.py",
      "fake_settings.py",
      "index.rst",
      "installation",
      "internationalization.rst",
      "invites.rst",
      "make.bat",
      "migrate_mysql_to_postgres.md",
      "mysql-anonymize.rst",
      "testing.rst"
    ]
  },
  "makefile": null,
  "readme": "[mozillians.org](https://mozillians.org) - Deprecated in favor of [People's directory](https://people.mozilla.org)\n========\n\nMinimal version of mozillians.org to display only vouches. No new features will be added to this site and access is restricted to only vouched users. The deletion of a profile is irreversible.\n\n## Code of Conduct\nBy participating in this project, you're agreeing to uphold the [Mozilla Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/). If you need to report a problem, please see our [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) guide."
},
{
  "name": "libaudit-go",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "audit_constant.go",
      "audit_events.go",
      "auditconstant_string.go",
      "auditprint",
      "buffer.go",
      "headers",
      "interpret.go",
      "libaudit.go",
      "libaudit_test.go",
      "lookup_tables.go",
      "parser.go",
      "parser_test.go",
      "rules.go",
      "rules_test.go",
      "s2i_type_conversion.json",
      "testdata",
      "vendor"
    ]
  },
  "makefile": "GOBIN    := $(shell which go)\nGO       := GO15VENDOREXPERIMENT=1 $(GOBIN)\nBUILDPRE := auditconstant_string.go\n\ntest: $(BUILDPRE)\n\tsudo $(GO) test -v -bench=. -covermode=count -coverprofile=coverage.out\n\nprofile: $(BUILDPRE)\n\tsudo $(GO) test -v -cpuprofile cpu.prof -memprofile mem.prof -bench=.\n\nauditconstant_string.go: audit_constant.go\n\t$(GO) get golang.org/x/tools/cmd/stringer\n\t$(GO) generate\n\nclean:\n\trm -f $(BUILDPRE)\n",
  "readme": "# libaudit in Go\n\nlibaudit-go is a go package for interfacing with Linux audit.\n\n[![Build Status](https://travis-ci.org/mozilla/libaudit-go.svg?branch=master)](https://travis-ci.org/mozilla/libaudit-go)\n[![Go Report Card](https://goreportcard.com/badge/mozilla/libaudit-go \"Go Report Card\")](https://goreportcard.com/report/mozilla/libaudit-go)\n\nlibaudit-go is a pure Go client library for interfacing with the Linux auditing framework. It provides functions\nto interact with the auditing subsystems over Netlink, including controlling the rule set and obtaining/interpreting\nincoming audit events.\n\nlibaudit-go can be used to build go applications which perform tasks similar to the standard Linux auditing daemon\n`auditd`.\n\nTo get started see package documentation at [godoc](https://godoc.org/github.com/mozilla/libaudit-go).\n\nFor a simple example of usage, see the [auditprint](./auditprint/) tool included in this repository.\n\n```bash\nsudo service stop auditd\ngo get -u github.com/mozilla/libaudit-go\ncd $GOPATH/src/github.com/mozilla/libaudit-go\ngo install github.com/mozilla/libaudit-go/auditprint\nsudo $GOPATH/bin/auditprint testdata/rules.json\n```\n\nSome key functions are discussed in the overview section below.\n\n## Overview\n\n### General \n\n##### NewNetlinkConnection \n\nTo use libaudit-go programs will need to initialize a new Netlink connection. `NewNetlinkConnection` can be used\nto allocate a new `NetlinkConnection` type which can then be passed to other functions in the library.\n\n```go\ns, err := libaudit.NewNetlinkConnection()\nif err != nil {\n        fmt.Printf(\"NewNetlinkConnection: %v\\n\", err)\n} \ndefer s.Close()\n```\n\n`NetlinkConnection` provides a `Send` and `Receive` method to send and receive Netlink messages to the kernel,\nhowever generally applications will use the various other functions included in libaudit-go and do not need to\ncall these functions directly.\n\n##### GetAuditEvents\n\nGetAuditEvents starts an audit event monitor in a go-routine and returns. Programs can call this function and\nspecify a callback function as an argument. When the audit event monitor receives a new event, this callback\nfunction will be called with the parsed `AuditEvent` as an argument.\n\n```go\n\nfunc myCallback(msg *libaudit.AuditEvent, err error) {\n        if err != nil {\n            // An error occurred getting or parsing the audit event\n            return\n        }\n\t// Print the fields\n        fmt.Println(msg.Data)\n\t// Print the raw event\n        fmt.Println(msg.Raw)\n}\n\nlibaudit.GetAuditEvents(s, myCallback)\n```\n\n##### GetRawAuditEvents\n\n`GetRawAuditEvents` behaves in a similar manner to `GetAuditEvents`, however programs can use this function\nto instead just retrieve raw audit events from the kernel as a string, instead of having libaudit-go parse\nthese audit events into an `AuditEvent` type.\n\n### Audit Rules\n\nAudit rules can be loaded into the kernel using libaudit-go, however the format differs from the common rule\nset used by userspace tools such as auditctl/auditd.\n\nlibaudit-go rulesets are defined as a JSON document. See [rules.json](./testdata/rules.json) as an example.\nThe libaudit-go type which stores the rule set is `AuditRules`.\n\n##### SetRules\n\n`SetRules` can be used to load an audit rule set into the kernel. The function takes a marshalled `AuditRules`\ntype as an argument (slice of bytes), and converts the JSON based rule set into a set of audit rules suitable\nfor submission to the kernel.\n\nThe function then makes the required Netlink calls to clear the existing rule set and load the new rules.\n\n```go\n// Load all rules from a file\ncontent, err := ioutil.ReadFile(\"audit.rules.json\")\nif err != nil {\n        fmt.Printf(\"error: %v\\n\", err)\n\tos.Exit(1)\n}\n\n// Set audit rules\nerr = libaudit.SetRules(s, content)\nif err != nil {\n        fmt.Printf(\"error: %v\\n\", err)\n        os.Exit(1)\n}\n```\n"
},
{
  "name": "trackingprotection-tools",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "requirements.txt",
      "setup.cfg",
      "setup.py",
      "trackingprotection_tools"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Tracking Protection Tools [![Build Status](https://travis-ci.org/mozilla/trackingprotection-tools.svg?branch=master)](https://travis-ci.org/mozilla/trackingprotection-tools)\n\nA collection of tools for working with and analyzing Firefox's Tracking\nProtection.\n\n## Tools\n\n1. **DisconnectParser**: A python parser for the Disconnect Tracking Protection\n   list.\n2. **DisconnectReporting**: A collection of tools to prepare and send tracking\n   domain reports to Disconnect.\n"
},
{
  "name": "GSOC2020",
  "files": {
    "/": [
      "APPLICATION-GUIDE.md",
      "LICENSE",
      "README.md",
      "proposals"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla and GSoC 2020\n\nI'm pleased to announce that Mozilla has been accepted as a mentoring organization in the 2020 Google Summer Of Code, and would like to express our gratitude to everyone on the GSoC team for their continued support.\n\nThis is the first year that we've organized Mozilla's application to GSoC on Github. Mozilla community members, please put your ideas in the */proposals/* directory, in some suitably-titled file, so that we can evaluate them and polish them up.\n\n## For Mozillians and friends of Mozilla:\n\nAs usual this will be an opportunity to coach a smart student through three months of work on an interesting but non-critical-path project that is open to any part of Mozilla, provided: \n\n* the project is primarily a coding project, \n* the proposal is well-scoped with clearly defined progress milestones and outcomes, taking roughly 3 months of effort for a capable student, and \n* there is a mentor specifically assigned to the project who is available for the duration of GSoC.\n\nYou may already have a student in mind for a specific project already; if so, please start that discussion now. The sooner we have well-specified project ideas lined up with potential mentors, the better. \n\nOtherwise please send us your proposals (via pull request) and feel free to bring us any questions you have about GSoC and Mozilla's participation in it. If Mozilla is accepted as a participating organization the student application period will begin March 16th. \n\nWith all that in mind, we're now accepting project proposals,via pull request to the */proposals/* directory of this repo.\n\n## Are you a student intending to apply to participate in GSoC with Mozilla? \n\nYour first step should be to look over the /proposals/ folder. Not all of those ideas will make the cut; it could be that they are not properly defined, the wrong size, or don't have a mentor, and that makes them less likely to get accepted. We may simply be awarded fewer GSoC slots than we have projects. \n\nWe see a lot of questions about what tasks or bugs students can be assigned in the leadup to GSoC, and while we're grateful for the enthuiasm we would like to discourage people from taking that approach. While it would be helpful for us to be able to assess applicants' skills ahead of time and we always want to be open to new contributors' help, we don't want to treat our GSOC projects as a prizes you might win if you volunteer hard enough. Asking people to labor performatively is unfair and exploitative, and that is not how we intend to operate or who we want to be. \n\nWith that in mind, while we encourage anyone who is interested to take a look at our code, download and build it - please do, that's why it's there! - we believe that your best approach as a GSoC applicant is to use what you learn there to help you *craft an excellent GSOC proposal*, rather than trying to rack up \"points\" in order to be considered.\n\n## Are you a student with a great idea for a GSOC project?\n\nYou can, of course, also submit your own ideas; a great idea doesn't need to come from Mozilla to be a great GSoC project.  For us to accept a proposal as a GSoC project, your proposal *must have* a mentor, a defined outcome of reasonable scope, and a calendar with meaningful milestones to have a shot at acceptance. You can submit your proposals either as pull requests to this repo, or as applications through the GSoC process itself. Bear in mind that projects that come as a surprise to the proposed mentor are unlikely to be accepted, so you should have had a conversation with the Mozillians whose support you'd like well ahead of time.\n\nThat said, there are a lot of moving parts to the Mozilla project, and figuring out who to talk to can be difficult even if you've got an amazing idea. If this is you, please start by *opening an issue describing your idea* so we can take a look at it. While we can't promise anything, if your idea looks promising we will do our best to connect you to somebody with domain-relevant experience to discuss turning it into a proposal.\n\n*How To Write A Good Project Proposal*\n\n* Be specific: It's hard to understand the impact of, or the size of, vague proposals.\n* Consider size: Participating students have approximately eight weeks to design, code, test and document the proposal. It needs to fill, but not overfill, that time.\n* Do your research: Support the idea with well-researched links to issues, bugs, patches, papers or pull requests.\n* Only put a name in the mentor slot if you know they are willing to take on the responsibility. If you think the GSoC admins won't know who you are, leave contact details.\n* Stay on top of your notifications: we may have questions about your idea that you will need to answer.\n\n## Please note\n\n* Participants in any Mozilla project are expected to respect and uphold the [Mozilla Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/)\n* The [GSoC FAQ](https://developers.google.com/open-source/gsoc/faq)\n* In light of the institutional closures resulting from COVID-19, we have consulted with the GSOC Support Team to find out what the \"proof of registration\" requirement for applicants specifically means. Their response was: \n\n    If you do not have one of the other acceptable forms of Proof of Enrollment (school ID, transcript) you can submit something with a recent date on it with your name and school name somewhere on the form such as:\n \n    * Your most recent semester exam schedule or syllabus \n    * Your most recent semester grade sheet\n    * The receipt showing you paid for this semester\n    * The letter showing your scholarship/financial aid etc covers this semester in early 2020\n\n    And it is fine to markout the grades or the funding or other personal details (we just need to see your name, your school name and the recent date on there).\n    Our team does not review proof of enrollment documents via email. Your documents will be reviewed after March 31 and the team will let you know at that time if your submission was not accepted. You will be able to submit a new proof of enrollment if the first one is not accepted by our team. \n"
},
{
  "name": "node-vtt",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Gruntfile.js",
      "LICENSE",
      "README.md",
      "lib",
      "package.json",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "node-vtt\n========\n\n[![Build Status](https://travis-ci.org/mozilla/node-vtt.svg?branch=master)](https://travis-ci.org/mozilla/node-vtt) [![npm-version](http://img.shields.io/npm/v/node-vtt.svg)](https://www.npmjs.org/package/node-vtt) [![Dependency Status](https://david-dm.org//mozilla/node-vtt.svg?theme=shields.io)](https://david-dm.org//mozilla/node-vtt)\n[![devDependency Status](https://david-dm.org/mozilla/node-vtt/dev-status.svg?theme=shields.io)](https://david-dm.org/mozilla/node-vtt#info=devDependencies)\n\nA node wrapper for [vtt.js](https://github.com/mozilla/vtt.js). It runs `vtt.js`\non [PhantomJS](http://phantomjs.org/) from Node.\n\n###Table of Contents###\n\n- [Install](#install)\n- [API](#api)\n  - [NodeVTT's Web Page](#nodevtts-web-page)\n  - [ready](#ready)\n  - [cues](#cues)\n  - [regions](#regions)\n  - [vtt](#vtt)\n  - [errors](#errors)\n  - [init(options, onInit)](#initoptions-oninit)\n  - [shutdown()](#shutdown)\n  - [parse(data, onParsed)](#parsedata-onparsed)\n  - [parseFile(file, onParsed)](#parsefilefile-onparsed)\n  - [flush(onFlush)](#flushonflush)\n  - [processParsedData(data, onProcessed)](#processparseddatadata-onprocessed)\n  - [processFile(file, onProcessed)](#processfilefile-onprocessed)\n  - [clear(onClear)](#clearonclear)\n  - [setupParser(encoding, onSetup)](#setupparserencoding-onsetup)\n  - [error](#error)\n- [License](#license)\n\nInstall\n=======\n\n`node-vtt` is on `npm`. To install run:\n\n```bash\n$ npm install node-vtt\n```\n\nYou'll need to install `PhantomJS` if you haven't already. You can download\nit from its [website](http://phantomjs.org/download.html) or simply use npm:\n\n```bash\n$ npm install -g phantomjs\n```\n\nOr include it in your `package.json` dependencies.\n\nAPI\n===\n\n`node-vtt` has a simple async API:\n\n```js\nvar NodeVTT = require(\"node-vtt\"),\n    nodeVTT = new NodeVTT();\n\nnodeVTT.init(function() {\n  nodeVTT.parseFile(\"someVTTFile\", function(error) {\n    if (error) {\n      // Do something with error.\n    }\n    // Do something with the vtt we parsed.\n    var vtt = nodeVTT.vtt;\n    nodeVTT.processParsedData(function(error, divs) {\n      if (error) {\n        // Do something with error.\n      }\n      console.log(divs);\n    });\n  });\n});\n```\n\n####NodeVTT's Web Page####\n\n`node-vtt` uses [PhantomJS](http://phantomjs.org/) to run `vtt.js` on a web\npage. Therefore, you need to have a simple HTML file for `node-vtt` to load. There\nis a default one provided for you, so read no further if you're not interested in\ncustomizing the page it uses.\n\nIf you provide your own page the page must have a few things.\n\n* It must have the `WebVTT`, `VTTCue`, and `VTTRegion` shims provided by\n[vtt.js](https://github.com/mozilla/vtt.js). Doing this is most easily accomplished\nby using the `vtt.js` bower distributable and including it as a script on the\npage. However, if you want more granularity in what is included on the page from\n`vtt.js` you can also `npm install vtt.js` and have access to the individual\nsource files through that.\n\n* The page must also have the `vttcue-extended` and `vttregion-extended` versions\nof the `VTTCue` and `VTTRegion` shims on the page.\n\n* If you'd like to run the processing model the page must have a `div` element on it\nwith an `id` property of `overlay` and a positioning of `relative`. `node-vtt` uses\nthis div as the container to display subtitles.\n\nSee the [default page](https://github.com/mozilla/node-vtt/blob/master/lib/basic.html)\nprovided for you for more information.\n\nOnce you've created your own customized page check out how you can load it with the\n[init](#initoptions-oninit) function.\n\n####ready####\n\nThe `ready` property describes whether or not `node-vtt` is ready to parse or process\nWebVTT. To get `node-vtt` ready you must call [init](#initoptions-oninit). It will become \"un-ready\"\nwhen you all [shutdown](#shutdown).\n\n####cues####\n\nThe `cues` property contains an array of the aggregated `VTTCues` that have been\nparsed from a WebVTT file. Calling [clear](#clearonclear) will empty the `cues` array.\n\n```js\nvar cues = nodeVTT.cues;\n```\n\n####regions####\n\nThe `regions` property contains an array of the aggregated `VTTRegions` that have been\nparsed from a WebVTT file. Calling [clear](#clearonclear) will empty the `regions` array.\n\n```js\nvar regions = nodeVTT.regions;\n```\n\n####vtt####\n\nThe `vtt` property contains an object that is the `cues` and `regions` properties.\nThis provides an easy way to get all the `VTTCues` and `VTTRegions` data parsed\nfrom a file.\n\n```js\nvar vtt = nodeVTT.vtt,\n    cues = vtt.cues,\n    regions = vtt.regions;\n```\n\n####errors####\n\nThe `errors` property contains an array of the aggregated\n[ParsingErrors](https://github.com/mozilla/vtt.js#parsingerror) from `vtt.js`\nthat have been received while parsing some WebVTT file. Calling\n[clear](#clearonclear) will empty the `errors` array.\n\n```js\nvar errors = nodeVTT.errors;\n```\n\n####init(options, onInit)####\n\nInitializes the `node-vtt` object. It optionally takes an options object that\ncan contain two config properties&mdash;`uri` and `encoding`. `uri` points at a custom\npage that you want `node-vtt` to load and run on. The page must have the WebVTT\nshim from `vtt.js` included on the page as well as the shims for VTTCue\n(extended) and VTTRegion (extended). If you don't want to  pass a `uri` a default\npage will be provided for you. The `encoding` property specifies the encoding of the\ndata that you want to parse. `node-vtt` currently supports two types&mdash;`string` or `utf8`.\n\nIf you'd like to make a custom page for `node-vtt` to work with then check out\nmore information on that [here](#nodevtts-web-page).\n\nUsing the default config of type `utf8` and the basic page provided for you.\n\n```js\nnodeVTT.init(function(error) {\n  if (error) {\n    return console.log(error.message);\n  }\n  // Run some node-vtt code.\n});\n```\n\nOr with an options object:\n\n```js\nnodeVTT.init({ uri: \"my-web-page.html\", encoding: \"string\" }, function(error) {\n  if (error) {\n    return console.log(error.message);\n  }\n  // Run some node-vtt code\n});\n```\n\n####shutdown()####\n\nShuts `node-vtt` down. This is necessary as `node-vtt` will keep a `PhantomJS`\ninstance alive until this method is called.\n\n```js\nnodeVTT.shutdown();\n```\n\n####parse(data, onParsed)####\n\nParses `data` as a chunk of WebVTT data. `data` can either be a UTF8 Node ArrayBuffer\nor a string. Make sure to call [init](#initoptions-oninit) or\n[setupParser](#setupparserencoding-onsetup) with the appropriate encoding specified\nbefore calling this function. `onParsed` will return an [error](#error) object that\nhas a `message` property if an error occurred. The parsed VTTCues and VTTRegions are\naggregated on the `node-vtt` object itself and can be accessed via the [vtt](#vtt),\n[cues](#cues), or [regions](#regions) properties.\n\n```js\nvar fs = require(\"fs\"),\n    data = fs.readFileSync(\"vtt-file\");\n\nnodeVTT.parse(data, function(error) {\n  if (error) {\n    return console.log(error.message);\n  }\n  var vtt = nodeVTT.vtt;\n});\n```\n\n####parseFile(file, onParsed)####\n\nA version of [parse(data, onParsed)](#parsedata-onparsed) that will read the\nWebVTT from a file for you and call [flush](#flushonflush) where needed.\n\n```js\nnodeVTT.parseFile(\"vtt-file\", function(error) {\n  if (error) {\n    // Do something\n  }\n  var vtt = nodeVTT.vtt;\n});\n```\n\n####flush(onFlush)####\n\nFlushes the parser. This indicates that no more data will be coming to the parser\nand so it should parse any unparsed data it may have. This is necessary when parsing\nstream data. See [flush](https://github.com/mozilla/vtt.js#flush) on `vtt.js` for\nmore information. `onFlush` will return an [error](#error) if something went\nwrong, otherwise, it will return nothing.\n\n```js\nnodeVTT.parse(data, function(){\n  nodeVTT.parse(moreData, function() {\n    nodeVTT.flush(function(error) {\n      if (error) {\n        console.log(error.message);\n      }\n      var vtt = nodeVTT.vtt;\n    });\n  });\n});\n```\n\n####processParsedData(data, onProcessed)####\n\nRuns the [processing level](http://dev.w3.org/html5/webvtt/#processing-model)\nsteps of the WebVTT specification over the cues contained in `data`. `data` should\nbe an object with a `cues` property on it that is an array\nof the `VTTCues` that should be processed. This turns the cues and regions into a\nseries of `div` elements that have CSS and positioning applied to them and are ready\nto be shown on a video. These divs will be returned through the `onProcessed`\ncallback and will also be automatically added as child elements to the `overlay` div.\nThe `overlay` div is a div used as a container for the subtitles. This overlay\ndiv comes from the page that `node-vtt` loaded with the [init](#inituri-oninit)\nfunction. The div on the page must have an `id` property set to 'overlay'.\n\n```js\nvar data = {\n  cuse: [ /* VTTCues go in here */ ]\n};\nnodeVTT.processParsedData(data, function(error, divs) {\n  if (error) {\n    return console.log(error.message);\n  }\n  // Do something with divs.\n});\n```\n\n**Note:** Processing regions isn't supported yet by `vtt.js`. It will be in the\nfuture though.\n\nIf you have just used the same instance of `node-vtt` to parse some data you\ncan leave out the `data` parameter. The default is for it to use the `cues`\nand `regions` that it has aggregated already.\n\n```js\nnodeVTT.parseFile(\"vtt-file\", function() {\n  // Leave out that 'data' parameter as we just parsed some WebVTT and we can use\n  // the VTTCues and VTTRegions aggregated by this nodeVTT instance in its cues\n  // and regions properties.\n  nodeVTT.processParsedData(function(error, divs) {\n    if (error) {\n      return console.log(error.message);\n    }\n    // Do something with divs.\n  });\n});\n```\n\n####processFile(file, onProcessed)####\n\nA version of [processParsedData](#processparseddatadata-onprocessed) except that\nit will read and parse the WebVTT data contained within the file and process it for\nyou in one go.\n\n```js\nnodeVTT.processFile(\"vtt-file\", function(error, divs) {\n  if (error) {\n    return console.log(error);\n  }\n  // Do something with divs\n});\n```\n\n####clear(onClear)####\n\nClears the state of `node-vtt`. This will create a fresh parser and empty the\n[vtt](#vtt), [cues](#cues), [regions](#regions), and [errors](#errors) arrays.\n`onClear` will be called with an [error](#error) if something went wrong.\n\nThis enables you to start parsing a new set of WebVTT data without creating a\nwhole new `node-vtt` object which is epensive since it has to start `PhantomJS`\nand establish a connection.\n\n```js\nnodeVTT.clear(function(error) {\n  if (error) {\n    console.log(error.message);\n  }\n  // Ready to do some more parsing.\n});\n```\n\n**Note:** Calling `clear` is only necessary if you want to parse a new set of\nWebVTT data. You do not need to call it if you're just calling the processing\nfunctions.\n\n####setupParser(encoding, onSetup)####\n\nClears the current state of `node-vtt`, see [clear](#clearonclear), and sets up a\nnew parser that is configured to parse the `encoding` specified. Only `string`\nand `utf8` are currently supported for encodings. If you don't pass `encoding`\nthis function has the exact same behaviour as [clear](#clearonclear).\n\n```js\nnodeVTT.setupParser(\"string\", function() {\n  var data = \"WEBVTT\\n00:00.000 --> 00:01.000\\nI'm a Cue!\";\n  nodeVTT.parse(data, function() {\n    console.log(nodeVTT.vtt);\n  });\n});\n```\n\n####error####\n\nThe error objects returned by `node-vtt` are simple JS objects with a `message` property\non them describing the error.\n\n```js\nnodeVTT.parseFile(\"wrong-file\", function(error) {\n  if (error) {\n    console.log(error.message);\n  }\n});\n```\n\nLicense\n=======\n\nApache v2.0. See [LICENSE](https://github.com/mozilla/node-vtt/blob/master/LICENSE).\n"
},
{
  "name": "glamvalid",
  "files": {
    "/": [
      "Dockerfile",
      "README.md",
      "libs.R",
      "run.sh",
      "runner.R",
      "sitegen.Rmd"
    ]
  },
  "makefile": null,
  "readme": "On OS X at least,inside the repo, run\n\n```\n docker build -t glamval .\n```\n\nThis will take some time (R packages take _so_ long to install, so many\ndependencies). Once done, \n\n```\nmkdir outputs\ndocker run -it   -v ~/.config:/root/.config  --mount type=bind,source=\"$(pwd)\"/outputs,target=/tmp/ \\\n                -e OS=Windows_NT -e CHANNEL=nightly -e BUILD_START=20200615 -e BUILD_END=20200619   \\\n                -e HISTOS=\"payload.histograms.fx_session_restore_file_size_bytes,payload.histograms.telemetry_compress,payload.histograms.cycle_collector_worker_visited_ref_counted\" \\\n                glamval\n```\n\n- inspect `run.sh` to see environment variables, but in summary they are\n  - `OS`, defaults is Windows_NT\n  - `CHANNEL`, default is nightly\n  - `BUILD_START`, `BUILD_END`, YYYYMMDD version of first end buildid. Use this\n    for pre-release.\n  - `DATE_START`,`DATE_END` for release channels. For pre-release, there isn't a\n    need for this as it's computed from `BUILD_START` and `BUILD_END` variables.\n  - `HISTOS` is a comma separated fully qualified histograms name. If this is\n    blank then a mount a newline separated file of histogram names and mount it\n    to `/root/histo.txt` e.g. `-v path-to-your-histo.txt:/root/histo.txt`.\n  - `MAJOR_VER` is optional, but you can specify something like 70 to restrict\n    to major versions\n- the arg `--mount type=bind,source=\"$(pwd)\"/outputs,target=/tmp/` will allow\n  the Docker app to write output to the folder `output/` in which the resultant\n  html file be created\n- Importantly the `-v ~/.config:/root/.config` will mount your BigQuery\n  credentials into the Docker app so the SQL queries can be made. Without this\n  it cannot connect to BigQuery.\n  \nAs the program runs, you will see SQL written to the screen (they will also be\n  in the HTML file) to give you an idea of the queries being run.\n  \n  \n"
},
{
  "name": "libprio",
  "files": {
    "/": [
      ".dockerignore",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Dockerfile.dist",
      "LICENSE",
      "README.md",
      "SConstruct",
      "docker-compose.yml",
      "include",
      "mpi",
      "pclient",
      "pclient_uint",
      "pfuzz",
      "prio",
      "ptest",
      "python",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# libprio - A Prio library in C using NSS \n\n# NOTE - this repository is being used for Firefox as part of a pilot experiment, this repository will be archived in favor of [the new Rust implementation, libprio-rs](https://github.com/abetterinternet/libprio-rs). Please direct contributions that are not related to the Firefox pilot to this new project.\n\n**Warning:**\nWe do our best to write bug-free code, but I have no doubt\nthat there are scary bugs, side-channel attacks, and memory leaks \nlurking herein. \n\n**Security bugs:**\nIf you find a security-critical bug in libprio, please report it to Mozilla\nusing the contact information on \n[this page](https://www.mozilla.org/security/#For_Developers).\n\n## Overview\n\nThis is an implementation of the core cryptographic routines\nfor the [Prio system](https://crypto.stanford.edu/prio/) \nfor the private computation of aggregate statistics:\n> \"Prio: Private, Robust, and Scalable Computation of Aggregate Statistics\"<br>\n> by Henry Corrigan-Gibbs and Dan Boneh<br>\n> USENIX Symposium on Networked Systems Design and Implementation<br>\n> March 2017\n>\n> Available online at:\n>    https://crypto.stanford.edu/prio/\n\n**Usage scenario.**\nThe library implements the cryptographic routines necessary\nfor the following application scenario:\nEach client holds a vector of boolean values.\nEach client uses the library to encode her private vector into two \nencoded packets&mdash;one for server A and one for server B.\n\nAfter receiving shares from a client, the servers can use the routines\nimplemented here to check whether the client-provided packets are \nwell formed. \n(Without this check, a single malicious client can corrupt the\noutput of the computation.)\n\nAfter collecting data packets from many clients, the servers\ncan combine their state to learn how many clients had the\n*i*th bit of their data vector set to `true` and how many\nclients had the *i*th bit of their data vector set to `false`.\nAs long as at least one of the two servers is honest \n(i.e., runs the correct code), \nthe servers learn *nothing else* about the clients' data, \nunder standard cryptographic assumptions.\n\nFor example, the *i*th bit of the data vector could indicate\nwhether the client ever visited the *i*th-ranked website\nin the Alexa Top 500.\nThe servers would learn how many clients visited each of the \nTop 500 websites *without learning* which clients visited\nwhich websites.\n\n**Efficiency considerations.**\nThe code makes no use of public-key crypto, so it should \nbe relatively fast.\nWhen each a data packet is of length *N*, \nall arithmetic is modulo a prime *p* (we use an 87-bit prime by default), \nand \"elements\" are integers modulo *p*, \nthe dominant costs of the system are:\n* **Client compute:** O(*N* log *N*) multiplications \n* **Client-to-server communication:** 2*N* + O(1) elements<br>\n* **Server compute:** O(*N* log *N*) multiplications to check each packet<br> \n    (NOTE: Using an optimization we haven't yet implemented, we can \n    drop this cost to O(*N*) multiplications per packet.)\n* **Server-to-server communication:** O(1) elements\n* **Server storage:** O(*N*) elements\n\n## Running the code\n\nYou must first install \n[NSS/NSPR](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS)\nwith NSPR version 4.13.1 (or newer?) and NSS version 3.35 (or newer?), \n[SCons](http://scons.org/) version 3.0.1 (or newer?), and\n[msgpack-c](https://github.com/msgpack/msgpack-c) version 2.1.5 (or newer?).\n\nOn Ubuntu Bionic (18.04LTS), you can install NSS and scons with:\n\n    $ sudo apt install scons libnspr4-dev libnss3-dev \n\nand you will have to download [msgpack-c 2.1.5 or newer here](https://github.com/msgpack/msgpack-c/releases),\nsince the Ubuntu packages for msgpack are far out of date.\n\nFor macOS using Homebrew:\n\n    $ brew install nss nspr scons msgpack\n\nTo compile the code, run:\n\n    $ scons\n\nTo run the test suite, execute:\n\n    $ build/ptest/ptest -v\n\nTo print debug messages while compiling:\n\n    $ scons VERBOSE=1\n\nTo compile with debug symbols, run:\n\n    $ scons DEBUG=1\n\nTo clean up the object and binary files, run:\n\n    $ scons -c\n\nThe library can be built and tested with docker.\n\n    $ docker-compose build\n    $ docker-compose run libprio\n\n\nThe files in this directory are:\n````\n/build      - Binaries, object files, etc.\n/include    - Exported header files\n              (Note: The public header is <mprio.h> since\n              NSPR already has a file called <prio.h>.)\n/mpi        - NSS MPI bignum library \n/pclient    - Example code that uses the Prio library\n/prio       - Prio library core code\n/ptest      - Tests and test runner\n````\n\n## Optimizations and features not yet implemented\n* **Server compute.**\n  By using a fast polynomial interpolation-and-evaluation\n  routine, we can reduce the cost of checking a single client\n  request from O(*N* log *N*) multiplications down to O(*N*)\n  multiplications, for a data packet of *N* items.\n* **Differential privacy.**\n  It would be very straightforward to add some small amount of \n  noise to the final statistics to provide differential privacy.\n  If this would be useful, I can add it.\n* **Misc.**\n  There are TODO notes scattered throughout code indicating\n  places for potential performance optimizations.\n  \n\n"
},
{
  "name": "discourse-mozilla-iam",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "app",
      "assets",
      "config",
      "db",
      "lib",
      "plugin.rb",
      "spec",
      "test"
    ]
  },
  "makefile": null,
  "readme": "Discourse + Mozilla IAM\n===========================\n\n[![Build Status](https://travis-ci.org/mozilla/discourse-mozilla-iam.svg?branch=master)](https://travis-ci.org/mozilla/discourse-mozilla-iam)\n [![Coverage Status](https://coveralls.io/repos/github/mozilla/discourse-mozilla-iam/badge.svg?branch=master)](https://coveralls.io/github/mozilla/discourse-mozilla-iam?branch=master)\n\nA plugin to integrate [Discourse](http://discourse.org) with Mozilla's Identity and Access Management (IAM) system.\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Running tests\n\nClone this plugin into `plugins/discourse-mozilla-iam` in the root of your Discourse source dir.\n\nUse `RAILS_ENV=test rake plugin:spec[discourse-mozilla-iam]` to run the tests.\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "onecrl-entry-checker",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "compare.py",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# Requirements\n\nYou'll need read access to the `prod` bucket and read/write access to the `staging` bucket on a suitable copy of Kinto.\n\nInstall deps with `pip install -r requirements.txt`\n\n# Usage\n\n`python3 compare.py`\n\n```\n[09:45:22] Stage-Stage: 1243 Stage-Preview: 1243 Stage-Published: 1240                 compare.py:59\n[09:45:24] Prod-Stage: 1243 Prod-Preview: 1243 Prod-Published: 1240                    compare.py:65\n           Verifying stage against preview                                             compare.py:71\n           stage/security-state-staging (1243) and stage/security-state-preview (1243) compare.py:74\n           are equivalent\n           stage/security-state-staging (1243) and prod/security-state-staging (1243)  compare.py:74\n           are equivalent\n           stage/security-state-staging (1243) and prod/security-state-preview (1243)  compare.py:74\n           are equivalent\n[09:45:25] stage/security-state-preview (1243) and prod/security-state-staging (1243)  compare.py:74\n           are equivalent\n           stage/security-state-preview (1243) and prod/security-state-preview (1243)  compare.py:74\n           are equivalent\n           prod/security-state-staging (1243) and prod/security-state-preview (1243)   compare.py:74\n           are equivalent\n           There are 3 changes waiting. Adding:                                        compare.py:79\n{\n    'details': {\n        'bug': 'https://bugzilla.mozilla.org/show_bug.cgi?id=1674587',\n        'who': '',\n        'why': '',\n        'name': '',\n        'created': ''\n    },\n    'enabled': False,\n    'issuerName':\n'MF0xCzAJBgNVBAYTAkpQMSUwIwYDVQQKExxTRUNPTSBUcnVzdCBTeXN0ZW1zIENPLixMVEQuMScwJQYDVQQLEx5TZWN1cml0eSB\n    'serialNumber': 'Irmw1g=='\n}\n{\n    'details': {\n        'bug': 'https://bugzilla.mozilla.org/show_bug.cgi?id=1674587',\n        'who': '',\n        'why': '',\n        'name': '',\n        'created': ''\n    },\n    'enabled': False,\n    'issuerName':\n'MGAxCzAJBgNVBAYTAk5MMR4wHAYDVQQKDBVTdGFhdCBkZXIgTmVkZXJsYW5kZW4xMTAvBgNVBAMMKFN0YWF0IGRlciBOZWRlcmx\n    'serialNumber': 'akWWZTytiy58nyLOsqpnaQ=='\n}\n{\n    'details': {\n        'bug': 'https://bugzilla.mozilla.org/show_bug.cgi?id=1674587',\n        'who': '',\n        'why': '',\n        'name': '',\n        'created': ''\n    },\n    'enabled': False,\n    'issuerName':\n'MFAxJDAiBgNVBAsTG0dsb2JhbFNpZ24gRUNDIFJvb3QgQ0EgLSBSNTETMBEGA1UEChMKR2xvYmFsU2lnbjETMBEGA1UEAxMKR2x\n    'serialNumber': 'Ae5fInnr9AhpWVIjkw=='\n}\n```"
},
{
  "name": "new-tab-scratchpad",
  "files": {
    "/": [
      ".gitignore",
      ".storybook",
      "README.md",
      "icon.svg",
      "manifest.json",
      "package-lock.json",
      "package.json",
      "public",
      "rollup.config.js",
      "scripts",
      "src",
      "web-ext-artifacts"
    ]
  },
  "makefile": null,
  "readme": "*Looking for a shareable component template? Go here --> [sveltejs/component-template](https://github.com/sveltejs/component-template)*\n\n---\n\n# svelte app\n\nThis is a project template for [Svelte](https://svelte.dev) apps. It lives at https://github.com/sveltejs/template.\n\nTo create a new project based on this template using [degit](https://github.com/Rich-Harris/degit):\n\n```bash\nnpx degit sveltejs/template svelte-app\ncd svelte-app\n```\n\n*Note that you will need to have [Node.js](https://nodejs.org) installed.*\n\n\n## Get started\n\nInstall the dependencies...\n\n```bash\ncd svelte-app\nnpm install\n```\n\n...then start [Rollup](https://rollupjs.org):\n\n```bash\nnpm run dev\n```\n\nNavigate to [localhost:5000](http://localhost:5000). You should see your app running. Edit a component file in `src`, save it, and reload the page to see your changes.\n\nBy default, the server will only respond to requests from localhost. To allow connections from other computers, edit the `sirv` commands in package.json to include the option `--host 0.0.0.0`.\n\n\n## Building and running in production mode\n\nTo create an optimised version of the app:\n\n```bash\nnpm run build\n```\n\nYou can run the newly built app with `npm run start`. This uses [sirv](https://github.com/lukeed/sirv), which is included in your package.json's `dependencies` so that the app will work when you deploy to platforms like [Heroku](https://heroku.com).\n\n\n## Single-page app mode\n\nBy default, sirv will only respond to requests that match files in `public`. This is to maximise compatibility with static fileservers, allowing you to deploy your app anywhere.\n\nIf you're building a single-page app (SPA) with multiple routes, sirv needs to be able to respond to requests for *any* path. You can make it so by editing the `\"start\"` command in package.json:\n\n```js\n\"start\": \"sirv public --single\"\n```\n\n## Using TypeScript\n\nThis template comes with a script to set up a TypeScript development environment, you can run it immediately after cloning the template with:\n\n```bash\nnode scripts/setupTypeScript.js\n```\n\nOr remove the script via:\n\n```bash\nrm scripts/setupTypeScript.js\n```\n\n## Deploying to the web\n\n### With [Vercel](https://vercel.com)\n\nInstall `vercel` if you haven't already:\n\n```bash\nnpm install -g vercel\n```\n\nThen, from within your project folder:\n\n```bash\ncd public\nvercel deploy --name my-project\n```\n\n### With [surge](https://surge.sh/)\n\nInstall `surge` if you haven't already:\n\n```bash\nnpm install -g surge\n```\n\nThen, from within your project folder:\n\n```bash\nnpm run build\nsurge public my-project.surge.sh\n```\n"
},
{
  "name": "restmail.net",
  "files": {
    "/": [
      ".eslintrc",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "ansible",
      "bin",
      "lib",
      "loadtest",
      "package-lock.json",
      "package.json",
      "tests",
      "website"
    ]
  },
  "makefile": null,
  "readme": "# Email addresses for testing\n\n[![Build Status](https://secure.travis-ci.org/mozilla/restmail.net.svg)](http://travis-ci.org/mozilla/restmail.net)\n\nHave you ever wanted to write an automated test of a service that\nsends email?  If you have, you might have wanted an email address that\nyou can check using a simple REST service that returns JSON...\n\n..that's what *restmail.net* is.\n\n## overview\n\nrestmail.net is an email server with a REST API.  Any time an\nemail is sent to a @restmail.net address, the email address springs\ninto existence and the message is stored (with a max of 10 emails per\n\"user\").  You can query the restmail API to check the email of any\nuser, and you can also delete outstanding messages via the API.\n\nThis lets you test services that deliver email.\n\nBy default, emails are deleted after one day.  You can change this\nby modifying the `expireAfter` parameter in `config.js`.  Set it to\n`0` for no auto-deletion, in which case you will want to keep an\neye on your redis database size.\n\n## security?\n\nAll mail sent to restmail email addresses is completely public.  anyone\nmay delete the messages associated with any user.  This service is\nwide open.  If that won't do, you should fork and deploy your own\ninstance of restmail, and add sekrets and stuff.\n\n## the API\n\n### `DELETE /mail/<user>`\n\nDelete all of the mail for the named user (where user is the user\nportion of the email address, not including `@restmail.net`).\n\nReturns `200` on success.\n\n### `GET /mail/<user>`\n\nReturns all mail for the specified user, as an array of JSON blobs,\nwith the newest messages first.  Here's example output (with lots of\nfields stripped out):\n\n```json\n[\n  {\n    \"headers\": {\n      \"date\": \"Fri, 11 May 2012 14:44:37 -0600\",\n      \"mime-version\": \"1.0 (Apple Message framework v1257)\"\n    },\n    \"from\": [\n      {\n        \"address\": \"lloyd@example.com\",\n        \"name\": \"Lloyd Hilaiel\"\n      }\n    ],\n    \"to\": [\n      {\n        \"address\": \"test@restmail.net\",\n        \"name\": \"\"\n      }\n    ],\n    \"subject\": \"this is my first message\",\n    \"text\": \"it is very pretty.\\n\"\n  },\n  {\n    \"headers\": {\n      \"date\": \"Fri, 11 May 2012 14:44:52 -0600\",\n      \"mime-version\": \"1.0 (Apple Message framework v1257)\"\n    },\n    \"from\": [\n      {\n        \"address\": \"lloyd@example.com\",\n        \"name\": \"Lloyd Hilaiel\"\n      }\n    ],\n    \"to\": [\n      {\n        \"address\": \"test@restmail.net\",\n        \"name\": \"\"\n      }\n    ],\n    \"subject\": \"this is my second message\",\n    \"text\": \"it's pretty awesome too.\\n\"\n  }\n]\n```\n\n## restmail on your domain (deprecated; the public restmail.net is now resricted).\n\nYou can use restmail from custom domains:  Just set restmail as your mail exchanger:\n\n    $ dig -t <my domain>\n    <my domain>            900     IN      MX      20 restmail.net.\n\nAnd fetch mail with the full email address:\n\n    GET /mail/<user>@<my domain>\n\ndone!\n\n"
},
{
  "name": "glad",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# glad\nThe Glean Auditing Dashboard\n"
},
{
  "name": "Reps",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "ISSUE_TEMPLATE.md",
      "README.md",
      "ThankYouEmma",
      "ThankYouRosana",
      "branding",
      "newreps",
      "newsletter",
      "okr-dashboard"
    ]
  },
  "makefile": null,
  "readme": "# The Reps Program Planning Repo\n\n![Participate](https://wiki.mozilla.org/images/thumb/e/e1/MozRep-Final-Outline.png/300px-MozRep-Final-Outline.png)\n\nThis repository is used for tracking [issues](https://github.com/mozilla/Reps/issues) directly related to the work of the Mozilla Reps program.\n\nIf you'd like to propose new work or get in touch with the program, head over to discourse here: https://discourse.mozilla-community.org/c/reps\n"
},
{
  "name": "ascendproject",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Best_Practices.md",
      "CNAME",
      "CONTRIBUTORS.md",
      "Gemfile",
      "README.md",
      "Rakefile",
      "_config.yml",
      "_data",
      "_includes",
      "_layouts",
      "_posts",
      "about.md",
      "application.md",
      "apply",
      "course_materials",
      "css",
      "faq.md",
      "feed.xml",
      "images",
      "index.html",
      "params.json",
      "participants",
      "press.md",
      "resources",
      "sponsors.md",
      "sponsors",
      "stylesheets",
      "timeline",
      "volunteer.md"
    ]
  },
  "makefile": null,
  "readme": "# Welcome to the Ascend Project [![Build Status](https://travis-ci.org/mozilla/ascendproject.png?branch=gh-pages)](https://travis-ci.org/mozilla/ascendproject)\n\n\nThe Ascend Project is a 6 week, full time training program that provides financial support, equipment, meals, transit, and childcare reimbursement in order to remove many of the barriers to immersive learning in Open Source technology.  The goal is to allow participants time and support as they focus on learning common open source practices: IRC, bug trackers, code review, version control, creating & committing patches, as well as the larger opportunities available to those who can leverage the tools and be developers of the open web. \n\nThis is a pilot project and as it is being developed this repo will hold all the material for:\n\n* Website [http://ascendproject.org][ascend_homepage]\n* Participant blog posts\n* Curriculum\n* Budgets\n* Resources & Materials\n\nThe point is to have all of this in the open and available to anyone who wants to see it, contribute to it, test it, or fork it. This repo also acts as a sandbox for participants to learn git and build up experience working in a shared project while also providing valuable archiving of assets for the project.\n\nCheck out the [homepage][ascend_homepage] for more information.\n\n\n[ascend_homepage]: http://ascendproject.org/about\n"
},
{
  "name": "mozilla.github.io",
  "files": {
    "/": [
      "Dino.svg",
      "GitHub-Mark.svg",
      "README.md",
      "index.html"
    ]
  },
  "makefile": null,
  "readme": "mozilla.github.io\n=================\n\nThe Mozilla Project uses GitHub!\n"
},
{
  "name": "One-Mozilla-blog",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "header-template.psd",
      "themes"
    ]
  },
  "makefile": null,
  "readme": "One-Mozilla-blog\n================\n\nA universal WordPress theme for Mozilla blogs.\n\nWhile this theme's code is open source, released under [GPLv3](http://www.gnu.org/licenses/gpl-3.0.html),\nthe distinctive visual design should be reserved for official Mozilla blogs. Certain assets -- especially logos --\nincluded with this theme are [protected property of Mozilla](https://www.mozilla.org/foundation/trademarks/).\n\nBy all means, take this code, modify it, improve it, learn from it... but please don't install the unaltered theme on\nyour own blog.\n"
},
{
  "name": "contentful-prototypes",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# contentful-prototypes\nShared Repo for Contentful Prototype Development\n"
},
{
  "name": "surf",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "package-lock.json",
      "package.json",
      "public",
      "server.js",
      "template-helpers",
      "views"
    ]
  },
  "makefile": null,
  "readme": "# SURF\nMozilla Security Engineering University Relationship Framework (SURF) // surf.mozilla.org\n"
},
{
  "name": "sunset-extension",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "_locales",
      "background.js",
      "goodbye",
      "icon.svg",
      "manifest.json"
    ]
  },
  "makefile": null,
  "readme": "Sunset\n======\n\nThis is a template extension you can use in case you no longer plan to maintain your add-on and\nwould like to offer users a way to export their data before it stops working. The add-on simply\nopens a tab such as this one:\n\n![Screenshot of a browser tab with a mountains and sunset background. Headline saying \"it's time to go\", some descriptive text, and suggested replacement add-ons](https://github.com/kewisch/sunset-extension/blob/assets/screenshot.png?raw=1)\n\nConfiguration\n-------------\n\n* Update the `manifest.json` with information from your add-on.\n  * Don't copy your whole manifest, you don't need all the permissions for example.\n  * If you previously had a browser action button uncomment those lines so users can click on the button they know.\n* Edit the HTML page to match your add-on's style. A few recommendations on what to keep:\n  * **Let users know what is happening**. Your add-on is being discontinued. Acknowledge users' frustration about this.\n  * **Talk about the benefits**. More time to work on your core product? Not shipping the add-on more secure? There must be something positive in it.\n  * **Give users an alternative**. Users liked your functionality. They might want a different add-on that does something similar, do you have a suggestion? This should be an add-on listed on AMO. We won't accept links to self-hosted add-ons or insisting to install a different browser.\n* Adjust the copy in `_locales/en/messages.json`, and translate as necessary.\n"
},
{
  "name": "chromium-src-build",
  "files": {
    "/": [
      ".gitignore",
      ".style.yapf",
      "BUILD.gn",
      "OWNERS",
      "OWNERS.setnoparent",
      "OWNERS.status",
      "README.md",
      "android",
      "apple",
      "apply_locales.py",
      "args",
      "build-ctags.sh",
      "build_config.h",
      "buildflag.h",
      "buildflag_header.gni",
      "check_gn_headers.py",
      "check_gn_headers_unittest.py",
      "check_gn_headers_whitelist.txt",
      "check_return_value.py",
      "chromeos",
      "ciopfs.sha1",
      "cipd",
      "clobber.py",
      "compiled_action.gni",
      "compute_build_timestamp.py",
      "config",
      "copy_test_data_ios.py",
      "cp.py",
      "detect_host_arch.py",
      "dir_exists.py",
      "docs",
      "dotfile_settings.gni",
      "download_nacl_toolchains.py",
      "env_dump.py",
      "extract_from_cab.py",
      "extract_partition.py",
      "find_depot_tools.py",
      "find_isolated_tests.py",
      "fix_gn_headers.py",
      "fuchsia",
      "gdb-add-index",
      "get_landmines.py",
      "get_symlink_targets.py",
      "gn_helpers.py",
      "gn_helpers_unittest.py",
      "gn_logs.gni",
      "gn_run_binary.py",
      "install-build-deps-android.sh",
      "install-build-deps.sh",
      "install-chroot.sh",
      "internal",
      "ios",
      "lacros",
      "landmine_utils.py",
      "landmines.py",
      "linux",
      "locale_tool.py",
      "mac",
      "mac_toolchain.py",
      "nocompile.gni",
      "noop.py",
      "partitioned_shared_library.gni",
      "precompile.cc",
      "precompile.h",
      "print_python_deps.py",
      "protoc_java.py",
      "protoc_java.pydeps",
      "redirect_stdout.py",
      "rm.py",
      "run_swarming_xcode_install.py",
      "sample_arg_file.gn",
      "sanitize-mac-build-log.sed",
      "sanitize-mac-build-log.sh",
      "sanitize-win-build-log.sed",
      "sanitize-win-build-log.sh",
      "sanitizers",
      "shim_headers.gni",
      "skia_gold_common",
      "swarming_xcode_install.py",
      "symlink.gni",
      "symlink.py",
      "timestamp.gni",
      "toolchain",
      "tree_truth.sh",
      "update-linux-sandbox.sh",
      "util",
      "vs_toolchain.py",
      "whitespace_file.txt",
      "win",
      "write_build_date_header.py",
      "write_buildflag_header.py",
      "xcode_binaries.yaml"
    ],
    "/docs": [
      "debugging_slow_builds.md",
      "mac_hermetic_toolchain.md",
      "writing_gn_templates.md"
    ]
  },
  "makefile": null,
  "readme": "# About\n`//build` contains:\n * Core GN templates and configuration\n * Core Python build scripts\n\nSince this directory is DEPS'ed in by some other repositories (webrtc, pdfium,\nv8, etc), it should be kept as self-contained as possible by not referring\nto files outside of it. Some exceptions exist (`//testing`, select\n`//third_party` subdirectories), but new dependencies tend to break these other\nprojects, and so should be avoided.\n\nChanges to `//build` should be landed in the Chromium repo. They will then be\nreplicated to the stand-alone [build repo](https://chromium.googlesource.com/chromium/src/build)\nby the [gsubtreed tool.](https://chromium.googlesource.com/infra/infra/+/master/infra/services/gsubtreed)\nNote: You can find all directories already  available through gsubtreed in the\n[list of all chromium repos](https://chromium.googlesource.com/).\n\n## Contents\n * `//build/config` - Common templates via `.gni` files.\n * `//build/toolchain` - GN toolchain definitions.\n * `Other .py files` - Some are used by GN/Ninja. Some by gclient hooks, some\n   are just random utilities.\n\nFiles referenced by `//.gn`:\n * `//build/BUILDCONFIG.gn` - Included by all `BUILD.gn` files.\n * `//build/secondary` - An overlay for `BUILD.gn` files. Enables adding\n   `BUILD.gn` to directories that live in sub-repositories.\n * `//build_overrides` -\n   Refer to [//build_overrides/README.md](../build_overrides/README.md).\n\n## Docs\n\n* [Writing GN Templates](docs/writing_gn_templates.md)\n* [Debugging Slow Builds](docs/debugging_slow_builds.md)\n* [Mac Hermetic Toolchains](docs/mac_hermetic_toolchain.md)\n* [Android Build Documentation](android/docs/README.md)\n"
},
{
  "name": "notes",
  "files": {
    "/": [
      ".babelrc",
      ".eslintignore",
      ".eslintrc.yml",
      ".gitignore",
      ".nvmrc",
      ".stylelintrc",
      ".travis.yml",
      "CHANGELOG.md",
      "CODEOWNERS",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "RELEASE.md",
      "bin",
      "circle.yml",
      "ckeditor-build",
      "docs",
      "karma.conf.js",
      "l10n.toml",
      "locales",
      "native",
      "package-lock.json",
      "package.json",
      "scripts",
      "src",
      "test",
      "webpack.config.js",
      "webpack.test-unit.js"
    ],
    "/docs": [
      "issue_template.md",
      "localization.md",
      "metrics.md"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Notes\n\nA notepad for Firefox\n\n[![CircleCI](https://circleci.com/gh/mozilla/notes/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla/notes/tree/master)\n[![Available on Test Pilot](https://img.shields.io/badge/available_on-Test_Pilot-0996F8.svg)](https://testpilot.firefox.com/experiments/notes)\n> Discussion Forum: [https://discourse.mozilla.org/c/archives/test-pilot](https://discourse.mozilla.org/c/archives/test-pilot)\n\n\n## Releases\n\nThe releases are packaged by CircleCI for [Android](https://app.circleci.com/pipelines/github/mozilla/notes/20/workflows/239f8590-cfae-491b-9db7-30971c73fd9a/jobs/3294/artifacts) and the [WebExtension](https://app.circleci.com/pipelines/github/mozilla/notes/20/workflows/239f8590-cfae-491b-9db7-30971c73fd9a/jobs/3292/artifacts).\n\n## Contribute\n\n* Step 0: If you plan on sending a pull-request, you should fork the repository.\n* Step 1: Clone the [notes](https://github.com/mozilla/notes) repository or your fork.\n```\ngit clone https://github.com/mozilla/notes.git\n# or\ngit clone https://github.com/[yourusername]/notes.git\n```\n* Step 2: Navigate to the root of the directory you cloned and run:\n> Make sure to use Node.js 8+.\n\n| Command         | Description                               |\n|-----------------|-------------------------------------------|\n| `npm install`   | Installs required Node.js dependencies.   |\n| `npm run build` | Builds the application as a Web Extension.|\n| `npm start`     | Launches Firefox with the Web Extension.  |\n\nYou can also open the `test/index.html` file in your browser to run the automated tests.\n\n## WebExtension Permissions\n\n| Permission      | Description                                                                    |\n|-----------------|--------------------------------------------------------------------------------|\n| `contextMenus`  | Used for \"Send to Note\" feature, sends text from pages to the Notes sidebar.   |\n| `storage`       | Storage for Notes.                                                             |\n| `identity`      | OAuth login to Firefox Accounts to sync your notes.                            |\n\n## Release\n\nSee [RELEASE.md](https://github.com/mozilla/notes/blob/master/RELEASE.md) for release steps.\n\n### Localization\n\nFirefox Notes localization is managed via [Pontoon](https://pontoon.mozilla.org/), not direct pull requests to the repository. If you want to fix a typo, add a new language, or simply know more about localization, please get in touch with the [existing localization team](https://pontoon.mozilla.org/teams/) for your language, or Mozilla\u2019s [l10n-drivers](https://wiki.mozilla.org/L10n:Mozilla_Team#Mozilla_Corporation) for guidance.\n\n## Licenses\n\n* [Mozilla Public License Version 2.0](LICENSE)\n* [CKEditor Text Editor License](https://github.com/ckeditor/ckeditor5/blob/master/LICENSE.md) used under MPL licence\n\n## Design\n\n* Design for reference: https://mozilla.invisionapp.com/share/6VBUYHMRB#/281041484_Firefox_Notes\n* Mobile design for reference: https://mozilla.invisionapp.com/share/BTGS26C2FE4\n\n## Screenshot\n\n![Notes v4](https://i.imgur.com/kOuI2uG.png)\n"
},
{
  "name": "measure-noise",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "config.json",
      "measure_noise",
      "requirements.txt",
      "resources",
      "setup.py",
      "setuptools.json",
      "tests",
      "vendor"
    ]
  },
  "makefile": null,
  "readme": "# measure-noise\n\nMeasure how our data deviates from normal distribution. ([project summary](https://docs.google.com/document/d/1TMUhY4zaOA_DV6hzmNY3Z2v9Hm6XF3zlKvWOTRL4xJs/edit#))\n\n\n|Branch      |Status   | Coverage |\n|------------|---------|----------|\n|master      | [![Build Status](https://travis-ci.org/mozilla/measure-noise.svg?branch=master)](https://travis-ci.org/mozilla/measure-noise) | |\n|dev         | [![Build Status](https://travis-ci.org/mozilla/measure-noise.svg?branch=dev)](https://travis-ci.org/mozilla/measure-noise)    | [![Coverage Status](https://coveralls.io/repos/github/mozilla/measure-noise/badge.svg)](https://coveralls.io/github/mozilla/measure-noise) |\n\n\n## Install\n\n    pip install measure-noise\n\n## Usage\n\nThe `deviance()` method will return a `(description, score)` pair describing how the samples deviate from a normal distribution, and by how much.  This is intended to screen samples for use in the t-test, and other statistics, that assume a normal distribution.\n\n* `SKEWED` - samples are heavily to one side of the mean\n* `OUTLIERS` - there are more outliers than would be expected from normal distribution\n* `MODAL` - few samples are near the mean (probably bimodal)\n* `OK` - no egregious deviation from normal\n* `N/A` - not enough data to make a conclusion (aka `OK`)\n\n#### Example\n\n    from measure_noise import deviance\n\n\t>>> desc, score = deviance([1,2,3,4,5,6,7,8])\n    >>> desc\n    'OK'\n\n## Development\n\nFor Linux/OSX\n\n    git clone https://github.com/mozilla/measure-noise.git\n    cd measure-noise\n    python -m venv .venv      \n    source .venv/bin/activate\n    pip install -r requirements.txt\n    pip install -r tests/requirements.txt\n    export PYTHONPATH=.:vendor\n    python -m unittest discover tests \n\nSimilar for Windows:\n\n    git clone https://github.com/mozilla/measure-noise.git\n    cd measure-noise\n    python.exe -m pip install virtualenv  \n    python.exe -m virtualenv .venv \n    .venv\\Scripts\\activate\n    pip install -r requirements.txt\n    pip install -r tests\\requirements.txt\n    set PYTHONPATH=.;vendor\n    python -m unittest discover tests \n\n\n---------------------\n\n## Analysis Configuration\n\nThe analysis is controlled by the `config.json` file, but it is missing the secrets required to connect to the Treeherder RO database and BigQuery. You must provide references to secrets by making your own config file. \n\nPlease look at the existing `resources/config-<user>.json` files. You may make a copy of one, and update it with references to the secrets on your dev machine. For example, `resources/kyle-config.json` looks like\n\n```json\n{\n    \"$ref\": \"../config.json\",\n    \"database.$ref\": \"~/private.json#treeherder\",\n    \"deviant_summary.account_info.$ref\": \"file:///e:/moz-fx-dev-ekyle-treeherder-a838a7718652.json\",\n    \"constants.measure_noise.analysis.SCATTER_RANGE\": \"month\",\n    \"constants.measure_noise.analysis.TREEHERDER_RANGE\": \"90day\"\n}\n```\n\nYou may be able to guess where each of my secrets are found on my machine. For example, I have a `private.json` file in my user directory. Here is an example of the contents:\n\n```javascript\n// Example contents of ~/private.json\n{\n    \"treeherder\": {\n        \"host\": \"treeherder-prod-ro.cd3i3txkp6c6.us-east-1.rds.amazonaws.com\",\n        \"port\" : 3306,\n        \"username\": \"username\",\n        \"password\": \"password\"\n    }\n}\n```\n\nFeel free to add your `resources/config-<user>.json` file and make a PR with it. Remember, **never put secrets in your project directory**; use references to secrets instead.   \n\n> You may use `{\"$ref\":\"env://MY_ENV_VARIABLE\"}` to use environment variables. [More details](https://github.com/klahnakoski/mo-json-config#environment-variables-reference) \n\n## Running Analysis\n\nEnsure you are in the main project directory, and point to your config file \n\n**Linux/OSX**\n\n    export PYTHONPATH=.:vendor\n    python measure_noise/analysis.py --config=resources/config-<user>.json\n\n**Windows**\n\n    set PYTHONPATH=.;vendor\n    python measure_noise\\analysis.py --config=resources\\config-kyle.json\n\n\nSome other options are \n\n* `--id` - show specific signature_ids\n* `--download=<filename>` - Download all deviant statistics to file\n* `--deviant=<int>` - Show number of series with most deviant noise \n* `--outliers=<int>` - Show number of series with most outliers noise \n* `--modal=<int>` - Show number of series that seem bi-modal \n* `--skewed=<int>` - Show number of series where mean and median differ \n* `--ok=<int>` - Show number of series that are worst, but good enough \n* `--noise=<int>` - Show number of series with largest relative standard deviation\n* `--extra=<int>` - Show number of series where Perfherder has detected steps, but not MWU\n* `--missing=<int>` - Show number of series where Perfherder missed alerting\n\n## Post Analysis\n\nThe `analysis.py` fills a local Sqlite database (as per the config file). It can be used to lookup other series that may be of interest, or to feed yet-another-program.\n\n\n## Windows\n\nYou must download the `scipy` and `numpy` binary packages. \n"
},
{
  "name": "ahab",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Ahab\nKubernetes AI Brainstorming\n\nAhab API - AI for Kubernetes\n\n- Machine learning based on observations about the cluster \n- May fit into an operator pattern - TBD\n- Operates in three progressively-responsible modes at the discretion of the cluster operators (these can all be constrained by label/namespace, etc) *Advisor* (tell about notable things but not do) vs. *mitigation* (fix automatically) vs. *manager mode* (tell & fix, deploy when\u2026  )\n    - Advisor recommends changes to the running cluster state, can also identify what it \u201cwould do\u201d if it was in mitigation or manager mode Data horizon: short i.e. week of training\n    - Mitigation (incident) mode is taking corrective/proactive actions based on intelligence stream + history + what is normal - aberrations that are \u201cnormal\u201d but outside of the expected state i.e. manual deploys/rollbacks, helm activity, deploys etc., re-establishes homeostasis : medium i.e. month or two\n    - Manager mode can perform proactive scaling, delay deployment until conditions are better (as defined by thresholds), optimization, tries to make a better normal  : long term i.e. 6 mo -- also generates ongoing stats that can help determine accuracy of the models\n- Integration with a bot : \u201cHow is today different than\u2026\u201d \u201cWhat\u2019s wrong with ____\u201d using natural language processing\n    - temperature of the cluster, i.e. Smoky the Bear forest fire risk model\n    - human-readable\n    - last failure, last deploy, changes generally so events can still be human-correlated\n    - current status (can be defined)\n- Understands organism boundaries (the area over which data can be gathered and acted on)\n    - Types of resources\n    - Actions against the cluster\n        - user generated\n        - end-user generated\n        - automated\n        - hostile\n        - expected/unexpected\n    - Architectural\n        - region\n        - phys nodes\n        - constraints\n        - failover path - warming up failure path when things seem unstable\n            - warm up the app\n            - start DB replication\n        - \u201cGoogle Power\u201d model\n- Node agents and central \u201cbrain\u201d \n- Can expert state for apples-to-apples upgrades, duplicate clusters\n- Integration with Helm\n    - Understanding Helm\u2019s release object\n    - How to feed geometry back to Helm/Tiller\n        - dry run mode reads chart and compares potential outcomes, Ahab can make recommendations\n        - if I increase memory to X does that still work\n        - fully model the interaction before you run it\n            - if you do X \u2026 the impact on the cluster is Y\n            - predictive failure analysis\n            - the Ahab data set allows for complete virtual analysis\n\nGoal - a method of constant prediction analysis that compares actual/predicted and adjusts the coinciding model. There will be an action stream and a prediction stream at all times. Eventually they should converge.\n"
},
{
  "name": "jsoncache",
  "files": {
    "/": [
      ".flake8",
      ".gitignore",
      ".pre-commit-config.yaml",
      "Makefile",
      "README.md",
      "enviroment.yml",
      "jsoncache",
      "setup.py",
      "tests"
    ]
  },
  "makefile": ".PHONY: build up tests flake8 ci tests-with-cov\n\nbuild:\n\tpython setup.py sdist\n\nupload:\n\ttwine upload --repository-url https://upload.pypi.org/legacy/ dist/*\n\n",
  "readme": "# jsoncache\nPython cache control for cloud storage models\n\n\nThis library exposes a multithreaded JSON object loader that support\nAmazon S3 and Google Cloud Storage.\n\n\n## Why do I care?\n\nBecause loading JSON files from the cloud is more annoying than you\nrealize.\n\n* Sometimes you're gonna get errors - log those errors.\n* Sometimes you're going to have compressed JSON blobs because Google\n  Cloud Storage has unmanageable timeouts for uploads\n  (https://github.com/googleapis/python-storage/issues/74)\n* You want your application to behave as if read errors from the cloud\n  weren't a problem, but you want those errors to show up in logging.\n\n\n## Quick Start\n\n\n1. Import the ThreadedObjectCache class.\n2. Instantiate it passing in the cloud type, bucket, path and time to\n   live in seconds.\n3. Call `.get()` on the ThreadedObjectCache instace.\n\n\nYou can optionally pass in a custom implementation of the `time`\nmodule to override how `time.time()` works.\n\nYou can optionally pass in a custom callable `transformer` that will\napply the `transformer` function to the data before it's returned.\nTypical use cases might involve initializing a sklearn model.\n\nYou can optionally pass in `block_until_cached`=True so that the\nconstructor will block until a model is loaded successfully from the\nnetwork.\n\nAll background threads are marked as daemon threads so using this code\nwon't cause your application to wait for thread death.\n\n```\nPython 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 02:37:09)\nType 'copyright', 'credits' or 'license' for more information\nIPython 7.17.0 -- An enhanced Interactive Python. Type '?' for help.\n\nIn [1]: from jsoncache import *\n\nIn [2]: t = ThreadedObjectCache('s3', 'telemetry-parquet', 'taar/similarity/lr_curves.json', 10)\n\nIn [3]: 2020-08-05 16:07:14,369 - botocore.credentials - INFO - Found credentials in environment variables.\nIn [3]:\n\nIn [3]: t.get()\nOut[3]:\n[[0.0, [0.029045735469752962, 0.02468400347868071]],\n [0.005000778819764661, [0.029530930135620918, 0.025088940785616222]],\n ...\n```\n"
},
{
  "name": "prospector",
  "files": {
    "/": [
      ".gitignore",
      "README",
      "aboutProfile",
      "aboutTrackers",
      "awesomeBarHD",
      "findSuggest",
      "homeDash",
      "instantPreview",
      "lessChromeHD",
      "oneLiner",
      "predictiveNewtab",
      "queryStats",
      "recallMonkey",
      "searchTabs",
      "siteSuggest",
      "snaporama",
      "speakWords",
      "tabFocus",
      "userData"
    ]
  },
  "makefile": null,
  "readme": "Prospector is a series of experiments from Mozilla Labs focused on analyzing, experimenting and prototyping improvements on how you search and discover content with Firefox.\n\nEach directory contains an individual Prospector experiment.\n\nFor more information about the overall project and individual experiments, visit https://mozillalabs.com/prospector/\n"
},
{
  "name": "slow_regression_detection",
  "files": {
    "/": [
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "__init__.py",
      "doc",
      "env_r.yaml",
      "etl.sh",
      "pyproject.toml",
      "requirements.txt",
      "requirements_dev.txt",
      "slow_regressions",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "slow_regressions\n==============================\n\n# Overview\n\nAt a high level, this project\n\n* pulls data from the taskcluster `moz-fx-data-derived-datasets.taskclusteretl.perfherder` table\n* arranges data by test suite and platform, performs preprocessing and outlier removal\n* fits a BRMS time series model \n\nThe primary output of this model are the MCMC samples of the BRMS model for a given day, test suite, platform combination. This allows for Bigquery calculations that, for a given test suite and platform, can make probabilistic comparisons of any day in the range.\n\nAn example usage is plotted below. \n\n![Output of model](https://github.com/wcbeard/slow_regression_detection/blob/master/doc/output.png)\n\n\nOn the right is a plot of the individual test scores over time, represented as circles. The error bands represent percentiles of the MCMC samples. \n\n**Note** that the test points (circles) and confidence intervals (bands) come from 2 different tables, since there isn't a clean mapping in time from one to the other. The tests (circles) can run multiple times in a day, and there can be significant gaps in time where there aren't any tests run (like in the days before/after a major release). The model samples and their intervals, however, represent the performance level for every single day in the range (the spline can automatically interpolate for days when there aren't any tests run). \n\nThe plot on the left shows comparisons of different days in the dataset. The reference date was 'today,' and the comparison dates were 7 days ago, 14 days ago, and the last day of the 2 previous versions (version 74 and 75). Using the MCMC samples, it's possible with the query to subtract today's samples from the samples for 7 days ago, and get a probability distribution over the difference in performance between the days. A threshold can then be chosen for alerting (say, when there's an 80% chance that we're worse today than we were last week).\n\n\n## The intermediate tables\n\nIt currently uses 3 tables, whose default locations are in the `tables` dict in [bq_utils.py](https://github.com/wcbeard/slow_regression_detection/blob/master/slow_regressions/utils/bq_utils.py).\n\nThe following are example usages of them\n\n- [samples](https://sql.telemetry.mozilla.org/queries/72740/source) ([Local](doc/sample.sql))\n- [input data](https://sql.telemetry.mozilla.org/queries/72659/source) ([Local](doc/input.sql))\n- [compare today vs prevdays](https://sql.telemetry.mozilla.org/queries/73466/source) ([Local](doc/compare_day_performance.sql))\n- [Diagnostic](doc/diagnostic.sql) (which days are filled in/missing)\n\n\n# Technical notes\n\nThe model is a BRMS spline model, specified to have 2 knots per release cycle. It could probably do with fewer, but currently the `EARLY_BETA_OR_EARLIER` flag changes the test scores half way through the cycle for some tests (see [Bugzilla](https://bugzilla.mozilla.org/show_bug.cgi?id=1611809)), so 2 flags per release allows enough flexibility.\n\nThe noise model with performance data is really unpredictable \n\n- [This repo](https://github.com/mozilla/measure-noise) is dedicated to classifying some of the noise\n- [This page](https://metrics.mozilla.com/protected/wbeard/slow_regr/distros.html) shows examples of what to expect\n\nA spline model with Gaussian noise _usually_ describes the movements in performance data well. Problems largely arise with multimodality and outliers.\n\n## Multimodality\n\nMultimodality can lead to wider than usual credible intervals, but when there are multiple modes, identifying regressions isn't well defined IMO. The best I can do is flag how many modes there are with the [find_modes](https://github.com/wcbeard/slow_regression_detection/blob/master/slow_regressions/stats/modal.py#L38) function.\n\n## Outliers\n\nOutliers are trickier because they vary so much depending on the test. I manually filter these using a zscore that's based on number of median absolute deviations away from the running median a point falls. To the naked eye, points that fall ~4 MAD's away appear to be outliers, but some distributions have outliers that are ~100 MADs away. What's more difficult is that some distributions have an outlier rate of ~1%, but others can have up to 5%. In order to make the model general purpose enough to accommodate the potentially hundreds of different tests, the rule of thumb I went with was to [classify](https://github.com/wcbeard/slow_regression_detection/blob/f41876327bc117351ad32b007eb40352d518a6aa/slow_regressions/etl.py#L69) points more than 5 MAD's away as outliers. \n\nFiltering out most of the outliers this way leads to pretty nice spline fits with BRMS and Stan. The remaining problem, though, is that there are some residual outliers that can still artificially widen the credible intervals, making comparisons of performance at different points in time less meaningful. In order to correct this issue, there's currently a step that recalibrates the credible intervals so that [90%] of the data points fall within the 90% CI's.\n\nThis works because the model assumes homoscedastic noise.\n\n\n### Potential future improvements \n\nOne slick way to deal with this would be to use a Student-t noise distribution. As long as the rate of outliers is relatively small (maybe ~1% of the overall observations in a local range?) the T distribution isn't swayed by them. Here's an example comparing how a Gaussian noise model responds to outliers vs. the T distribution.\n\n\n\n![T-dist vs Gaussian](https://github.com/wcbeard/slow_regression_detection/blob/master/doc/gaussian%20vs%20tdist.png)\n\n\n\nI went with Gaussian + post-processing because for some test suites, Stan had a harder time fitting the T distribution. In hindsight, I'm wondering if it would work easier by getting a MLE of the `nu` (and/or the scale) parameter for a given dataset, and passing a constrained version of this as a [prior](https://rdrr.io/cran/brms/man/set_prior.html) to BRMS. It should be straightforward to get the t-distribution parameters by looking at the time series residuals `res` from, say, the rolling median, and plugging them into\n\n```python\nfrom stats.outliers import fwd_backwd_resid\nfrom scipy import stats as sts\n\nres = fwd_backwd_resid(y, ret_z=False, ret_resid=True).res_min\nnu, loc, scale = sts.t.fit(res)\n```\n\n\n- if a model is fit on Monday, and then again on Tuesday, \n\n\n\n# How to run the model\n\nTo run locally:\n1. Ensure Docker has at least 8GB memory. This could be overkill, but the Stan models are voracious\n2. Ensure BigQuery credentials exist. Run the following from the CLI:\n   - `gcloud auth application-default login`\n\nNext, run the following CLI commands to clone the repo, set a few paths, and build and run Docker.\n```bash\ngit clone https://github.com/wcbeard/slow_regression_detection.git slow_reg\nexport SR_LOCATION=\"$(pwd)/slow_reg\"\nexport GCLOUD_CREDS=\"$HOME/.config/gcloud\"\n\ncd $SR_LOCATION\ndocker build --tag ds_546_prod .\ndocker run -v=$GCLOUD_CREDS:/root/.config/gcloud -v $SR_LOCATION:/sreg --interactive --tty ds_546_prod\n\n```\n\n## Sundry notes\n\n### Shell aliases\n\n```sh\nalias db=\"docker build -t ds_546_prod .\"\nalias dr=\"docker run -v=$HOME/.config/gcloud:/root/.config/gcloud -v ~/repos/sreg:/sreg -it ds_546_prod /bin/bash\"\n\n\nfunction da () {\n    export CONTAINER=`docker container ls | pcregrep -o \"^[a-z0-9]+\"`\n    docker exec -it $CONTAINER /bin/bash\n}\n```\n\n### Status\n\n`python -m slow_regressions.utils.diagnose_etl dates`\n\n\n### Ipython\n\n```\n%run slow_regressions/ipyimp.py\n```\n\n### Workflow\n\n#### etl.sh layout\n\n- `slow_regressions.load_raw_test_data etl`\n  - Downloads summary data from\n        `moz-fx-data-derived-datasets.taskclusteretl.perfherder` to\n        temp table\n- `python -m slow_regressions.etl load_brms --brms_dir='/sreg/data/' --date=\"$yesterday\"`\n    - pulls summarized data, splits it out by suite, does some preprocessing,\n    saves it to local directory that R can read from\n- `time Rscript slow_regressions/model/smod.r \"/sreg/data/$yesterday/\"`\n    - Run the R model on local data, and save MCMC samples locally\n- `python -m slow_regressions.etl upload_model_data \\\n      --subdate=\"$yesterday\" --model_data_dir='/sreg/data/'`\n    - read MCMC samples and the preprocessed data that was fed into the model,\n        upload it to BQ\n\n\n### Python\n\n```python\nimport slow_regressions.utils.model_eval as me\nimport slow_regressions.etl as sr\n# import slow_regressions.utils.suite_utils as su\n# import slow_regressions.utils.bq_utils as bq\n\nbv = sr.load_beta_versions()\nmm = me.ModelManager(\"/sreg/data/\", \"2020-07-06\", bv)\n\n# Upload input\ndf_inp = sr.transform_model_input_data(mm.suite_plats)\nsr.upload_model_input_data(df_inp, replace=False, skip_existing=True)\n\n# Upload samples\ndraws_ul = sr.transform_model_posterior(mm.suite_plats)\nsr.upload_model_draws(\n    draws_ul, replace=False, skip_existing=True,\n)\n```\n\n# Trouble with paths\n\nMay be helpful to add this to the top of the file\n\n```python\nfrom pathlib import Path\nimport sys\nproject_dir = Path(__file__).parent.parent.absolute()\nsys.path.insert(0, str(project_dir))\nprint(sys.path)\n```\n\n\n\nOr, call from command line with `python -m module.mod2` (but _no_ `.py`!) \n"
},
{
  "name": "tls-interop",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "README.md",
      "cases.json",
      "cipher_map.json",
      "fail.json",
      "run.sh",
      "src",
      "travis_script.sh"
    ]
  },
  "makefile": null,
  "readme": "\n[![Build Status](https://travis-ci.org/mozilla/tls-interop.svg?branch=master)](https://travis-ci.org/mozilla/tls-interop)\n\nPrimitive TLS interop Harness\n=============================\n\nThis program tests interop between two TLS stacks. In order to use it,\neach stack needs to be wrapped in a BoringSSL runner-compatible\n[shim](https://boringssl.googlesource.com/boringssl/+/master/ssl/test/PORTING.md).\nThe runner then runs the shims against each other in a variety (currently small)\nof configurations).\n\n\nBasic Execution Instructions\n============================\nThe harness is run as:\n\n```\ntls_interop --client [shim-client] --server [shim-server] --rootdir=[path-to-key-files] --test-cases [test-case-descriptions]\n```\nFor instance:\n\n```\ntls_interop --client ${NSS_ROOT}/dist/Darwin15.6.0_cc_64_DBG.OBJ/bin/nss_bogo_shim --server ${NSS_ROOT}/dist/Darwin15.6.0_cc_64_DBG.OBJ/bin/nss_bogo_shim --rootdir=${BORINGSSL_ROOT}/ssl/test/runner/ --test-cases cases.json\n```\n\nTo swap client and server, you need to run it twice.\n\nThe run.sh script makes it easier to run a certain configuration of shims and \ntest cases, provided the shims for nss, boringssl and openssl executables can\nbe found in the default locations as specified under Cargo Test Instructions.\n\nExample:\n\n```\n./run.sh -m loopback -c cases.json -v\n```\n\n-v is for verbose output.\n-m is for mode, the configuration of shims used as client and server.\n-c is for case file, the file with test cases. \n\nAll available modes are: all, loopback, boring_client, boring_server, ossl_client, ossl_server.\n\n\nCargo Test Instructions\n============================\nSome of the internal rust test cases run with \"cargo test\" assume readily built\nversions of nss, boringssl and openssl being available in the parent directory.\nThe NSS shim is expected to be found at \"../dist/Debug/bin/nss_bogo_shim\".  \nThe BoringSSL shim is expected to be found at \"../boringssl/build/ssl/test/bssl_shim\".  \nThe OpenSSL shim is expected to be found at \"../openssl/tests/ossl_shim/ossl_shim\".\n\nNOTE: OpenSSL needs to be built with the \"enable-external-tests\" flag. Otherwise\nthe ossl_shim is not built.\n\nAll three default paths can be overwritten by setting the following environment variables:  \nNSS_SHIM_PATH = ${NSS_ROOT}/bin/nss_bogo_shim  \nBORING_ROOT_DIR = ${BORINGSSL_ROOT}  \nOSSL_SHIM_PATH = ${OPENSSL_ROOT}/tests/ossl_shim/ossl_shim  \n\n```\ncargo test\n```\nRuns a set of very basic connection tests between nss and the other two \nshims and additionally all test cases specified in the cases.json file, in each \navailable combination of shims.\n"
},
{
  "name": "responsible-design",
  "files": {
    "/": [
      ".editorconfig",
      ".gitattributes",
      ".gitignore",
      ".htaccess",
      "404.html",
      "LICENSE.txt",
      "README.md",
      "about.html",
      "browserconfig.xml",
      "checklist.html",
      "community-stories.html",
      "css",
      "favicon.ico",
      "human-element.html",
      "humans.txt",
      "icon.png",
      "img",
      "index.html",
      "js",
      "package-lock.json",
      "package.json",
      "pdf",
      "robots.txt",
      "sass",
      "site.webmanifest",
      "tile.png"
    ]
  },
  "makefile": null,
  "readme": "# delight\nstatic site for Responsible Design for Digital Communities\n\nThis is a static site with all dependencies included in the repo. To run locally:\n\n1. git clone\n2. cd to cloned dir\n3. run `python -m SimpleHTTPServer` (this might not even be needed in most cases)\n"
},
{
  "name": "django-dnt",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "dnt",
      "requirements.dev.txt",
      "setup.cfg",
      "setup.py",
      "testapp",
      "tests",
      "tox.ini"
    ]
  },
  "makefile": "export DJANGO_SETTINGS_MODULE = testapp.settings\nexport PYTHONPATH := $(shell pwd)\n.PHONY: help clean coverage coveragehtml develop lint qa qa-all release sdist test test-all\n\nhelp:\n\t@echo \"clean - remove all artifacts\"\n\t@echo \"coverage - check code coverage\"\n\t@echo \"coveragehtml - display code coverage in browser\"\n\t@echo \"develop - install development requirements\"\n\t@echo \"lint - check style with flake8\"\n\t@echo \"qa - run linters and test coverage\"\n\t@echo \"qa-all - run QA plus tox and packaging\"\n\t@echo \"release - package and upload a release\"\n\t@echo \"sdist - package\"\n\t@echo \"test - run tests\"\n\t@echo \"test-all - run tests on every Python version with tox\"\n\t@echo \"test-release - upload a release to the test PyPI server\"\n\nclean:\n\tgit clean -Xfd\n\ndevelop:\n\tpip install -r requirements.dev.txt\n\nlint:\n\tflake8 .\n\ntest:\n\tdjango-admin test\n\ntest-all:\n\ttox --skip_missing_interpreters\n\ncoverage: clean\n\tcoverage erase\n\tcoverage run --branch --source=dnt `which django-admin` test\n\ncoveragehtml: coverage\n\tcoverage html\n\tpython -m webbrowser file://$(CURDIR)/htmlcov/index.html\n\nqa: lint coveragehtml\n\nqa-all: qa sdist test-all\n\nsdist:\n\tpython setup.py sdist bdist_wheel\n\tls -l dist\n\tcheck-manifest\n\tpyroma dist/`ls -t dist | grep tar.gz | head -n1`\n\nrelease: clean sdist\n\ttwine upload dist/*\n\tpython -m webbrowser -n https://pypi.python.org/pypi/django-dnt\n\n# Add [test] section to ~/.pypirc, https://testpypi.python.org/pypi\ntest-release: clean sdist\n\ttwine upload --repository test dist/*\n\tpython -m webbrowser -n https://testpypi.python.org/pypi/django-dnt\n",
  "readme": "==========\nDjango-DNT\n==========\n\n.. image:: http://img.shields.io/travis/mozilla/django-dnt/master.svg\n    :alt: The status of Travis continuous integration tests\n    :target: https://travis-ci.org/mozilla/django-dnt\n\n.. image:: https://img.shields.io/codecov/c/github/mozilla/django-dnt.svg\n    :target: https://codecov.io/gh/mozilla/django-dnt\n    :alt: The code coverage\n\n.. image:: https://img.shields.io/pypi/v/django-dnt.svg\n    :alt: The PyPI package\n    :target: https://pypi.python.org/pypi/django-dnt\n\n.. Omit badges from docs\n\n``django-dnt`` offers an easy way to pay attention to the ``DNT``\n(`Do Not Track <https://en.wikipedia.org/wiki/Do_Not_Track>`_) HTTP header. If\nusers are sending ``DNT: 1``, ``DoNotTrackMiddleware`` will set ``request.DNT =\nTrue``, else it will set ``request.DNT = False``.\n\nJust add ``dnt.middleware.DoNotTrackMiddleware`` to your ``MIDDLEWARE_CLASSES``\n(Django 1.9 and earlier) or ``MIDDLEWARE`` (Django 1.10 and later) and you're\ngood to go.\n"
},
{
  "name": "MoDataSubmission",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "modatasubmission",
      "pyLibrary",
      "requirements.txt",
      "resources",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "MoDataSubmission\n================\n\nA service that accepts HTTP POST requests of JSON, stores them in S3, and \nreturns a link to its location.  Requests are authenticated using \n[Hawk](https://github.com/hueniverse/hawk).\n\n\nRequirements\n------------\n\n* Python27, including Pip\n* Server with write access to Amazon S3\n\n \nInstallation\n------------\n\nThere is an installation script in [resources/scripts/install.sh](https://github.com/klahnakoski/MoDataSubmission/blob/dev/resources/scripts/install.sh) \nwhich can be used as a guide for installing the server.\n\nSetup\n-----\n\nThe server requires a configuration file.  An example configuration file is at \n[tests/resources/config/server.json](https://github.com/klahnakoski/MoDataSubmission/blob/dev/tests/resources/config/server.json)\n\n```javascript\n\t{\n\t\t\"flask\": {\n\t\t\t\"host\": \"0.0.0.0\",\n\t\t\t\"port\": 5000,\n\t\t\t\"debug\": true,\n\t\t\t\"threaded\": true,\n\t\t\t\"processes\": 1\n\t\t},\n\t\t\"aws\": {\n\t        \"aws_access_key_id\": \"sometring\",\n\t        \"aws_secret_access_key\" : \"somebase64\",\n\t        \"region\": \"us-west-2\"\n\t    },\n\t\t\"users\": [\n\t\t\t{\n\t\t\t\t\"hawk\": {\n\t\t\t\t\t\"id\": \"kyle@example.com\",\n\t\t\t\t\t\"key\": \"secret\",\n\t\t\t\t\t\"algorithm\": \"sha256\"\n\t\t\t\t},\n\t\t\t\t\"resources\": [\n\t\t\t\t\t\"testing\"\n\t\t\t\t]\n\t\t\t}\n\t\t],\n\t\t\"debug\": {\n\t\t\t\"trace\": true,\n\t\t\t\"log\": [\n\t\t\t\t{\n\t\t\t\t\t\"log_type\": \"console\"\n\t\t\t\t}\n\t\t\t]\n\t\t}\n\t}\n```\n\n###Setup Properties\n\n* *`flask`* - properties delivered to the `Flask.run()` method.  Be sure you have `debug=false` in production!\n* *`aws`* - your AWS permissions.  These should NOT be in this configuration file, but rather in a separate key file.  See the `server.json`file for an example  \n* *`users`* - An array of users and permissions.  The hawk credentials are used to identify the POST requests.  And `resources` is a list of bucket names that user is allowed to POST to.\n* *`debug`* - [Logging configuration](https://github.com/klahnakoski/pyLibrary/tree/dev/pyLibrary/debugs#configuration), which lacks good documentation.\n\nRunning\n-------\n\nThe [run script](https://github.com/klahnakoski/MoDataSubmission/blob/dev/resources/scripts/run.sh) details the steps to run the server.   Do not use the `sudo -i` step if you are not listening on port 80.\n\nTesting\n-------\n\nThere is a simple client in the [test code](https://github.com/klahnakoski/MoDataSubmission/blob/dev/tests/test_production.py#L27) to save some JSON in S3.   The only important line is the one that sends the data; here is that same line, expanded with parameters matching the server configuration above:\n\n```python\n\tlink, id = Client(\n\t\turl = \"http://example.com:80/testing\",\n\t\thawk_credentials = {\n\t\t\t\"id\": \"kyle@example.com\",\n\t\t\t\"key\": \"secret\",\n\t\t\t\"algorithm\": \"sha256\"\n\t\t}\n\t).send(data)\n```\n"
},
{
  "name": "sotp",
  "files": {
    "/": [
      ".circleci",
      ".sops.yaml",
      "README.md",
      "config.yaml",
      "docs",
      "go.mod",
      "go.sum",
      "main.go",
      "main_test.go",
      "sops_functional_tests_key.asc"
    ],
    "/docs": [
      "aws_vmfa.png"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# SOTP: Sops OTP\n\nSmall utility to store AWS TOTP secrets into Sops encrypted files and generate OTP on the command line.\n\n[![CircleCI](https://circleci.com/gh/mozilla/sotp.svg?style=svg)](https://circleci.com/gh/mozilla/sotp)\n\nusage:\n\n```\n$ sotp test1\ncurrent one-time password is: 693190\n```\n\nSotp reads it's configuration from `config.yaml` in the local directory.\nThe config must be a valid Sops encrypted YAML file with the syntax:\n\n```yaml\naccounts:\n    - name: test1\n      totpsecret: YAGQP5IP77OO3HMPS3D2KPMSNLNDIB7EO22EGAN3JEGE3DAR37Z2U5YDGKGN44VA\n    - name: test2\n      totpsecret: xyzabcd....\n```\n\n* `name` is just a reference name that you'll use when invoking Sotp\n* `totpsecret` is a the secret seed you get from the `Adding a virtual MFA` screen in the AWS IAM console of a given user\n\n![aws_mfa_screen](./docs/aws_vmfa.png)"
},
{
  "name": "fission-test-tracker",
  "files": {
    "/": [
      "README.md",
      "authentication.py",
      "groups.py",
      "merge.py",
      "requirements.txt",
      "spreadsheet.py",
      "xorigin-and-fission.ini"
    ]
  },
  "makefile": null,
  "readme": "fission test tracker\n============\nAutomation scripts for updating Fission mochitest reports on Google Spreadsheets.  \n\n## General info\n\nThese scripts pull down the latest fission and xorigin mochitest results against mozilla-central via the Treeherder API and merge any changes with an existing report in Google Spreadsheets. They also preserve any comments/additional annotations in the spreadsheet between updates. \n\nFor more info, see [Project Fission/Enabling Tests with Fission](https://wiki.mozilla.org/Project_Fission/Enabling_Tests_with_Fission) on the Mozilla Wiki.\n\n## Setup\n\nYou'll need to create a Google API account and project, enable the Google Sheets API on that project and generate a \"credentials.json\" file via the Google Cloud console. Run through Step 1 of the [Sheets APIv4 Python Quickstart](https://developers.google.com/sheets/api/quickstart/python) to generate the file and save it in the root of this directory. \n\nThen initialize a python3 virtual environment:\n```sh\npython3 -m venv [VENV_PATH]\nsource [VENV_PATH]/bin/activate\npip install -r requirements.txt \n```\n\n## Usage\n\n\n`spreadsheet.py -c [path-to-config]`\n\n\nThis repo includes a config file that points to the existing Fission+XOrigin mochitest spreadsheet in xorigin-and-fission.ini:\n\n\n`spreadsheet.py --config xorigin-and-fission.ini`\n\n\nA daily cronjob could look something like this:\n\n```sh\n0 10 * * * [VENV_PATH]/bin/python [REPO_PATH]/spreadsheet.py -c xorigin-and-fission.ini >> /var/log/fission.spreadsheet.log 2>&1\n```\n\n## Development\n\n#### Adding a new column to the spreadsheet\n\n* Add the column name in the header row @ spreadsheet.py:MainHeader.fields\n* Add the column value @ spreadsheet.py:TestRows.__init__\n\nBoth should be added at the exact index you want them to appear in your spreadsheet. \n\n#### Filtering based on a different skip-if/fail-if annotation\n\nThis script grabs test data based on test-info reports generated in TaskCluster CI runs, specifically [test-info-fission and test-info-xorigin](https://searchfox.org/mozilla-central/source/taskcluster/ci/source-test/file-metadata.yml#37,63).\n\nIt then filters and merges the tests from both reports based on regex matches (e.g. at the time of writing, we're only interested in 'xorigin && !fission' matches from the xorigin report). To change the filtering, see merge.py:get_full_report().\n\n## Improvements\n\n* No need to query both reports for the Fission team's current needs - can just query the xorigin report and filter for tests that are either \"xorigin && fission\" or \"xorigin && !fission\"\n* This script outputs any changes in tests status (newly failing, newly passing, existing passing). These results could be outputted to #fission slack.\n\n## Acknowledgements \n\nThanks to Kris Maglione ([@kmaglione](https://github.com/kmaglione)) for the initial auth scripts and for answering my many questions every week! \n"
},
{
  "name": "pibal",
  "files": {
    "/": [
      ".editorconfig",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".prettierignore",
      ".prettierrc.js",
      "LICENSE",
      "README.md",
      "cache.js",
      "docs",
      "package-lock.json",
      "package.json",
      "public",
      "rollup.config.js",
      "src"
    ],
    "/docs": [
      "development.md"
    ]
  },
  "makefile": null,
  "readme": "# Pibal\n"
},
{
  "name": "stt-android-tflite-tutorial",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "app",
      "build.gradle",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "settings.gradle"
    ]
  },
  "makefile": null,
  "readme": "# Android STT Tutorial\n\nAndroid application that streams audio from the microphone and transcribes it using STT.\n\n## Prerequisites\n\n#### Download model\n\nDownload the pre-trained English model and extract it:\n```\ncurl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.tflite\ncurl -LO https://github.com/mozilla/DeepSpeech/releases/download/v0.7.4/deepspeech-0.7.4-models.scorer\n```\n\nMove the model files `deepspeech-0.7.4-models.tflite` and `deepspeech-0.7.4-models.scorer`, to the demo application's data directory on your android device.\nMind that the data directory will only be present after installing and launching the app once.\n\n```\nadb push deepspeech-0.7.4-models.tflite deepspeech-0.7.4-models.scorer /storage/emulated/0/Android/data/org.deepspeechdemo/files/\n```\n\nYou can also copy the files from your file browser to the device.\n\n#### Android device with USB Debugging\n\nConnect an android device and make sure to enable USB-Debugging in the developer settings of the device.\nIf haven't already, you can activate your developer settings by following [this guide from android](https://developer.android.com/studio/debug/dev-options#enable).\n\n## Installation\n\nTo install the example app on your connected android device you can either use the command line or Android Studio.\n\n### Command Line\n\n```\ncd android-tflite-tutorial\n./gradlew installDebug\n``` \n\n### Android Studio\n\nOpen the `android-tflite-tutorial` directory in Android Studio.\nRun the app and your connected android device.\n\n## Usage\n\nStart recording by pressing the button and the app will transcribe the spoken text.\n"
},
{
  "name": "GitHub-action-testing",
  "files": {
    "/": [
      ".github",
      "LICENSE",
      "README.md",
      "action-a"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "## Nothing to see here\n\nWe're using this repo to test how GitHub Actions might affect the security of Mozilla products developed on GitHub.\n\nAny findings will be published in the normal places, and we'll probably delete this repo.\n"
},
{
  "name": "DemoStumbler",
  "files": {
    "/": [
      ".gitignore",
      ".idea",
      ".travis.yml",
      "DemoStumbler.iml",
      "Makefile",
      "README.md",
      "app",
      "build.gradle",
      "docs",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "settings.gradle"
    ],
    "/docs": [
      "README.md",
      "Run_and_Menubar.png"
    ]
  },
  "makefile": "all:\n\t./gradlew build\n\ninstall: all\n\t./gradlew installDebug\n\nclean:\n\t./gradlew build\n",
  "readme": "[![Build\nStatus](https://travis-ci.org/mozilla/DemoStumbler.svg?branch=master)](https://travis-ci.org/mozilla/DemoStumbler)\n\n# DemoStumbler\n\n## Embed the Mozilla Stumbler as a library in your Android Application\n\nThe [Mozilla Stumbler](https://github.com/mozilla/MozStumbler/) is an Android wifi and cell tower stumbling application which \nis designed to collect location data and submit it to our location service [Ichnaea](http://github.com/mozilla/ichnaea/).\n\nThe standard Android application runs in what we call 'active' mode where we aggressively scan for Wi-fi routers, cell tower data \nand GPS supplied latitude and longitude. \n\nWe have a second mode though - for passive stumbling.  We use a version of this code in Firefox for Android to do extremely low power\nstumbling.\n\nNow you can use it too!\n\nFull documentation can be found on the primary Mozilla Stumbler [wiki](https://github.com/mozilla/MozStumbler/wiki/Using-libstumbler)\n\n## But I'm impatient. Just show me the code.\n\nYou'll need to import some classes:\n```java\nimport org.mozilla.mozstumbler.service.core.http.IHttpUtil;\nimport org.mozilla.mozstumbler.service.mainthread.PassiveServiceReceiver;\nimport org.mozilla.mozstumbler.svclocator.ServiceConfig;\nimport org.mozilla.mozstumbler.svclocator.ServiceLocator;\nimport org.mozilla.mozstumbler.svclocator.services.ISystemClock;\nimport org.mozilla.mozstumbler.svclocator.services.log.ILogger;\n```\n\nNext, you'll need to configure the ServiceLocator and then start the service with an Intent. \n\n```java\n// Setup ServiceConfig and ServiceLocator\nServiceConfig svcConfig = new ServiceConfig();\nsvcConfig.put(IHttpUtil.class,\n        ServiceConfig.load(\"org.mozilla.mozstumbler.service.core.http.HttpUtil\"));\nsvcConfig.put(ISystemClock.class,\n        ServiceConfig.load(\"org.mozilla.mozstumbler.svclocator.services.SystemClock\"));\nsvcConfig.put(ILocationService.class,\n        ServiceConfig.load(\"org.mozilla.mozstumbler.service.core.http.MLS\"));\nsvcConfig.put(ILogger.class,\n        ServiceConfig.load(\"org.mozilla.mozstumbler.svclocator.services.log.ProductionLogger\"));\n\nServiceLocator.newRoot(svcConfig);\n\nIntent i = PassiveServiceReceiver.createStartIntent(this, \"a_moz_api_key\",\n        \"Just Another User-Agent\");\nstartService(i);\n```\n\n\n## Build and install the code\n\n### Enable Developer Options if you haven't done so already\n\n1. Open Settings> About> Software Information> More.\n2. Then tap \u201cBuild number\u201d seven times to enable Developer options. ...\n3. Go back to Settings menu and now you'll be able to see \u201cDeveloper options\u201d there.\n4. Tap it and turn on USB Debugging from the menu on the next screen.\n    \n### Connect your Android device over USB, build and install.\n\nFrom command line on UNIX: \n```bash\n./gradlew installDebug\n```\n\nYou can alternately just open Android Studio and use the IDE to build and install the application.\n"
},
{
  "name": "git-repo",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      ".project",
      ".pydevproject",
      ".pylintrc",
      "COPYING",
      "README.md",
      "SUBMITTING_PATCHES",
      "color.py",
      "command.py",
      "docs",
      "editor.py",
      "error.py",
      "git_command.py",
      "git_config.py",
      "git_refs.py",
      "git_ssh",
      "hooks",
      "main.py",
      "manifest_xml.py",
      "pager.py",
      "progress.py",
      "project.py",
      "pyversion.py",
      "repo",
      "subcmds",
      "tests",
      "trace.py",
      "wrapper.py"
    ],
    "/docs": [
      "manifest-format.txt"
    ]
  },
  "makefile": null,
  "readme": "\nThe modification herein allows use of git ssh\n\n= Manifest\n\nPull repo from our git-repo repo (isn't that nice) as shown below.\n\"--no-repo-verify\" is necessary to pull the master branch that has the patch\nfor git ssh.\n\n```\nmkdir projdir && cd projdir\ncurl https://raw.githubusercontent.com/mozilla/git-repo/master/repo > ./repo\nchmod u+x ./repo\n./repo init --no-repo-verify -u git@github.com:githubuser/manifests.git\n./repo sync\n```\n\n= Manifest\n\n```\n<?xml version=\"1.0\" ?>\n<manifest>\n  <remote fetch=\"git@github.com:mozilla/\" name=\"mozilla\"/>\n\n  <project name=\"privaterepo\" path=\"localpath/privaterepo\" remote=\"mozilla\" revision=\"master\"/>\n</manifest>\n```"
},
{
  "name": "klaatu",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".pyup.yml",
      "Dockerfile",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "fixtures",
      "mypy.ini",
      "pipenv.txt",
      "setup.cfg",
      "tests",
      "tox.ini",
      "utilities"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Experiments validator - Klaatu\n\nA tool used to validate firefox experiments\n\n## Using the docker hub image\n\nTo use the docker hub image, you must mount your local dir as a volume in the container. I suggest mounting the volume like `-v {LOCAL-DIR}:/code/test_files`.\n\nHere is an example: ```docker run --rm --name \"klaatu\" -v $PWD/{PATH-TO-XPI-FOLDER}:/code/test_files mozilla/klaatu:latest tox -e exp-tests -- --experiment=test_files/{NAME-OF-FILE}.xpi --html=report.html```\n\n## Included tests\n- ```test_experiment_does_not_stop_startup```: Experiment does not stop browser startup, or prohibit a clean exit.\n- ```test_private_browsing_disables_experiment```: Experiment should be disabled in private browsing mode.\n- ```test_experiment_does_not_drastically_slow_firefox```: Experiment should not slow firefox down by more then 20%.\n- ```test_experiment_shows_on_support_page```: Experiment should show on about:support page.\n- ```test_experiment_shows_on_studies_page```: Experiment should show on about:studies page.\n- ```test_experiment_expires_correctly```: Experiment should not be included in the sent pings when it is disabled/expired.\n- ```test_experiment_remains_disabled_after_user_disables_it```: Disable experiment, restart Firefox to make sure it stays disabled.\n- ```test_experiment_sends_correct_telemetry```: Make sure telemetry is sent and recieved properly.\n- ```test_experiment_does_not_stop_update```: Experiment should not block firefox updates.\n\n\n## Prerequisites\n\nYou should have docker and git installed.\n\n## How to use\n\n1. Clone repository\n2. Fill out this JSON file, name it `variables.json` and place it in the root directory:\n\n```\n    {\n        \"recipeId\":\n        \"slug\":\n        \"userFacingName\":\n        \"userFacingDescription\":\n        \"branch\":\n        \"active\": true,\n        \"addonId\":\n        \"addonUrl\": \"https://example.com/{YOUR ADDON ID HERE}@mozilla.org-signed.xpi\",\n        \"addonVersion\":\n        \"extensionApiId\":\n        \"extensionHash\": \"badhash\",\n        \"hashAlgorithm\": \"sha256\",\n        \"studyEndDate\": null\n    }\n```\nAdd the path using the ```--variables``` option. ```--variables={PATH/TO/variables.json}```\n\n3. Build docker image with command ```docker build -t klaatu .``` in the projects root directory.\n4. Run tests with docker, example: ```docker run --rm --name=\"klaatu\" klaatu tox -e exp-tests -- --experiment=fixtures/normandy-nextgen-study-example-a@mozilla.org-0.3-signed.xpi --variables={PATH/TO/variables.json} --private-browsing-enabled```\n\n## CLI Options\n\n- ```--run-release-firefox```: Runs the suite with an unbranded release Firefox version.\n- ```--private-browsing-enabled```: If your experiment runs within private browsing windows please include this option.\n- ```--run-update-test```: Includes the update test in the test run using Firefox Nightly.\n- ```--experiment```: Path to the experiment you want to test."
},
{
  "name": "PRESC-Outreachy-archive",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "datasets",
      "dev",
      "environment.yml"
    ]
  },
  "makefile": null,
  "readme": "# PRESC Outreachy archive\n\nThis repo serves as the archive for the submissions received from Outreachy\ncandidates during the Spring 2020 application process for the\n[PRESC](https://github.com/mozilla/PRESC) project.\nThanks to everyone who participated!\n\nCode will be integrated as needed into the main PRESC repo, and the full\noriginal submissions will be preserved here. Commit history is available from\nthe closed PRs in the main repo.\n"
},
{
  "name": "npm-lockdown",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "ChangeLog",
      "LICENSE",
      "README.md",
      "example",
      "lockdown.js",
      "npm-lockdown.png",
      "package.json",
      "relock.js"
    ]
  },
  "makefile": null,
  "readme": "# npm-lockdown\n\nPut your dependencies on lockdown.\n\n![lockdown](https://github.com/mozilla/npm-lockdown/raw/master/npm-lockdown.png)\n\n## What's this?\n\nNPM Lockdown is a tool that locks your node.js app to\nspecific versions of dependencies... So that you can:\n\n  1. know that the code you develop against is what you test and deploy\n  2. `npm install` and get the same code, every time.\n  3. not have to copy all of your dependencies into your project\n  4. not have to stand up a private npm repository to solve this problem.\n\n## Who is this for?\n\nNode.JS application developers, but not library authors.  Stuff published\nin npm as libraries probably wouldn't be interested.\n\n## Why Care?\n\nEven if you express verbatim versions in your package.json file, you're still\nvulnerable to your code breaking at any time.  This can happen if a dependency\nof a project you depend on with a specific version *itself* depends on another\npackages with a version range.\n\nHow can other people accidentally or intentionally break your node.js app?\nWell, they might...\n\n  * ... push a new version that no longer supports your preferred version of node.js.\n  * ... fix a subtle bug that you actually depend on.\n  * ... accidentally introduce a subtle bug.\n  * ... be having a bad day.\n\n## Usage!\n\n\n\n    npm install --save foo@0.8.1\n    ./node_modules/.bin/lockdown-relock\n\n`npm-lockdown` is easy to get started with.  It generates a single file that lists\nthe versions and check-sums of the software you depend on, so any time something\nchanges out from under you, `npm install` will fail and tell you what package has\nchanged.\n\n### One Time Project Setup\n\n  1. npm install the version of lockdown you want: `npm install --save lockdown`\n  2. add a line to your package.json file: `\"scripts\": { \"preinstall\": \"lockdown\" }`\n  3. generate a lockdown.json: `node_modules/.bin/lockdown-relock`\n  4. commit: `git add package.json lockdown.json && git commit -m \"be safe\"`\n\n### Adding new modules\n\n  1. npm install the specific dependencies of your app `npm install --save foo@0.8.1`\n  4. re-generate your lockdown.json: `node_modules/.bin/lockdown-relock`\n  5. commit: `git add package.json lockdown.json && git commit -m \"be safe\"`\n\n### Changing dependencies once locked down\n\nYou update your dependencies explicitly, relock, and commit:\n\n    npm install --save foo@1.2.3\n    node_modules/.bin/lockdown-relock\n    git add lockdown.json package.json\n    git commit -m \"move to foo v1.2.3\"\n\ndone!\n\n### Using an npm mirror\n\nYou can fetch resources from an npm mirror by specifying the NPM_CONFIG_REGISTRY\nenvironment variable when invoking `npm install`. If NPM_CONFIG_REGISTRY is not\nspecified, http://registry.npmjs.org will be used.\n\n    NPM_CONFIG_REGISTRY=http://registry.npmjs.eu/ npm install\n\n## Notes:\n\n  * You should use the latest stable version of lockdown, find it from the [npm registry](https://npmjs.org/package/lockdown)\n\n## Installing dependencies once locked down\n\n    npm install\n\n## Related Tools\n\n**[yarn][]** - Fast, reliable, and secure dependency management.\n\n**Fast:** Yarn caches every package it downloads so it never needs to download the same package again. It also parallelizes operations to maximize resource utilization so install times are faster than ever.\n\n**Reliable:** Using a detailed, concise lockfile format and a deterministic algorithm for installs, Yarn is able to guarantee that an install that worked on one system will work exactly the same way on any other system.\n\n**Secure:** Yarn uses checksums to verify the integrity of every installed package before its code is executed.\n\n  [yarn]: https://yarnpkg.com/\n\n**[npm shrinkwrap][]** - NPM itself has a feature called \"shrinkwrap\" that\n\n> locks down the versions of a package's dependencies so that you can control exactly which\n> versions of each dependency will be used when your package is installed.\n\nAt present (as of npm v1.1.33), the implementation of shrinkwrap has a couple flaws\nwhich make it unusable for certain applications:\n\n  1. No checksums!  NPM shrinkwrap does not guarantee bit-wise equality of the installed\n     dependencies, so if an upstream server or author decides to change the contents of\n     version 1.2.3 of `foo`, you'll install something different than you intended without\n     knowing.\n  2. Does not play nice with `optionalDependencies` - If you \"shrinkwrap\" your app and you\n     have an installed dep that is optional, the dependency is no longer optional.  This might\n     not be what you want.\n\n  [npm shrinkwrap]: https://docs.npmjs.com/cli/shrinkwrap\n\n*NOTE:* you can combine lockdown with shrinkwrap just fine.  If all you care about is #1 above.\n\nThe path forward is to build checksums into shrinkwrap and kick lockdown to the curb, but until\nthen, lockdown solves some problems.  (@izs is [interested in patches][]).\n\n  [interested in patches]: https://twitter.com/izs/status/234330784931143682\n\n**[npm-seal][]** - Solves the same problem as lockdown in a very different way.  Because seal\nis built to be used in concert with shrinkwrap, it suffers from the `optionalDependencies` issue\ndescribed above.\n\n  [npm-seal]: https://github.com/zaach/npm-seal\n"
},
{
  "name": "mortar-app-stub",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "css",
      "favicon.ico",
      "img",
      "index.html",
      "js",
      "manifest.webapp"
    ]
  },
  "makefile": null,
  "readme": "# A Blank Template for Open Web Apps\n\nThis is a minimal template that has a little HTML, CSS, and JavaScript to help\nyou start writing an [Open Web App](https://developer.mozilla.org/en-US/Apps).\n\nThis is part of the [mortar](https://github.com/mozilla/mortar/)\ntemplate collection for building Open Web Apps.\n\n# Usage\n\n```\ngit clone git://github.com/mozilla/mortar-app-stub.git myapp\n```\n\n# Readme template\n\nTo help others get a good overview of your app, you can use the example below as a template for your actual README.md.\n\n# My Sample App\n\n## What it is\n\n*Enter a short description about what your app does.*\n\n## How to install\n\nFor example:\n\n```bash\ngit clone git://github.com/your-username/your-app.git myapp\n```\n\nOpen `myapp/index.html` into your browser.\n\n*The above example assumes you are using Github for your app's repository. If you are not, provide alternative instructions here.*\n\n## Running Tests\n\n*If you have tests for your app, provide instructions here.*\n\n## Help and support\n\n*Provide contact information or links to where users can submit issues and/or feature requests here.*\n\n## License\n\n*Enter your license information here or provide a link.*\n"
},
{
  "name": "send-static",
  "files": {
    "/": [
      "Inter-Medium.woff",
      "Inter-Medium.woff2",
      "body-bg.svg",
      "favicon.ico",
      "index.html",
      "logo.svg",
      "mozilla.svg"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "mts",
  "files": {
    "/": [
      ".compute",
      ".gitignore",
      ".gitmodules",
      "3rd_party",
      "CMakeLists.txt",
      "LICENSE",
      "README.md",
      "VERSION",
      "cmake",
      "docs",
      "src"
    ],
    "/docs": [
      ".gitignore",
      "Makefile",
      "make.bat",
      "source"
    ]
  },
  "makefile": null,
  "readme": "# Marian Translation Service\n\nThe code in this repository implements a Marian-based translation service. \nCurrently, it offers an REST http server. The lack of https is currently on purpose,\nas in the vast majority of the use cases that I envision, the service will run locally\nand not through a registered domain address, so only self-signed certificates would work.\nIf you need https, go through a proxy and put the Marian REST server behind a firewall.\n\nCurrently, the server can serve only a single configuration (model or ensemble of models).\n\nThe REST API is described [here](https://github.com/ugermann/mts/wiki/BergamotAPI/wiki/BergamotAPI).\n\n## Compilation\n### Local\nNotice that you need all the dependencies and libraries for a full compilation of \n[Marian](http://github.com/marian-nmt/marian-dev).\n\n```\ngit clone https://github.com/ugermann/mts /path/to/your/local/clone/of/mts\nmkdir /path/to/where/you/want/to/build\ncd /path/to/where/you/want/to/build/mts\ncmake /path/to/your/local/clone/of/mts\nmake -j\n```\n\n### Dockerized Version\n(coming soon, based on http://github.com/ugermann/marian-docker ...)\n\n## Configuring a Model to serve\nThese instructions assume that you have the following files from a marian training process:\n\n- at least one model (more for ensemble decoding; do not try ensembed decoding unless your REST server has access to a GPU)\n- the respective vocabulary file(s)\n- a shortlist file for faster hypothesis generation (optional)\n\n### Preparation\n- Convert the model file(s) to binary format with \n  ```\n  marian-conv -f model.npz -t model.bin\n  ```\n  This is optional, but makes the model load faster.\n  \n- Create a decoder.yml file, e.g. like this one:\n  ```\n  relative-paths: true\n  models: [ model.bin ]\n  vocabs: [ joint-vocab.spm, joint-vocab.spm ]\n  alignment: true\n  beam-size: 4\n  normalize: 1\n  word-penalty: 0\n  mini-batch: 128\n  maxi-batch: 100\n  maxi-batch-sort: src\n\n  # The following are specific to the marian REST server\n  # source-language and target-language are used for the Demo\n  # interface; the ssplit-prefix-file is from the Moses sentence splitter\n  # and comes with the marian REST server image. Pick the right one\n  # for your source language. SSPLIT_ROOT_DIR is set to the appropriate\n  # value in the `mariannmt/marian-rest-server` image.\n  source-language: German\n  target-language: English\n  ssplit-prefix-file: ${SSPLIT_ROOT_DIR}/nonbreaking_prefixes/nonbreaking_prefix.de\n  ```\n- Copy the appropriate `nonbreaking_prefix.*` file for sentence splitting into an appropriate\n  location, or \n  ```\n  export SSPLIT_ROOT_DIR=/path/to/your/local/clone/of/mts/3rd_party/ssplit-cpp\n  ```\n  \n## Running the Server\n```\n/path/to/your/build/directory/rest-server -c /path/to/decoder.yml -p <port of your choice>\n```\n\n# Known bugs\n- Rest-server currently inherits the version info from the marian submodule, which is obviously incorrect.\n"
},
{
  "name": "ssplit-cpp",
  "files": {
    "/": [
      ".gitignore",
      "CMakeLists.txt",
      "LICENSE.md",
      "README.md",
      "cmake",
      "nonbreaking_prefixes",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# ssplit-cpp\nThis is an approximate reimplementation of the sentence splitter from the Moses toolkit.\n\n- Currently doesn't support CJK character sets.\n- requires the pcrecpp libraries\n  On Ubuntu, `sudo apt-get install libpcre3 and libpcre3-cpp` should do the trick\n  \n## Build instructions\n```\nmkdir build\ncd build\ncmake ..\nmake -j\n```\nThis produces an executable `ssplit`.\n\n## Usage\n\ncat \\<text with one paragraph per line\\> | ssplit \\<path to nonbreaking_prefix file\\>\n\n\n\n\n\n"
},
{
  "name": "marian-dev",
  "files": {
    "/": [
      ".clang-format",
      ".compute",
      ".gitattributes",
      ".github",
      ".gitignore",
      ".gitmodules",
      ".taskcluster.yml",
      "CHANGELOG.md",
      "CMakeLists.txt",
      "CMakeSettings.json",
      "CONTRIBUTING.md",
      "Doxyfile.in",
      "LICENSE.md",
      "README.md",
      "VERSION",
      "cmake",
      "contrib",
      "doc",
      "examples",
      "regression-tests",
      "scripts",
      "src",
      "training",
      "vs"
    ],
    "/.github": [
      "ISSUE_TEMPLATE",
      "pull_request_template.md"
    ]
  },
  "makefile": null,
  "readme": "Marian\n======\n\n[![Task Status](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/marian-dev/master/badge.svg)](https://community-tc.services.mozilla.com/api/github/v1/repository/mozilla/marian-dev/master/latest)\n[![Latest release](https://img.shields.io/github/release/marian-nmt/marian.svg?label=release)](https://github.com/marian-nmt/marian/releases)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE.md)\n[![Twitter](https://img.shields.io/twitter/follow/marian_nmt.svg?style=social)](https://twitter.com/intent/follow?screen_name=marian_nmt)\n\n*Marian* is an efficient Neural Machine Translation framework written in pure\nC++ with minimal dependencies.\n\nNamed in honour of Marian Rejewski, a Polish mathematician and cryptologist.\n\nMain features:\n\n- Efficient pure C++ implementation\n- Fast multi-GPU training and GPU/CPU translation\n- State-of-the-art NMT architectures: deep RNN and transformer\n- Permissive open source license (MIT)\n- [more detail...](https://marian-nmt.github.io/features)\n\nIf you use this, please cite:\n\nMarcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth\nHeafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay\nBogoychev, Andr\u00e9 F. T. Martins, Alexandra Birch (2018). Marian: Fast Neural\nMachine Translation in C++ (http://www.aclweb.org/anthology/P18-4020)\n\n    @InProceedings{mariannmt,\n        title     = {Marian: Fast Neural Machine Translation in {C++}},\n        author    = {Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and\n                     Dwojak, Tomasz and Hoang, Hieu and Heafield, Kenneth and\n                     Neckermann, Tom and Seide, Frank and Germann, Ulrich and\n                     Fikri Aji, Alham and Bogoychev, Nikolay and\n                     Martins, Andr\\'{e} F. T. and Birch, Alexandra},\n        booktitle = {Proceedings of ACL 2018, System Demonstrations},\n        pages     = {116--121},\n        publisher = {Association for Computational Linguistics},\n        year      = {2018},\n        month     = {July},\n        address   = {Melbourne, Australia},\n        url       = {http://www.aclweb.org/anthology/P18-4020}\n    }\n\n## Amun\n\nThe handwritten decoder for RNN models compatible with Marian and Nematus has\nbeen superseded by the Marian decoder. The code is available in a separate\nrepository: https://github.com/marian-nmt/amun\n\n## Website\n\nMore information on https://marian-nmt.github.io\n\n- [Quick start](https://marian-nmt.github.io/quickstart)\n- [Installation and usage documentation](https://marian-nmt.github.io/docs)\n- [Usage examples](https://marian-nmt.github.io/examples)\n\n## Acknowledgements\n\nThe development of Marian received funding from the European Union's\n_Horizon 2020 Research and Innovation Programme_ under grant agreements\n688139 ([SUMMA](http://www.summa-project.eu); 2016-2019),\n645487 ([Modern MT](http://www.modernmt.eu); 2015-2017),\n644333 ([TraMOOC](http://tramooc.eu/); 2015-2017),\n644402 ([HiML](http://www.himl.eu/); 2015-2017),\n825303 ([Bergamot](https://browser.mt/); 2019-2021),\nthe Amazon Academic Research Awards program,\nthe World Intellectual Property Organization,\nand is based upon work supported in part by the Office of the Director of\nNational Intelligence (ODNI), Intelligence Advanced Research Projects Activity\n(IARPA), via contract #FA8650-17-C-9117.\n\nThis software contains source code provided by NVIDIA Corporation.\n"
},
{
  "name": "whatthetrain",
  "files": {
    "/": [
      ".gitignore",
      "CNAME",
      "README.md",
      "gh-fork-ribbon.css",
      "gh-fork-ribbon.ie.css",
      "images",
      "index.html",
      "moment-timezone-with-data-2012-2022.js",
      "moment.min.js",
      "style.css",
      "trains.js"
    ]
  },
  "makefile": null,
  "readme": "https://whattrainisitnow.com/\n"
},
{
  "name": "Simple-WebSocket-Server",
  "files": {
    "/": [
      ".clang-format",
      ".gitignore",
      ".gitlab-ci.yml",
      "CMakeLists.txt",
      "LICENSE",
      "README.md",
      "asio_compatibility.hpp",
      "client_ws.hpp",
      "client_wss.hpp",
      "crypto.hpp",
      "javascript_client_example.html",
      "mutex.hpp",
      "server_ws.hpp",
      "server_wss.hpp",
      "status_code.hpp",
      "tests",
      "utility.hpp",
      "ws_examples.cpp",
      "wss_examples.cpp"
    ]
  },
  "makefile": null,
  "readme": "**_This project has moved to https://gitlab.com/eidheim/Simple-WebSocket-Server._**\n\nSimple-WebSocket-Server\n=================\n\nA very simple, fast, multithreaded, platform independent WebSocket (WS) and WebSocket Secure (WSS) server and client library implemented using C++11, Asio (both Boost.Asio and standalone Asio can be used) and OpenSSL. Created to be an easy way to make WebSocket endpoints in C++.\n\nSee https://gitlab.com/eidheim/Simple-Web-Server for an easy way to make REST resources available from C++ applications. Also, feel free to check out the new C++ IDE supporting C++11/14/17: https://gitlab.com/cppit/jucipp. \n\n### Features\n\n* RFC 6455 mostly supported: text/binary frames, fragmented messages, ping-pong, connection close with status and reason.\n* Asynchronous message handling\n* Thread pool if needed\n* Platform independent\n* WebSocket Secure support\n* Timeouts, if any of SocketServer::timeout_request and SocketServer::timeout_idle are >0 (default: SocketServer::timeout_request=5 seconds, and SocketServer::timeout_idle=0 seconds; no timeout on idle connections)\n* Simple way to add WebSocket endpoints using regex for path, and anonymous functions\n* An easy to use WebSocket and WebSocket Secure client library\n* C++ bindings to the following OpenSSL methods: Base64, MD5, SHA1, SHA256 and SHA512 (found in crypto.hpp)\n\n### Usage\n\nSee [ws_examples.cpp](ws_examples.cpp) or [wss_examples.cpp](wss_examples.cpp) for example usage. \n\n### Dependencies\n\n* Boost.Asio or standalone Asio\n* OpenSSL libraries\n\n### Compile\n\nCompile with a C++11 supported compiler:\n\n```sh\nmkdir build\ncd build\ncmake ..\nmake\ncd ..\n```\n\n#### Run server and client examples\n\n### WS\n\n```sh\n./build/ws_examples\n```\n\n### WSS\n\nBefore running the WSS-examples, an RSA private key (server.key) and an SSL certificate (server.crt) must be created.\n\nThen:\n```\n./build/wss_examples\n```\n"
},
{
  "name": "marian-training",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE.md",
      "README.md",
      "importers",
      "tools",
      "training-basics-sentencepiece",
      "training-basics",
      "transformer",
      "translating-amun",
      "wmt2017-transformer",
      "wmt2017-uedin"
    ]
  },
  "makefile": null,
  "readme": "# Marian examples\n\nExamples, tutorials and use cases for the Marian toolkit.\n\nMore information on https://marian-nmt.github.io\n\nList of examples:\n* `translating-amun` -- examples for translating with Amun\n* `training-basics` -- the complete example for training a WMT16-scale model\n* `training-basics-sentencepiece` -- as `training-basics`, but uses built-in SentencePiece for data processing, requires Marian v1.7+\n* `transformer` -- scripts for training the transformer model\n* `wmt2017-uedin` -- scripts for building a WMT2017-grade model for en-de based on Edinburgh's WMT2017 submission\n* `wmt2017-transformer` -- scripts for building a better than WMT2017-grade model for en-de, beating WMT2017 submission by 1.2 BLEU\n\n## Usage\n\nFirst download common tools:\n\n    cd tools\n    make all\n    cd ..\n\nNext, go to the chosen directory and run `run-me.sh`, e.g.:\n\n    cd training-basics\n    ./run-me.sh\n\nThe README file in each directory provides more detailed description.\n\n## Acknowledgements\n\nThe development of Marian received funding from the European Union's\n_Horizon 2020 Research and Innovation Programme_ under grant agreements\n688139 ([SUMMA](http://www.summa-project.eu); 2016-2019),\n645487 ([Modern MT](http://www.modernmt.eu); 2015-2017),\n644333 ([TraMOOC](http://tramooc.eu/); 2015-2017),\n644402 ([HiML](http://www.himl.eu/); 2015-2017),\nthe Amazon Academic Research Awards program, and\nthe World Intellectual Property Organization.\n\nThis software contains source code provided by NVIDIA Corporation.\n\n"
},
{
  "name": "ad-library-download-tools",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "external_files",
      "src",
      "test"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "discourse-mozilla-travis",
  "files": {
    "/": [
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "before_script.sh",
      "entrypoint.sh",
      "plugin_helper.rb",
      "script.sh"
    ]
  },
  "makefile": null,
  "readme": "# discourse-mozilla-travis\n\n*Inspired by https://meta.discourse.org/t/setting-up-plugin-continuous-integration-tests-on-travis-ci/59612*\n\nThis repo holds the scripts we use across our [Discourse plugins](https://github.com/search?q=org%3Amozilla+topic%3Adiscourse-plugin) to run tests in Travis CI and check coverage on Coveralls.\n\nIncorporating these scripts into a plugin is a multi-step process:\n\n## Set up Travis CI\n\nEnable [Travis CI](https://travis-ci.org/) for the repo of the plugin in question.\n\n[`.travis.yml`](.travis.yml) is a travis config file which runs these scripts, it should be placed in the root of a plugin repo.\n\nA [Travis cron job](https://docs.travis-ci.com/user/cron-jobs/) should be set up to run every day to test the plugin against the latest upstream code.\n\n## Set up Coveralls\n\nEnable [Coveralls](https://coveralls.io/) for the repo of the plugin in question.\n\nThis code must be run at the **very top** of every spec:\n\n```ruby\nif ENV[\"COVERALLS\"] || ENV[\"SIMPLECOV\"]\n  require \"simplecov\"\n\n  if ENV[\"COVERALLS\"]\n    require \"coveralls\"\n    SimpleCov.formatter = Coveralls::SimpleCov::Formatter\n  end\n\n  SimpleCov.start do\n    root File.expand_path(\"../..\", __FILE__)\n    add_filter \"spec/\"\n    add_filter \"db/migrate\"\n    add_filter \"gems/\"\n  end\nend\n\nrequire \"rails_helper\"\n```\n\nFor inconvenience [`plugin_helper.rb`](plugin_helper.rb) contains this code, and if placed in the plugin's `spec/` folder, it can be included in specs with:\n\n```ruby\nrequire_relative \"plugin_helper\"\n```\n\n(which must also go at the **very top** of every spec)\n\nThis won't test coverage of code in a plugin's `plugin.rb` file, so all testable code should be moved into a separate file which can be included with `require_relative`.\n\nWhen run locally it'll also place output in the `coverage/` folder, so this should be added to the repo's `.gitignore`.\n\n## Checklist\n\n* Enable Travis CI\n* Include [`.travis.yml`](.travis.yml)\n* Set up Travis cron job\n* Enable Coveralls\n* Include [`plugin_helper.rb`](plugin_helper.rb)\n* Move testable code out of `plugin.rb`\n* Add `coverage` to `.gitignore`\n* Add build status and coverage status badges to `README.md`\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Licence\n\n[GPL-2.0](LICENSE)\n\n[`.travis.yml`](.travis.yml) and [`plugin_helper.rb`](plugin_helper.rb) also licensed under [MPL-2.0](https://www.mozilla.org/en-US/MPL/2.0/)\n"
},
{
  "name": "demo_docker",
  "files": {
    "/": [
      ".circleci",
      "Dockerfile",
      "README.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# demo_docker\n"
},
{
  "name": "node-srp",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "index.js",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "[![build status](https://secure.travis-ci.org/jedp/node-srp.png)](http://travis-ci.org/jedp/node-srp)\n\n# SRP - Secure Remote Password\n\nImplementation of the [SRP Authentication and Key Exchange\nSystem](http://tools.ietf.org/html/rfc2945) and protocols in [Secure\nRemote Password (SRP) Protocol for TLS\nAuthentication](http://tools.ietf.org/html/rfc5054)\n\nSRP is an interactive protocol which allows a server to confirm that some client knows a password, and to derive a strong shared session key, without revealing what the password is to an eavesdropper. In addition, the server does not hold the actual password: instead it stores a \"verifier\" created by the client. If the server's private data is revealed (by a server compromise), the verifier cannot be used directly to impersonate the client.\n\nThis module provides both client and server implementations of SRP-6a for node.js. They are interoperable with [Mozilla Identity-Attached Services](https://wiki.mozilla.org/Identity/AttachedServices/KeyServerProtocol)\n\n* [Installation](#installation)\n* [Running Tests](#running-tests)\n* [Usage](#how-to-use-it)\n* [API Reference](#api-reference)\n* [Resources](#resources)\n\n## Installation\n\n`npm install srp`\n\nor `git clone` this archive and run `npm install` in it.\n\n## Running Tests\n\nRun `npm test`.\n\nTests include vectors from:\n- [RFC 5054, Appendix B](https://tools.ietf.org/html/rfc5054#appendix-B).\n- [Mozilla Identity Attached Services](https://wiki.mozilla.org/Identity/AttachedServices/KeyServerProtocol)\n\n## How to use it\n\nFirst, you must decide on the \"parameters\". This module provides a variety of pre-packaged parameter sets, at various levels of security (more secure parameters take longer to run). The \"2048\"-bit parameters are probably fairly secure for the near future. Both client and server must use the same parameters.\n\n    var params = srp.params[\"2048\"];\n\nEach client will have a unique \"identity\" string. This is typically a username or email address. Clients will also use a unique \"salt\", which can be randomly generated during account creation. The salt is generally stored on the server, and must be provided to the client each time they try to connect.\n\nNote that all APIs accept and return node.js Buffer objects, not strings.\n\n### Client Setup: Account Creation\n\nThe client feeds their identity string, password, and salt, into `computeVerifier`. This returns the Verifier buffer. The Verifier must be delivered to the server, typically during a \"create account\" process. Note that the Verifier can be used to mount a dictionary attack against the user's password, so it should be treated with care (and delivered securely to the server).\n\n    var verifier = srp.computeVerifier(params, salt, identity, password);\n    createAccount(identity, verifier);\n\nThe server should store the identity, salt, and verifier in a table, indexed by the identity for later access. The server will provide the salt to anyone who asks, but should never reveal the verifier to anybody.\n\n### Login\n\nLater, when the client wants to connect to the server, it starts by submitting its identity string, and retrieving the salt.\n\nThen the client needs to create a secret random string called `secret1`, using `srp.genKey()`. The protocol uses this to make sure that each instance of the protocol is unique.\n\nThen, create a new `srp.Client` instance with parameters, identity, salt, password, and `secret1`.\n\nThe client must then ask this object for the derived `srpA` value, and deliver srpA to the server.\n\n    srp.genKey(function(secret1) {\n        var c = new srp.Client(params, salt, identity, password, secret1);\n        var srpA = c.computeA();\n        sendToServer(srpA);\n    });\n\nMeanwhile, the server is doing something similar, except using the Verifier instead of the salt/identity/password, and using its own secret random string.\n\n    srp.genKey(function(secret2) {\n        var s = new srp.Server(params, verifier, secret2);\n        var srpB = s.computeB();\n        sendToClient(srpB);\n    });\n\nWhen the client receives the server's `srpB` value, it stuffs it into the Client instance. This allows it to extract two values: `M1` and `K`.\n\n    c.setB(srpB);\n    var M1 = c.computeM1();\n    sendToServer(M1);\n    var K = c.computeK();\n\n`M1` is a challenge value, created by the client and delivered to the server. After accepting the client's `A` value, the server can check `M1` to determine whether or not the client really knew the password. The server can also obtain its own `K` value.\n\n    s.setA(srpA)\n    s.checkM1(M1); // throws error if wrong\n    var K = s.computeK();\n\nIf the password passed into `srp.Client()` is the same as the one passed into `srp.computeVerifier()`, then the server will accept `M1`, and the `K` on both sides will be the same.\n\n`K` is a strong random string, suitable for use as a session key to encrypt or authenticate subsequent messages.\n\nIf the password was different, then `s.checkM1()` will throw an error, and the two `K` values will be unrelated random strings.\n\nThe overall conversation looks like this:\n\n    Client:                             Server:\n     p = params[\"2048\"]                  p = params[\"2048\"]\n     s1 = genKey()                       s2 = genKey()\n     c = new Client(p,salt,id,pw,s1)     s = new Server(p,verifier,s2)\n     A = c.computeA()            A---->  s.setA(A)\n     c.setB(B)                <-----B    B = s.computeB()\n     M1 = c.computeM1()         M1---->  s.checkM1(M1) // may throw error\n     K = c.computeK()                    K = s.computeK()\n\n### What a \"Session\" Means\n\nBasic login can be done by simply calling `s.checkM1()`: if it doesn't throw an exception, the client knew the right password. However, by itself, this does not bind knowledge of the password to anything else. If the A/B/M1 values were delivered over an insecure channel, controlled by an attacker, they could simply wait until `M1` was accepted, and then take control of the channel.\n\nDelivering these values over a secure channel, such as an HTTPS connection, is better. If the HTTP client correctly checks the server certificate, and the certificate was correctly issued, then you can exclude a man-in-the-middle attacker.\n\nThe safest approach is to *create* a secure channel with the generated session key `K`, using it to encrypt and authenticate all the messages which follow.\n\n## API Reference\n\nModule contents:\n\n- **`params[]`**\n - table of parameter sets. Pass a property from this object into the Client and Server constructors.\n- **`genKey(numBytes, callback)`**\n - async function to generate the ephemeral secrets passed into the Client and Server constructors.\n- **`computeVerifier(params, salt, identity, password) -> V`**\n - produces a Verifier, which should be given to the server during account creation. The Verifier will be passed into the Server constructor during login.\n- **`Client(params, salt, identity, password, secret1) -> c`**\n - constructor for the client-side of SRP. secret1 should come from genKey(). The Client object has the following methods:\n- **`Server(params, verifier, secret2) -> s`**\n - constructor for the server-side of SRP. secret2 should come from genKey(). If the Server object must be persisted (e.g. in a database) between protocol phases, simply store secret2 and re-construct the Server with the same values. The Server object has the following methods:\n\n`Client` methods:\n\n- **`computeA() -> A`**\n - produce the A value that will be sent to the server.\n- **`setB(B)`**\n - this accepts the B value from the server. M1 and K cannot be accessed until setB() has been called.\n- **`computeM1() -> M1`**\n - produce the M1 key-confirmation message. This should be sent to the server, which can check it to make sure the client really knew the correct password. setB must be called before computeM1.\n- **`computeK() -> K`**\n - produce the shared key K. If the password and verifier matched, both client and server will get the same value for K. setB must be called before computeK.\n\n`Server` methods:\n\n- **`computeB() -> B`**\n - produce the B value that will be sent to the client.\n- **`setA(A)`**\n - this accepts the A value from the client. checkM1 and computeK cannot be called until setA has been called.\n- **`checkM1(M1)`**\n - this checks the client's M1 key-confirmation message. If the client's password matched the server's verifier, checkM1() will complete without error. If they do not match, checkM1() will throw an error.\n- **`computeK() -> K`**\n - produce the shared key K. setA must be called before computeK.\n\n## Resources\n\n- [The Stanford SRP Homepage](http://srp.stanford.edu/)\n- RFC 2945: [The SRP Authentication and Key Exchange System](http://tools.ietf.org/html/rfc2945)\n- RFC 5054: [Using the Secure Remote Password (SRP) Protocol for TLS Authentication](http://tools.ietf.org/html/rfc5054)\n- Wikipedia: [The Secure Remote Password protocol](http://en.wikipedia.org/wiki/Secure_Remote_Password_protocol)\n\n## License\n\nMIT\n"
},
{
  "name": "moz-git-tools",
  "files": {
    "/": [
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.markdown",
      "git-branchname",
      "git-bz",
      "git-bz-moz",
      "git-bzexport",
      "git-edit-files",
      "git-fix-whitespace",
      "git-new-workdir",
      "git-patch-to-hg-patch",
      "git-push-to-hg",
      "git-push-to-mozreview",
      "git-push-to-try",
      "git-push-to-trychooser",
      "git-qapplied",
      "git-qparent",
      "git-qrebase",
      "git-remote-link",
      "git-root",
      "git-to-hg-commit",
      "git-tracks",
      "hg-patch-to-git-patch",
      "pre-commit",
      "private",
      "test.sh"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "research-repo-webconf-crawl-representativeness",
  "files": {
    "/": [
      ".gitignore",
      "3366423.3380104.pdf",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "human-browsing-top-sites",
      "list-comparison",
      "lists"
    ]
  },
  "makefile": null,
  "readme": "# The representativeness of automated Web crawls as a surrogate for human browsing: companion repository\n\nThis repository contains or links to all assets relevant to the WWW'20 paper: [The representative of automated Web crawls as a surrogate for human browsing](https://dl.acm.org/doi/abs/10.1145/3366423.3380104). All listed assets will be made publicly available pending internal privacy/trust audit processes required prior to data release. For specific inquiries pertaining to data access and collaborations on privacy enhancing technologies research please reach out to the corresponding authors listed on the manuscript.\n\n* Lists used for crawls: under [lists](./lists/) directory\n* Trexa repo: https://github.com/mozilla/trexa\n* Crawl preparation (pre crawl and depth crawl code): https://github.com/mozilla/crawl-prep\n* Crawl database: [Google Doc](https://docs.google.com/spreadsheets/d/1HlocB39Ujaw2JH4Nm_0lXFqQ6GcQjJ7ONHHLFq-NReI/)\n* Crawl downloads: all the crawl data is stored in a S3 bucket. The total size of the data is __184.2GB__ comprised of:\n    + 18.4GB for the 44 time sequence crawls\n    + 36.4GB for the two large companion crawls of ~100k sites\n    + 129.4GB for the remaining 60 crawls\n\n    The compressed crawl data (64GB) is available on BitTorrent on\n[AcademicTorrents](https://academictorrents.com/details/5e9ef2b5531ce3b965681be6eccab1fbd114af62/tech)\n    \n* Alternate orchestration repo: https://github.com/birdsarah/faust-selenium\n* List comparison analysis: under [list-comparison](./list-comparison/top-site-list-comparison.ipynb) directory\n* DP-protected top-level domain ranking for opt-in human users [August-2019]: under [human-browsing-top-sites](./human-browsing-top-sites/top_dp_domains_human_browsing.csv) directory\n\nIf you find any of the resources contained int his repository valuable for your research please cite the original manuscript for which this work was produced:\n\n```\n@inproceedings{10.1145/3366423.3380104,\nauthor = {Zeber, David and Bird, Sarah and Oliveira, Camila and Rudametkin, Walter and Segall, Ilana and Wolls\\'{e}n, Fredrik and Lopatka, Martin},\ntitle = {The Representativeness of Automated Web Crawls as a Surrogate for Human Browsing},\nyear = {2020},\nisbn = {9781450370233},\npublisher = {Association for Computing Machinery},\naddress = {New York, NY, USA},\nurl = {https://doi.org/10.1145/3366423.3380104},\ndoi = {10.1145/3366423.3380104},\nbooktitle = {Proceedings of The Web Conference 2020},\npages = {167\u2013178},\nnumpages = {12},\nkeywords = {Web Crawling, Online Privacy, Tracking, Browser Fingerprinting, World Wide Web},\nlocation = {Taipei, Taiwan},\nseries = {WWW \u201920}\n}\n```\n"
},
{
  "name": "apiaah",
  "files": {
    "/": [
      ".circleci",
      "Dockerfile",
      "api.R"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "moz-chargers",
  "files": {
    "/": [
      ".gitignore",
      ".glitch-assets",
      "README.md",
      "index.js",
      "package.json",
      "static"
    ]
  },
  "makefile": null,
  "readme": "# moz-chargers\n\nScrape Chargepoint's web site to determine the state of the Mozilla-employee only chargers at the Mountain View office and optionally post the status to a slack\n"
},
{
  "name": "fx-topline-release-dash",
  "files": {
    "/": [
      ".babelrc",
      ".gitignore",
      "README.md",
      "index.html",
      "package.json",
      "server",
      "src",
      "static",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# firefox-topline-release-dashboard\n\n## install\n\nrun `npm install`, like you'd expect\n\n## further development requires an understanding of\n\n1. React, which I used for the main layout to speed up development\n2. D3 / Metrics-Graphics, which we're using for now to show lines\n\n## build\n\nrun `npm run build` to have webpack build the output. Will watch this directory for changes and rebuild.\n"
},
{
  "name": "tracking-test",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "ads.html",
      "analytics.html",
      "content.html",
      "cryptomining.html",
      "disconnect.html",
      "etp-test.html",
      "fingerprinting.html",
      "iframe-bug1108017.html",
      "index.html",
      "social.html"
    ]
  },
  "makefile": null,
  "readme": "# Shavar list testing using test pages\n\nPlease use https://senglehardt.com/test/trackingprotection/test_pages/ until https://github.com/mozilla/tracking-test/pull/13 is ready.\n"
},
{
  "name": "firefox-accounts-campaign",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "fonts",
      "images",
      "index.html",
      "main.js",
      "number",
      "style.css"
    ]
  },
  "makefile": null,
  "readme": "This was the campaign website for the Firefox Accounts Volunteer Campaign.\n"
},
{
  "name": "missioncontrol-v2",
  "files": {
    "/": [
      ".circleci",
      ".dockerignore",
      ".gitignore",
      "Dockerfile",
      "Makefile",
      "README.md",
      "backup.firefox.desktop.R",
      "build.models.firefox.desktop.R",
      "complete.runner.sh",
      "create.dashboards.static.R",
      "environment.yml",
      "etl.R",
      "mc2",
      "missioncontrol.lib.R",
      "pipeline.md",
      "process.model.firefox.desktop.R",
      "pyproject.toml",
      "tox.ini",
      "visual_demo"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: build shell\n\nbuild:\n\tdocker build -t missioncontrol-v2 .\n\nshell:\n\tdocker run -v $(PWD):/mc-etl -it missioncontrol-v2 /bin/bash\n",
  "readme": "# missioncontrol-v2\n\n[![CircleCI](https://img.shields.io/circleci/project/github/mozilla/missioncontrol-v2/master.svg)](https://circleci.com/gh/mozilla/missioncontrol-v2)\n\n\nAn alternate view of crash and stability\n\n## Installation instructions\n\nThis code is designed to be 'easy' to install and repeatable. That is if the underlying data doesn't change the output should not either.\n\nThis code should either work inside GCP or on a local computer. In either case, we recommend\na powerful one with at least 4 cores and a decent amount (8Gb+) of memory. If using Docker, you will \nwant to increase the amount of resources available to containers if you haven't already: the \ndefaults are likely to be insufficient.\n\nYou will also want a [GCP service account](https://docs.telemetry.mozilla.org/cookbooks/bigquery.html#gcp-bigquery-api-access) with permission to read from the datasets in\n`fx-data-shared-prod` and write access to a cloud storage bucket. Typically one would\ndo this using a sandbox project.\n\nAfter getting a service setup, download the credentials into a file called `gcloud.json` in\nthe root of your checkout.\n\n## Development instructions\n\nThe ETL pipeline is based on running a number of scripts in succession, performing\nthe following operations:\n\n* Download the latest crash and usage data for a recent set of versions, and upload\n  the results to a temporary table in BigQuery.\n* Build a statistical model based on the above data downloaded as well as historical\n  data that we have seen before.\n* Generate an Rmarkdown-based report based on the output of the above model and upload\n  it to google cloud storage.\n\n### Option 1: Use the Docker container\n\nThis is the most deterministic approach and closest to what we are using in production, \nthough it is likely to be slower on non-Linux hosts. These instructions assume that you have \n[Docker](https://www.docker.com/) and a basic set of developer tools installed on your machine.\n\nFirst, build the container:\n\n```bash\nmake build\n```\n\nThen, create a shell session inside it:\n\n```bash\nmake shell\n```\n\nSkip to the next section to run the code.\n\n### Option 2: Use Conda\n\nThis should run on the bare metal of your machine, and should be much faster on Mac. These instructions assume you have either [conda](https://docs.conda.io/projects/conda/en/latest/) \nor [miniconda](https://docs.conda.io/en/latest/miniconda.html) installed, as well as the \n[Google Cloud SDK](https://cloud.google.com/sdk/).\n\nFrom the root checkout, creating and activating a conda environment is a two step process:\n\n```bash\nconda env create -n mc2 -f environment.yml\nconda activate mc2\n```\n\n### Running\n\nOnce you have a shell (either in the docker container or activated conda environment), \nset some environment variables corresponding to your GCP settings:\n\n```bash\nexport GOOGLE_APPLICATION_CREDENTIALS=$PWD/gcloud.json\nexport RAW_OUTPUT_TABLE=missioncontrol_v2_test_raw\nexport MODEL_OUTPUT_TABLE=missioncontrol_v2_test_model\nexport GCP_PROJECT_ID=my-gcp-project-id\nexport GCS_OUTPUT_PREFIX=gs://my-cloud-storage-bucket\n```\n\nThe `RAW_OUTPUT_TABLE` and `MODEL_OUTPUT_TABLE` settings specify the GCP table names for temporary\ndata written during the run.\n\nThen run the model:\n\n```bash\n./complete.runner.sh\n```\n\nIf running on an underpowered machine, or you just want to get results more quickly, you can also \nenable \"simple\" mode, which (as the name implifies) speeds up the model generation significantly by\nusing a simplified statistical model:\n\n```bash\nSIMPLE=1 ./complete.runner.sh\n```\n\n### Gotchas\n\nIf you run the data pulling code shortly after a new release, and did not pull data in the\nprevious days, then those days' data could be missing for the previous major release versions.\n\nTo avoid this problem, you can copy the bigquery table used in production (`moz-fx-data-derived-datasets.analysis.missioncontrol_v2_raw_data`) to your own GCP project.\n"
},
{
  "name": "gecko-projects",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "gecko-projects\n==============\n\nThis repository is synced from hg.mozilla.org and should be read-only.\n\nNote: 'master' branch is rairly updated - view the branch you care about\nto see how recent it is. It should be within ~30 minutes of\nhg.mozilla.org copy.\n\nThis repo is intended to be used with gecko-dev.  For instance, to add the 'alder' branch to your local repository:\n\n    git clone https://github.com/mozilla/gecko-dev.git\n    cd gecko-dev\n    # Add a new 'projects' remote\n    git remote add projects https://github.com/mozilla/gecko-projects.git\n    # Add a new branch from gecko-projects.  For this example, let's say 'alder'.\n    BRANCH=alder\n    # Set the list of branches we care about; this will discard any previous branches\n    git remote set-branches projects $BRANCH\n    # To append to the list, use --add:\n    #     git remote set-branches --add projects $BRANCH\n    git fetch projects\n    git checkout $BRANCH\n\nSince the mercurial repositories being synced have less strict rules, there is a higher likelihood of a commit that may cause sync issues.  Mozilla Release Engineering reserves the right to reset this repo and resync at any point, should it become necessary.\n"
},
{
  "name": "marketing-analytics",
  "files": {
    "/": [
      ".gitignore",
      "Airflow",
      "AppEngine_moz_mktg_prod_001",
      "CODE_OF_CONDUCT.md",
      "ETL Jobs",
      "Projects",
      "Rcode",
      "Xuan",
      "bqQueries",
      "telemetry"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "CANOSP-2019",
  "files": {
    "/": [
      ".circleci",
      ".flake8",
      ".gitattributes",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "contributors.md",
      "datasets",
      "environment.yml",
      "mozfldp",
      "notebooks",
      "setup.py",
      "tests"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": ".PHONY: upload_pypi tests image up stop setup_conda lint pytest\n\nIMAGE_NAME=mozfldp:latest\n\nall: pytest\n\nbuild_image:\n\t# Build the docker image\n\tdocker build . -t $(IMAGE_NAME)\n\nupload_pypi:\n\ttwine upload --repository-url https://upload.pypi.org/legacy/ dist/*\n\nsetup_conda:\n\t# Install dependencies\n\tconda env update -n mozfldp -f environment.yml\n\npytest: lint\n\tpytest\n\n# tests:\n# \tpython setup.py pytest\n\nlint:\n\tflake8 mozfldp tests\n\ndocker_tests:\n\tdocker run -it \\\n\t\t-p 127.0.0.1:8090:8000 \\\n\t\t--name mozfldp \\\n\t\t-t --rm $(IMAGE_NAME) \\\n\t\ttest\n\n####\n#### Use `make up` and `make stop` to run the Docker container locally\n####\n\nup:\n\t# Bind 127.0.0.1, port 8090 to the container's port 8000\n\t# and start the server.\n\t#\n\t# Name the container 'mozfldp' so that we can stop the container\n\t# easily\n\t# docker container stop mozfldp || true\n\tdocker run -eit \\\n\t\t-p 127.0.0.1:8090:8000 \\\n\t\t--name mozfldp \\\n\t\t-t --rm $(IMAGE_NAME) \\\n\t\tweb\n\nstop:\n\tdocker stop mozfldp\n",
  "readme": "[![CircleCI](https://circleci.com/gh/mozilla/CANOSP-2019/tree/master.svg?style=svg)](https://circleci.com/gh/mozilla/CANOSP-2019/tree/master)\n\n# CANOSP-2019\n\nThis project implements a minimal server that can perform federated learning\nwith differential privacy and accepts messages from clients.\n\n\n## Getting Started\n\nWe are using Miniconda to manage the environment. It can be installed using one\nof the installers available [here](https://docs.conda.io/en/latest/miniconda.html).\nFor MacOS, the bash installer is recommended.\n\nMake sure to have `conda init` run during conda installation so that\nyour PATH is set properly.\n\n\n## Installing locally to run tests\n\nTo install and run the tests for this project you can run:\n\n```bash\n# Set up the environment.\n$ make setup_conda\n$ conda activate mozfldp\n\n# Run tests\n$ make pytest\n```\n\n## Running the server\n\nYou can run the server locally, serving requests on port 8000, using:\n\n```bash\n$ python -m mozfldp.server\n```\n\n## Building a release\n\n```bash\npython setup.py sdist\n```\n\n\n## Running from Docker\n\nThe server can also be built and run as a Docker container. \nFirst, install [Docker](https://docs.docker.com/get-docker/).\n\nOnce you have Docker installed, you can build the container and run tests using:\n\n```bash\n$ make build_image\n$ make docker_tests\n```\n\nTo run the service in the container, use:\n\n```bash\n$ make up\n```\n\nNote that in the above command, we are exposing the container's port\n8000 by binding it to port 8090 on the host computer.\n\n\n## Sending data to the server\n\n\nYou can submit arbitrary JSON blobs to the server using HTTP POST.\n\nA sample curl invocation that will work is:\n\n```bash\ncurl -X POST http://127.0.0.1:8000/api/v1/compute_new_weights\n\n{\"result\":\"ok\",\"weights\":[[[0.0,0.0,0.0,.... }\n```\n\nNote: If you are running locally, the port will be 8000. Port 8090 is used if you are running in a docker container.\n\n"
},
{
  "name": "releasehealth",
  "files": {
    "/": [
      ".gitignore",
      ".project",
      "CODE_OF_CONDUCT.md",
      "css",
      "fonts",
      "images",
      "index.html",
      "js"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "discourse-mozilla-theme",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "about.json",
      "assets",
      "common",
      "desktop",
      "gulpfile.js",
      "mobile",
      "package-lock.json",
      "package.json",
      "scss",
      "settings.yml",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# discourse-mozilla-theme\n\n*Theme used on discourse.mozilla.org*\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "i18n-abide",
  "files": {
    "/": [
      ".gitignore",
      ".jshintrc",
      ".travis.yml",
      "README.md",
      "bin",
      "docs",
      "examples",
      "lib",
      "package.json",
      "static",
      "tests"
    ],
    "/docs": [
      "API.md",
      "GETTEXT.md",
      "USAGE.md"
    ]
  },
  "makefile": null,
  "readme": "# i18n-abide\n\nThis module **abides by the user's language preferences** and makes it available\nthroughout the app.\n\nThis module **abides by the Mozilla L10n way of doing things**.\n\n**The module abides**.\n\n# Status\n\nUsed in production systems, such as the\n[Mozilla Persona](https://github.com/mozilla/browserid) service in 40+\nlanguages.\n\nAlso used on other websites including:\n* Mozilla Webmaker\n\n# Supported Localization Technologies\n\nThis module supports several localization backends:\n* Gettext PO files (default and documented below)\n* Plist files\n* Transifex key-value-JSON files\n\nThis module supports client side as well as server side localization.\n\n# Usage\n\n    npm install i18n-abide\n\nIn this README, we'll use express and EJS templates, but other\nintegrations are possible.\n\nIn your app where you setup express:\n\n    var i18n = require('i18n-abide');\n\n    app.use(i18n.abide({\n      supported_languages: ['en-US', 'de', 'es', 'db-LB', 'it-CH'],\n      default_lang: 'en-US',\n      debug_lang: 'it-CH',\n      translation_directory: 'i18n'\n    }));\n\nThis block sets up the middleware and views with gettext support.\nWe declare support for English, German, Spanish, and two debug locales\n(more on this later).\n\nIn your routes, you can use the gettext function in `.js` files.\n\n    exports.homepage = function(req, resp) {\n      resp.render('home', {title: req.gettext(\"Hey, careful, man, there's a beverage here!\")});\n    };\n\nIn your layout files, you can add\n\n    <!DOCTYPE html>\n    <html lang=\"<%= lang %>\" dir=\"<%= lang_dir %>\">\n      <head>\n        <meta charset=\"utf-8\">\n        ...\n\nIn your templates files, you can use the gettext function in `.ejs` files:\n\n    <p><%= gettext(\"This will not stand, ya know, this aggression will not stand, man.\") %></p>\n\ni18n-abide also provides a `format` function for string interpolation.\n\nThis module provides both server side translations and client side translations.\nServer side works out of the box and is the most common use case.\n\nIf you also want to do client-side translations,\ni18n-abide provides `lib/gettext.js` and you can do the same in `.js` and\n`.ejs` files.\n\n## Setting Language via HTTP Header\n\nThe `i18n-abide` module uses the\n[`accept-language` HTTP header](http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4)\nto determine which language to use.\n\nSee [API docs](./docs/API.md) for overriding this via URL or the API directly.\n\n## Translation files\n\nThe `i18n-abide` module currently supports three file formats.\n\n1) PO/POT files, which get transformed to JSON via provided command line tools.\n\n2) [PLIST](https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man5/plist.5.html) (i.e., XML)\nfiles, which require no transformation prior to use.\n\n3) [Transifex](http://support.transifex.com/customer/portal/articles/1223004-key-value-json-files) [JSON](https://developer.mozilla.org/en/docs/JSON)\n(JavaScript Object Notation) a key-value JSON type,\nwhich require no transformation prior to use.\n\n### PO/POT files\n\nThis is the default and assumed for documentation in this README.\n\nPO files can be compiled to .json or Gettext binary `.mo` files.\n\nFor use on the client side,\nPO files are compiled to JavaScript for easy inclusion into your page or build\nscript.\n\nNOTE: The PO/POT files are also transformed into .JSON,\nbut do not follow the same layout as the Transifex JSON files.\n\nSee [GETTEXT.md](docs/GETTEXT.md) for more details.\n\n### Other file formats\n\nSee [API](docs/API.md) for configuration and details around using Plist or Transifex localization files.\n\n\n# Debugging and Testing\n\n`db-LB` is a special **debug** locale.\nTo trigger it, set your Browser or Operating System language to Italian\n(Switzerland) which is `it-CH`.\nThis fake locale `db-LB` will be triggered,\nit is David Bowie speak for the region of Labyrinth.\n\nOh, hell ya a \"The Dude\" / Bowie Mashup.\nThat just happened.\n\nNow,\nstart up your Node server and visit a page you've wrapped strings in Gettext...\n\n# Tutorial\n\nMozilla Hacks blog has a three part introduction.\n\n* [Localize Your Node.js Service](https://hacks.mozilla.org/2013/04/localize-your-node-js-service-part-1-of-3-a-node-js-holiday-season-part-9/)\n* [Localization community, tools & process](https://hacks.mozilla.org/2013/04/localization-community-tools-process-part-2-of-3-a-node-js-holiday-season-part-10/)\n* [Localization in Action](https://hacks.mozilla.org/2013/04/localization-in-action-part-3-of-3-a-node-js-holiday-season-part-11/)\n\n# Docs\n* See [USAGE](./docs/USAGE.md) for full details.\n* [API docs](./docs/API.md) has more advanced config options and APIs\n* [GETTEXT](./docs/GETTEXT.md) documents how to use PO/POT files\n"
},
{
  "name": "sandboxed-regexp",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "Makefile",
      "README.md",
      "src",
      "tools"
    ]
  },
  "makefile": "# No school like the old school...\n\nbuild: pkg/sandboxed-regexp.js\n\npkg/sandboxed-regexp.js: src/lib.rs src/sandboxed-regexp.js Cargo.toml\n\t# We use a custom smaller-than-normal stack size in the hope of reducing memory usage.\n\t# This might prove to be a bad idea in practice...\n\tRUSTFLAGS=\"-C link-arg=-zstack-size=16384\" wasm-pack build --target nodejs --release --out-name=\"sandboxed-regexp\" --no-typescript\n\t#wasm-pack build --target nodejs --release --out-name=\"sandboxed-regexp\" --no-typescript\n\t# We have our own custom JS wrapper, overwrite the generated one.\n\tcp src/sandboxed-regexp.js pkg/sandboxed-regexp.js\n\ntest: build\n\tnode ./src/test.js\n\nbench: build\n\tnode ./tools/bench.js\n\nclean:\n\trm -rf ./pkg\n\npublish:\n\tcd ./pkg && npm publish",
  "readme": "\n# sandboxed-regexp\n\n## Process untrusted regexes in JavaScript, with the power of Rust!\n\nJavaScript's builtin [`RegExp`](https://developer.mozilla.org/docs/Web/JavaScript/Reference/Global_Objects/RegExp)\nclass is not suitable for working with regular expressions obtained from untrusted sources, because it can easily\nfall victim to [Catastrophic Backtracking](https://www.regular-expressions.info/catastrophic.html).\n\nRust's [`regex`](https://docs.rs/regex/) crate, by contrast, is specifically designed to\n[safely handle untrusted input](https://docs.rs/regex/#untrusted-input) without blowing up.\n\nSo let's use WebAssembly to bring this same safe handling of untrusted regexes from Rust to JavaScript,\nwith the addition of extra sandboxing!\n\n\n## Building it\n\nYou'll need [`wasm-pack`](https://rustwasm.github.io/docs/wasm-pack/tutorials/npm-browser-packages/index.html).\n\nBuild the package using `make build`. Test that it works using `make test`.\nPublish the resulting bundle under `./pkg` like a regular nodejs library.\n\n\n## Using it\n\nLike so:\n\n```\nconst { SandboxedRegExp } = require('sandboxed-regexp');\n\n// Problematic regex example from http://www.rexegg.com/regex-explosive-quantifiers.html\nconst FastRE = new SandboxedRegExp(\"^(A+)*B\")\n\n// This will run quickly to completion and output `false`.\nconsole.log(FastRE.test(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC\"));\n\n// But if you try the same thing with the builtin RegExp class.\nconst SlowRE = new RegExp(\"^(A+)*B\");\n\n// Then this will churn CPU for several seconds before outputting `false`.\nconsole.log(SlowRE.test(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC\"));\n\n// And this will churn CPU until you get sick of waiting and interrupt it.\nconsole.log(SlowRE.test(\"AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC\"));\n```\n\n\n## Being Careful with it\n\nUsing the `SandboxedRegExp` class should feel fairly similar to using the\nbuiltin `RegExp` class for simple use-cases, but it is in no way intended\nto be a drop-in replacement.\n\nIt's missing the following features of the builtin class that might conceivably\nbe added by someone who needed them:\n\n* No support for any methods besides `test` (such as `exec` or `match`).\n* No support for capturing groups.\n* No support for updating the `lastIndex` property after a match.\n\nIt has the following differences from the builtin class that will probably\nalways remain:\n\n* It uses the [Rust crate's regex syntax](https://docs.rs/regex/#syntax).\n  This should be the same as the JS syntax for simple cases but is likely\n  to be very different around the edges.\n    * In particular, this means no support for look-ahead or backreferences,\n      which are popular non-regular enhancements to reglar expression syntax\n      that are resistant to safe execution of untrusted inputs.\n* Thorough unicode handling is on by default. This is most likely to show\n  up in practice as character classes like `\\w` matching unicode character\n  classes rather than ASCII.\n\nIt has the following non-functional differences that might matter to you\nat scale:\n\n* You have to slurp in a few hundred kilobytes of wasm code.\n* Testing against a `SandboxedRegExp` is cheap, but creating one is likely to\n  be much more expensive than creating a builtin `RegExp`. If you're creating\n  `SandboxedRegExp` instances in a loop, you're likely to have a bad time.\n* Each `SandboxedRegExp` instance consumes more memory than a builtin `RegExp`\n  instance. *Significantly* more. We might expose some knobs for tuning the\n  memory use in future.\n* Testing a string against a `SandboxedRegExp` involves copying that string\n  into the wasm linear memory, which might get expensive if you're testing\n  very large strings.\n* If you're working with a particularly-badly-behaved regex, it might run out\n  of memory or similar catastrophic failure behaviour that will be surfaced\n  as an opaque wasm-related error. This could render the `SandboxedRegExp`\n  object unusable, but the rest of your program should be fine.\n"
},
{
  "name": "pushlog-addon",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "background.js",
      "buglist-link.js",
      "content.js",
      "icons",
      "manifest.json"
    ]
  },
  "makefile": null,
  "readme": "# pushlog-addon\n\npushlog addon adds an item in context menu to get the pushlog of the selected buildid.\n\n## Bugs\n\nhttps://github.com/mozilla/pushlog-addon/issues/new\n\n## Contact\n\nEmail: calixte@mozilla.com\n"
},
{
  "name": "doh-rollout",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "docs",
      "src"
    ],
    "/docs": [
      "code_of_conduct.md",
      "contributing.md",
      "security.md"
    ]
  },
  "makefile": null,
  "readme": "# This repo hosts strings used by the DNS Over HTTPS doorhanger UI shipping in Firefox.\n\nThis is a temporary setup until the strings are migrated to conform to standard mozilla-central protocol.\n\n-------\n\n# Former README below for archival purposes\n\n***Note - This repository has been archived as of Tuesday, January 7th, 2020. The DNS-over-HTTPS extension [has been migrated](https://searchfox.org/mozilla-release/source/browser/extensions/doh-rollout) to the Firefox browser as of [Release 72](https://www.mozilla.org/en-US/firefox/72.0/releasenotes/). Please file any issues regarding this extension through [Bugzilla](https://bugzilla.mozilla.org/enter_bug.cgi?format=__default__&blocked=1598218&product=Firefox&component=Security).***\n\n## DoH Rollout\n\nThis is a WebExtension + Experimental API to gradually roll out [DNS-over-HTTP](https://support.mozilla.org/en-US/kb/firefox-dns-over-https) to Firefox users.\n\n## How to install WebExtension\n\n1. Install web-ext `npm install -g web-ext`\n2. Install the dependencies `npm install`\n3. Build the addon `npm run build`\n\n## How to run WebExtension\n1. `web-ext run --verbose -f Nightly`\n\nOR\n\n1. Run a non-release build (Nightly or Aurora) version 61+.\n2. Set `extensions.legacy.enabled` to true in about:config\n3. Navigate to `about:debugging`, choose\n   \"Load Temporary Add-on\" and select a file from this project:\n   - Select `manifest.json`\n   - Select `src/web-ext-artifacts/dns_over_https-${VERSION}.zip`\n\nYou should see a new entry in the list of extensions titled \"DoH Rollout\".\n\nTo see the banner [create a new boolean pref](https://support.mozilla.org/en-US/kb/about-config-editor-firefox#w_adding-changing-and-resetting-preferences) `doh-rollout.enabled` in [about:config](about:config) and set to `true`.\n\nThis will run heuristics against your network settings to determine if DoH can safely be enabled. If it can, it will trigger a doorhanger notification announcing that is has been enabled.\n\nTo check if DoH is turned on, check the value of pref `network.trr.mode`. If it is enabled, it will be set to `2`.\n\n[Telemetry][telemetry-schema-link] will be sent each time heuristics are evaluated.\n\n## Dependencies\n\n- [web-ext](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Getting_started_with_web-ext)\n- Firefox 61+\n\n## Contributing\n\nSee the [guidelines][contributing-link] for contributing to this project \u2013 including on how best to report an issue with this project.\n\nThis project is governed by a [Code Of Conduct][coc-link].\n\nTo disclose potential a security vulnerability please see our [security][security-link] documentation.\n\n## [License][license-link]\n\nThis module is licensed under the [Mozilla Public License, version 2.0][license-link].\n\n[docs-link]: docs/\n[contributing-link]: docs/contributing.md\n[coc-link]: docs/code_of_conduct.md\n[security-link]: docs/security.md\n[license-link]: /LICENSE\n[telemetry-schema-link]: docs/telemetry.md\n\n## Press Releases\n\n### Related press releases:\n\n- [What\u2019s next in making Encrypted DNS-over-HTTPS the Default](https://blog.mozilla.org/futurereleases/2019/09/06/whats-next-in-making-dns-over-https-the-default/) _September 9, 2019_\n- [DNS-over-HTTPS (DoH) Update \u2013 Detecting Managed Networks and User Choice ](https://blog.mozilla.org/futurereleases/2019/07/31/dns-over-https-doh-update-detecting-managed-networks-and-user-choice/) _July 31, 2019_\n- [DNS-over-HTTPS (DoH) Update \u2013 Recent Testing Results and Next Steps ](https://blog.mozilla.org/futurereleases/2019/04/02/dns-over-https-doh-update-recent-testing-results-and-next-steps/) _April 2, 2019_\n- [Next Steps in DNS-over-HTTPS Testing ](https://blog.mozilla.org/futurereleases/2018/11/27/next-steps-in-dns-over-https-testing/) _November 11, 2018_\n- [DNS over HTTPS (DoH) \u2013 Testing on Beta ](https://blog.mozilla.org/futurereleases/2018/09/13/dns-over-https-doh-testing-on-beta/) _September 13, 2018_\n"
},
{
  "name": "dummytracker",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "cookie_access_test.html",
      "index.html",
      "set_cookie.html",
      "test_blocked.png",
      "test_not_blocked.png",
      "tracker.js"
    ]
  },
  "makefile": null,
  "readme": "# dummytracker.org\n\nThis repo contains the code behind <https://dummytracker.org>.\n"
},
{
  "name": "private-distributed-learning",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# private-distributed-learning"
},
{
  "name": "marian-examples",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE.md",
      "README.md",
      "tools",
      "training-basics-sentencepiece",
      "training-basics",
      "transformer",
      "translating-amun",
      "wmt2017-transformer",
      "wmt2017-uedin"
    ]
  },
  "makefile": null,
  "readme": "# Marian examples\n\nExamples, tutorials and use cases for the Marian toolkit.\n\nMore information on https://marian-nmt.github.io\n\nList of examples:\n* `translating-amun` -- examples for translating with Amun\n* `training-basics` -- the complete example for training a WMT16-scale model\n* `training-basics-sentencepiece` -- as `training-basics`, but uses built-in SentencePiece for data processing, requires Marian v1.7+\n* `transformer` -- scripts for training the transformer model\n* `wmt2017-uedin` -- scripts for building a WMT2017-grade model for en-de based on Edinburgh's WMT2017 submission\n* `wmt2017-transformer` -- scripts for building a better than WMT2017-grade model for en-de, beating WMT2017 submission by 1.2 BLEU\n\n## Usage\n\nFirst download common tools:\n\n    cd tools\n    make all\n    cd ..\n\nNext, go to the chosen directory and run `run-me.sh`, e.g.:\n\n    cd training-basics\n    ./run-me.sh\n\nThe README file in each directory provides more detailed description.\n\n## Acknowledgements\n\nThe development of Marian received funding from the European Union's\n_Horizon 2020 Research and Innovation Programme_ under grant agreements\n688139 ([SUMMA](http://www.summa-project.eu); 2016-2019),\n645487 ([Modern MT](http://www.modernmt.eu); 2015-2017),\n644333 ([TraMOOC](http://tramooc.eu/); 2015-2017),\n644402 ([HiML](http://www.himl.eu/); 2015-2017),\nthe Amazon Academic Research Awards program, and\nthe World Intellectual Property Organization.\n\nThis software contains source code provided by NVIDIA Corporation.\n\n"
},
{
  "name": "misp-integrations",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "eis2zeek.py",
      "eis2zeek.yml",
      "et2zeek.py",
      "et2zeek.yml",
      "misptozeek.yml",
      "otx2zeek.map.yml",
      "otx2zeek.py",
      "otx2zeek.yml",
      "whitelist.yml"
    ]
  },
  "makefile": null,
  "readme": "# misp-integrations\nThe MISP integration code\n"
},
{
  "name": "sacreBLEU",
  "files": {
    "/": [
      ".gitignore",
      "CHANGELOG.md",
      "LICENSE.txt",
      "README.md",
      "pytest.ini",
      "sacrebleu.py",
      "setup.cfg",
      "setup.py",
      "test.sh",
      "test"
    ]
  },
  "makefile": null,
  "readme": "SacreBLEU ([Post, 2018](https://arxiv.org/abs/1804.08771)) provides hassle-free computation of shareable, comparable, and reproducible BLEU scores.\nInspired by Rico Sennrich's `multi-bleu-detok.perl`, it produces the official WMT scores but works with plain text.\nIt also knows all the standard test sets and handles downloading, processing, and tokenization for you.\n\nWhy use this version of BLEU?\n- It automatically downloads common WMT test sets and processes them to plain text\n- It produces a short version string that facilitates cross-paper comparisons\n- It properly computes scores on detokenized outputs, using WMT ([Conference on Machine Translation](http://statmt.org/wmt17)) standard tokenization\n- It produces the same values as official script (`mteval-v13a.pl`) used by WMT\n- It outputs the BLEU score without the comma, so you don't have to remove it with `sed` (Looking at you, `multi-bleu.perl`)\n\n# QUICK START\n\nInstall the Python module (Python 3 only)\n\n    pip3 install sacrebleu\n\nThis installs a shell script, `sacrebleu`.\n(You can also directly run the shell script `sacrebleu.py` in the source repository).\n\nGet a list of available test sets:\n\n    sacrebleu\n\nDownload the source for one of the pre-defined test sets:\n\n    sacrebleu -t wmt14 -l de-en --echo src > wmt14-de-en.src\n\n(you can also use long parameter names for readability):\n\n    sacrebleu --test-set wmt14 --langpair de-en --echo src > wmt14-de-en.src\n\nAfter tokenizing, translating, and detokenizing it, you can score your decoder output easily:\n\n    cat output.detok.txt | sacrebleu -t wmt14 -l de-en\n\nSacreBLEU knows about common WMT test sets, but you can also use it to score system outputs with arbitrary references.\nIt also works in backwards compatible model where you manually specify the reference(s), similar to the format of `multi-bleu.txt`:\n\n    cat output.detok.txt | sacrebleu REF1 [REF2 ...]\n\nNote that the system output and references will all be tokenized internally.\n\nSacreBLEU generates version strings like the following.\nPut them in a footnote in your paper!\nUse `--short` for a shorter hash if you like.\n\n    BLEU+case.mixed+lang.de-en+test.wmt17 = 32.97 66.1/40.2/26.6/18.1 (BP = 0.980 ratio = 0.980 hyp_len = 63134 ref_len = 64399)\n\n# MOTIVATION\n\nComparing BLEU scores is harder than it should be.\nEvery decoder has its own implementation, often borrowed from Moses, but maybe with subtle changes.\nMoses itself has a number of implementations as standalone scripts, with little indication of how they differ (note: they mostly don't, but `multi-bleu.pl` expects tokenized input).\nDifferent flags passed to each of these scripts can produce wide swings in the final score.\nAll of these may handle tokenization in different ways.\nOn top of this, downloading and managing test sets is a moderate annoyance.\nSacre bleu!\nWhat a mess.\n\nSacreBLEU aims to solve these problems by wrapping the original Papineni reference implementation together with other useful features.\nThe defaults are set the way that BLEU should be computed, and furthermore, the script outputs a short version string that allows others to know exactly what you did.\nAs an added bonus, it automatically downloads and manages test sets for you, so that you can simply tell it to score against 'wmt14', without having to hunt down a path on your local file system.\nIt is all designed to take BLEU a little more seriously.\nAfter all, even with all its problems, BLEU is the default and---admit it---well-loved metric of our entire research community.\nSacre BLEU.\n\n# LICENSE\n\nSacreBLEU is licensed under the Apache 2.0 License.\n\n# CREDITS\n\nThis was all Rico Sennrich's idea.\nOriginally written by Matt Post.\nThe official version can be found at https://github.com/awslabs/sockeye/tree/master/sockeye_contrib/sacrebleu.\n"
},
{
  "name": "subword-nmt",
  "files": {
    "/": [
      ".github",
      ".gitignore",
      "CHANGELOG.md",
      "LICENSE",
      "README.md",
      "apply_bpe.py",
      "get_vocab.py",
      "learn_bpe.py",
      "learn_joint_bpe_and_vocab.py",
      "setup.py",
      "subword_nmt"
    ],
    "/.github": [
      "workflows"
    ]
  },
  "makefile": null,
  "readme": "Subword Neural Machine Translation\n==================================\n\nThis repository contains preprocessing scripts to segment text into subword\nunits. The primary purpose is to facilitate the reproduction of our experiments\non Neural Machine Translation with subword units (see below for reference).\n\nINSTALLATION\n------------\n\ninstall via pip (from PyPI):\n\n    pip install subword-nmt\n\ninstall via pip (from Github):\n\n    pip install https://github.com/rsennrich/subword-nmt/archive/master.zip\n\nalternatively, clone this repository; the scripts are executable stand-alone.\n\n\nUSAGE INSTRUCTIONS\n------------------\n\nCheck the individual files for usage instructions.\n\nTo apply byte pair encoding to word segmentation, invoke these commands:\n\n    subword-nmt learn-bpe -s {num_operations} < {train_file} > {codes_file}\n    subword-nmt apply-bpe -c {codes_file} < {test_file} > {out_file}\n\nTo segment rare words into character n-grams, do the following:\n\n    subword-nmt get-vocab --train_file {train_file} --vocab_file {vocab_file}\n    subword-nmt segment-char-ngrams --vocab {vocab_file} -n {order} --shortlist {size} < {test_file} > {out_file}\n\nThe original segmentation can be restored with a simple replacement:\n\n    sed -r 's/(@@ )|(@@ ?$)//g'\n\nIf you cloned the repository and did not install a package, you can also run the individual commands as scripts:\n\n    ./subword_nmt/learn_bpe.py -s {num_operations} < {train_file} > {codes_file}\n\nBEST PRACTICE ADVICE FOR BYTE PAIR ENCODING IN NMT\n--------------------------------------------------\n\nWe found that for languages that share an alphabet, learning BPE on the\nconcatenation of the (two or more) involved languages increases the consistency\nof segmentation, and reduces the problem of inserting/deleting characters when\ncopying/transliterating names.\n\nHowever, this introduces undesirable edge cases in that a word may be segmented\nin a way that has only been observed in the other language, and is thus unknown\nat test time. To prevent this, `apply_bpe.py` accepts a `--vocabulary` and a\n`--vocabulary-threshold` option so that the script will only produce symbols\nwhich also appear in the vocabulary (with at least some frequency).\n\nTo use this functionality, we recommend the following recipe (assuming L1 and L2\nare the two languages):\n\nLearn byte pair encoding on the concatenation of the training text, and get resulting vocabulary for each:\n\n    cat {train_file}.L1 {train_file}.L2 | subword-nmt learn-bpe -s {num_operations} -o {codes_file}\n    subword-nmt apply-bpe -c {codes_file} < {train_file}.L1 | subword-nmt get-vocab > {vocab_file}.L1\n    subword-nmt apply-bpe -c {codes_file} < {train_file}.L2 | subword-nmt get-vocab > {vocab_file}.L2\n\nmore conventiently, you can do the same with with this command:\n\n    subword-nmt learn-joint-bpe-and-vocab --input {train_file}.L1 {train_file}.L2 -s {num_operations} -o {codes_file} --write-vocabulary {vocab_file}.L1 {vocab_file}.L2\n\nre-apply byte pair encoding with vocabulary filter:\n\n    subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L1 --vocabulary-threshold 50 < {train_file}.L1 > {train_file}.BPE.L1\n    subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L2 --vocabulary-threshold 50 < {train_file}.L2 > {train_file}.BPE.L2\n\nas a last step, extract the vocabulary to be used by the neural network. Example with Nematus:\n\n    nematus/data/build_dictionary.py {train_file}.BPE.L1 {train_file}.BPE.L2\n\n[you may want to take the union of all vocabularies to support multilingual systems]\n\nfor test/dev data, re-use the same options for consistency:\n\n    subword-nmt apply-bpe -c {codes_file} --vocabulary {vocab_file}.L1 --vocabulary-threshold 50 < {test_file}.L1 > {test_file}.BPE.L1\n\nADVANCED FEATURES\n-----------------\n\nOn top of the basic BPE implementation, this repository supports:\n\n- BPE dropout (Provilkov, Emelianenko and Voita, 2019): https://arxiv.org/abs/1910.13267\n  use the argument `--dropout 0.1` for `subword-nmt apply-bpe` to randomly drop out possible merges.\n  Doing this on the training corpus can improve quality of the final system; at test time, use BPE without dropout\n\n- support for glossaries:\n  use the argument `--glossaries` for `subword-nmt apply-bpe` to provide a list of words and/or regular expressions\n  that should always be passed to the output without subword segmentation\n\nPUBLICATIONS\n------------\n\nThe segmentation methods are described in:\n\nRico Sennrich, Barry Haddow and Alexandra Birch (2016):\n    Neural Machine Translation of Rare Words with Subword Units\n    Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016). Berlin, Germany.\n\nHOW IMPLEMENTATION DIFFERS FROM Sennrich et al. (2016)\n------------------------------------------------------\n\nThis repository implements the subword segmentation as described in Sennrich et al. (2016),\nbut since version 0.2, there is one core difference related to end-of-word tokens.\n\nIn Sennrich et al. (2016), the end-of-word token `</w>` is initially represented as a separate token, which can be merged with other subwords over time:\n\n```\nu n d </w>\nf u n d </w>\n```\n\nSince 0.2, end-of-word tokens are initially concatenated with the word-final character:\n\n```\nu n d</w>\nf u n d</w>\n```\n\nThe new representation ensures that when BPE codes are learned from the above examples and then applied to new text, it is clear that a subword unit `und` is unambiguously word-final, and `un` is unambiguously word-internal, preventing the production of up to two different subword units from each BPE merge operation.\n\n`apply_bpe.py` is backward-compatible and continues to accept old-style BPE files. New-style BPE files are identified by having the following first line: `#version: 0.2`\n\nACKNOWLEDGMENTS\n---------------\nThis project has received funding from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland, and from the European Union\u2019s Horizon 2020 research and innovation programme under grant agreement 645452 (QT21).\n"
},
{
  "name": "moses-scripts",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# moses-scripts\n\nA number of preprocessing scripts extracted from Moses (https://github.com/moses-smt/mosesdecoder)\n"
},
{
  "name": "scorertool",
  "files": {
    "/": [
      ".compute",
      ".gitignore",
      ".idea",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bin",
      "oscarlm",
      "requirements.txt"
    ]
  },
  "makefile": null,
  "readme": "# OscarLM\nGenerate language models from [OSCAR](https://traces1.inria.fr/oscar/) corpora.\n"
},
{
  "name": "fix-the-internet-lab",
  "files": {
    "/": [
      "fx-browser.png",
      "index.html",
      "style.css"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "FBGEMM",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      ".gitmodules",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "bench",
      "cmake",
      "include",
      "src",
      "test",
      "third_party"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# FBGEMM\n\n## Linux Build: [![CircleCI](https://circleci.com/gh/pytorch/FBGEMM.svg?style=svg)](https://circleci.com/gh/pytorch/FBGEMM)\n\nFBGEMM (Facebook GEneral Matrix Multiplication) is a low-precision,\nhigh-performance matrix-matrix multiplications and convolution library for\nserver-side inference.\n\nThe library provides efficient low-precision general matrix multiplication for\nsmall batch sizes and support for accuracy-loss minimizing techniques such as\nrow-wise quantization and outlier-aware quantization. FBGEMM also exploits\nfusion opportunities in order to overcome the unique challenges of matrix\nmultiplication at lower precision with bandwidth-bound operations.\n\nFBGEMM is used as a backend of Caffe2 and PyTorch quantized operators for x86 machines:\n* Caffe2: https://github.com/pytorch/pytorch/tree/master/caffe2/quantization/server\n* PyTorch: https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native/quantized/cpu\n\n## Examples\n\nThe tests (in test folder) and benchmarks (in bench folder) are some great\nexamples of using FBGEMM. For instance, SpMDMTest test in\ntest/PackedRequantizeAcc16Test.cc shows how to combine row offset calculations\nwith packing of A (PackAWithRowOffset), how to pack B matrix (PackBMatrix) and\nconstruct output pipeline (sparse\\_matrix\\*dense\\_matrix --> requantization -->\nnop) fused with inner GEMM macro kernel.\n\n## Build Notes\nFBGEMM uses the standard CMAKE-based build flow.\n\n### Dependencies\nFBGEMM requires gcc 4.9+ and a CPU with support for avx2 instruction set or\nhigher. It's been tested on Mac OS X and Linux.\n\n+ ###### asmjit\nWith inner kernels, FBGEMM takes a \u201cone size doesn't fit all\u201d approach, so the\nimplementation dynamically generates efficient matrix-shape specific vectorized\ncode using a third-party library called [asmjit][1]. **asmjit is required** to\nbuild FBGEMM.\n\n+ ###### cpuinfo\nFBGEMM detects CPU instruction set support at runtime using cpuinfo library and\ndispatches optimized kernels for the detected instruction set. Therefore,\n**cpuinfo is required** to detect CPU type.\n\n+ ###### googletest\ngoogletest is required to build and run FBGEMM's tests. **googletest is not\nrequired** if you don't want to run FBGEMM tests. By default, building of tests\nis **on**. Turn it off by setting FBGEMM\\_BUILD\\_TESTS to off.\n\nYou can download [asmjit][1], [cpuinfo][2], [googletest][3] and set\nASMJIT\\_SRC\\_DIR, CPUINFO\\_SRC\\_DIR, GOOGLETEST\\_SOURCE\\_DIR respectively for\ncmake to find these libraries. If any of these variables is not set, cmake will\nbuild the git submodules found in the third\\_party directory.\n\nFBGEMM, in general, does not have any dependency on Intel MKL. However, for\nperformance comparison, some benchmarks use MKL functions. If MKL is found or\nMKL path is provided with INTEL\\_MKL\\_DIR benchmarks are built with MKL and\nperformance numbers are reported for MKL functions as well. However, if MKL is\nnot found, the benchmarks are not built.\n\nGeneral build instructions are as follows:\n\n```\ngit clone --recursive https://github.com/pytorch/FBGEMM.git\ncd FBGEMM\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\nmkdir build && cd build\ncmake ..\nmake\n```\n\nTo run the tests after building FBGEMM (if tests are built), use the following\ncommand:\n```\nmake test\n```\n\n## Installing  FBGEMM\n```\nmake install\n```\n\n## How FBGEMM works\nFor a high-level overview, design philosophy and brief descriptions of various\nparts of FBGEMM please see [our blog][4].\n\n## Full documentation\nWe have extensively used comments in our source files. The best and up-do-date\ndocumentation is available in the source files.\n\n## Join the FBGEMM community\nSee the [`CONTRIBUTING`](CONTRIBUTING.md) file for how to help out.\n\n## License\nFBGEMM is BSD licensed, as found in the [`LICENSE`](LICENSE) file.\n\n\n[1]:https://github.com/asmjit/asmjit\n[2]:https://github.com/pytorch/cpuinfo\n[3]:https://github.com/google/googletest\n[4]:https://code.fb.com/ml-applications/fbgemm\n"
},
{
  "name": "nccl",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE.txt",
      "Makefile",
      "README.md",
      "ext-net",
      "makefiles",
      "pkg",
      "src"
    ]
  },
  "makefile": "#\n# Copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n#\n# See LICENSE.txt for license information\n#\n.PHONY : all clean\n\ndefault : src.build\ninstall : src.install\nBUILDDIR ?= $(abspath ./build)\nABSBUILDDIR := $(abspath $(BUILDDIR))\nTARGETS := src pkg\nclean: ${TARGETS:%=%.clean}\ntest.build: src.build\nLICENSE_FILES := LICENSE.txt\nLICENSE_TARGETS := $(LICENSE_FILES:%=$(BUILDDIR)/%)\nlic: $(LICENSE_TARGETS)\n\n${BUILDDIR}/%.txt: %.txt\n\t@printf \"Copying    %-35s > %s\\n\" $< $@\n\tmkdir -p ${BUILDDIR}\n\tcp $< $@\n\nsrc.%:\n\t${MAKE} -C src $* BUILDDIR=${ABSBUILDDIR}\n\npkg.%:\n\t${MAKE} -C pkg $* BUILDDIR=${ABSBUILDDIR}\n\npkg.debian.prep: lic\npkg.txz.prep: lic\n",
  "readme": "# NCCL\n\nOptimized primitives for collective multi-GPU communication.\n\n## Introduction\n\nNCCL (pronounced \"Nickel\") is a stand-alone library of standard collective communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, and reduce-scatter. It has been optimized to achieve high bandwidth on platforms using PCIe, NVLink, NVswitch, as well as networking using InfiniBand Verbs or TCP/IP sockets. NCCL supports an arbitrary number of GPUs installed in a single node or across multiple nodes, and can be used in either single- or multi-process (e.g., MPI) applications.\n\nFor more information on NCCL usage, please refer to the [NCCL documentation](https://docs.nvidia.com/deeplearning/sdk/nccl-developer-guide/index.html).\n\n## What's inside\n\nAt present, the library implements the following collectives operations:\n\n- all-reduce\n- all-gather\n- reduce-scatter\n- reduce\n- broadcast\n\nThese operations are implemented using ring algorithms and have been optimized for throughput and latency. For best performance, small operations can be either batched into larger operations or aggregated through the API.\n\n## Requirements\n\nNCCL requires at least CUDA 7.0 and Kepler or newer GPUs. For PCIe based platforms, best performance is achieved when all GPUs are located on a common PCIe root complex, but multi-socket configurations are also supported.\n\n## Build\n\nNote: the official and tested builds of NCCL can be downloaded from: https://developer.nvidia.com/nccl. You can skip the following build steps if you choose to use the official builds.\n\nTo build the library :\n\n```shell\n$ cd nccl\n$ make -j src.build\n```\n\nIf CUDA is not installed in the default /usr/local/cuda path, you can define the CUDA path with :\n\n```shell\n$ make src.build CUDA_HOME=<path to cuda install>\n```\n\nNCCL will be compiled and installed in `build/` unless `BUILDDIR` is set.\n\nBy default, NCCL is compiled for all supported architectures. To accelerate the compilation and reduce the binary size, consider redefining `NVCC_GENCODE` (defined in `makefiles/common.mk`) to only include the architecture of the target platform :\n```shell\n$ make -j src.build NVCC_GENCODE=\"-gencode=arch=compute_70,code=sm_70\"\n```\n\n## Install\n\nTo install NCCL on the system, create a package then install it as root.\n\nDebian/Ubuntu :\n```shell\n$ # Install tools to create debian packages\n$ sudo apt install build-essential devscripts debhelper fakeroot\n$ # Build NCCL deb package\n$ make pkg.debian.build\n$ ls build/pkg/deb/\n```\n\nRedHat/CentOS :\n```shell\n$ # Install tools to create rpm packages\n$ sudo yum install rpm-build rpmdevtools\n$ # Build NCCL rpm package\n$ make pkg.redhat.build\n$ ls build/pkg/rpm/\n```\n\nOS-agnostic tarball :\n```shell\n$ make pkg.txz.build\n$ ls build/pkg/txz/\n```\n\n## Tests\n\nTests for NCCL are maintained separately at https://github.com/nvidia/nccl-tests.\n\n```shell\n$ git clone https://github.com/NVIDIA/nccl-tests.git\n$ cd nccl-tests\n$ make\n$ ./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>\n```\n\n## Copyright\n\nAll source code and accompanying documentation is copyright (c) 2015-2019, NVIDIA CORPORATION. All rights reserved.\n"
},
{
  "name": "sentencepiece",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CMakeLists.txt",
      "CMakeLists_minexport.txt",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "VERSION",
      "appveyor.yml",
      "config.h.in",
      "data",
      "doc",
      "python",
      "sentencepiece.pc.in",
      "src",
      "tensorflow",
      "test.bat",
      "test.sh",
      "test_minexport.bat",
      "third_party"
    ]
  },
  "makefile": null,
  "readme": "# SentencePiece\n\n[![Build Status](https://travis-ci.org/google/sentencepiece.svg?branch=master)](https://travis-ci.org/google/sentencepiece)\n[![Build status](https://ci.appveyor.com/api/projects/status/vxoub3qx4fwpysyq?svg=true)](https://ci.appveyor.com/project/taku910/sentencepiece)\n[![Coverage Status](https://coveralls.io/repos/github/google/sentencepiece/badge.svg?branch=master)](https://coveralls.io/github/google/sentencepiece?branch=master)\n[![GitHub Issues](https://img.shields.io/github/issues/google/sentencepiece.svg)](https://github.com/google/sentencepiece/issues)\n[![PyPI version](https://badge.fury.io/py/sentencepiece.svg)](https://badge.fury.io/py/sentencepiece)\n[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg)](CONTRIBUTING.md)\n[![License](https://img.shields.io/badge/License-Apache%202.0-brightgreen.svg)](https://opensource.org/licenses/Apache-2.0)\n\nSentencePiece is an unsupervised text tokenizer and detokenizer mainly for\nNeural Network-based text generation systems where the vocabulary size\nis predetermined prior to the neural model training. SentencePiece implements\n**subword units** (e.g., **byte-pair-encoding (BPE)** [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)]) and \n**unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)])\nwith the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\n\n**This is not an official Google product.**\n\n## Technical highlights\n- **Purely data driven**: SentencePiece trains tokenization and detokenization\n  models from sentences. Pre-tokenization ([Moses tokenizer](https://github.com/moses-smt/mosesdecoder/blob/master/scripts/tokenizer/tokenizer.perl)/[MeCab](http://taku910.github.io/mecab/)/[KyTea](http://www.phontron.com/kytea/)) is not always required.\n- **Language independent**: SentencePiece treats the sentences just as sequences of Unicode characters. There is no language-dependent logic.\n- **Multiple subword algorithms**: **BPE**  [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)] and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)] are supported.\n- **Subword regularization**: SentencePiece implements subword sampling for [subword regularization](https://arxiv.org/abs/1804.10959) which helps to improve the robustness and accuracy of NMT models.\n- **Fast and lightweight**: Segmentation speed is around 50k sentences/sec, and memory footprint is around 6MB.\n- **Self-contained**: The same tokenization/detokenization is obtained as long as the same model file is used.\n- **Direct vocabulary id generation**: SentencePiece manages vocabulary to id mapping and can directly generate vocabulary id sequences from raw sentences.\n- **NFKC-based normalization**: SentencePiece performs NFKC-based text normalization.\n\n## Comparisons with other implementations\n|Feature|SentencePiece|[subword-nmt](https://github.com/rsennrich/subword-nmt)|[WordPiece](https://arxiv.org/pdf/1609.08144.pdf)|\n|:---|:---:|:---:|:---:|\n|Supported algorithm|BPE, unigram, char, word|BPE|BPE*|\n|OSS?|Yes|Yes|Google internal|\n|Subword regularization|[Yes](#subword-regularization)|No|No|\n|Python Library (pip)|[Yes](python/README.md)|No|N/A|\n|C++ Library|[Yes](doc/api.md)|No|N/A|\n|Pre-segmentation required?|[No](#whitespace-is-treated-as-a-basic-symbol)|Yes|Yes|\n|Customizable normalization (e.g., NFKC)|[Yes](doc/normalization.md)|No|N/A|\n|Direct id generation|[Yes](#end-to-end-example)|No|N/A|\n\nNote that BPE algorithm used in WordPiece is slightly different from the original BPE.\n\n## Overview\n### What is SentencePiece?\nSentencePiece is a re-implementation of **sub-word units**, an effective way to alleviate the open vocabulary\n  problems in neural machine translation. SentencePiece supports two segmentation algorithms, **byte-pair-encoding (BPE)** [[Sennrich et al.](http://www.aclweb.org/anthology/P16-1162)] and **unigram language model** [[Kudo.](https://arxiv.org/abs/1804.10959)]. Here are the high level differences from other implementations.\n\n#### The number of unique tokens is predetermined\nNeural Machine Translation models typically operate with a fixed\nvocabulary. Unlike most unsupervised word segmentation algorithms, which\nassume an infinite vocabulary, SentencePiece trains the segmentation model such\nthat the final vocabulary size is fixed, e.g., 8k, 16k, or 32k.\n\nNote that SentencePices specifies the final vocabulary size for training, which is different from \n[subword-nmt](https://github.com/rsennrich/subword-nmt) that uses the number of merge operations.\nThe number of merge operations is a BPE-specific parameter and not applicable to other segmentation algorithms, including unigram, word and character.\n\n#### Trains from raw sentences\nPrevious sub-word implementations assume that the input sentences are pre-tokenized. This constraint was required for efficient training, but makes the preprocessing complicated as we have to run language dependent tokenizers in advance.\nThe implementation of SentencePiece is fast enough to train the model from raw sentences. This is useful for training the tokenizer and detokenizer for Chinese, Japanese and Korean where no explicit spaces exist between words.\n\n#### Whitespace is treated as a basic symbol\nThe first step of Natural Language processing is text tokenization. For\nexample, a standard English tokenizer would segment the text \"Hello world.\" into the\nfollowing three tokens.\n\n> [Hello] [World] [.]\n\nOne observation is that the original input and tokenized sequence are **NOT\nreversibly convertible**. For instance, the information that is no space between\n\u201cWorld\u201d and \u201c.\u201d is dropped from the tokenized sequence, since e.g., `Tokenize(\u201cWorld.\u201d) == Tokenize(\u201cWorld .\u201d)`\n\nSentencePiece treats the input text just as a sequence of Unicode characters. Whitespace is also handled as a normal symbol. To handle the whitespace as a basic token explicitly, SentencePiece first escapes the whitespace with a meta symbol \"\u2581\" (U+2581) as follows.\n\n> Hello\u2581World.\n\nThen, this text is segmented into small pieces, for example:\n\n> [Hello] [\u2581Wor] [ld] [.]\n\nSince the whitespace is preserved in the segmented text, we can detokenize the text without any ambiguities.\n\n```\n  detokenized = ''.join(pieces).replace('_', ' ')\n```\n\nThis feature makes it possible to perform detokenization without relying on language-specific resources.\n\nNote that we cannot apply the same lossless conversions when splitting the\nsentence with standard word segmenters, since they treat the whitespace as a\nspecial symbol. Tokenized sequences do not preserve the necessary information to restore the original sentence.\n\n* (en) Hello world.   \u2192 [Hello] [World] [.]   \\(A space between Hello and World\\)\n* (ja) \u3053\u3093\u306b\u3061\u306f\u4e16\u754c\u3002  \u2192 [\u3053\u3093\u306b\u3061\u306f] [\u4e16\u754c] [\u3002] \\(No space between \u3053\u3093\u306b\u3061\u306f and \u4e16\u754c\\)\n\n#### Subword regularization\nSubword regularization [[Kudo.](https://arxiv.org/abs/1804.10959)] is a simple regularization method\nthat virtually augments training data with on-the-fly subword sampling, which helps to improve the accuracy as well as robustness of NMT models.\n\nTo enable subword regularization, you would like to integrate SentencePiece library \n([C++](doc/api.md#sampling-subword-regularization)/[Python](python/README.md)) into the NMT system to sample one segmentation for each parameter update, which is different from the standard off-line data preparations. Here's the example of [Python library](python/README.md). You can find that 'New York' is segmented differently on each ``SampleEncode`` call. The details of sampling parameters are found in [sentencepiece_processor.h](src/sentencepiece_processor.h).\n\n```\n>>> import sentencepiece as spm\n>>> s = spm.SentencePieceProcessor()\n>>> s.Load('spm.model')\n>>> for n in range(5):\n...     s.SampleEncodeAsPiece('New York', -1, 0.1)\n... \n['\u2581', 'N', 'e', 'w', '\u2581York']\n['\u2581', 'New', '\u2581York']\n['\u2581', 'New', '\u2581Y', 'o', 'r', 'k']\n['\u2581', 'New', '\u2581York']\n['\u2581', 'New', '\u2581York']\n```\n\n## Installation\n\n### Python module\nSentencePiece provides Python wrapper that supports both SentencePiece training and segmentation.\nYou can install Python binary package of SentencePiece with.\n\n```\n% pip install sentencepiece\n```\n\nFor more detail, see [Python module](python/README.md)\n\n### C++ (from source)\nThe following tools and libraries are required to build SentencePiece:\n\n* [cmake](https://cmake.org/)\n* C++11 compiler\n* [protobuf](https://github.com/google/protobuf) library\n* [gperftool](https://github.com/gperftools/gperftools) library (optional, 10-40% performance improvement can be obtained.)\n\nOn Ubuntu, autotools can be installed with apt-get:\n```\n% sudo apt-get install cmake pkg-config libprotobuf9v5 protobuf-compiler libprotobuf-dev libgoogle-perftools-dev \n```\nThe name of the protobuf library is different between ubuntu distros. Please enter appropriate command for your Ubuntu version.\n\nOn ubuntu 14.04 LTS (Trusty Tahr):\n```\n% sudo apt-get install libprotobuf8\n```\n\nOn ubuntu 16.04 LTS (Xenial Xerus):\n```\n% sudo apt-get install libprotobuf9v5\n```\n\nOn ubuntu 17.10 (Artful Aardvark) and Later:\n```\n% sudo apt-get install libprotobuf10\n```\n\nOn OSX, you can use brew:\n```\n% brew install protobuf cmake\n```\n\nIf want to use self-prepared protobuf library, specify protbof prefix before build:\n```\n% cmake .. -DCMAKE_PREFIX_PATH=<prefix_path_to_protobuf>\n```\n\n### Build and Install SentencePiece\n```\n% cd /path/to/sentencepiece\n% mkdir build\n% cd build\n% cmake ..\n% make -j $(nproc)\n% sudo make install\n% sudo ldconfig -v\n```\nOn OSX/macOS, replace the last command with the following: \n``% sudo update_dyld_shared_cache```\n\n## Usage instructions\n### Train SentencePiece Model\n```\n% spm_train --input=<input> --model_prefix=<model_name> --vocab_size=8000 --character_coverage=1.0 --model_type=<type>\n```\n* `--input`: one-sentence-per-line **raw** corpus file. No need to run\n  tokenizer, normalizer or preprocessor. By default, SentencePiece normalizes\n  the input with Unicode NFKC. You can pass a comma-separated list of files.\n* `--model_prefix`: output model name prefix. `<model_name>.model` and `<model_name>.vocab` are generated.\n* `--vocab_size`: vocabulary size, e.g., 8000, 16000, or 32000\n* `--character_coverage`: amount of characters covered by the model, good defaults are: `0.9995` for languages with rich character set like Japanse or Chinese and `1.0` for other languages with small character set.\n* `--model_type`: model type. Choose from `unigram` (default), `bpe`, `char`, or `word`. The input sentence must be pretokenized when using `word` type.\n\nNote that `spm_train` loads only the first `--input_sentence_size` sentences (default value is 10M).\n\nUse `--help` flag to display all parameters for training.\n\n### Encode raw text into sentence pieces/ids\n```\n% spm_encode --model=<model_file> --output_format=piece < input > output\n% spm_encode --model=<model_file> --output_format=id < input > output\n```\n\nUse `--extra_options` flag to insert the BOS/EOS markers or reverse the input sequence.\n```\n% spm_encode --extra_options=eos (add </s> only)\n% spm_encode --extra_options=bos:eos (add <s> and </s>)\n% spm_encode --extra_options=reverse:bos:eos (reverse input and add <s> and </s>)\n```\n\nSentencePiece supports nbest segmentation and segmentation sampling with `--output_format=(nbest|sample)_(piece|id)` flags.\n```\n% spm_encode --model=<model_file> --output_format=sample_piece --nbest_size=-1 --alpha=0.5 < input > output\n% spm_encode --model=<model_file> --output_format=nbest_id --nbest_size=10 < input > output\n```\n\n### Decode sentence pieces/ids into raw text\n```\n% spm_decode --model=<model_file> --input_format=piece < input > output\n% spm_decode --model=<model_file> --input_format=id < input > output\n```\nUse `--extra_options` flag to decode the text in reverse order.\n```\n% spm_decode --extra_options=reverse < input > output\n```\n\n### End-to-End Example\n```\n% spm_train --input=data/botchan.txt --model_prefix=m --vocab_size=1000\nunigram_model_trainer.cc(494) LOG(INFO) Starts training with :\ninput: \"../data/botchan.txt\"\n... <snip>\nunigram_model_trainer.cc(529) LOG(INFO) EM sub_iter=1 size=1100 obj=10.4973 num_tokens=37630 num_tokens/piece=34.2091\ntrainer_interface.cc(272) LOG(INFO) Saving model: m.model\ntrainer_interface.cc(281) LOG(INFO) Saving vocabs: m.vocab\n\n% echo \"I saw a girl with a telescope.\" | spm_encode --model=m.model\n\u2581I \u2581saw \u2581a \u2581girl \u2581with \u2581a \u2581 te le s c o pe .\n\n% echo \"I saw a girl with a telescope.\" | spm_encode --model=m.model --output_format=id\n9 459 11 939 44 11 4 142 82 8 28 21 132 6\n\n% echo \"9 459 11 939 44 11 4 142 82 8 28 21 132 6\" | spm_decode --model=m.model --input_format=id\nI saw a girl with a telescope.\n```\nYou can find that the original input sentence is restored from the vocabulary id sequence.\n\n### Export vocabulary list\n```\n% spm_export_vocab --model=<model_file> --output=<output file>\n```\n```<output file>``` stores a list of vocabulary and emission log probabilities. The vocabulary id corresponds to the line number in this file.\n\n### Redefine special meta tokens\n  By default, SentencePiece uses Unknown (&lt;unk&gt;), BOS (&lt;s&gt;) and EOS (&lt;/s&gt;) tokens which have the ids of 0, 1, and 2 respectively. We can redefine this mapping in the training phase as follows.\n  \n```\n% spm_train --bos_id=0 --eos_id=1 --unk_id=5 --input=... --model_prefix=... --character_coverage=... \n```\nWhen setting -1 id e.g., ```bos_id=-1```, this special token is disabled. Note that the unknow id cannot be disabled.  We can define an id for padding (&lt;pad&gt;) as ```--pad_id=3```. \u00a0\n\nIf you want to assign another special tokens, please see [Use custom symbols](doc/special_symbols.md).\n\n### Vocabulary restriction\n```spm_encode``` accepts a ```--vocabulary``` and a ```--vocabulary_threshold``` option so that ```spm_encode``` will only produce symbols which also appear in the vocabulary (with at least some frequency). The background of this feature is decribed in [subword-nmt page](https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-pair-encoding-in-nmt).\n\nThe usage is basically the same as that of ```subword-nmt```. Assming that L1 and L2 are the two languages (source/target languages), train the shared spm model, and get resulting vocabulary for each:\n\n```\n% cat {train_file}.L1 {train_file}.L2 | shuffle > train\n% spm_train --input=train --model_prefix=spm --vocab_size=8000 --character_coverage=0.9995\n% spm_encode --model=spm.model --generate_vocabulary < {train_file}.L1 > {vocab_file}.L1\n% spm_encode --model=spm.model --generate_vocabulary < {train_file}.L2 > {vocab_file}.L2\n```\n\n```shuffle``` command is used just in case because ```spm_train``` loads the first 10M lines of corpus by default.\n\n\nThen segment train/test corpus with ```--vocabulary``` option\n```\n% spm_encode --model=spm.model --vocabulary={vocab_file}.L1 --vocabulary_threshold=50 < {test_file}.L1 > {test_file}.seg.L1\n% spm_encode --model=spm.model --vocabulary={vocab_file}.L2 --vocabulary_threshold=50 < {test_file}.L2 > {test_file}.seg.L2\n```\n\n## Advanced topics\n\n* [SentencePiece Experiments](doc/experiments.md)\n* [SentencePieceProcessor C++ API](doc/api.md)\n* [Use custom text normalization rules](doc/normalization.md)\n* [Use custom symbols](doc/special_symbols.md)\n* [Segmentation and training algorithms in detail]\n"
},
{
  "name": "omniscient",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "assets",
      "manifest.json",
      "translations"
    ]
  },
  "makefile": null,
  "readme": "# omniscient\nA custom Zendesk app that will provide subscription data in relation to help desk tickets.\n\n## Expected Behavior\nThis app determines if a ticket is a subscription services ticket by checking the form id and the ticket tags. If the \nform id matches the Subscription Services form or the 'subscription_services' tag is used the app will attempt to load\nthe user's subscription information. If the app determines that the ticket is not a subscription services ticket, a\nmessage will display explaining that the ticket is not of an applicable type.\n\nThe app will then check to see that the ticket requester has a user id set. If the user id is not set, the app will\ndisplay text describing that the user object does not have an id associated with it. If the user id is found, the app\nwill then attempt to load the subscription data for the user.\n\nIf there is an error fetching the user's subscription data, a message will display describing that the app was unable to\nload subscription data. Otherwise, the app will display any subscription data for the user or indicate that the user\ndoes not have any subscription data to display.\n\n## Local Development\n\nBefore beginning development, you will need to install Zendesk App Tools (ZAT). Installation instructions can be followed\nhere: https://develop.zendesk.com/hc/en-us/articles/360001075048-Installing-and-using-the-Zendesk-apps-tools\n\nFrom within a terminal, you can run the app locally and test the app within the Zendesk instance.\n\nTo start the application locally execute `zat server` from the application's root directory.\n\nAs you start the server, you will be prompted to provide configuration data - these settings are defined within\n`manifest.json`.\n\nOnce the server is running, navigate to a ticket on the Zendesk instance and append `?zat=true` to the end of the url \nand refresh.\n\nYou may have to set your browser to unblock content on the page to view the app. The application will appear on the \nright-hand side of the ticket. Once it is unblocked, the app may be hidden: there is a button `Apps` (located on the upper \nright-hand side of the page) that will hide/show the applications that are loaded.\n\nAs you make changes to the application, you can use the \"reload all apps\" button to reload the application. This is a\nsmall button with a circular arrow located above the application on the right.\n\n## Zendesk Installation\nOnce you are ready to load the app into Zendesk, you will first need to prepare the application. From the application \nroot directory execute the following:\n* `zat validate` to validate the application, if it fails validation, fix any reported errors and run the validation \ncommand again.\n* `zat package` to package up the application into a zip file. The console output will give you the location of the zip\nfile.\n\nOnce you have the application packaged up, you are ready to install the application on Zendesk.\n\n* In Zendesk Support, click the Admin icon in the sidebar, then select Manage from the Apps category.\n* Click Upload Private App in the upper-right corner of the page.\n* Enter the name of your app.\n* Click Choose File and select the zip file of your app.\n* Click Upload to upload the app to Zendesk Support.\n* When prompted, click Install.\n\nAfter the application has been installed, the configurations can be updated from the Apps Manager.\n"
},
{
  "name": "marian",
  "files": {
    "/": [
      ".clang-format",
      ".gitattributes",
      ".gitignore",
      ".gitmodules",
      "CHANGELOG.md",
      "CMakeLists.txt",
      "CMakeSettings.json",
      "CONTRIBUTING.md",
      "Doxyfile.in",
      "LICENSE.md",
      "README.md",
      "VERSION",
      "cmake",
      "contrib",
      "doc",
      "examples",
      "regression-tests",
      "scripts",
      "src",
      "vs"
    ]
  },
  "makefile": null,
  "readme": "Marian\n======\n\n[![Build Status CUDA 9](https://img.shields.io/jenkins/s/http/vali.inf.ed.ac.uk/jenkins/view/marian/job/marian-dev-cuda-9.2.svg?label=CUDA%209)](http://vali.inf.ed.ac.uk/jenkins/job/marian-dev-cuda-9.2/)\n[![Build Status CUDA 10](https://img.shields.io/jenkins/s/http/vali.inf.ed.ac.uk/jenkins/view/marian/job/marian-dev-cuda-10.1.svg?label=CUDA%2010)](http://vali.inf.ed.ac.uk/jenkins/job/marian-dev-cuda-10.1/)\n[![Build Status CPU](https://img.shields.io/jenkins/s/http/vali.inf.ed.ac.uk/jenkins/view/marian/job/marian-dev-cpu.svg?label=CPU)](http://vali.inf.ed.ac.uk/jenkins/job/marian-dev-cpu/)\n[![Tests Status](https://img.shields.io/jenkins/s/http/vali.inf.ed.ac.uk/jenkins/view/marian/job/marian-regression-tests.svg?label=tests)](http://vali.inf.ed.ac.uk/jenkins/job/marian-regression-tests/)\n[![Latest release](https://img.shields.io/github/release/marian-nmt/marian.svg?label=release)](https://github.com/marian-nmt/marian/releases)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](./LICENSE.md)\n[![Twitter](https://img.shields.io/twitter/follow/marian_nmt.svg?style=social)](https://twitter.com/intent/follow?screen_name=marian_nmt)\n\n*Marian* is an efficient Neural Machine Translation framework written in pure\nC++ with minimal dependencies.\n\nNamed in honour of Marian Rejewski, a Polish mathematician and cryptologist.\n\nMain features:\n\n- Efficient pure C++ implementation\n- Fast multi-GPU training and GPU/CPU translation\n- State-of-the-art NMT architectures: deep RNN and transformer\n- Permissive open source license (MIT)\n- [more detail...](https://marian-nmt.github.io/features)\n\nIf you use this, please cite:\n\nMarcin Junczys-Dowmunt, Roman Grundkiewicz, Tomasz Dwojak, Hieu Hoang, Kenneth\nHeafield, Tom Neckermann, Frank Seide, Ulrich Germann, Alham Fikri Aji, Nikolay\nBogoychev, Andr\u00e9 F. T. Martins, Alexandra Birch (2018). Marian: Fast Neural\nMachine Translation in C++ (http://www.aclweb.org/anthology/P18-4020)\n\n    @InProceedings{mariannmt,\n        title     = {Marian: Fast Neural Machine Translation in {C++}},\n        author    = {Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and\n                     Dwojak, Tomasz and Hoang, Hieu and Heafield, Kenneth and\n                     Neckermann, Tom and Seide, Frank and Germann, Ulrich and\n                     Fikri Aji, Alham and Bogoychev, Nikolay and\n                     Martins, Andr\\'{e} F. T. and Birch, Alexandra},\n        booktitle = {Proceedings of ACL 2018, System Demonstrations},\n        pages     = {116--121},\n        publisher = {Association for Computational Linguistics},\n        year      = {2018},\n        month     = {July},\n        address   = {Melbourne, Australia},\n        url       = {http://www.aclweb.org/anthology/P18-4020}\n    }\n\n## Amun\n\nThe handwritten decoder for RNN models compatible with Marian and Nematus has\nbeen superseded by the Marian decoder. The code is available in a separate\nrepository: https://github.com/marian-nmt/amun\n\n## Website\n\nMore information on https://marian-nmt.github.io\n\n- [Quick start](https://marian-nmt.github.io/quickstart)\n- [Installation and usage documentation](https://marian-nmt.github.io/docs)\n- [Usage examples](https://marian-nmt.github.io/examples)\n\n## Acknowledgements\n\nThe development of Marian received funding from the European Union's\n_Horizon 2020 Research and Innovation Programme_ under grant agreements\n688139 ([SUMMA](http://www.summa-project.eu); 2016-2019),\n645487 ([Modern MT](http://www.modernmt.eu); 2015-2017),\n644333 ([TraMOOC](http://tramooc.eu/); 2015-2017),\n644402 ([HiML](http://www.himl.eu/); 2015-2017),\n825303 ([Bergamot](https://browser.mt/); 2019-2021),\nthe Amazon Academic Research Awards program,\nthe World Intellectual Property Organization,\nand is based upon work supported in part by the Office of the Director of\nNational Intelligence (ODNI), Intelligence Advanced Research Projects Activity\n(IARPA), via contract #FA8650-17-C-9117.\n\nThis software contains source code provided by NVIDIA Corporation.\n"
},
{
  "name": "renard",
  "files": {
    "/": [
      ".circleci",
      "README.md",
      "cmd",
      "encoders.go",
      "go.mod",
      "go.sum",
      "renard.go",
      "timestamp.go",
      "writer.go",
      "zip"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# The RENARD signing scheme\n\n[![GoDoc](https://godoc.org/go.mozilla.org/renard?status.svg)](https://godoc.org/go.mozilla.org/renard)\n[![CircleCI](https://circleci.com/gh/mozilla/renard.svg?style=svg)](https://circleci.com/gh/mozilla/renard)\n\nRenard is a signing scheme designed primarily for Firefox add-ons and update files.\n\nMore information can be found at [add-ons signing v3](https://docs.google.com/document/d/1irzthww3vvtwGyNeJOZWWmqXIcuUHtMLmR0QZzqyKcI/edit#)."
},
{
  "name": "django-tidings",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "docs",
      "requirements.dev.txt",
      "setup.cfg",
      "setup.py",
      "tests",
      "tidings",
      "tox.ini"
    ],
    "/docs": [
      "Makefile",
      "changes.rst",
      "conf.py",
      "design.rst",
      "dev.rst",
      "index.rst",
      "installation.rst",
      "introduction.rst",
      "make.bat",
      "reference.rst",
      "requirements.txt",
      "settings.rst",
      "views.rst"
    ]
  },
  "makefile": "export DJANGO_SETTINGS_MODULE = tests.mockapp.settings\nexport PYTHONPATH := $(shell pwd)\n\n.PHONY: help\nhelp:\n\t@echo \"clean - remove all artifacts\"\n\t@echo \"coverage - check code coverage\"\n\t@echo \"coveragehtml - display code coverage in browser\"\n\t@echo \"develop - install development requirements\"\n\t@echo \"lint - check style with flake8\"\n\t@echo \"qa - run linters and test coverage\"\n\t@echo \"qa-all - run QA plus packaging and cross-version tests\"\n\t@echo \"release - package and upload a release\"\n\t@echo \"sdist - package\"\n\t@echo \"test - run tests\"\n\t@echo \"test-all - run tests against eacy Django/Python version\"\n\t@echo \"test-release - upload a release to the test PyPI server\"\n\n.PHONY: clean\nclean:\n\tgit clean -Xfd\n\n.PHONY: develop\ndevelop:\n\tpip install -r requirements.dev.txt\n\n.PHONY: shell\nshell:\n\tdjango-admin.py shell\n\n.PHONY: test\ntest:\n\tdjango-admin.py test tests\n\n.PHONY: migrations\nmigrations:\n\tdjango-admin.py makemigrations tidings\n\n.PHONY: docs\ndocs:\n\t$(MAKE) -C docs clean html\n\n.PHONY: test-all\ntest-all:\n\ttox --skip-missing-interpreters\n\n.PHONY: coverage\ncoverage:\n\tcoverage erase\n\tcoverage run --branch --source=tidings `which django-admin` test tests\n\n.PHONY: coveragehtml\ncoveragehtml: coverage\n\tcoverage html\n\tpython -m webbrowser file://$(CURDIR)/htmlcov/index.html\n\n.PHONY: lint\nlint:\n\tflake8 .\n\n.PHONY: qa\nqa: lint coveragehtml\n\n.PHONY: qa-all\nqa-all: qa sdist test-all\n\n.PHONY: sdist\nsdist:\n\tpython setup.py sdist bdist_wheel\n\tls -l dist\n\tcheck-manifest\n\tpyroma dist/`ls -t dist | grep tar.gz | head -n1`\n\n.PHONY: release\nrelease: clean sdist\n\tgpg --detach-sign -a dist/*.tar.gz\n\tgpg --detach-sign -a dist/*.whl\n\ttwine upload dist/*\n\tpython -m webbrowser -n https://pypi.python.org/pypi/django-tidings\n\n.PHONY: test-release\n# Add [test] section to ~/.pypirc, https://testpypi.python.org/pypi\ntest-release: clean sdist\n\tgpg --detach-sign -a dist/*.tar.gz\n\tgpg --detach-sign -a dist/*.whl\n\ttwine upload --repository test dist/*\n\tpython -m webbrowser -n https://testpypi.python.org/pypi/django-tidings\n",
  "readme": "==============\ndjango-tidings\n==============\n\n.. image:: https://img.shields.io/pypi/v/django-tidings.svg\n   :target: https://pypi.python.org/pypi/django-tidings\n\n.. image:: https://img.shields.io/travis/mozilla/django-tidings.svg\n   :target: http://travis-ci.org/mozilla/django-tidings\n\n.. image:: https://img.shields.io/coveralls/mozilla/django-tidings.svg\n   :target: https://coveralls.io/github/mozilla/django-tidings\n\n.. image:: https://readthedocs.org/projects/django-tidings/badge/\n   :target: https://django-tidings.readthedocs.io/en/latest/\n\n.. Omit badges from docs\n\ndjango-tidings is a framework for sending email notifications to users who have\nregistered interest in certain events, such as the modification of some model\nobject. Used by support.mozilla.org_ and developer.mozilla.org_, it is\noptimized for large-scale installations. Its features include...\n\n* Asynchronous operation using the celery_ task queue\n* De-duplication of notifications\n* Association of subscriptions with either registered Django users or anonymous\n  email addresses\n* Optional confirmation of anonymous subscriptions\n* Hook points for customizing any page drawn and any email sent\n\nPlease see the full documentation at django-tidings.readthedocs.io_.\n\n.. _celery: http://www.celeryproject.org/\n.. _support.mozilla.org: https://support.mozilla.org/en-US/\n.. _developer.mozilla.org: https://developer.mozilla.org/en-US/\n.. _django-tidings.readthedocs.io: https://django-tidings.readthedocs.io/en/latest/\n"
},
{
  "name": "mozilla-analytics",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# mozilla-analytics\nRepo for mozilla analytics team to store work, share queries and code\n"
},
{
  "name": "cvelist",
  "files": {
    "/": [
      "1999",
      "2000",
      "2001",
      "2002",
      "2003",
      "2004",
      "2005",
      "2006",
      "2007",
      "2008",
      "2009",
      "2010",
      "2011",
      "2012",
      "2013",
      "2014",
      "2015",
      "2016",
      "2017",
      "2018",
      "2019",
      "2020",
      "CODEOWNERS",
      "CONTRIBUTING.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# CVE Automation Working Group Git Pilot\n\nThe [CVE Automation Working\nGroup](https://github.com/CVEProject/automation-working-group) is\npiloting use of git to share information about public vulnerabilities. \nThe goal is to learn not only what features are necessary to support\nthe \"plumbing\" of sending and receiving the data, but also which\nattributes and metadata are needed in the CVE format to support\nautomation. \n\nSee [How to Contribute](https://github.com/CVEProject/cvelist/blob/master/CONTRIBUTING.md)\nfor details on participating in this pilot.\n\nThis repository holds information included in the [CVE\nList](https://cve.mitre.org/cve/) formatted using the [CVE JSON\nformat](https://github.com/CVEProject/automation-working-group/tree/master/cve_json_schema). \n\nUse of the CVE information in this repository is subject to the [CVE\nTerms of Use](https://cve.mitre.org/about/termsofuse.html). \n\n\n## Overview of the Repository\n\nInformation about each CVE id is stored as a unique file in the repo\nin a subdirectory based on the year as well as the numeric portion of\nthe id, truncated by 1,000.  Thus, [2017/3xxx](2017/3xxx) is for\nCVE-2017-3000 - CVE-2017-3999, and [2017/1002xxx](2017/1002xxx) is for\nCVE-2017-1002000 - CVE-2017-1002999. \n\nThe CVE Team updates these files automatically every hour using\ninformation from the CVE List, provided there have been changes.  The\nsynchronization job kicks off at the top of the hour and should\ncomplete within 5 minutes. \n\nFor ids that have been populated, the files contain the description\nand references that appear in the [CVE\nList](https://cve.mitre.org/cve/).  They may also contain\ninformation about the affected product(s) and problem type(s), which\nCNAs have been supplying when making assignments during the past year\nbut which is not included in the CVE List.  And going forward, it is\nhoped that they will contain a richer collection of information about\nthe vulnerability, as supported by the full CVE JSON schema. \n\n\n## Contact\n\nDirect questions, comments, or concerns about use of this repo to the CVE\nTeam using the [CVE Request web form](https://cveform.mitre.org). \n"
},
{
  "name": "security-training-helper",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Pipfile",
      "README.md",
      "sample-data",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# security-training-helper\nA repo to hold helper scripts for the security training effort.\n"
},
{
  "name": "macOS-install-drive-with-wipe-disk0",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "build_macos_usb_wipe.sh"
    ]
  },
  "makefile": null,
  "readme": "# macOS-install-drive-with-wipe-disk0\nCreate a bootable macOS USB install drive and automatically download the wipe-disk0.sh script to the USB drive\n"
},
{
  "name": "camdtest",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE.txt",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "A repository for Cameron Dawson [:camd] to test Treeherder functionality of handling\npull requests and pushes from github repos.  If you haven't seen activity in\na month, then it can be deleted, if you like.\n\n\n# camdtest\n\nAnd another thing\ndis and dat\nfdsfds\n\nfdsafffds\nfds\nfdsfds\nvdsfdsfdfdsfdsfds\nfdsfdsafdfdsfdsfdsafdsfdsafdfds\ncam yayfdfds\nssssss\nfdfds\nfdfds\nwaa\nfdsafdfdas\nffdsfdfdsfdfasdfdssfdfds\nfdsfds\nfdfdsfdfdsfdfdsssfdsfdsfdsfsfdfd\na\nb\nc\n\nsaa\nfdfds\nfdfds\nfdsfds\nfdsfds\nwwww''\nddd\ndddss\nfdfd\nsddds\nfdsf\naaaa\nfdsa\n"
},
{
  "name": "jschannel",
  "files": {
    "/": [
      ".gitignore",
      "PROTOCOL.md",
      "README.md",
      "docs",
      "example",
      "perf",
      "src"
    ],
    "/docs": [
      "ajax-loader.gif",
      "doctestjs",
      "github-ribbon.png",
      "index.html",
      "index_child.html",
      "jquery.min.js",
      "json2.js",
      "main.css",
      "run-tests.js",
      "sh.css",
      "sh_javascript.min.js",
      "sh_main.js"
    ]
  },
  "makefile": null,
  "readme": "## Welcome to JSChannel\n\nJSChannel is a small JavaScript abstraction layer on top of HTML5\ncross document messaging.  It builds rich messaging\nsemantics out of postMessage.\n\n## Overview\n\nHTML5's cross document messaging opens up a ton of possibilities for\ninteresting client side creations.  But, the specification only\nprovides the bare minimum amount of plumbing required to support cross\ndomain client side communication.  JSChannel is a very small\nabstraction that provides higher level messaging semantics on top of\npostMessage, which include:\n\n  * Query/Response - including support for *callback arguments*\n  * Notifications - Fire and forget style.\n\nIn addition to support for these messaging semantics, the library sports\nthe following additional features:\n\n  * \"scoping\", all messages will be safely tucked under the namespace you\n    provide.\n  * multiple channel support - message handling properly stops propagation\n    once messages are handled, and when they're not it leaves\n    them alone.  This means you can have an arbitrary number of different\n    channels in a single page.\n  * Rich messaging semantics - Query/Response and Notifications\n  * An idiomatic api - exceptions inside message handlers automatically\n    turn into error responses.  return values are likewise converted into\n    \"success\" responses.\n  * support for errors: the library provides the basic structure for error\n    propagation while the client define error codes.\n  * very compatible with asynchronicity.\n  * supports any browser with JSON parsing (native or as a JS library) and\n    postMessage.\n  * designed primarily for inter-frame communication.\n\n## Sample usage\n\nThe simplest possible use case.  A client (parent.html) loads up another\ndocument in an iframe (child.html) and invokes a function on her.\n\n### parent.html\n\n    <html>\n    <head><script src=\"src/jschannel.js\"></script></head>\n    <body>\n    <iframe id=\"childId\" src=\"child.html\"></iframe>\n    </body>\n    <script>\n    \n    var chan = Channel.build({\n        window: document.getElementById(\"childId\").contentWindow,\n        origin: \"*\",\n        scope: \"testScope\"\n    });\n    chan.call({\n        method: \"reverse\",\n        params: \"hello world!\",\n        success: function(v) {\n            console.log(v);\n        }\n    });\n    \n    </script>\n    </html>\n\n### child.html\n\n    <html><head>\n    <script src=\"src/jschannel.js\"></script>\n    <script>\n    \n    var chan = Channel.build({window: window.parent, origin: \"*\", scope: \"testScope\"});\n    chan.bind(\"reverse\", function(trans, s) {\n        return s.split(\"\").reverse().join(\"\");\n    });\n    \n    </script>\n    </head>\n    </html>\n\n## Documentation\n\nFull documentation for JSChannel can be found\n[here](http://mozilla.github.com/jschannel/docs/).\n\n## Influences\n\n[JSON-RPC](http://groups.google.com/group/json-rpc/web/json-rpc-2-0)\nprovided some design inspiration for message formats.\n\nThe existence of [pmrpc](http://code.google.com/p/pmrpc/) inspired me there's\nroom for pure js abstractions in this area.\n\nThe API design was influenced by\n[postmessage](http://github.com/daepark/postmessage).\n\n## LICENSE\n\nAll files that are part of this project are covered by the following\nlicense.  The one exception is code under test/doctestjs which\nincludes licensing information inside.\n    \n    Version: MPL 1.1/GPL 2.0/LGPL 2.1\n    \n    The contents of this file are subject to the Mozilla Public License Version \n    1.1 (the \"License\"); you may not use this file except in compliance with \n    the License. You may obtain a copy of the License at \n    http://www.mozilla.org/MPL/\n    \n    Software distributed under the License is distributed on an \"AS IS\" basis,\n    WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License\n    for the specific language governing rights and limitations under the\n    License.\n    \n    The Original Code is jschannel.\n    \n    The Initial Developer of the Original Code is Lloyd Hilaiel.\n\n    Portions created by the Initial Developer are Copyright (C) 2010\n    the Initial Developer. All Rights Reserved.\n    \n    Contributor(s):\n    \n    Alternatively, the contents of this file may be used under the terms of\n    either the GNU General Public License Version 2 or later (the \"GPL\"), or\n    the GNU Lesser General Public License Version 2.1 or later (the \"LGPL\"),\n    in which case the provisions of the GPL or the LGPL are applicable instead\n    of those above. If you wish to allow use of your version of this file only\n    under the terms of either the GPL or the LGPL, and not to allow others to\n    use your version of this file under the terms of the MPL, indicate your\n    decision by deleting the provisions above and replace them with the notice\n    and other provisions required by the GPL or the LGPL. If you do not delete\n    the provisions above, a recipient may use your version of this file under\n    the terms of any one of the MPL, the GPL or the LGPL.\n"
},
{
  "name": "jx-sqlite",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "docs",
      "jx_sqlite",
      "requirements.txt",
      "resources",
      "setup.py",
      "setuptools.json",
      "tests",
      "vendor"
    ],
    "/docs": [
      "JSON in Database.md",
      "JSON in Database.pptx",
      "Logical Equality.md",
      "Nomenclature.md",
      "Perspective.md",
      "Student Project 190124.md",
      "The Future.md",
      "gsoc.md",
      "jx_expressions_leaves.md",
      "nested1.png",
      "nested2.png",
      "perspectives.pptx",
      "ref1.png",
      "ref2.png",
      "ref3.png",
      "schema1.png",
      "schema2.png",
      "schema3.png",
      "schema4.png",
      "t1.png",
      "t2.png",
      "t3.png",
      "t3b.png",
      "t4.png",
      "t4b.png"
    ]
  },
  "makefile": null,
  "readme": "# jx-sqlite \n\nJSON query expressions using SQLite\n\n## Summary\n\nThis library will manage your database schema to store JSON documents. You get all the speed of a well-formed database schema without the schema migration headaches. \n\nhttps://www.youtube.com/watch?v=0_YLzb7BegI&list=PLSE8ODhjZXja7K1hjZ01UTVDnGQdx5v5U&index=26&t=260s\n\n## Status\n\nSignificant updates to the supporting libraries has broken this ode.  It still works works for the simple cases that require it\n\n**Jan 2020** - 96/283 test failing  \n\n\n\n## Installation\n\n    pip install jx-sqlite\n\n## Code Example\n\nOpen a database \n\n```python\ncontainer = Container()\n```\n\nDeclare a table\n\n```python\ntable = container.get_or_create_facts(\"my_table\")\n```\n\nPour JSON documents into it\n\n```python\ntable.add({\"os\":\"linux\", \"value\":42})\n```\n\nQuery the table\n\n```python\ntable.query({\n    \"select\": \"os\", \n    \"where\": {\"gt\": {\"value\": 0}}\n})\n```\n\n## More\n\nAn attempt to store JSON documents in SQLite so that they are accessible via SQL. The hope is this will serve a basis for a general document-relational map (DRM), and leverage the database's query optimizer.\njx-sqlite  is also responsible for making the schema, and changing it dynamically as new JSON schema are encountered and to ensure that the old queries against the new schema have the same meaning.\n\nThe most interesting, and most important feature is that we query nested object arrays as if they were just another table.  This is important for two reasons:\n\n1. Inner objects `{\"a\": {\"b\": 0}}` are a shortcut for nested arrays `{\"a\": [{\"b\": 0}]}`, plus\n2. Schemas can be expanded from one-to-one  to one-to-many `{\"a\": [{\"b\": 0}, {\"b\": 1}]}`.\n\n\n## Motivation\n\nJSON is a nice format to store data, and it has become quite prevalent. Unfortunately, databases do not handle it well, often a human is required to declare a schema that can hold the JSON before it can be queried. If we are not overwhelmed by the diversity of JSON now, we soon will be. There will be more JSON, of more different shapes, as the number of connected devices( and the information they generate) continues to increase.\n\n## Contributing\n\nContributions are always welcome! The best thing to do is find a failing test, and try to fix it.\n\nThese instructions will get you a copy of the project up and running on your local machine for development and testing purposes.\n\n    $ git clone https://github.com/mozilla/jx-sqlite\n    $ cd jx-sqlite\n\n### Running tests\n\nThere are over 200 tests used to confirm the expected behaviour: They test a variety of JSON forms, and the queries that can be performed on them. Most tests are further split into three different output formats ( list, table and cube).\n\n    export PYTHONPATH=.\n    python -m unittest discover -v -s tests\n\n### Technical Docs\n\n* [Json Query Expression](https://github.com/klahnakoski/ActiveData/blob/dev/docs/jx.md)\n* [Nomenclature](https://github.com/mozilla/jx-sqlite/blob/master/docs/Nomenclature.md)\n* [Snowflake](https://github.com/mozilla/jx-sqlite/blob/master/docs/Perspective.md)\n* [JSON in Database](https://github.com/mozilla/jx-sqlite/blob/master/docs/JSON%20in%20Database.md)\n* [The Future](https://github.com/mozilla/jx-sqlite/blob/master/docs/The%20Future.md)\n\n## License\n\nThis project is licensed under Mozilla Public License, v. 2.0. If a copy of the MPL was not distributed with this file, You can obtain one at http://mozilla.org/MPL/2.0/.\n\n\n## History\n\n*Sep 2018* - Upgrade libs, start refactoring to work with other libs\n\n*Dec 2017* - A number of tests were added, but they do not pass.\n\n*Sep 2017* - GSoC work completed, all but a few tests pass.\n \n\n## GSOC\n\nWork done upto the deadline of GSoC'17:\n\n* [Pull Requests](https://github.com/mozilla/jx-sqlite/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Arohit-rk)\n* [Commits](https://github.com/mozilla/jx-sqlite/commits?author=rohit-rk)\n\n\n## More Documentation\n\n* [The Future](https://github.com/mozilla/jx-sqlite/blob/master/docs/The%20Future.md)\n"
},
{
  "name": "webcompat-ml",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "datasets",
      "docs",
      "setup.py",
      "src"
    ],
    "/docs": [
      "architecture.md",
      "pipeline.md"
    ]
  },
  "makefile": null,
  "readme": "# webcompat-ml\n\n## About\n\nThis project includes all the models and helper utils used in the ML pipeline of webcompat.com.\n\n## Installation\n\n```\n> python setup.py build\n> python setup.py install\n```\n\n## Documentation\n\nFull project documentation is hosted on https://webcompat-ml.readthedocs.io/\n\n## License\nMozilla Public License Version 2.0\n\n## Credits\n\n* `automl-gs` was used as a starting point of our experiments to quickly evaluate different classifiers and how well they perform with our data.\n"
},
{
  "name": "openinnovation",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# openinnovation\nOpen Innovation Website\n"
},
{
  "name": "curriculum-final",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "blank-template",
      "color-picker-challenge",
      "encryption",
      "intermediate-web-lit-one",
      "intermediate-web-lit-two",
      "internet-health-basics",
      "kenya-dso-interventions",
      "make-your-first-webpage",
      "offline-icebreakers",
      "privacy-avatar",
      "privacy-basics",
      "privacy-fill-in-the-blanks",
      "privacy-rock-paper-scissors",
      "privacy-toolkit-v10.pdf",
      "template-assets",
      "web-lit-basics-one",
      "web-lit-basics-two",
      "what-if-they-had-the-web"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Curriculum Repo\nOpen repo for Web Literacy curriculum content and templates.\n"
},
{
  "name": "legacy_sync_queries",
  "files": {
    "/": [
      ".gitignore",
      "misc",
      "sync_android_dashboard",
      "sync_bookmark_validation_dashboard_old_engine",
      "sync_engine_error_success_dashboard",
      "sync_engine_failure_reasons_dashboard",
      "sync_ios_dashboard",
      "sync_send_tab_dashboard"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "OpenDesign",
  "files": {
    "/": [
      ".DS_Store",
      "A-Frame",
      "Activate Mozilla",
      "Addons",
      "Analysis and Research",
      "CCLICENSE",
      "CODE_OF_CONDUCT.md",
      "Campaigns",
      "Community Activities",
      "Firefox",
      "IHR",
      "ISSUE_TEMPLATE.md",
      "L10n",
      "LICENSE",
      "Mixed Reality",
      "Mozilla Deep Speech",
      "Mozilla Quality Assurance",
      "Mozilla Security",
      "Mozillians",
      "Open Innovation",
      "Other icons",
      "README.md",
      "Rust"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Open Design\n\nWelcome to the Mozilla Open Design Repo. Here you can file issues with your design requests for the community design group, find collaborators, or find design tasks to tackle. Don't forget to use the issue template below to make sure you are including all the relevant information!\n\nAnyone can take (or make) a design issue, as long as it is related to Mozilla!\n\nIf you'd like to join the community design group mailing list please [sign up here](http://ow.ly/WgQ9S) and watch this repo!\n\nNEED HELP? Watch a tutorial and ask your questions [here](https://discourse.mozilla-community.org/t/love-community-design-but-not-comfortable-with-github/6626)!\n\nNOTE: Once you've completed a project please include the source files!\n\n## Issue Template: \n\n[Click here](https://github.com/mozilla/Community-Design/issues/new?body=%23%23%20Goal%3A%20%0A%5BWhat%20you%27d%20like%20created%5D%0A%0A%23%23%20Info%3A%20%0A%5BWhat%20it%20will%20be%20used%20for%2Fwhere%20it%20will%20go%5D%0A%0A%23%23%20Style%20Information%3A%20%0A%5BWhat%20you%20want%20it%20to%20look%20like%2C%20link%20to%20references%2C%20similar%20things%20etc.%5D%0A%0A%23%23%20Deadline%3A%20%0A%5BWhen%20would%20you%20like%20this%20done%20by%5D%0A%0A%23%23%20Tag%3A%20%0A%5BDesign%20Needed%2C%20Developer%20Needed%2C%20Question%2C%20Staff%20Support%20Needed%5D) to create issue with default template\n\n```\n## Goal: \n[What you'd like created]\n\n## Info: \n[What it will be used for/where it will go]\n\n## Style Information: \n[What you want it to look like, link to references, similar things etc.]\n\n## Deadline: \n[When would you like this done by]\n\n## Tag: \n[Design Needed, Developer Needed, Question, Staff Support Needed]\n```\n\nNote: Because of the way permissions work on this repo, if you can't add a label yourself indicate the label you want added in the body of the issue and someone will add it for you. \n"
},
{
  "name": "iris_android",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# iris_android\nA project that uses Iris to automate the Android emulator\n"
},
{
  "name": "legacy-browser-support",
  "files": {
    "/": [
      "CONTRIBUTING",
      "DESCRIPTION",
      "LICENSE",
      "README.md",
      "chrome_extension",
      "core",
      "ie_bho",
      "makefile",
      "makefile64",
      "native_component",
      "npapi",
      "policy_templates",
      "third_party"
    ]
  },
  "makefile": null,
  "readme": "This repo is a port of the Chrome legacy browser extension and plug-in to Firefox\n\n---\n\n# Goal\nBounce the navigation back to Chrome whenever the user is trying to open a web\npage that is meant to be opened in Chrome. It is meant to be a companion to the\nChrome Legacy Browser Support Extension to allow for seamless switching between\nChrome and IE.\n\n\n# Implementation\nInternet Explorer:\nThe plug-in is implemented as an Internet Explorer Add-on, and is packaged in a\nWindows installer. The installer adds a DLL to the user's machine and registers\nthe plug-in in the browser (Tools -> Manage Add-ons). Once installed, the Add-on\nmonitors navigation requests in IE and checks if the url is any of the\nwhitelisted ones to be opened in IE. If not it will close the tab that started\nthe navigation and reopen the url in Chrome.\n\n# policies.json\n\n```\n{\n  \"policies\": {\n    \"3rdparty\": {\n      \"Extensions\": {\n        \"legacy-browser-support@mozilla.org\": {\n          \"alternative_browser_path\": \"${ie}\",\n          \"alternative_browser_arguments\": \"${url}\",\n          \"firefox_path\": \"${firefox}\",\n          \"firefox_arguments\": \"${url}\",\n          \"url_list\": [\"example.com\"],\n          \"url_greylist\": [\"example.org\"],\n          \"keep_last_firefox_tab\": true | false,\n          \"show_transition_screen\": 0-60\n          \"use_ie_site_list\": true | false,         \n        }\n      }\n    }\n  }\n}\n```\n\n`alternative_browser_path` - Allows you to specify the program to be used as an alternative browser.\n\nIf enabled you can either specify an absolute path in the policy or use one of the following variables:\n\n${ie} - The default install location for Internet Explorer\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\IEXPLORE.EXE is used to pick the location of the browser in this case.\n\n${chrome} - The default install location for Chrome\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\Chrome.EXE is used to pick the location of the browser in this case.\n\n${safari} - The default install location for Safari\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\SAFARI.EXE is used to pick the location of the browser in this case.\n\nIf the policy is not set or left empty the default alternative browser will be used which is Internet Explorer determined as if the ${ie} value is used.\n\n`alternative_browser_arguments` - Allows you to specify the arguments to be passed to the executable of the alternative browser.\n\nIf the policy is enabled the provided parameters will be used when the alternative browser is invoked.\n\nYou can also use the special placeholder ${url} to specify where the url should appear in the command line. E.g. \"--url=${url} --kiosk\".\n\nYou don't have to specify the placeholder if it should be appended to the end or if it should be the only argument you can leave that policy empty.\n\nIf the policy is not set or left empty only the url will be passed as a parameter to the browser.\n\n`firefox_path` - Allows you to specify the executable of Firefox to be launched when returning from the alternative browser(*).\n\nWhen the policy is enabled you can either specify an absolute path in the policy or use the following variable:\n\n${firefox} - The default install location for Firefox\n\nHKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\App Paths\\FIREFOX.EXE is used to pick the location of the browser in this case.\n\nIf the policy is not set or left empty the default installation of Firefox will be used as if the ${firefox} value has been used.\n\n*: Presently only Internet Explorer supports returning to Firefox automatically.\n\n`firefox_arguments` - Allows you to specify the arguments to be passed to the Firefox when it is executed.\n\nIf the policy is enabled the provided paramters will be used when Firefox is invoked.\n\nYou can also use the special placeholder ${url} to specify where the url should appear in the command line. E.g. \"--url=${url} --kiosk\".\n\nYou don't have to specify the placeholder if it should be appended to the end or if it should be the only argument you can leave that policy empty.\n\nIf the policy is not specified only the url will be passed as a parameter to the browser.\n\n`url_list` - Allows you to specify a list of host domain names to be opened in the Alternative browser.  \n\nIf the policy is enabled you have to provide a list of filters which will trigger the transition to the alternative browser. Every entry should be one of the following four types:\n\nHost-name part: Either complete domain names like \"www.example.com\" should be specified or parts of them like \"example.com\" or even \"example\". Wildcards are not supported yet.  \n\nURL prefix: Only proper URL prefixes are matched complete with protocol and port if needed. E.g. \"http://login.example.com\" or \"https://www.example.com:8080/login/\".  \n\nNegative entry: Starts with \"!\" and continues as host-name part or URL prefix as described above. Negative entries are always opened in Firefox. E.g. \"!example.com\" or \"!file:///c:/localapp/\".  \n\nWildcard entry: Contains only a single \"*\" character. Matches any URL. Intended to be used together with negative entries, if most URLs should be open in the alternative browser and only a small selection of URLs should be opened in Firefox.  \n\nNegative entries have higher priorities than positive entries which allows to whitelist large portions of a domain while saving smaller sections to be opened in Firefox.\n\nThe wildcard entry if present is applied only after all other rules have been checked.\n\nThe following protocols are monitored for redirecting: http:, https:.\n\nIf not specified or left empty, no transition to the alternative browser will be triggered.\n\n`url_greylist` - Allows you to specify a list of host domain names to be opened in either browser.\n\nWhen this policy is enabled, the domains in this list will be available in both browsers and will not trigger a transition either way. A possible use case are any authentication domains that are shared between new and legacy apps in this list.\n\nHost-name parts: Either complete domain names like \"www.example.com\" should be specified or parts of them like \"example.com: or even \"example\". Wildcards are not supported yet.\n\nURL prefixes: Only proper URL prefixes are matched complete with protocol and port if needed. E.g. \"http://login.example.com\" or \"https://www.example.com:8080/login/\".\n\nIf not specified or left empty, any domain not in the \"Hosts to Open In the Alternative Browser\" list will trigger a transition back to Firefox(*).\n\n*: Presently only Internet Explorer supports returning to Firefox automatically.\n\n`keep_last_firefox_tab` - Allows you to specify what happens when the last open tab in Firefox is being redirected to the alternative browser.\n\nWhen this policy is true or not set the last tab will be left open and point to the new tab page after the transition. If it is set to false then the last tab will be closed which will lead to closing Firefox if there are no other Firefox windows open.\n\n`show_transition_screen` - Allows you to specify how long the transition message in the tab will be visible in Firefox before transitioning to the other browser. This helps visualize the transition. The policy is specified in seconds.\n\nIf it is not set then the tab will be closed immediately when the transition can be completed.\n\n`use_ie_site_list` - When enabled, transitions between browsers will also be triggered by entries in the SiteList policy for Internet Explorer as defined in (https://technet.microsoft.com/itpro/internet-explorer/ie11-deploy-guide/turn-on-enterprise-mode-and-use-a-site-list).\n\nIf it is not set then only the urls in the extension policy will be used.\n"
},
{
  "name": "webowonder",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "__init__.py",
      "apps",
      "bin",
      "docs",
      "lib",
      "manage.py",
      "media",
      "migrations",
      "requirements",
      "settings.py",
      "settings_local.py-dist",
      "templates",
      "urls.py",
      "wsgi"
    ],
    "/docs": [
      "Makefile",
      "_build",
      "_static",
      "_templates",
      "conf.py",
      "gettingstarted.rst",
      "index.rst",
      "libs.rst"
    ]
  },
  "makefile": null,
  "readme": "Web O' Wonder\n=============\n\nBased off Mozilla's [Playdoh][gh-playdoh] web application template,\nWeb O Wonder is [hosted on github][gh-wow]. \n\nPlease follow [Playdoh's docs][gh-playdoh], except for the follow:\n\n* cd webowonder\n* git clone --recursive git://github.com/mozilla/webowonder-lib.git vendor\n* svn co [gh-wow-l10n] into a directory named locale\n* Setup Apache or another web server on another domain to host your demos\n* Run schema migrations instead of syncdb (see bin/update_site.py)\n** clone [gh-wow-demos] which is a bunch of static files\n** Map Doc Root to the root of this repository\n** Test that you can go to http://demoland/demos/shadows/ and see a demo load\n* settings_local.py\n** DEMOLAND = 'http://demoland/'\n\n[gh-playdoh]: http://mozilla.github.com/playdoh\n[gh-wow]: https://github.com/mozilla/webowonder\n[gh-wow-lib]: https://github.com/mozilla/webowonder\n[gh-wow-demos]: https://github.com/mozilla/webowonder\n[gh-wow-l10n]: http://svn.mozilla.org/projects/TBD\n\n\nVendor vs Virtualenvs\n---------------------\nOptional, but the new way of setting up virtual envs is:\n\n* pip install only compiled requirements into a virtualenv\n* Django will pickup other requirements from vendor\n\nRemember vendor and locale are not submodules. You'll need to cd into them and make commits.\n::\n    webowonder\n        apps\n        ...\n        locale <- SVN\n        vendor <- git\n\nPhases\n------\nThis demo site will be released in phases... To hide later phases do;\n1) set hidden flag in database\n2) Comment out webtrends tracking in base.html\n3) Never make this repo public ;)\n\nLicense\n-------\nThis software is licensed under the [New BSD License][BSD]. For more\ninformation, read the file ``LICENSE``.\n\n[BSD]: http://creativecommons.org/licenses/BSD/\n\n"
},
{
  "name": "discourse-tldr",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "index.js",
      "newsletter.js",
      "package-lock.json",
      "package.json",
      "post_newsletter",
      "zip.sh"
    ]
  },
  "makefile": null,
  "readme": "# Discourse + tl;dr newsletter\n\nScripts to fetch the [tl;dr newsletter](https://mana.mozilla.org/wiki/pages/viewpage.action?pageId=70485683) and post it to Discourse.\n\nIn operation here: https://discourse.mozilla.org/c/mozillians/tldr\n\nFile issues here: https://github.com/mozilla/discourse/issues\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "discourse-email-in-lambda",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# discourse-email-in-lambda\n\n## Building\n\n`rustup target install x86_64-unknown-linux-musl`\n\n`cargo build --release --features vendored --target x86_64-unknown-linux-musl`\n\n`zip -j lambda.zip ./target/x86_64-unknown-linux-musl/release/bootstrap`\n\n## Setting up\n\nThis lambda takes 4 environment variables:\n* `DISCOURSE_EMAIL_IN_BUCKET`: name of s3 bucket raw emails are placed in\n* `DISCOURSE_URL`: base url of Discourse, without a trailing slash, eg: \"https://discourse.mozilla.org\"\n* `DISCOURSE_API_KEY`\n* `DISCOURSE_API_USERNAME`\n* `REJECTED_RECIPIENTS`: comma separated list of recipients to not process email for\n\n## Logging\n\nThis lambda uses [env_logger](https://docs.rs/env_logger/0.6.2/env_logger/) for logging, meaning you can change the logging level and style with environment variables.\n\nBy default:\n* `RUST_LOG=info`\n* `RUST_LOG_STYLE=never`\n"
},
{
  "name": "GitHub_Selenium",
  "files": {
    "/": [
      ".editorconfig",
      ".github",
      ".gitignore",
      ".travis.yml",
      "AUTHORS.rst",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.rst",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "docs",
      "github_selenium",
      "requirements.txt",
      "requirements_dev.txt",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini",
      "travis_pypi_setup.py"
    ],
    "/docs": [
      ".gitignore",
      "Makefile",
      "authors.rst",
      "conf.py",
      "contributing.rst",
      "history.rst",
      "index.rst",
      "installation.rst",
      "make.bat",
      "readme.rst",
      "usage.rst"
    ],
    "/.github": [
      "ISSUE_TEMPLATE.md"
    ]
  },
  "makefile": ".PHONY: clean clean-test clean-pyc clean-build docs help\n.DEFAULT_GOAL := help\ndefine BROWSER_PYSCRIPT\nimport os, webbrowser, sys\ntry:\n\tfrom urllib import pathname2url\nexcept:\n\tfrom urllib.request import pathname2url\n\nwebbrowser.open(\"file://\" + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\n\ndefine PRINT_HELP_PYSCRIPT\nimport re, sys\n\nfor line in sys.stdin:\n\tmatch = re.match(r'^([a-zA-Z_-]+):.*?## (.*)$$', line)\n\tif match:\n\t\ttarget, help = match.groups()\n\t\tprint(\"%-20s %s\" % (target, help))\nendef\nexport PRINT_HELP_PYSCRIPT\nBROWSER := python -c \"$$BROWSER_PYSCRIPT\"\n\nhelp:\n\t@python -c \"$$PRINT_HELP_PYSCRIPT\" < $(MAKEFILE_LIST)\n\nclean: clean-build clean-pyc clean-test ## remove all build, test, coverage and Python artifacts\n\n\nclean-build: ## remove build artifacts\n\trm -fr build/\n\trm -fr dist/\n\trm -fr .eggs/\n\tfind . -name '*.egg-info' -exec rm -fr {} +\n\tfind . -name '*.egg' -exec rm -f {} +\n\nclean-pyc: ## remove Python file artifacts\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -fr {} +\n\nclean-test: ## remove test and coverage artifacts\n\trm -fr .tox/\n\trm -f .coverage\n\trm -fr htmlcov/\n\nlint: ## check style with flake8\n\tflake8 github_selenium tests\n\ntest: ## run tests quickly with the default Python\n\tpy.test\n\t\n\ntest-all: ## run tests on every Python version with tox\n\ttox\n\ncoverage: ## check code coverage quickly with the default Python\n\tcoverage run --source github_selenium -m pytest\n\tcoverage report -m\n\tcoverage html\n\t$(BROWSER) htmlcov/index.html\n\ndocs: ## generate Sphinx HTML documentation, including API docs\n\trm -f docs/github_selenium.rst\n\trm -f docs/modules.rst\n\tsphinx-apidoc -o docs/ github_selenium\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C docs html\n\t$(BROWSER) docs/_build/html/index.html\n\nservedocs: docs ## compile the docs watching for changes\n\twatchmedo shell-command -p '*.rst' -c '$(MAKE) -C docs html' -R -D .\n\nrelease: clean ## package and upload a release\n\tpython setup.py sdist upload\n\tpython setup.py bdist_wheel upload\n\ndist: clean ## builds source and wheel package\n\tpython setup.py sdist\n\tpython setup.py bdist_wheel\n\tls -l dist\n\ninstall: clean ## install the package to the active Python's site-packages\n\tpython setup.py install\n",
  "readme": "===============\nGitHub Selenium\n===============\n\n..\n    .. image:: https://img.shields.io/pypi/v/github_selenium.svg\n            :target: https://pypi.python.org/pypi/github_selenium\n\n    .. image:: https://img.shields.io/travis/hwine/github_selenium.svg\n            :target: https://travis-ci.org/hwine/github_selenium\n\n    .. image:: https://readthedocs.org/projects/github-selenium/badge/?version=latest\n            :target: https://github-selenium.readthedocs.io/en/latest/?badge=latest\n            :alt: Documentation Status\n\n    .. image:: https://pyup.io/repos/github/hwine/github_selenium/shield.svg\n        :target: https://pyup.io/repos/github/hwine/github_selenium/\n        :alt: Updates\n\n\nHelper for 2FA login to GitHub\n\n\n* Free software: MPL v2 license\n\n..\n    * Documentation: https://github-selenium.readthedocs.io.\n\n\nFeatures\n--------\n\nAllows for automated browser login to GitHub accounts requiring 2FA, and\nprovides a Selenium_ web driver instance for further work. Uses a\nheadless Firefox instance driven by geckodriver_.\n\nPrerequites\n-----------\n\nOutside of the dependencies which will be installed when you ``pip\ninstall`` this package, you need:\n\n    - Python (3 prefered, should work with 2)\n    - Firefox \n    - Latest geckodriver_\n\nDevelopment dependencies are managed via pipenv_. Use ``pipenv install\n--dev``\nto build your virtual environment and install development dependencies.\n\n* TODO\n\nThis works *just* enough to be useful on a couple of scripts. YMMV.\n\n  * more docs!\n  * lots! PR's & Issues welcome\n\nCredits\n---------\n\nThis package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter\n.. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage\n.. _selenium: http://www.seleniumhq.org/\n.. _geckodriver: https://github.com/mozilla/geckodriver\n.. _pipenv: https://github.com/pypa/pipenv\n"
},
{
  "name": "nightlytt",
  "files": {
    "/": [
      ".editorconfig",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "History.md",
      "README.md",
      "extension",
      "screenshot.png"
    ]
  },
  "makefile": null,
  "readme": "# Nightly Tester Tools\n\nNightly Tester Tools (NTT) is an add-on for aiding testers of Nightly builds of Mozilla apps including Firefox.\n\n<img src=\"screenshot.png\" alt=\"Screenshot\" width=\"400\" height=\"300\">\n\n## Features\n\n* Copy the build ID or list of extensions to the clipboard using the toolbar button\n* Insert the build ID or list of extensions into a textbox using the context menu\n* Option to customize the Title Bar\n\n## Limitations\n\nThere are a couple of known issues due to the limited [WebExtensions APIs](https://developer.mozilla.org/Add-ons/WebExtensions/API):\n\n* The changeset cannot be retrieved\n* The extension list does not include system add-ons\n* The textbox inserting options don't work on [some Mozilla sites](https://bugzilla.mozilla.org/show_bug.cgi?id=1310082) (including AMO due to a security restriction) as well as with a [textbox within an iframe](https://bugzilla.mozilla.org/show_bug.cgi?id=1405723) and certain rich text editors\n* Some variables are not available for the custom title template\n\nThe following features found in the original XUL-based extension are not yet implemented, and some of them may not be implemented again:\n\n* Copy `about:support` to Pastebin\n* Open the profile folder (Use `about:support` or `about:profile` page instead to open it)\n* Open the pushlog\n* Screenshot utility (Use [Firefox Screenshots](https://support.mozilla.org/kb/firefox-screenshots) instead)\n* Crash options\n* Extension compatibility fixer\n* Menu items under Tools\n\n## Compatibility\n\nThe current WebExtension version is only compatible with Firefox. Use legacy [version 3.10](https://addons.mozilla.org/firefox/addon/nightly-tester-tools/versions/3.10) for Thunderbird and SeaMonkey.\n\n## Install\n\nYou can install the latest stable NTT from [addons.mozilla.org](https://addons.mozilla.org/firefox/addon/nightly-tester-tools/).\n\nTo install for development, clone the repo:\n\n```\ngit clone git://github.com/mozilla/nightlytt.git\n```\n\nInstall the [`web-ext` command line tool](https://developer.mozilla.org/Add-ons/WebExtensions/Getting_started_with_web-ext):\n\n```\nnpm install --global web-ext\n```\n\nTo test with Firefox:\n\n```\nweb-ext run -s extension\n```\n\nTo build for release:\n```\nweb-ext build -s extension\n```\n\nYou can also [temporarily install the extension](https://developer.mozilla.org/Add-ons/WebExtensions/Temporary_Installation_in_Firefox) in your Firefox without having to use `web-ext`.\n\n## Development\n\nAll bugs and feature requests are filed in the Nightly Tester Tools project at GitHub. You can [view the list of open issues](https://github.com/mozilla/nightlytt/issues), or you can [file a new issue](https://github.com/mozilla/nightlytt/issues/new). Check out [the wiki](https://wiki.mozilla.org/Auto-tools/Projects/NightlyTesterTools) for a list of current and proposed features and feel free to file bugs and submit patches.\n\nThis project uses [`.editorconfig`](http://editorconfig.org/#overview), which sets defaults for the formatting of the code. So enjoy the use of [compatible editor](http://editorconfig.org/#download). Just download and install the corresponding plugin.\n\nAlso, it's encouraged to use the `web-ext lint` command to [check the code](https://developer.mozilla.org/Add-ons/WebExtensions/Getting_started_with_web-ext#Checking_for_code_lint).\n"
},
{
  "name": "wg-decisions",
  "files": {
    "/": [
      "README.md",
      "config.toml"
    ]
  },
  "makefile": null,
  "readme": "# wg-decisions\nA place for Gecko developers to track CSSWG decisions.\n\nIssues are filed automatically by\n[wg-tracker](https://github.com/heycam/wg-tracker) upon CSS Working Group\nresolutions being made.\n"
},
{
  "name": "guardduty-multi-account-manager",
  "files": {
    "/": [
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Makefile",
      "README.md",
      "cloudformation",
      "docs",
      "lambda_functions",
      "requirements.txt",
      "tests"
    ],
    "/docs": [
      "dgram.png",
      "example-organizations-reader-iam-role.yml",
      "plan.md"
    ]
  },
  "makefile": "ROOT_DIR\t:= $(shell dirname $(realpath $(lastword $(MAKEFILE_LIST))))\nPARENTDIR       := $(realpath ../)\nAWS_REGION = us-west-2\n# We only need to publish to the us-west-2 bucket as it uses bucket replication\n# to replicate all data to the buckets in other regions\nS3_BUCKET_NAME  := public.us-west-2.infosec.mozilla.org\nS3_BUCKET_TEMPLATE_PATH\t:= guardduty-multi-account-manager/cf\nS3_BUCKET_LAMBDA_PATH\t:= guardduty-multi-account-manager/lambda\nS3_BUCKET_TEMPLATE_URI\t:= s3://$(S3_BUCKET_NAME)/$(S3_BUCKET_TEMPLATE_PATH)\nS3_BUCKET_LAMBDA_URI\t:= s3://$(S3_BUCKET_NAME)/$(S3_BUCKET_LAMBDA_PATH)\nPARENT_TEMPLATE_URI\t:= https://s3.amazonaws.com/$(S3_BUCKET_NAME)/$(S3_BUCKET_TEMPLATE_PATH)/guardduty-multi-account-manager-parent.yml\nLAMBDA_BUCKET_PREFIX\t:= $(shell v='$(S3_BUCKET_NAME)'; echo \"$${v%%us-west-2*}\")\nLAMBDA_BUCKET_SUFFIX\t:= $(shell v='$(S3_BUCKET_NAME)'; echo \"$${v\\#\\#*us-west-2}\")\n\nall:\n\t@echo 'Available make targets:'\n\t@grep '^[^#[:space:]].*:' Makefile\n\n.PHONY: test\ntest:\n\tpy.test tests/ --capture=no\n\n.PHONY: cfn-lint test\ntest: cfn-lint\ncfn-lint: ## Verify the CloudFormation template pass linting tests\n\t-cfn-lint cloudformation/*.yml\n\n.PHONY: upload-templates\nupload-templates:\n\t@export AWS_REGION=$(AWS_REGION)\n\taws s3 sync cloudformation/ $(S3_BUCKET_TEMPLATE_URI) --exclude=\"*\" --include=\"*.yml\"\n\n.PHONY: upload-gd2md-lambda\nupload-gd2md-lambda:\n\t@export AWS_REGION=$(AWS_REGION)\n\tzip lambda_functions/gd2md.zip lambda_functions/normalization.py\n\taws s3 cp lambda_functions/gd2md.zip $(S3_BUCKET_LAMBDA_URI)/gd2md.zip\n\trm lambda_functions/gd2md.zip\n\n.PHONY: upload-plumbing-lambda\nupload-plumbing-lambda:\n\t@export AWS_REGION=$(AWS_REGION)\n\tzip lambda_functions/plumbing.zip lambda_functions/plumbing.py\n\taws s3 cp lambda_functions/plumbing.zip $(S3_BUCKET_LAMBDA_URI)/plumbing.zip\n\trm lambda_functions/plumbing.zip\n\n.PHONY: upload-invitation_manager-lambda\nupload-invitation_manager-lambda:\n\tzip lambda_functions/invitation_manager.zip lambda_functions/invitation_manager.py\n\tAWS_REGION=$(AWS_REGION) aws s3 cp lambda_functions/invitation_manager.zip $(S3_BUCKET_LAMBDA_URI)/invitation_manager.zip\n\trm lambda_functions/invitation_manager.zip\n\n.PHONY: upload-templates create-stack\ncreate-stack:\n\t@export AWS_REGION=$(AWS_REGION)\n\n\t# $${$(S3_BUCKET_NAME)##us-west-2*}\n\t# https://github.com/aws/aws-cli/issues/870#issuecomment-51629161\n\taws cloudformation create-stack --stack-name guardduty-multi-account-manager \\\n\t  --capabilities CAPABILITY_IAM \\\n\t  --parameters \\\n\t    ParameterKey=CloudFormationTemplatePrefix,ParameterValue=https://s3.amazonaws.com/$(S3_BUCKET_NAME)/$(S3_BUCKET_TEMPLATE_PATH)/ \\\n\t    ParameterKey=LambdaCodeS3BucketNamePrefix,ParameterValue=$(LAMBDA_BUCKET_PREFIX) \\\n\t    ParameterKey=LambdaCodeS3BucketNameSuffix,ParameterValue=$(LAMBDA_BUCKET_SUFFIX) \\\n\t    ParameterKey=LambdaCodeS3Path,ParameterValue=$(S3_BUCKET_LAMBDA_PATH)/ \\\n\t    ParameterKey=OrganizationAccountArns,ParameterValue=\\'$(ORGANIZAION_ACCOUNT_ARNS)\\' \\\n\t    ParameterKey=AccountFilterList,ParameterValue=$(ACCOUNT_FILTER_LIST) \\\n\t  --template-url $(PARENT_TEMPLATE_URI)\n\n.PHONY: create-invitation-manager-stack\ncreate-invitation-manager-stack:\n\tAWS_REGION=$(AWS_REGION) aws cloudformation create-stack --stack-name guardduty-invitation-manager \\\n\t  --capabilities CAPABILITY_IAM \\\n\t  --parameters \\\n\t    ParameterKey=LambdaCodeS3BucketNamePrefix,ParameterValue=$(LAMBDA_BUCKET_PREFIX) \\\n\t    ParameterKey=LambdaCodeS3BucketNameSuffix,ParameterValue=$(LAMBDA_BUCKET_SUFFIX) \\\n\t    ParameterKey=LambdaCodeS3Path,ParameterValue=$(S3_BUCKET_LAMBDA_PATH)/ \\\n\t    ParameterKey=OrganizationAccountArns,ParameterValue=\\'$(ORGANIZAION_ACCOUNT_ARNS)\\' \\\n\t    ParameterKey=AccountFilterList,ParameterValue=$(ACCOUNT_FILTER_LIST) \\\n\t  --template-url https://s3.amazonaws.com/$(S3_BUCKET_NAME)/$(S3_BUCKET_TEMPLATE_PATH)/guardduty-invitation-manager.yml\n\n.PHONY: upload-templates update-stack\nupdate-stack:\n\t@export AWS_REGION=$(AWS_REGION)\n\taws cloudformation update-stack --stack-name guardduty-multi-account-manager \\\n\t  --capabilities CAPABILITY_IAM \\\n\t  --template-url $(PARENT_TEMPLATE_URI)\n\n\n# To upload to staging and test\n# logged into infosec-dev AWS account\n#  make upload-templates S3_BUCKET_NAME=public.us-west-2.security.allizom.org\n#  make upload-invitation_manager-lambda S3_BUCKET_NAME=public.us-west-2.security.allizom.org\n# logged into infosec-prod (because this is the account that's trusted)\n#  make create-stack S3_BUCKET_NAME=public.us-west-2.security.allizom.org\n# or\n#  make create-invitation-manager-stack S3_BUCKET_NAME=public.us-west-2.security.allizom.org ORGANIZAION_ACCOUNT_ARNS=arn:aws:iam::329567179436:role/Infosec-Organization-Reader,arn:aws:iam::943761894018:role/Infosec-Organization-Reader\n\n\n",
  "readme": "# GuardDuty Multi-Account Manager\n\nAutomate the AWS GuardDuty account invitation lifecycle for all of your \norganizations AWS accounts in all regions as well as aggregate and normalize \nthe GuardDuty findings\n\n## Architecture\n\n!['docs/dgram.png'](docs/dgram.png)\n\n> Above is an example architecture for a master account with a member account. \n> Note: The member account has GuardDuty detectors in every region as does the \n> master account.\n\n## Why This?\n\nAs a multi-account user of Amazon Web Services you have a few choices when\ndeciding to turn on GuardDuty across your accounts.\n\nYour options are:\n\n1. Stack Sets\n2. Human invitations\n3. Something else.\n\nDue to the nature of stack sets and the distributed governance of Mozilla it\nbreaks our trust model to grant the needed permissions to run stack sets.\nHuman behavior consistently generates inconsistent results.\n\nThis is why we elected to create GuardDuty Multi-Account Manager\n\n## What is it?\n\nGuardDuty Multi-Account Manager is a series of lambda functions designed to do\nthe following:\n\n* Enable GuardDuty Masters in all AWS Regions present and future.\n* Empower account owners to decide to enable GuardDuty\n* Manage the lifecycle of invitations to the member accounts\n* Aggregate all findings from all detectors in all regions, normalize the data,\n  and send to a single SQS queue\n\n## How do I deploy it?\n\n### Dependencies\n\n* AWS Organizations\n  * Either run the GuardDuty Multi-Account Manager from within an AWS\n    Organizations parent account or\n  * Establish an IAM Role in the AWS Organizations parent account that can be\n    assumed by the GuardDuty Multi-Account Manager.\n    [Example IAM Role](docs/example-organizations-reader-iam-role.yml)\n* Deploy the\n  [Cloudformation Cross Account Outputs](https://github.com/mozilla/cloudformation-cross-account-outputs/)\n  service which allows CloudFormation stacks in other AWS accounts to report\n  back output. This is used to convey the\n  [GuardDuty Member Account IAM Role](cloudformation/guardduty-member-account-role.yml)\n  information. In order to deploy this service \n  [follow the instructions in the README](https://github.com/mozilla/cloudformation-cross-account-outputs#deploy-the-infrastructure)\n  which explains how. \n  * Make sure that in Step 1 and 2 you deploy each template in only one region. These resources shouldn't be deployed multiple times in an AWS account.\n  * Make sure that in Step 3, you deploy the `cloudformation-sns-emission-consumer.yml`\n  template in every region that you want to allow your GuardDuty members to potentially\n  deploy the GuardDuty member role in. For example, in the included \n  [`guardduty-member-account-role.yml`](cloudformation/guardduty-member-account-role.yml),\n  it assumes that you'll have deployed `cloudformation-sns-emission-consumer.yml`\n  in both `us-west-2` and `us-east-1`\n* Customize the \n  [`guardduty-member-account-role.yml`](cloudformation/guardduty-member-account-role.yml)\n  CloudFormation template which you'll distribute to your members. \n  * You need to set two values in the `Mappings` section of the template\n    * `MasterAccount`:`Principal` : Set this to the \n      [root principal](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html#Principal_specifying)\n      of your AWS account in which you're running the GuardDuty master. For\n      example `arn:aws:iam::123456789012:root`\n    * `SNSTopicForPublishingIAMRoleArn`:`Account` : Set this to the \n      [AWS Account ID](https://docs.aws.amazon.com/general/latest/gr/acct-identifiers.html#FindingYourAccountIdentifiers)\n      of the AWS account that you've deployed the \n      [Cloudformation Cross Account Outputs](https://github.com/mozilla/cloudformation-cross-account-outputs/)\n      service in. For example `123456789012`.\n  * Add any additional regions that you wish to support (which you've deployed \n    Cloudformation Cross Account Outputs in) into the \n    `TheRegionYouAreDeployingIn` mapping following the example of the existing\n    two regions listed there already.\n  \n### Getting Started\n\n* Deploy the Cloudformation Stack from\n  [`cloudformation/guardduty-multi-account-manager-parent.yml`](https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/guardduty-multi-account-manager/cf/guardduty-multi-account-manager-parent.yml) in the master\n  account. [![Launch GuardDuty Multi Account Manager](https://s3.amazonaws.com/cloudformation-examples/cloudformation-launch-stack.png)](https://console.aws.amazon.com/cloudformation/home?region=us-west-2#/stacks/new?stackName=guardduty-multi-account-manager&templateURL=https://s3-us-west-2.amazonaws.com/public.us-west-2.infosec.mozilla.org/guardduty-multi-account-manager/cf/guardduty-multi-account-manager-parent.yml)\n\n* The stack will spin up and create all Master Detectors in all regions, a\n  normalization functions, and all SNS Topics with CloudWatch events.\n\n### Onboarding Accounts\n\n1. Ensure that the the mappings are configured in the\n   [`cloudformation/guardduty-member-account-role.yml`](cloudformation/guardduty-member-account-role.yml)\n   template as described above\n2. Deploy the customized [`cloudformation/guardduty-member-account-role.yml`](cloudformation/guardduty-member-account-role.yml)\n   CloudFormation template in your member AWS accounts. This CloudFormation template should only be deployed once in a single\n   region in each member AWS account. The account will then register with the master account and go through the invitation\n   process automatically for every region.\n\n## AWS re:invent 2018 SEC403 Presentation\n\n* [Watch our presentation on GuardDuty Multi Account Manager](https://www.youtube.com/watch?v=M5yQpegaYF8&t=1889) at AWS re:Invent 2018\n* [Read the slides](https://www.slideshare.net/AmazonWebServices/five-new-security-automations-using-aws-security-services-open-source-sec403-aws-reinvent-2018/47)\n\n## License\n\nguardduty-multi-account-manager is Licensed under the\n[Mozilla Public License 2.0 ( MPL2.0 )](https://www.mozilla.org/en-US/MPL/2.0/)\n\n## Contributors\n\n* [Gene Wood](https://github.com/gene1wood/)\n* [Andrew Krug](https://github.com/andrewkrug/)\n"
},
{
  "name": "perf-consultations",
  "files": {
    "/": [
      ".github",
      "README.md",
      "_config.yml",
      "fluent-request.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": "# perf-consultations"
},
{
  "name": "monolith-client",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.rst",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "elasticsearch.yml",
      "jenkins.sh",
      "monolith",
      "requirements",
      "setup.py"
    ]
  },
  "makefile": "HERE = $(shell pwd)\nPYTHON = python\n\nINSTALL = pip install --no-deps\nVTENV_OPTS ?= --distribute\n\n.PHONY: all clean test\n\nall: build\n\nbuild:\n\t$(INSTALL) -r requirements/prod.txt\n\t$(INSTALL) -r requirements/dev.txt\n\t$(INSTALL) -r requirements/test.txt\n\t$(PYTHON) setup.py develop\n\ntest:\n\tnosetests -s -d -v --with-xunit --with-coverage --cover-package \"monolith.client\" monolith\n",
  "readme": "Monolith Client\n===============\n\nMozilla Monolith is an application to provide statistics and charts for\nanalytics like time series data.\n\nThis library implements a client to interact with the REST-ful web service.\n\nYou can read more about it at https://mozilla-monolith.readthedocs.org/\n\n.. note::\n\n    Note: monolith is still under heavy development and not yet useful.\n"
},
{
  "name": "PyHawk",
  "files": {
    "/": [
      ".gitignore",
      "CHANGES.txt",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "compatibility",
      "dev_requirements.txt",
      "hawk",
      "sample_client.py",
      "sample_server.py",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": ".. important::\n\n    **2017: THIS LIBRARY IS UNMAINTAINED AND MAY BE INSECURE. PLEASE USE ONE OF\n    THE ALTERNATIVES LISTED BELOW.**\n\n\nPython Libraries for HAWK\n==========================\n\nHawk_ is an HTTP authentication scheme using a message authentication code (MAC) algorithm to provide partial HTTP request cryptographic verification.\n\n.. _Hawk: https://github.com/hueniverse/hawk\n\nPyHawk is great for consuming or providing webservices from Python.\n\nAlternatives\n------------\n\nPyHawk's goal is to track as closely to the original NodeJS' hawk code,\nbecause hawk is a primarily an authentication scheme documented by\nthe implementaiton (as opposed to a standard).\n\nIf you find this module un-pythonic, also consider:\n\n* mohawk_ Pythonic Hawk library\n\n* hawkauthlib_\n\n.. _mohawk: https://github.com/kumar303/mohawk\n.. _hawkauthlib: https://github.com/mozilla-services/hawkauthlib\n\nUsage (Client Side)\n-------------------\n\nIf you had code that consumed a HAWK authenticated webservice,\nyou could do something like the following\n\n::\n\n    import hawk\n    import requests\n\n    # Hawk is secured with a shared secret\n    credentials = db.lookup_secrets(some_id)\n\n    # Prepare your request headers\n    header = hawk.client.header(url, 'GET', {\n        'credentials': credentials,\n        'ext': 'Yo Yo'})\n\n    # Which goes into Authorization field of HTTP headers\n    headers = [('Authorization', header['field'])]\n    res = requests.get(url, data=params, headers=headers)\n\n    response = { 'headers': res.headers }\n\n    # We can verify we're talking to our trusted server\n    verified = hawk.client.authenticate(response, credentials,\n                                        header['artifacts'],\n                                        {'payload': res.text})\n    if verified:\n        print res.text\n    else:\n        print \"Something fishy going on.\"\n\nSee `sample_client.py`_ for details.\n\n.. _`sample_client.py`: https://github.com/mozilla/PyHawk/blob/master/sample_client.py\n\nUsage (Server side)\n-------------------\nIf you provide a webservice and want to do authentication via HAWK,\ndo something like the following:\n\n::\n\n\n    import hawk\n\n    # A callback function for looking up credentials\n    def lookup_hawk_credentials(id):\n        # Some collection of secrets\n        return db.lookup(id)\n\n    # req is a Request object from your webserver framework\n    if 'Hawk ' in req.headers['Authorization']:\n        return check_auth_via_hawk(req)\n    else:\n        return failure(req, res)\n\n    def check_auth_via_hawk(req):\n        server = hawk.Server(req, lookup_hawk_credentials)\n\n        # This will raise a hawk.util.HawkException if it fails\n        artifacts = server.authenticate()\n\n        # Sign our response, so clients can trust us\n        auth = server.header(artifacts,\n                                 { 'payload': payload,\n                                   'contentType': 'text/plain' })\n\n        headers = [('Content-Type', 'text/plain'),\n                       ('Server-Authorization', auth)]\n\n        start_response(status, headers)\n\n        return payload\n\nSee `sample_server.py`_ for details.\n\n.. _`sample_server.py`: https://github.com/mozilla/PyHawk/blob/master/sample_client.py\n\nLogging\n-------\n\nPyHawk uses `python logging`_ to emit information about why authorization is\nfailing and so on. You can configure these logger channels with ``INFO``,\n``DEBUG``, etc, to get some helpful output.\n\n**hawk**\n    All hawk logging, including everything below.\n\n**hawk.client**\n    All hawk client related messages, including header construction.\n\n**hawk.server**\n    All hawk server related messages, including authorization.\n\n**hawk.hcrypto**\n    All hawk crypto related messages, including bewit handling.\n\n**hawk.util**\n    All shared hawk code such as header normalization.\n\n\n.. _`python logging`: http://docs.python.org/2/library/logging.html\n\n\nStatus\n------\n\n**2017: This library is unmaintained and probably insecure.**\n\nDevelopment\n-----------\n\nOptionally use `env` as a virtualenv\n\n::\n\n    virtualenv env\n    source env/bin/activate\n\n\nLocally install source:\n\n::\n\n    python setup.py develop\n\nUnit tests are in `hawk/tests`.\n\n::\n\n    python hawk/tests/test_*.py\n\n\nAdditionally, one can test compatibility:\n\nThe `compatibility/nodejs` directory has a server.js and a client.js (Node code) which are from HAWK's usage.js.\n\nTo test the server, do the following:\n\n1) python sample_server.py\n2) cd compatibility/nodejs/\n3) node client.js\n\nOutput should be\n\n::\n\n    Authenticated Request is 200 (OK)\n    Response validates (OK)\n    Unauthenticated request should 401 - (OK)\n\nNote: the port numbers in test_pyhawk.py and client.js must match.\n\nTo test the client, do the following:\n\n1) cd compatibility/nodejs/\n2) node server.js\n3) cd ../..\n4) python sample_client.py\n\nOutput should be\n\n::\n\n    Response validates (OK)\n\nPublishing Versions\n-------------------\n\nEdit setup.py and bump the version number.\n\n::\n\n    python setup.py sdist upload\n\nYou should see your updates at https://pypi.python.org/pypi?%3Aaction=pkg_edit&name=PyHawk\n"
},
{
  "name": "esFrontLine",
  "files": {
    "/": [
      ".gitignore",
      "MANIFEST.in",
      "README.md",
      "esFrontLine",
      "resources",
      "setup.py",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "esFrontLine\n===========\n\nLimit restful requests to backend ElasticSearch cluster:  Queries only.\n\n\nRequirements\n------------\n\n  * Python 2.7\n  * An ElasticSearch cluster to forward queries to\n\n\nInstall\n------------\n\nI will assume you have Python installed (if not, here are [Windows7 instructions](https://github.com/klahnakoski/pyLibrary#windows-7-install-instructions-))\n\n    pip install esFrontLine\n\nSetup\n-----\n\nYou must write your own setting.json file with the following properties set:\n\n  * **elasticsearch** - (Array of) ElasticSearch nodes\n\n  * **elasticsearch.host** - URL of the ElasticSearch node that will accept query requests\n\n  * **elasticsearch.port** - port for ES (default = 9200)\n\n  * **flask** - flask.run() parameters (default port = 5000)\n\n  * **debug** - turn on debugging\n\n  * **whitelist** - list of indexes that are allowed\n\nHere is an example of my ```settings.json``` file\n\n    {\n        \"elasticsearch\":[{\n            \"host\":\"http://elasticsearch4.metrics.scl3.mozilla.com\",\n            \"port\":9200\n        },{\n            \"host\":\"http://elasticsearch5.metrics.scl3.mozilla.com\",\n            \"port\":9200\n        },{\n            \"host\":\"http://elasticsearch7.metrics.scl3.mozilla.com\",\n            \"port\":9200\n        },{\n            \"host\":\"http://elasticsearch8.metrics.scl3.mozilla.com\",\n            \"port\":9200\n        }],\n        \"flask\":{\n            \"host\":\"0.0.0.0\",\n            \"port\":9292,\n            \"debug\":false,\n            \"threaded\":true,\n            \"processes\":1\n        },\n        \"whitelist\":[\"bugs\", \"org_chart\", \"bug_summary\", \"reviews\"],\n        \"debug\":{\n            \"log\":[{\n                \"filename\": \"./tests/results/logs/app.log\",\n                \"maxBytes\": 10000000,\n                \"backupCount\": 200,\n                \"encoding\": \"utf8\"\n            },{\n                \"stream\":\"sys.stdout\"\n            }]\n        }\n\n    }\n\nExecution\n---------\n\n    python app.py --settings-file <path_to_file_with_JSON_settings>\n\nCode Source\n-----------\n\n[https://github.com/klahnakoski/esFrontLine](https://github.com/klahnakoski/esFrontLine)\n"
},
{
  "name": "MakeAPI",
  "files": {
    "/": [
      ".bowerrc",
      ".gitignore",
      ".npmignore",
      ".travis.yml",
      "Gruntfile.js",
      "LICENSE",
      "Procfile",
      "README.md",
      "bower.json",
      "env.sample",
      "lib",
      "migrations",
      "package.json",
      "public",
      "routes",
      "scripts",
      "server.js",
      "test",
      "views"
    ]
  },
  "makefile": null,
  "readme": "##### This project is no longer under active development\n\n[![Build Status](https://travis-ci.org/mozilla/webmaker.org.png)](https://travis-ci.org/mozilla/MakeAPI)\n[![Dependency Status](https://gemnasium.com/mozilla/webmaker.org.png)](https://gemnasium.com/mozilla/MakeAPI)\n\n# MakeAPI (core)\nThe MakeAPI is a node.js based service for storing and exposing metadata about user created web content, called \"makes\". It provides consumers with an API to Create, Update, Delete, and Search for metadata about a make.\n\n**NOTE: This README assumes that you have all the required external dependencies installed and have a working dev environment. New to Webmaker? Make sure to read our <a href=\"https://wiki.mozilla.org/Webmaker/Code\">developer guide</a> for everything you need in order to get started!**\n\n## 1. Installing & Running the Server\n\n### Dependencies\n\n- [MongoDB](http://www.mongodb.org/)\n  - *Mac OS X*: Run this with `mongod`\n  - *Ubuntu-based linux systems.*: Run this with `sudo service mongodb start`\n- [ElasticSearch](http://www.elasticsearch.org/)\n  - *Mac OS X*: Run this with `elasticsearch -f`\n  - *Ubuntu-based linux systems.*: Run this with `sudo service elasticsearch start`\n- [Login server](https://github.com/mozilla/login.webmaker.org)\n\n\n### Installation\n\n1. Clone the git repository with `git clone https://github.com/mozilla/MakeAPI.git`\n2. Execute `npm install` in the application directory.\n3. In the root directory of the application, copy `env.sample` to a new file called `.env`\n\n    **NOTE**: The `.env` file contains settings for various aspects of the MakeAPI server's operation. The default settings should be enough for local development. For non-standard configurations, you may need to adjust where variables point to match the locations of your external dependancies. See the [ENV file reference](https://github.com/mozilla/MakeAPI/wiki/ENV-File-Reference) for more details.\n\n#### Running the Node Server\n\n1. Ensure all external dependencies are running\n2. Run `node server.js` from the root of the MakeAPI application\n\n    By default the server will run at http://localhost:5000. You can change this by adding PORT=<port> to your .env file.\n\n#### Clearing make data\n\nClear elastic search:\n\n`curl -XDELETE \"<URL of ElasticSearch Cluster>/<ElasticSearch Index Name>\"`\n\nFind your mongo files and clear them.\n\n1. In a terminal run `mongo` add the --host <YOUR_MONGO_HOST> flag to connect to a remote mongo service\n2. From the mongo shell run `use makeapi` to switch to the makeapi index\n3. Run `db.dropDatabase()` to remove all existing make data and API keys from mongoDB\n\n## 2. API Documentation\n\nAPI documentation can be found at [https://mozilla.github.io/makeapi-docs/](https://mozilla.github.io/makeapi-docs/)\n\nThe [makeapi-client JavaScript library](https://github.com/mozilla/makeapi-client) can be used to facilitate interaction with the API in web browser and node contexts. Documentation for the client library can be found at [http://mozilla.github.io/makeapi-docs/client-docs/](http://mozilla.github.io/makeapi-docs/client-docs/)\n\n**The Make API Does not sanitize Data it receives or outputs, so it is up to consumer applications to sanitize data appropriately.**\n\n## 3. Resources\n\n### Env Variable Reference Section\nAll the environment variables are listed and detailed here: [https://mozilla.github.io/makeapi-docs/#configuration](https://mozilla.github.io/makeapi-docs/#configuration)\n\n## 4. Testing\n### How to test\nWe use a combination of technologies to \"lint\" and test our CSS and JavaScript code. These tests **must** pass in order for a pull request to be merged into the Mozilla repository. To run them locally,\n\n1.  Navigate to the root folder of the MakeAPI server\n2.  Run `npm test`\n\n### TravisCI\nWhen a pull request is made to the Mozilla repository, it is automatically scheduled for testing on the [Travis-CI continuous-integration platform](https://travis-ci.org/). This verifies that the code passes linting requirements as well as all of its unit tests. You can see the status of these tests on the Github page for the pull request, and on the <a href=\"https://travis-ci.org/mozilla/MakeAPI/pull_requests\">MakeAPI travisCI page</a>.\n\n### Updating tests\nMost developers won't need to update the tests, but changes to certain parts of the MakeAPI require that the tests be revised. Keeping these tests accurate is essential for easy maintenence of this code base, and pull requests that change these parts will be rejected without proper unit tests.\n\nIf you need help understanding the unit tests, hop on the #webmaker IRC channel and we'll be happy to help! No idea what IRC is? Check out our [IRC guide](https://wiki.mozilla.org/IRC).\n\n## 5. Accessing the MakeAPI with your service\n\nThe MakeAPI uses [HAWK](https://github.com/hueniverse/hawk) to authenticate MakeAPI calls. Hawk is an HTTP authentication scheme that can be used to verify the authenticity of messages using cryptographically generated message authentication codes (MACs). Hawk does not provide Transport Layer security, and only serves to identify the sender and verify message integrity.\n\nAll applications that wish to make authenticated calls must be issued a pair of keys to sign all requests. Keys are optional for search requests.\n\n### Authentication in a dev environment\nFor convenience of development and testing, the `USE_DEV_KEY_LOOKUP` variable can be set to true in the environment file. This flag will use a **DEVELOPMENT ONLY** strategy when verifying keys.\n\nWhen development key mode is enabled, clients can sign their requests by passing hawk `\"00000000-0000-0000-000000000000\" as their public and private key. Any other key combination will fail to authenticate.\n\n**DO NOT USE DEV KEYS OUTSIDE OF A DEVELOPMENT ENVIRONMENT!**\n\n### Generating keys for production or staging servers w/ `generateKeys.js`\n\nThe generation of keys can be done using the [generateKeys](https://github.com/mozilla/MakeAPI/blob/master/scripts/generateKeys.js) script.\n\nThe script is called with two arguments: an email address to associate with the keys and a integer indicating the number of pairs to be generated. Generated keys are added to the database and then outputted to the console.\n\n### Generating keys in the admin console\n\nThere is a tool in the Admin Make Editor that generates keys. It can be reached by visiting the /admin page of the MakeAPI in your browser. You must have a Webmaker admin account on the login server that the MakeAPI is using for authentication.\n\n##  6. Metrics and logging\nThe MakeAPI server uses a number of technologies, like [STATSD](https://github.com/etsy/statsd/) and [New Relic](http://newrelic.com/), to optionally collect and analyze useful performance data. For local development, this shouldn't be a concern.\n\nFor more information on configuring the MakeAPI server's New Relic module, see: https://github.com/newrelic/node-newrelic/#configuring-the-agent\n"
},
{
  "name": "betachannel",
  "files": {
    "/": [
      ".gitignore",
      "Gruntfile.js",
      "README.md",
      "build",
      "docs",
      "i18n",
      "node_modules",
      "package.json",
      "scripts",
      "server",
      "www"
    ],
    "/docs": [
      "AWS.md",
      "DEVELOPER_NICETIES.md",
      "OPERATIONS.md",
      "SPEC.md",
      "db",
      "deployment_architecture.png",
      "deployment_architecture.svg",
      "wireframes.png",
      "wireframes.svg"
    ]
  },
  "makefile": null,
  "readme": "# BetaFox\n\nGive beta testers access to your Apps\n\nThis is the source code to the BetaFox webapp.\n\nFirefoxOS, Firefox for Android and other future runtimes let developers write Open Web Apps.\nDistributing privileged apps to remote testers is very difficult.\n\nThis webapp allows developers to upload their packaged apps and\ngives them versioned install links.\nBeta testers can install the app from the link.\n\n## Dependencies\n\n* NodeJS 10.x or greater\n* Python 2.7 or greater, but less than Python 3.x\n* MySQL\n* [NSS Tools](https://developer.mozilla.org/en-US/docs/Mozilla/Projects/NSS/Tools)\n* zip/unzip (UnZip, by Info-ZIP will work)\n\nSee docs/AWS.md for a fast, simple Ubuntu/EC2 based install.\n\n## Developer Installation\n\n    $ cp server/config/default.js  server/config/developer.js\n    # customize developer.js config file as you see fit... then\n    $ mysql < docs/db/schema_up_000.sql\n    $ npm install\n    $ npm start\n\n`npm install` isn't required for stage and production servers.\nThe node modules required are checked in with the source code.\n`npm rebuild` is required for all environments.\n\nThe webapp is available at http://localhost:8000\n\n## Development\n\nClient side assets are prepared and built with [GruntJS](http://gruntjs.com/).\n\n    grunt\n\nMinified or built assets are commited into `git`.\n\n## Get Involved\n\nhttps://wiki.mozilla.org/Mobile/Projects/BetaFox\n"
},
{
  "name": "gitribution",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Procfile",
      "README.md",
      "fetch_data.js",
      "lib",
      "newrelic.js",
      "package.json",
      "reset.js",
      "sample.env",
      "sql",
      "tasks.todo",
      "test.js",
      "to_track.js",
      "web.js",
      "worker.js"
    ]
  },
  "makefile": null,
  "readme": "gitribution\n===========\n\nExtracts data from the Github API and allows it to be queried for particular contribution activities\n\nLooks at the people (github logins) involved in:\n* commits\n* issues\n\nThis app uses membership of the github organisations to flag staff, plus a list of extra github logins in to_track.js. This is not 100% accurate, but a pretty good indicator of who is staff.\n\n## Prerequisites:\n\n* node\n* heroku toolbelt\n* mysql db\n\n## Setup an activities table in mysql\nSee script in sql/create_table.sql\n\n## Environment Config\n\nFor local dev, copy sample.env to .env and add credentials\nSet equivilent environment variables on Heroku\n\n## Tracking Config\n```\nto_track.js\n```\n\n## Running the app:\n\n```\nforeman start\n```\n\n## Clear and rebuild the database\n\nRun this script to clear and rebuild the database (useful if we change repo names or move around org accounts etc...)\n\n```\nforeman run node reset\n```"
},
{
  "name": "all-aboard",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      ".jpmignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "data",
      "index.js",
      "lib",
      "locale",
      "package.json",
      "test",
      "tests-functional"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/mozilla/all-aboard.svg?branch=master)](https://travis-ci.org/mozilla/all-aboard)\n\n#All Aboard\nThe Mozilla Firefox Educational Tool\n\n## Installing All-Aboard\n\nYour first step in trying this, in development add-on, is to tweak your Firefox config a little. Open up Firefox and go to `about:config`. Click on the blue button to acknowledge that you will be careful, and search for:\n\n```\nxpinstall.signatures.required\n```\n\n[See SUMO if you get stuck](https://support.mozilla.org/en-US/kb/add-on-signing-in-firefox?#w_override-add-on-signing-advanced-users)\n\nDouble click on the item and it's value should change to false. You can now [download and install](https://github.com/mozilla/all-aboard) the add-on.\n\n## First run\n\nIn order to trigger the onboarding experiment, you must visit the firstrun page and answer the two questions your are prompted with.\n\n```\nhttps://www.mozilla.org/firefox/47.0/firstrun/\n```\n\n## Contributing\n\nTo run the add-on during development and testing, you will need to first follow the `jpm` installation instructions found on MDN here:\nhttps://developer.mozilla.org/en-US/Add-ons/SDK/Tools/jpm#Installation\n\nOnce installed you can launch the add-on as follows:\n\n```\njpm run --binary-args www.mozilla.org/en-US/firefox/47.0/firstrun/\n```\n\nTo run the addon in keeping with proper sessioning, you can launch the addon using:\n```\njpm run --binary-args www.mozilla.org/en-US/firefox/47.0/firstrun/ --profile PROFILE_NAME --no-copy\n```\n\nTo ease development and testing of the add-on, it is possible to configure the time intervals and elapsed time formula using a JSON config file.\n\nIn the root of the project folder, add a file called `config.json` as follows:\n\n```\n{\n    \"afterInteractionCloseTime\": 5000,\n    \"defaultSidebarInterval\": 10,\n    \"defaultSidebarCloseTime\": 7000,\n    \"timeElapsedFormula\": 1000,\n    \"waitInterval\": 10000\n}\n```\n\nThe values are defined as follows:\n\n* `afterInteractionCloseTime` - The amount of time to wait before auto closing the sidebar after user interaction.\n* `defaultSidebarInterval` - Time between sidebars defined in seconds\n* `defaultSidebarCloseTime` - Time to wait before auto closing the sidebar if there is no interaction by the user.\n* `timeElapsedFormula` - This is the formula used to convert milliseconds to either minutes, hours etc. If you for example set this to `1000`, it will devide the milliseconds to seconds.\n* `waitInterval` - This is the interval, set in milliseconds, that the timer will wait until triggering the next badge update and notification.\n\nIf the above file is not present, the add-on will use it\u2019s defaults of 24 hours.\n\n### Functional tests\n\nFunctional tests are implemented using Selenium and the python Marionette client to allow access to both web content and the browser chrome.\n\nTo write and run the tests you need a couple of additional dependecies:\n\n* (Python)[https://www.python.org/]\n* (Virtualenvwrapper)[https://pypi.python.org/pypi/virtualenvwrapper]\n* (Gecko Driver)[https://github.com/mozilla/geckodriver/releases]\n\nOnce you have the above installed, create a virtual environment for the project:\n\n```\nmkvirtualenv all-aboard\n```\n\nOnce the environment has been created, you need to install Tox:\n\n```\npip install tox\n```\n\nIn order to run the tests, you will either have to have the `geckodriver` on your path or, pass it on the command line. To add it to your path, run the following:\n\n```\nexport PATH=$PATH:/path/to/gecko/driver/\n```\n\nNote that currently you need the change the name of the driver from `geckodriver` to `wires` or else selenium will not detect the driver.\n\nRun the tests with Tox:\n\n```\ntox -e tests\n```\n\nYou can also specifiy a specific Firefox binary to run the tests again as follows:\n\n```\ntox -e tests -- --firefox-path=/Applications/FirefoxDeveloperEdition.app/Contents/MacOS/firefox\n```\n"
},
{
  "name": "loady",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "bin",
      "docs",
      "lib",
      "package.json"
    ],
    "/docs": [
      "development-tips.md"
    ]
  },
  "makefile": null,
  "readme": "Loady\n=====\n\nWe'll have your services crying loady, loady, loady\n\nStatus\n------\n\n**Note:** This code is very early and not ready for use.\n\nLoady is a load testing framework. You provide a config file and a set of\nNode modules that simulate user activity. Loady will generate thousands of\nrequests per second. You can estimate maximum capacity, stress test code, etc.\n\nInstallation\n------------\n\n    npm install loady\n\nUsage\n-----\n\n    ./node_module/.bin/loady -c ./tests/load_gen.js\n\nExample Loady Script ``load_gen.js``:\n\n    exports.config = function () {\n      return {\n        num_activities_per_day: 40,\n        app_name: 'Foobar 5000',\n        activities_dir: './lib/load/activities'\n      };\n    };\n\nOn average, a user does about 40 activities per day.  If you provide an application\nname it will be displayed with usage via `--help`. ``activities_dir`` is a path\n(relative to this config file) which contains different Activities -- simulated\nuser interactions.\n\nLoady Activities\n----------------\n\nYour load testing is accomplished through a series of Loady Activities.\nThese are simply Node modules with a ``startFunc`` function and a\n``probability`` float.\n\nExample ``lib/load/activities/simple.js``:\n    var common = require('../common');\n\n    exports.probability = 30 / 40;\n    exports.startFunc = function (cfg, cb) {\n      common.requestHomepage(cfg, function (err, statusCode) {\n        if (err) {\n            cb(err);\n        } else if (statusCode >= 500) {\n            cb('too much load');\n        } else if (statusCode !== 200) {\n            cb('error');\n        } else {\n            cb(null);\n        }\n      }\n    };\n\nWhich activity is run is random, but weighted by the probability. You'll\nwant to look at your different user activities and weight them accordingly. If\nthey sign up for an account once per year, this will be a very low number\n(say 1 / 365 * 24 * 60 * 60) and if the sign in ever two weeks\n(1 / 14 * 24 * 60 * 60) and if they hit an active_session activity almost\nevery time (39/40). Etc.\n\nLoady will do the math to normalize the probabilities and make sure they all\nadd up to 100% and\nscale them so that the frequency is roughly proportional over time.\n\nAs you change your application, you'll add new activities and may want to\ntweak the probabilities on older activties.\n\nListing Loady Activities\n------------------------\n\n    $ ./node_module/.bin/loady -c ./tests/load_gen.js -l\n    available activities: well_known, provision_no_session, auth, provision\n\n\nUser Database\n-------------\n\nLoady uses an in-memory representation of users. If you wish to control user\ncreation than add an Activity named ``signup``.\n\nHere is the simplest possible example ``lib/load/activities/signup.js``:\n\n    exports.startFunc = function (cfg, cb) {\n        var user = userdb.getNewUser();\n        if (!user) {\n          winston.error(\".getNewUser() should *never* return undefined!\");\n          process.exit(1);\n        }\n        userdb.addNewUser(user);\n        cb(null);\n      };\n\nReading Loady Output\n--------------------\n\nAverage active users simulated over the last 1s/5s/60s:\n\n    0.00    0.00    0.00    4 R, 4 S (4wn 0pn)\n    8657.28     8657.28     8657.28     4 R, 5 S (4wn 0pn)\n    10832.40    9744.84     9744.84     7 R, 8 S (7wn 0pn)\n    17331.84    12273.84    12273.84    10 R, 12 S (10wn 0pn) (1 503s)\n    25997.76    15704.82    15704.82    18 R, 18 S (18wn 0pn)\n    38918.88    20347.63    20347.63    24 R, 27 S (24wn 0pn) (3 ERRORS!)\n\n### ADU\n\nThe first three columns give you an idea of how many active daily users (ADU)\nyou can concurrently support. First column is on average in the last second,\nhow many users.\nSimilarly, the last column is averaged over the last 60 seconds.\n\n### Activities\n\nThe R column is the number of activities that are still running.\n\nThe S column is the total number of activities started in the last second.\n\nThis is followed by a list of activities still running broken down by firsta\nand last character of the activity name.\n\nSo for example:\n\n    8657.28     8657.28     8657.28     4 R, 5 S (4wn 0pn)\n\n4 **w**ake_o**n** activities are running. One other process was completed in\nthe last second. The typoe of the process completed is unknown, it could have\nbeen a wn or pn.\n\n### Errors\n\nThe output will also catch errors. These may be common as your app stabelizes\nunder load. Don't worry.\n\n    8657.28     8657.28     8657.28     4 R, 7 S (4wn 0pn) (1 ERRORS!) (2 503s)\n\nThis indicates that 1 of the activities ended in an error condition and\n2 of the activities finished with a 503.\n\nLoady Options\n-------------\n\nAdding options to loady CLI\n---------------------------\n\nYou can override or extend the command line options for loady to give yourself\nhooks to your own load testing activities:\n\n\n    var _ = require('underscore'),\n          loady = require('loady');\n\n    exports.config = function () {\n      return {\n        app_name: 'Foobar 5000',\n        activities_dir: './lib/load/activities'\n      };\n    };\n\n    exports.cli = function (defaults) {\n      var my_help = defaults['help'].slice();\n      my_help[loady.DESC_KEY] = 'Super cool load gen. Call 555-1212 for Help.';\n      var my_opts = {\n        help: loady_help\n      };\n      return _.extend(my_opts, defaults);\n    };\n\nIf you define a ``cli`` property which is a function that returns\n[optimist](https://github.com/substack/node-optimist) compatible configuration\nin the following format:\n\n    {\n        key: [SHORTCUT, DESC, FUNC],\n        ...\n    }\n\nThen for each key:\n\n* ``alias`` will be called with your SHORTCUT and KEY\n* ``describe`` will be called with your SHORTCUT and DESC\n* if present in arguments, FUNC callback will be invoked with your the value\n"
},
{
  "name": "make.mozilla.org",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "Vagrantfile",
      "bin",
      "docs",
      "fabfile.py",
      "fabric",
      "lib",
      "make_mozilla",
      "manage.py",
      "media",
      "migrations",
      "puppet",
      "requirements",
      "setup.py",
      "vagrantconfig.yaml",
      "vagrantconfig_local.yaml-dist",
      "vendor",
      "vendor-local",
      "wsgi"
    ],
    "/docs": [
      "Makefile",
      "_build",
      "_static",
      "_templates",
      "build-github.zsh",
      "conf.py",
      "index.rst"
    ]
  },
  "makefile": null,
  "readme": "****\n**This repo is now deprecated**\n\nwebmaker.org can now been found at https://github.com/mozilla/webmaker.org\n****\n\nLocal development\n=================\n\nIn addition to the more specific requirements below you need the standard set of software\nto get playdoh installed:\n\n* Xcode (with the Command Line Tools installed)\n* mySQL (a playdoh dependancy - though we're not using it for this project)\n* node and lessc\n* wget\n\nWe're using PostGIS + GeoDjango for the DB, so you'll also need the following installed:\n\n* Postgresql 8.4 +\n* PostGIS >= 1.4 && < 2.0\n* Geos\n* Proj4\n* GDAL\n\nMac OS X\n--------\n\nWith [Homebrew][brew], installing PostGIS and GDAL will install all you need:\n\n    brew update\n    brew install postgis gdal\n\n[brew]: http://mxcl.github.com/homebrew/\n\n**Note** The current version of PostGIS in Homebrew is version 2.0.0 - you need to install 1.5.3 (due to a numder of compatibility issues)\n\nUbuntu\n------\n\nFor Ubuntu 10.04, see the information in our Puppet config (`./puppet/manifests/dev.pp` and the classes in `./puppet/manifests/classes`)\n\nFor Ubuntu release >= 11.10, this should work:\n\n    sudo apt-get install binutils gdal-bin libproj-dev postgresql-9.1-postgis \\\n         postgresql-server-dev-9.1\n\nOnce those are installed, then `pip install -r requirements/compiled.txt` should \nwork as expected.\n\nMaking it run locally\n=====================\n\nCheckout the sourcecode and init the git submodules:\n\n    git clone --recursive git://github.com/mozilla/make.mozilla.org\n\nIf at any point you realize you forgot to clone with the recursive flag, you\ncan fix that by running:\n\n    git submodule update --init --recursive\n\nOnce you have all the dependencies installed, you need to create a settings\nfile.\n\n    cp make_mozilla/settings/local.py{-dist,}\n\nAnd then edit the settings file to match your needs, here is the section about\nthe databases:\n\n    DATABASES = {\n        'default': {\n            'ENGINE': 'django.contrib.gis.db.backends.postgis',\n            'NAME': 'webmaker',\n            'USER': 'postgres',\n            'PASSWORD': 'postgres',\n            'HOST': 'localhost',\n            'PORT': '5432',\n            'TEST_CHARSET': 'utf8',\n            'TEST_COLLATION': 'utf8_general_ci',\n        },\n    }\n\n\nSetting up the database\n=====================\n\nGeoDjango\n---------\n\nGeoDjango is installed as part of Django. You need to take a look at the installation \ninstructions at [https://docs.djangoproject.com/en/1.3/ref/contrib/gis/install/#post-installation] \nto get PostGIS configured properly with Django. Ubuntu's version ships with a postgis-template generation script, which you can see used in `./puppet/manifests/classes/postgis.pp`\n\nOnce you've installed postgis, you need to create a template for it, and then\ncreate your database with this template. For postgis 1.5:\n\n    cd /tmp && wget https://raw.github.com/django/django/master/docs/ref/contrib/gis/install/create_template_postgis-1.5.sh\n    sudo su postgres\n    /tmp/create_template_postgis-1.5.sh\n\nAnd finally create the database:\n\n    createdb -T template_postgis webmaker\n\nPopulating it\n-------------\n\n    ./manage.py syncdb\n    ./manage.py migrate\n    \n\nCompiling the assets\n--------------------\n\nYou also need to compile the assets, be sure to update your settings with the path to your LESS executable:\n\n    LESS_BIN = \"/usr/local/bin/lessc\"\n\nAnd then run:\n\n    ./manage.py compress_assets\n\nplaydoh: about the framework\n============================\n\nThis app is built using Mozilla's Playdoh.\n\nMozilla's Playdoh is a web application template based on [Django][django].\n\nPatches are welcome! Feel free to fork and contribute to this project on\n[github][gh-playdoh].\n\nFull Playdoh [documentation][docs] is available as well.\n\n[django]: http://www.djangoproject.com/\n[gh-playdoh]: https://github.com/mozilla/playdoh\n[docs]: http://playdoh.rtfd.org/\n\nLicense\n-------\nThis software is licensed under the [New BSD License][BSD]. For more\ninformation, read the file ``LICENSE``.\n\n[BSD]: http://creativecommons.org/licenses/BSD/\n\n"
},
{
  "name": "telemetry-next-node",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "index.js",
      "package.json",
      "test.js"
    ]
  },
  "makefile": null,
  "readme": "Telemetry.js v2 for Node\n========================\n\nThe `telemetry-next-node` module loads `telemetry.js` from `https://telemetry.mozilla.org/v2/telemetry.js` and make the functions available in node.js, so that telemetry dashboard aggregates can be analyzed server-side.\n\n**Warning**, this module downloads and **loads Javascript code** from `https://telemetry.mozilla.org/v2/telemetry.js` via HTTPS. If security is very important, run this inside an isolated environment such as a Docker container.\n\nFor long-running applications, it is recommended that the module be re-initialized every so often in order to obtain new data and account for any aggregate API changes. This can be done by calling `Telemetry.init()` again - see below for details.\n\nUsage\n-----\n\nA simple usage example:\n\n```js\nvar Telemetry = require('telemetry-next-node');\n\n// Initialize library\nTelemetry.init(function() {\n  var version = Telemetry.getVersions()[0]; // Get the first available version\n\n  // Optain a mapping from filter names to possible values of those filters\n  var parts = version.split(\"/\");\n  Telemetry.getFilterOptions(parts[0], parts[1], function(filters) {\n    console.log(\"Measures available:\");\n    filters.metric.forEach(function(measure) {\n      console.log(measure);\n    });\n  });\n});\n```\n\nNote that until `Telemetry.init(callback)` executes, this module will not have other methods than `Telemetry.init`. In the browser, these methods will be available, but they will throw an exception if `Telemetry.init` has not completed yet. This is a minor difference, but it may show up if testing for the existance of specific methods in the library.\n\n### Reloading\n\nJust like `telemetry.js` in the browser, you can reload version information by calling `Telemetry.init()` again. This also causes the code to be reloaded from `https://telemetry.mozilla.org/v2/telemetry.js`. Reloading is necessary in order to get updated information, such as aggregates published after the module has been loaded.\n\nReloading is necessary for long-running applications to obtain fresh data. An application that does not reload may behave erratically if changes are made to the backend.\n\nLicense\n-------\n\nThe `telemetry-next-node` library is released on the MPL license - see `LICENSE` for details.\n"
},
{
  "name": "infra-x-design",
  "files": {
    "/": [
      "README.md",
      "overview.png"
    ]
  },
  "makefile": null,
  "readme": "# Infra-X-Design (we are working on the name :p)\n\nWARNING: The below is an etherpad copy/paste for now. The intent is the README.md will be a very high level overview and \neach component will have its own .md file with a detailed description.\n\n![Overview](overview.png)\n\n[Source for image](https://cacoo.com/diagrams/UVEoW1R0nyYd7k37/edit)\n\n# Terminology\n\n  * Job - Has a 1-1 relationship with a commit / pull request\n  * Task - Type of Container and what is run on the container\n  * Container Provider - API to manage containers\n  * Task Request -  A list of tasks and treeherder \"job GUID\"\n  * Task Partition - task with a restricted set of files\n  * Execution Plan - group of task \n  * Container - Test environment (i.e. AWS, TravisCI, Mozilla Virtual Machine, or Mozilla physical hardware); managed by the Provisioner\n  * Commit - Could be a pull request or hg push, in treeherder this corresponds to a result set\n\n## Treeherder object mapping\n  * Job -> revision hash\n  * Job group (string) -> Task\n  * Job GUID -> Task Partition\n\n# Components\n\n## Job Master\n\n#### Role: Generates *jobs* and provides corresponding *Task Requests* to the Scheduler\n\nInitiation begins with a GitHub pull request or hg push\nIndependent piece of the system which deals with per-project buisness logic. Initates *jobs* via talking to the scheduler.  Different projects will implement their own task masters; there won't be a generic component that can be used by everyone.\nThe task master receives result notifications via pusle for each task that is initiated as the result of a job, and is the component that can be queried to determine the overall status of the job.\nCommunicates the status of a given Job (result set) to treeherder.\nShepherd version:  initiate job with treeherder via pulse, initiate job with scheduler via rest.  Implements rest service and/or pulse notification for job status.\n\n## Optimizer\n#### Role:\n\n## Scheduler\n\n#### Role: Central dispatching piece.\n\n  - Receive API request for a collection of tasks\n  - Given a particular set of *tasks requests* determine the types of *containers* to pass to the optimizer which returns an execution plan\n  - Give plan to provisioner\n  - Listen to status of *task-partition* and update the overall status of the *task* (pulse)\n  - Task Subdivisions map to \"Job\" groups in treeherder\n  - Maintains a queue of tasks that the provisioner can't service immediately\n\n## Provisioner (Machine Poofer)\n\n```js\n// Example object which would be sent to the provisioner\nvar Task =  {\n    name: \"...\",\n    files: [...],\n    containerTag: [\"linux_x86_64\"],\n    bootstrapper: [\"http://...\"],\n    bootstrapperArguments: [\"--revision=asdfasdf\", ...]\n}\n```\n\n#### Role: An interface to the *container providers*\n- Prioritization of container providers\n- Instantiation of containers (e.g., vm's) and related rules\n- Monitor container status/health (i.e. mozpool?)\n\n\n#### REST interface to:\n - types of containers\n - queue a task partition via an accept or reject response\n \n## Executioner\n#### Role: Inteface directly with machines via ip/port. Reports logs.\n\n  - is given the container ip + port number\n  - connect to the agent\n    1. pass the task to the agent\n    2. reads output / status from socket / write to log file\n    3. notifiy pulse with status and log file\n    4. notify provisioner completion of task-partition\n\n## Container + Agent\n#### Role: Agent reads task and builds a executable/script and passes output/status to the executioner\n"
},
{
  "name": "vaani.microphone-check",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "calculate-wer",
      "calculate-wer-baseline",
      "lib",
      "microphone-check",
      "package.json",
      "resources",
      "results",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# Vaani Microphone Check\n\n## Abstract \n\nThis repo contains a suite of tools used to compare microphones used for the Vaani project.\n\n## Introduction\n\nCurrently we are using this suite to compare the Nascent Object device microphones to other USB microphones. Generally, when comparing such microphones we will arrange a set of *n* Nascent Object devices at various distances from a sound source. Next to each such Nascent Object device we will place a Raspberry Pi connected to the other USB microphone we wish to evaluate. The general test set up is as follows:\n\n![Image of Test Setup](https://raw.githubusercontent.com/mozilla/vaani.microphone-check/master/resources/images/TestSetUp.png)\n\nThe sound source then \"reads\" aloud various example sentences, apropos for the Vaani project, and the various devices, Nascent Objects and RPi's, then record the produced audio.\n\n## Set-Up\n\nTo compare Nascent Object device microphones to other USB microphones we first prepare *n* Nascent Object devices with the host names *vaani-1*, *vaani-2*,...*vaani-n*. We then place them at specific measured distances from a sound source. (For our current tests we place *vaani-1* at 1 meter from the sound source, *vaani-2* at 2 meters from the sound source...)\n\nNext we prepare *n* Raspberry Pi devices with the host names *raspberrypi-1*, *raspberrypi-2*,...*raspberrypi-n* each connected the USB microphone we wish to evaluate. We then place the *m*-th Raspberry Pi device next to the *m*-th Nascent Object device, as pictured above.\n\nNext we have all the devices *vaani-1*, *vaani-2*,...*vaani-n*, *raspberrypi-1*, *raspberrypi-2*,...*raspberrypi-n* join the same WiFi network. This allows us, from this WiFi network, to login to *vaani-1* using the hostname *vaani-1.local*, to *vaani-2* using the hostname *vaani-2.local*,..., and to *raspberrypi-n* using the hostname *raspberrypi-n.local*.\n\nNext we clone this repository onto a computer connected to the sound source and on the same WiFi network as all of the devices. (We have only tested this with OS X.) This computer must then be configured to ssh into all devices without using a password. (This process is described here[[1]](http://www.linuxproblem.org/art_9.html).)\n\nThe final configuration step that must occur is adjusting the audio level of the sound source such that its volume emulates that of conversational speech. To do so one first requires a dB meter. (We used Decibel 10th[[2]](https://itunes.apple.com/us/app/decibel-10th-professional/id448155923?mt=8)) One then palces the dB meter at a distance of 1m from the sound source, plays any of the audio files in `resources/audio`, and then adjusts the volume of the sound source such that the audio files are 65 dB at 1m from the sound source. (65dB at 1m is an approximation of conversational speech[[3]](http://www.hearnet.com/at_risk/risk_trivia.shtml)).\n\n## Execution\n\nOnce on has completed all of the set-up steps, execution of the code is straight-forward. One `cd`'s into the `vaani.microphone-check` directory. Then one calls `./microphone-check` as follows\n```\nkdaviss-MacBook-Pro:vaani.microphone-check kdavis$ ./microphone-check <n> <corpus>\n```\nwhere `<n>` is replaced with the number of Nascent Object devices and `<corpus>`with the corpus one wishes to test. (The various corpora are identified by their directory name under the `resources/audio/` directory.\n\nUpon completion, the recordings from the various devices will be placed in the `results` directory. For the `n=3` case the results will appear as follows\n```\nresults\n\u251c\u2500\u2500 <corpus>\n    \u251c\u2500\u2500 no\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 device-1\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_nemorosas_to_my_list.wav\n    |   |   |   ...\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_tetonensis_to_my_list_please.wav\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 can_you_please_add_on_pilsners_to_my_list.wav\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 device-2\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_nemorosas_to_my_list.wav\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_tetonensis_to_my_list_please.wav\n    |   |   |   ...\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 can_you_please_add_on_pilsners_to_my_list.wav\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 device-3\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 add_anemone_nemorosas_to_my_list.wav\n    \u2502\u00a0\u00a0     \u251c\u2500\u2500 add_anemone_tetonensis_to_my_list_please.wav\n    |       |   ...\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 can_you_please_add_on_pilsners_to_my_list.wav\n    \u2514\u2500\u2500 rpi\n        \u251c\u2500\u2500 device-1\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_nemorosas_to_my_list.wav\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_tetonensis_to_my_list_please.wav\n        |   |   ...\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 can_you_please_add_on_pilsners_to_my_list.wav\n        \u251c\u2500\u2500 device-2\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_nemorosas_to_my_list.wav\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 add_anemone_tetonensis_to_my_list_please.wav\n        |   |   ...\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 can_you_please_add_on_pilsners_to_my_list.wav\n        \u2514\u2500\u2500 device-3\n            \u251c\u2500\u2500 add_anemone_nemorosas_to_my_list.wav\n            \u251c\u2500\u2500 add_anemone_tetonensis_to_my_list_please.wav\n            |   ...\n            \u2514\u2500\u2500 can_you_please_add_on_pilsners_to_my_list.wav\n```\nwhere `<corpus>` is the selected corpus, the `rpi` directory contains the Raspberry Pi results, and the `no` directory the Nascent Object results.\n\n## Evaluation\n\nEvaluation is done through calculation of the [WER](https://en.wikipedia.org/wiki/Word_error_rate) on the result and resource sets. (The resource set is located in `resource/audio/<corpus>/` and consists of the phrases used to drive the sound source.) Evaluation of the WER on the resource set provides a baseline WER from which the result WER's can be judged, as the resource set WER is not colored by microphones or distances.\n\n### Evaluation: Resource Set\n\nTo dertermine the WER for resource set, the repository contains a script `calculate-wer-baseline` that when executed as follows\n```\nkdaviss-MacBook-Pro:vaani.microphone-check kdavis$ ./calculate-wer-baseline\n```\npasses the resource set speech corpora through a STT engine and measures the WER of the resulting transcripts.\n\nThe WER result is then written to files of the form\n```\nresources/audio/<corpus>/RESULTS\n```\nwhich contain a single line of the form\n```\nWER: 0.1553679653679652\n```\n\n### Evaluation: Result Set\n\nTo determine the WER for the various microphone/distance pairings of the result set, the repository contains a script `calculate-wer` that when executed as follows\n```\nkdaviss-MacBook-Pro:vaani.microphone-check kdavis$ ./calculate-wer --corpus corpus-1\n```\npasses the result set speech `corpus-1` through a STT engine and measures the WER of the resulting transcripts.\n\nThe WER results are then written to files of the form\n```\nresults/<corpus-1>/no/device-1/RESULTS\nresults/<corpus-1>/no/device-2/RESULTS\nresults/<corpus-1>/no/device-3/RESULTS\n...\n```\ncorresponding to the various microphone/distance pairings for `corpus-1`. Each such file contains a single line of the form\n```\nWER: 0.2053679653679652\n```\n"
},
{
  "name": "navras",
  "files": {
    "/": [
      ".env-dist",
      ".gitignore",
      "LICENSE",
      "README.md",
      "manage.py",
      "navras",
      "requirements"
    ]
  },
  "makefile": null,
  "readme": "# navras\n\nContribution Pathways\n\n## install\n\nThis assumes that you have already cloned the repo locally and installed `python-virtualenvwrapper`.\n\nOn first run you should create a new virtualenv\n\n`mkvirtualenv navras -a [your_code_path]`\n\nActivate your virtualenv and install requirements:\n\n```\nworkon navras\npip install -r requirements/dev.txt\n```\n\nSet your enviromental variables:\n\n`cp .env-dist .env`\n\nSetup the database:\n\n`./manage.py migrate`\n\nAnd run the app\n\n`./manage.py runserver`\n"
},
{
  "name": "reclama",
  "files": {
    "/": [
      ".env-dist",
      ".gitignore",
      "Dockerfile",
      "LICENSE",
      "README.md",
      "bin",
      "docker-compose.yml",
      "manage.py",
      "reclama",
      "requirements"
    ]
  },
  "makefile": null,
  "readme": "# reclama\n\nA bug sprint experiment for events\n\n## install\n\nThis assumes that you have already cloned the repo locally and installed `python-virtualenvwrapper`.\n\nOn first run you should create a new virtualenv\n\n`mkvirtualenv reclama -a [your_code_path]`\n\nActivate your virtualenv and install requirements:\n\n```\nworkon reclama\npip install -r requirements/dev.txt\n```\n\nSet your enviromental variables:\n\n`cp .env-dist .env`\n\nSetup the database:\n\n`./manage.py migrate`\n\nAnd run the app\n\n`./manage.py runserver`\n"
},
{
  "name": "Bugzilla-ETL",
  "files": {
    "/": [
      ".circleci",
      ".editorconfig",
      ".gitignore",
      "LICENSE",
      "MANIFEST.in",
      "README.md",
      "bugzilla_etl",
      "docs",
      "requirements.txt",
      "resources",
      "setup.py",
      "tests",
      "vendor"
    ],
    "/docs": [
      "2018 Q1 Overview and Plan.md",
      "Architecture.png",
      "Architecture_2015.png",
      "Architecture_2015.pptx",
      "Architecture_2018.png",
      "Architecture_2018.pptx",
      "Bugzilla Lightning 161207.pdf",
      "Bugzilla Lightning 161207.ppt",
      "Getting Started.md",
      "Operations Support.md",
      "Replication.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "\n# Bugzilla-ETL\n\nExtract Bugzilla change history; Transform into bug snapshots; and Load into Elasticsearch  \n\n\n## Support\n\nIf you are here because the Mozilla's instance is down, please read the [Operation Support Document](docs/Operations%20Support.md)\n\n\n## Motivation and Details\n\n[https://wiki.mozilla.org/BMO/ElasticSearch](https://wiki.mozilla.org/BMO/ElasticSearch)\n\n## Requirements\n\n  * Python 3.6 (or PyPy to run fast)\n  * MySQL/Maria database with Mozilla's Bugzilla schema \n  * ElasticSearch >= 6.1 cluster to hold the bug snapshot documents\n\n## Installation\n\nPython and SetupTools are required.  It is best you install on Linux, but if \nyou do install on Windows please [follow instructions to get these installed]\n(https://github.com/klahnakoski/pyLibrary#windows-7-install-instructions-for-python).  \nWhen done, installation is easy:\n\n    git clone https://github.com/klahnakoski/Bugzilla-ETL.git\n\nthen install requirements:\n\n    cd Bugzilla-ETL\n    pip install -r requirements.txt\n\n**WARNING: `pip install Bugzilla-ETL` does not work** - I have been unable \nto get Pip to install resource files consistently across platforms and Python \nversions.\n\n## Installation with PyPy\n\nPyPy will execute 4 to 5 times faster then CPython.  PyPy maintains its own \nenvironment, and its own version of the module binaries.  This means running \nSetupTools is just a little different.  After\n\n    git clone https://github.com/klahnakoski/Bugzilla-ETL.git\n\nthen install requirements with PyPy's version of pip:\n\n    cd Bugzilla-ETL\n    c:\\PyPy27\\bin\\pip.exe install -r requirements.txt\n\nDespite my Windows example, the equivalent must be done in Linux.\n\n## Setup\n\nYou must prepare a `settings.json` file to reference the resources,\nand its filename must be provided as an argument in the command line.\nExamples of settings files can be found in [resources/settings](resources/settings)\n\n### Inter-Run State\n\nBugzilla-ETL keeps local run state in the form of two files:\n`first_run_time` and `last_run_time`.  These are both parameters\nin the ``settings.json` file.\n\n  * `first_run_time` is written only if it does not exist, and triggers a \n    full ETL refresh.  Delete this file if you want to create a new ES index \n    and start ETL from the beginning.\n  * `last_run_time` is recorded whenever there has been a successful ETL.  \n    This file will not exist until the initial full ETL has completed \n    successfully.  Deleting this file should have no net effect, other than \n    making the program work harder then it should.\n\n### Alias Analysis\n\nYou will require an alias file that matches the various email addresses that \nusers have over time.  This analysis is necessary for proper CC list history \nand patch review history.  [More on alias analysis](https://wiki.mozilla.org/BMO/ElasticSearch#Alias_Analysis).\n\n  * Make an `alias_analysis_settings.json` file.  Which can be the same \n    main ETL settings.json file.\n  * The `param.alias_file.key` can be `null`, or set to a AES256 key \n    of your choice.\n  * Run [alias_analysis.py](https://github.com/klahnakoski/Bugzilla-ETL/blob/master/resources/scripts/alias_analysis.bat)\n\n\n## Running bz_etl.py\n\nAsuming your `settings.json` file is in `~/Bugzilla_ETL`:\n\n    cd ~/Bugzilla_ETL\n\n    pypy bugzilla_etl\\bz_etl.py --settings=settings.json\n\nUse `--help` for more options, and see [example command line script](resources/scripts/bz_etl.bat)\n\n## Got it working?\n\nThe initial ETL will take over two hours.  If you want something\nquicker to confirm your configuration is correct, use `--reset\n--quick` arguments on the command line. This will limit ETL\nto the first 1000, and last 1000 bugs.\n\n    cd ~/Bugzilla_ETL\n    pypy bugzilla_etl\\bz_etl.py  --settings=settings.json --reset --quick\n\n## Using Cron\n\nBugzilla-ETL is meant to be triggered by cron; usually every 10 minutes.\nBugzilla-ETL limits itself to only one instance *per `settings.json`\nfile*:  That way, if more then one instance is accidentally run, the\nsubsequent instances will do no work and shutdown cleanly.\n\n## Running Tests\n\nThe Git clone will include test code. You can run those tests, but you must...\n\n  * Have MySQL installed (no Bugzilla schema required)\n  * Have an ElasticSearch (v 6.x+) cluster to hold the test results\n  * A complete `test_settings.json` file to point to the resources ([example](./resources/settings/test_settings.json))\n  * Use pypy (v5.9+) for 4x the speed: `pypy .\\tests\\test_etl.py --settings=test_settings.json`\n\n```python\npython -m pip install virtualenv\ncd ~/Bugzilla-ETL\n\npython -m virtualenv .env\n.env\\Scripts\\activate\npip install -r requirements.txt\nset PYTHONPATH=.;vendor\n\npython -m unittest discover -v -s tests\n```\n\n## Fixing tests\n\nTest runs are compared to documents found in the reference files at `tests/resources/reference`. They may need updating after changing the code.   \n\n    python -m unittest test_examples \n\nThe output file is found in `tests/results`, and can replace the reference file. Be sure to review the `git diff`; it will show the change in the reference file, just to be sure nothing went wrong.\n\n\n## Upgrades\n\nThere may be enhancements from time to time.  To get them\n\n    cd ~/Bugzilla-ETL\n    git pull origin master\n    pip install -r requirements.txt\n\nAfter upgrading the code, you may want to trigger a full ETL.  To do this,\nyou may either\n\n1.  run `bz_etl.py` with the `--reset` flag directly, or\n2.  remove the `first_run_time` file (and the next cron event will trigger a full ETL)\n\n## Submitting Bugs\n\nWe use Bugzilla for tracking bugs.  If you want to submit a bug or feature\nrequest, please [add a dependency to BZ ETL Metabug](https://bugzilla.mozilla.org/showdependencytree.cgi?id=959670&hide_resolved=0)\n\n\n## More on ElasticSearch\n\nIf you are new to ElasticSearch, I recommend using [ElasticSearch Head](https://github.com/mobz/elasticsearch-head)\nfor getting cluster status, current schema definitions, viewing individual\nrecords, and more.  Clone it off of GitHub, and open the `index.html` file\nfrom in your browser.  Here are some alternate [instructions](http://mobz.github.io/elasticsearch-head/).\n"
},
{
  "name": "activity-stream-okrs",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# activity-stream-okrs\nThese are quarterly team level OKR projects for the Activity Stream team.\n\n\n![Activity Stream](https://trello-attachments.s3.amazonaws.com/56f56a284ae0796984e3ca8e/59dbee134b3a6727ac0f835a/b5efb47fdc833746b75753db37a1f8e2/Screen_Shot_2017-10-16_at_12.58.26_PM.png)\n\n## 12 Month Vision\nActivity Stream is more than just a way of finding places on the web you\u2019ve already visited.  It is an extensible platform for intelligently aggregating user activity and for making recommendations on new content.   \n\nOver the next twelve months, Activity Stream will integrate external data sources and provide users with a surface to explore and experience their history in new ways.  We are rewriting the Firefox Library view to give users better access to their history, while at the same time adding new data types such as Videos, Synced Tabs, Screenshots, Pocketed Stories, and Downloads.\n\nWe are extending the Pocket integration to make smarter recomendations to users.  \n\nIn addition to work on about:home and about:newtab, we are generalizing the Snippets and Onboarding Tour messaging functionality into a centralized Message Center service, that will allow users to view and manage their Firefox notifications.  This system will be extended to be a general service for communication, notifications and messaging with the user.\n\nWe will be releasing a Web Extension API that will allow users/developers to add new sections the the Newtab page.  \n\nAs always, we remain commited to high performance and high stability.  We are implementing an interface to Sentry to give a better view into 'in the wild' runtime errors.  We are building a process to capture A/B perceived performance videos in order to prevent  perf regressions.  \n\nThe team is commited to product improvement through experimentation and data collection.  Each release will be guided and informed by multiple experiments to challenge assumptions and add confidence that we are release the most usable and useful features for our users.  \n"
},
{
  "name": "deepspeech-pt-br-scripts",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "dataframe_utils.py",
      "parallel_download_urls.sh"
    ]
  },
  "makefile": null,
  "readme": "# deepspeech-pt-br-scripts\nScripts and tools used to train pt-BR models for DeepSpeech\n"
},
{
  "name": "nmap-agent",
  "files": {
    "/": [
      ".rspec",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "Gemfile",
      "README.md",
      "Rakefile",
      "bin",
      "docker-compose.yml",
      "lambda",
      "lib",
      "nmap_agent.gemspec",
      "s3",
      "spec"
    ]
  },
  "makefile": null,
  "readme": "# nmap-agent (client)\n\nA container that performs NMAP scans and send results to S3 for post analysis\n\nInputs:\n  - target(s)\n  - scan options\n  - reporting endpoint\n\nOutputs\n  - Raw NMAP XML results sent to S3\n\nBenefits:\n  - simplified format\n  - deployable via docker\n  - pass inputs via ENV vars\n  - No running services\n  - Multiple perspectives...\n      * Scan from Docker => Prod Endpoint\n      * Scan from Docker => Docker Network\n      * Scan from Docker => VPC\n      \n# S3 bucket (server)\n\nA receiving location for scan results\n\nInputs:\n  - Uploads scan results via write only access (limit exposure if a single node is corrupted)\n  \nOutputs:\n  - S3 bucket scan results via read-only access (limit exposure if policy node is corrupted)\n  \nBenefits:\n  - No web application to secure/maintain\n  - Easy access to raw data for alternative uses\n  - Easy programmatics access to data store\n  - AWS/DevOps friendly\n\n# nmap2json post processing (Lambda function)\n\na simple lambda function, which is run on any file that changes in an S3 bucket ./xml folder and produces a simplified ./json equivalent.  JSON is simply an easier format to work with and reduces the barrier of entry for really anything to use this data, including the policy framework.\n\n# nmap-policy (TBD)\n\na policy/expectations framework for describing service expectations for a given perspective\n"
},
{
  "name": "JSBugMon",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README",
      "bugmon.py",
      "compileShell.py",
      "countCpus.py",
      "ionmonkey.py",
      "langfuzz.py",
      "marked.py",
      "secbugfixed.py",
      "subprocesses.py",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "== What JSBugMon is ==\n\nJSBugMon is a tool for automated tracking of JavaScript engine bug reports in\nMozilla's Bugzilla database. It is capable of automatically extracting tests\nfrom bug reports that crash or assert. It is also able to extract the affected\nrevision, guess required runtime options and architecture and determine what\nbuild type is required to reproduce.\n\n== What JSBugMon needs to track a bug ==\n\nIn order to track a bug, JSBugMon must first be able to find a working test\n(that crashes or asserts). The test can be an attachment to the bug or be part\nof the first comment (comment 0), even if the comment includes other text that\ndoes not belong to the test. The second requirement is an HG revision specified\nin comment 0.\n\nUsing this data, JSBugMon will try to build a JS shell with the specified HG\nrevision and try to reproduce the problem using the given test. It try different\nbuild types, runtime options and architectures, but will prefer the architecture\nspecified in the bug. If tracking for a bug was requested, but JSBugMon was not\nable to reproduce/track the bug, it will state this in the bug comments.\n\n== How JSBugMon can be used ==\n\nWhile Mozilla's JSBugMon instance performs several tasks on its own and fully\nautomatic (e.g. verification of fixed security bugs), most interaction needs to\nbe explicitly requested using the whiteboard. Commands for JSBugMon are added in\nthe whiteboard with a special tag that looks like this:\n\n[jsbugmon:cmd1,cmd2,cmd3...]\n\n\nValid commands are:\n\nupdate - The most basic command, it requests tracking for the bug. JSBugMon will\n  first attempt to reproduce the bug. If reproduction fails, it will comment and\n  unset the update flag. If reproduction succeeds, it will remain quiet and\n  comment once the bug no longer reproduces.\n\nreconfirm - Only valid together with the update command. It will perform the\n  same steps as for tracking, but even on successful reproduction, it will comment\n  in the bug. After doing so, it will ignore the bug (by setting the ignore flag,\n  see below) until otherwise requested.\n\nignore - JSBugMon will ignore any bug that has this command in it, regardless of\n  other commands. JSBugMon also sets this flag automatically after certain\n  commands.\n\nbisect - This command will cause JSBugMon to perform a bisection to find the\n  regressing changeset (the changeset that introduced the bug).\n\nbisectfix - This command will cause JSBugMon to perform a bisection to find the\n  fixing changeset in case the bug does no longer reproduce.\n\nverify-branch=b1;b2;b3.. - Using this command, it is possible to check if the\n  test reproduces on one or more specified branches (e.g. mozilla-aurora).\n  JSBugMon will comment with the result for every branch in the bug.\n\n\n\n== What JSBugMon currently supports (or does not support) ==\n\nJSBugMon currently only works with regular 32 and 64 bit Linux builds. It does\nnot support:\n\n  * Threadsafe builds \n  * Builds with additional flags like --enable-more-determinism\n    or --enable-root-analysis\n  * Checking a test with Valgrind or ASan\n"
},
{
  "name": "macos-desktop",
  "files": {
    "/": [
      ".gitignore",
      "Catalina Tests",
      "LICENSE.txt",
      "README.md",
      "set-desktop-catalina.sh",
      "set-desktop-mojave.sh"
    ]
  },
  "makefile": null,
  "readme": "# Set macOS Desktop Image\n\nJump to information on [macOS Catalina 10.15](#macos-mojave-1015)\n\nJump to information on [macOS Mojave 10.14](#macos-mojave-1014)\n\n---\n\n# macOS Mojave 10.15\n\n**IMPORTANT**: `set-desktop-catalina.sh` has been tested using **macOS Catalina Beta [ Developer 19A487l and Public 19A487m ]**\n\n## Purpose\nSet a user's Desktop image in macOS Catalina 10.15 from the command line. See [Limitations](#limitations)\n\n## Background\nAn explanation of how Desktop images appear to be managed by macOS since Mojave (10.14) can be found at [Setting the Desktop Image in macOS Mojave From the Command Line](https://www.tech-otaku.com/mac/setting-desktop-image-macos-mojave-from-command-line). While this article was written before the release of the Catalina Beta, the majority is still relevant to 10.15. \n\n## Usage\n`[bash] /path/to/set-desktop-catalina.sh <desktop image>`\n\n## Examples\n\n#### HEIF (.heic) images\n\n###### Catalina Automatic\n`[bash] /path/to/set-desktop-catalina.sh catalina`\n\n###### Catalina Light (Still)\n`[bash] /path/to/set-desktop-catalina.sh light`\n\n###### Catalina Dark (Still)\n`[bash] /path/to/set-desktop-catalina.sh dark`\n\n###### Mojave Dynamic\n`[bash] /path/to/set-desktop-catalina.sh mojave`\n\n###### Mojave Light (Still)\n`[bash] /path/to/set-desktop-catalina.sh mojave-light`\n\n###### Mojave Dark (Still)\n`[bash] /path/to/set-desktop-catalina.sh mojave-dark`\n\n###### Solar Gradients\n`[bash] /path/to/set-desktop-catalina.sh solar`\n\n#### non-HEIF (.heic) images\n\n###### High Sierra\n`[bash] /path/to/set-desktop-catalina.sh \"/System/Library/Desktop Pictures/High Sierra.jpg\"`\n\n###### Ink Cloud\n`[bash] /path/to/set-desktop-catalina.sh \"/System/Library/Desktop Pictures/Ink Cloud.jpg\"`\n\n#### Other\n\n###### Default\n`[bash] /path/to/set-desktop-catalina.sh default`\n\n## Error Messages\n\n###### Not running macOS Catalina\n`ERROR: For use with macOS Catalina 10.15.x only.`\n\n###### No Desktop image passed on the command line\n`ERROR: No image was specified.`\n\n###### Supplied Desktop image is a file that doesn't exist\n`ERROR: '/path/to/desktop/image.jpg' doesn't exist.`\n\n###### Invalid option passed on the command line\n`ERROR: 'option' is not a valid option.`\n\n###### Attempt to reset a database other than desktoppicture.db to the default\n`ERROR: The 'default' option is only appropriate when the database is ~/Library/Application Support/Dock/desktoppicture.db`\n\n###### Possible dual-monitor environment detected\n`ERROR: This script should not be used in a dual-monitor environment.`\n\n###### Two or more Desktops (Spaces) detected\n`ERROR: This script should not be used when multiple Desktops (Spaces) are configured.`\n\n\n## Limitations\nWorks only with a single Desktop (Space) in a single-display environment.\n\n__Not for use with multiple Desktops (Spaces) or in a dual-display environment.__\n\n---\n\n# macOS Mojave 10.14\n\n## Purpose\nSet a user's Desktop image in macOS Mojave 10.14 from the command line. See [Limitations](#limitations)\n\n## Background\nAn explanation for why this script does what it does can be found at [Setting the Desktop Image in macOS Mojave From the Command Line](https://www.tech-otaku.com/mac/setting-desktop-image-macos-mojave-from-command-line)\n\n## Usage\n`[bash] /path/to/set-desktop-mojave.sh <desktop image>`\n\n## Examples\n\n#### HEIF (.heic) images\n\n###### Mojave Dynamic\n`[bash] /path/to/set-desktop-mojave.sh mojave`\n\n###### Mojave Light (Still)\n`[bash] /path/to/set-desktop-mojave.sh light`\n\n###### Mojave Dark (Still)\n`[bash] /path/to/set-desktop-mojave.sh dark`\n\n###### Solar Gradients\n`[bash] /path/to/set-desktop-mojave.sh solar`\n\n#### non-HEIF (.heic) images\n\n###### High Sierra\n`[bash] /path/to/set-desktop-mojave.sh \"/Library/Desktop Pictures/High Sierra.jpg\"`\n\n###### Ink Cloud\n`[bash] /path/to/set-desktop-mojave.sh \"/Library/Desktop Pictures/Ink Cloud.jpg\"`\n\n#### Other\n\n###### Default\n`[bash] /path/to/set-desktop-mojave.sh default`\n\n## Error Messages\n\n###### Not running macOS Mojave\n`ERROR: For use with macOS Mojave 10.14.x only.`\n\n###### No Desktop image passed on the command line\n`ERROR: No image was specified.`\n\n###### Supplied Desktop image is a file that doesn't exist\n`ERROR: '/path/to/desktop/image.jpg' doesn't exist.`\n\n###### Invalid option passed on the command line\n`ERROR: 'option' is not a valid option.`\n\n###### Attempt to reset a database other than desktoppicture.db to the default\n`ERROR: The 'default' option is only appropriate when the database is ~/Library/Application Support/Dock/desktoppicture.db`\n\n###### Possible dual-monitor environment detected\n`ERROR: This script should not be used in a dual-monitor environment.`\n\n###### Two or more Desktops (Spaces) detected\n`ERROR: This script should not be used when multiple Desktops (Spaces) are configured.`\n\n\n## Limitations\nWorks only with a single Desktop (Space) in a single-display environment.\n\n__Not for use with multiple Desktops (Spaces) or in a dual-display environment.__\n"
},
{
  "name": "blok",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      ".web-extension-id",
      "LICENSE",
      "README.md",
      "docs",
      "package.json",
      "src",
      "tests",
      "web-ext-artifacts"
    ],
    "/docs": [
      "kpi-1.png",
      "metrics.md",
      "page-action-screenshot.png",
      "pop-up-screenshot.png",
      "profile-editor-screenshot.png",
      "testplan.md"
    ]
  },
  "makefile": null,
  "readme": "# Tracking Protection: Test Pilot Experiment\n## AKA Blok\n\n[![Build Status](https://travis-ci.org/mozilla/blok.svg?branch=master)](https://travis-ci.org/mozilla/blok)\n[![Coverage\nStatus](https://coveralls.io/repos/github/mozilla/blok/badge.svg)](https://coveralls.io/github/mozilla/blok)\n[![Available on Test Pilot](https://img.shields.io/badge/available_on-Test_Pilot-0996F8.svg)](https://testpilot.firefox.com/experiments/tracking-protection)\n\n[Web Extension](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/) re-implementation of [Tracking Protection for Firefox](https://support.mozilla.org/en-US/kb/tracking-protection-pbm).\n\nWe will run this add-on thru [Test Pilot experimentation](https://testpilot.firefox.com/experiments) to:\n\n* Measure web content breakage\n* Collect user feedback\n\nWhen we have breakage data and user feedback, we will change the tracking protection implementation, so users get better web experience with tracking protection.\n\n\n## Requirements\n\n* Firefox 48+\n\n\n## Run it\n\n1. [Download the latest `.xpi`](https://github.com/mozilla/blok/tree/master/web-ext-artifacts)\n2. In Firefox, \"Open File\" and select the `.xpi`\n\nWhen the add-on blocks tracker requests, you will see a Tracking Protection pageAction icon:\n\n![pageAction Screenshot](docs/page-action-screenshot.png)\n\nWhen you click the icon, you will see a pop-up, so you can report\nbroken/working pages, or toggle Tracking Protection on or off:\n\n![Pop-up Screenshot](docs/pop-up-screenshot.png)\n\n\n## Development\n\n1. Clone this repo locally\n2. `cd blok`\n3. `npm install`\n4. `npm run bundle`\n\n### Running the Code\n\nThis add-on depends on [`web-ext`](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext). Using `web-ext`, start a version of Firefox running the add-on like so:\n\n`./node_modules/.bin/web-ext run --source-dir=src --firefox-binary {path to Firefox 49+ binary}`\n\n### Development Environment\n\nAdd-on development is better with [a particular  environment](https://developer.mozilla.org/en-US/Add-ons/Setting_up_extension_development_environment). One simple way to get that environment set up is to install the [DevPrefs add-on](https://addons.mozilla.org/en-US/firefox/addon/devprefs/). You can make a custom Firefox profile that includes the DevPrefs add-on, and use that profile when you run the code in this repository. \n\n![profileEditor Screenshot](docs/profile-editor-screenshot.png)\n\n1. Make a new profile by running `{path to Firefox binary} -no-remote -P {new_profile_name}`, which launches the profile editor. \"Create Profile\" -- name it whatever you wish (e.g. 'blok_dev') and store it in the default location. It's probably best to deselect the option to \"Use without asking,\" since you probably don't want to use this as your default profile.\n\n2. Once you've created your profile, click \"Start Firefox\". A new instance of Firefox should launch. Go to Tools->Add-ons and search for \"DevPrefs\". Install it. Quit Firefox.\n\n3. Now you have a new, vanilla Firefox profile with the DevPrefs add-on installed. You can use your new profile with the code in _this_ repository like so:\n\n`./node_modules/.bin/web-ext run --source-dir=src --firefox-binary {path to Firefox 49+ binary} --firefox-profile {new_profile_name}`\n\nCheck out the [Browser Toolbox](https://developer.mozilla.org/en-US/docs/Tools/Browser_Toolbox) for more information about debugging add-on code.\n\n## Testing\n\nRequires node 6+\n\n`npm test`\n\n## Distributing\n\nTo distribute, you will need AMO access credentials. See the `web-ext` docs.\n\n1. Use [`web-ext\n   sign`](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/web-ext_command_reference#web-ext_sign)\n\n## FAQ, Footnotes, Appendices, etc.\n#### How does this compare with ublock, privacy badger, ghostery, etc.\nThe primary goal of this add-on experiment is to create a feedback loop for\ntracking protection users to provide data on problems & breakage, so we\n(Mozilla) can learn how to improve tracking protection technologies in ways\nthat maximize user privacy AND minimize web breakage.\n\nThe other privacy add-ons are great tools as well. The focus on reporting\nwebsite problems & breakages is the key difference here. In fact, we deferred a\nnumber of other features in favor of simplicity and to make the feedback\nmechanism the primary focus of the add-on.\n\nRead more on [the discourse forum](https://discourse.mozilla-community.org/c/test-pilot/tracking-protection).\n\n### How do I run the add-on without `web-ext`\n\n1. Go to `about:config` and set `xpinstall.signatures.required` to `false`\n2. Go to `about:debugging`\n3. Click \"Load Temporary Add-on\"\n4. Select this repo's `src/manifest.json` file\n"
},
{
  "name": "node-janus",
  "files": {
    "/": [
      ".gitignore",
      ".jscsrc",
      ".jshintignore",
      ".jshintrc",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "config",
      "keys",
      "lib",
      "package.json",
      "proxy",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# Janus - Privacy & Compression Proxy\n[![Build Status](https://travis-ci.org/mozilla/node-janus.svg?branch=develop)](https://travis-ci.org/mozilla/node-janus)\n\n## Requirements\n* [Node.js v10.0](http://nodejs.org)\n* [FFmpeg](http://ffmpeg.org/)\n\n## Installation\nGet the code first.\n\n    git clone https://github.com/mozilla/node-janus\n\nNext, use NPM to install all the dependencies.\n\n    cd node-janus  \n    npm install\n\n## Configuration and Usage\n### Proxy\nYou can find the default proxy configuration in `config/default.yml`.\nAll settings are exposed and documented there.\n\nYou may edit the settings directly in the default configuration file or\npreferably override some of settings using a custom configuration file,\nsee the [node-config documentation](https://lorenwest.github.io/node-config/latest/)\nfor more details about the configuration system.\n\nTo start the proxy, just run\n\n    ./proxy\n\nThe only command-line arguments supported are `-h` for help and `-v` for\nshowing the version.\n\n### Firefox\n#### Minimal Version\nYou need at least Firefox 33 for SPDY proxy support.\n\n#### Self-Signed Certificates\nWhen using a self-signed certificate, you need to add it to Firefox first. To do\nthis, use Firefox to open the proxy via its host-port combination.\n\n    https://<proxy.host>:<proxy.port>/\n\nThis should prompt you to add an exception for the self-signed certificate.\n\n### Automatic Client Configuration Using the Add-On\nThe prefered way for using the proxy is by installing the [Janus\nadd-on](https://addons.mozilla.org/en-US/firefox/addon/janus-proxy-configurator/).\nWhen using the add-on, you can conveniently configure the optional features of\nthe proxy and view some statistics on bandwidth savings.\n\nShould you have reasons to set up the proxy without the add-on, please follow\nthe manual instructions next.\n\n### Manual Client Configuration\n#### Desktop\nYou can configure the secure proxy in `Preferences/Advanced/Network/Settings`.\nSelect `Automatic proxy configuration URL` and set it to your custom PAC file or\nuse the default configuration served by the integrated PAC server.\n\n    http://<pac.host>:<pac.port>\n\nThis will serve a suitable PAC file with the proper host and ports set.\nCheck `config/default.yml` for the default PAC server connection details.\n\n#### Android\nFor Fennec the steps are similar. Open `about:config` and set\n`network.proxy.autoconfig_url` to the location of your PAC file or the Janus\nPAC server.\nTo load the PAC file and activate the proxy, set `network.proxy.type` to `2`.\n\n## Production Deployment\n### Additional Requirements\n* `optional` [Redis](http://redis.io)\n* `optional` [StatsD](https://github.com/etsy/statsd)\n\nBy default, the proxy uses a basic in-memory cache and does only log basic\nmetric stats. Additionally, the proxy supports a Redis-based caching solution\nand StatsD metrics reporting.\n\nTo enable the Redis cache, you need to have a running Redis server instance.\nThe proxy-side configuration is straight-forward using `config/default.yml`,\nwhere you set the host and port accordingly and switch caching modes by setting\n`cache.type`.\n\nTo view and process the full metrics, you need a receiver compatible to StatsD\nmetrics. To establish a connection, simply set the `metrics.statsd` settings\naccordingly in `config/default.yml` or your local overriding config files.\n\n### Self-Signed Certificate\nYou will also need to use your own certificate for your server FQDN. You can\ngenerate a new key and a new certificate simply by executing this command from\n`node-janus` root directory:\n\n    openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout keys/key.pem -out keys/crt.pem\n\nBe careful to correctly set the `Common Name` with your server FQDN, e.g., for\n`example.com`:\n\n    Common Name (e.g. server FQDN or YOUR name) []:example.com\n\nBecause a self-signed certificate is not delivered by a trusted CA, you will\nhave to manually add it to your browser. Please have a look to the [Firefox](#firefox)\nsection for more details.\n\n## Development\nWe would be happy to accept pull requests for known issues or useful new\nfeatures, so we encourage you to contribute!\n\nPlease make sure all tests pass locally before putting the request up for\nreview, additional tests for new features would be great, too.\n\n### Tests\nTo run all tests use\n\n    npm test\n\nTo get coverage statistics use\n\n    npm run-script coverage\n\nTo run performance tests using Marionette you need to point the configuration\nto your Firefox binary in file `config/test/test.yml`, setting\n`test.firefoxPath`. Then launch the tests using\n\n    npm run-script marionette\n\nTo simulate different mobile network environments, use\n\n    npm run-script networksimulation 2G|3G|4G\n\nand stop the system-wide simulation by reverting to the defaults using\n\n    npm run-script networksimulation default\n\n"
},
{
  "name": "discourse-auto-email-in",
  "files": {
    "/": [
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assets",
      "config",
      "plugin.rb",
      "spec"
    ]
  },
  "makefile": null,
  "readme": "# auto-email-in\n*Discourse plugin which automatically sets category email-in addresses based on their slug*\n\n[![Build Status](https://travis-ci.org/mozilla/discourse-auto-email-in.svg?branch=master)](https://travis-ci.org/mozilla/discourse-auto-email-in)\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Installation\n\nFollow the Discourse [Install a Plugin](https://meta.discourse.org/t/install-a-plugin/19157) guide.\n\n## Usage\n\nEmail-in address autogeneration can be disabled on particular categories by opening their category edit modal, going to \"Settings\", and ticking the \"Disable auto-email-in plugin on this category and subcategories\" option. Now the email-in address can be manually set, or removed altogether.\n\nIf `auto_email_in_append` is enabled then this plugin won't overwrite existing email-in addresses, but will append newly generated addresses to the chain of possible addresses.\n\nThen, if an admin manually edits the email in value, the generated address will be appended (if it doesn't already exist). This can be used to clear old addresses on categories.\n\nIf `auto_email_in_append` is disabled, then this plugin will overwrite all existing email-in addresses with the ones it generates, and admins won't be able to edit addresses.\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "zept",
  "files": {
    "/": [
      "LICENSE",
      "ZeekWeek 2019.pdf",
      "counters.rst",
      "link_to_talk.rst",
      "links.rst",
      "set_irq_affinity",
      "softnet_stat.rst",
      "softnet_stat.sh"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "discourse-group-category-notification",
  "files": {
    "/": [
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "plugin.rb",
      "spec"
    ]
  },
  "makefile": null,
  "readme": "# group-category-notification\n*A plugin to enable automatic subscription to categories based on group membership*\n\n[![Build Status](https://travis-ci.org/mozilla/discourse-group-category-notification.svg?branch=master)](https://travis-ci.org/mozilla/discourse-group-category-notification)\n\n## Usage\n\nThis plugin doesn't currently have a UI, interactions must be carried out using `rails c`.\n\nA category can be added with `GroupCategoryNotification.add(group, category)`, and removed with `GroupCategoryNotification.remove(group, category)`.\n\nAdded categories will be automatically watched by all current and future members of that group. A user can still go and set their notification level on that category to something different, and the plugin won't override it.\n\nIf a category is removed all users in that group watching the category will have their notification level set to regular on their categories, regardless of their preference.\n\nIf a group is removed, all users in that group will have their notification level on previously added categories set to regular, regardless of their preference.\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Running tests\n\nClone this plugin into `plugins/discourse-group-category-notification` in the root of your Discourse source dir.\n\nUse `RAILS_ENV=test rake plugin:spec[discourse-group-category-notification]` to run the tests.\n"
},
{
  "name": "restore-logins-extension",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE.txt",
      "README.md",
      "_locales",
      "api.js",
      "manifest.json",
      "schema.json"
    ]
  },
  "makefile": null,
  "readme": "# Password Restore for AVG Internet Security by Firefox\n\n[Install the extension from addons.mozilla.org](https://addons.mozilla.org/firefox/addon/restore-logins/)\n\nIssue bug: https://bugzilla.mozilla.org/show_bug.cgi?id=1558765\n\nExtension bug: https://bugzilla.mozilla.org/show_bug.cgi?id=1559458\n\nSupport article: https://support.mozilla.org/kb/passwords-disappearing-avg-security\n\nExtension translations: https://pontoon.mozilla.org/projects/restore-logins-extension/\n\n\nThe \u201cPassword Protection\u201d feature of AVG security software has caused logins and passwords stored in Firefox to disappear after recent updates to Firefox. AVG has since issued a fix for this issue. However, if you were impacted by this issue, a recovery method has been detailed in [this support article](https://support.mozilla.org/kb/passwords-disappearing-avg-security).\n\nThis extension will restore saved passwords and logins without the need to go through all of the steps. Ensure that your AVG Internet Security software is up to date, then install this extension. And your passwords will be recovered.\n"
},
{
  "name": "crypto-inventory",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# crypto-inventory\nThe Mozilla Cryptography Inventory\n"
},
{
  "name": "discourse-post-read-email",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assets",
      "config",
      "plugin.rb",
      "plugin_code.rb",
      "spec",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# post-read-email\n\n[![Build Status](https://travis-ci.org/mozilla/discourse-post-read-email.svg?branch=master)](https://travis-ci.org/mozilla/discourse-post-read-email)\n [![Coverage Status](https://coveralls.io/repos/github/mozilla/discourse-post-read-email/badge.svg)](https://coveralls.io/github/mozilla/discourse-post-read-email)\n\n*A plugin to give users the option of marking posts as read when emailed*\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n"
},
{
  "name": "cef",
  "files": {
    "/": [
      "MANIFEST.in",
      "README.txt",
      "cef.py",
      "setup.py",
      "test_cef.py"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "csp-logger",
  "files": {
    "/": [
      ".gitignore",
      ".jsbeautifyrc",
      ".jshintrc",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "README.md",
      "conf",
      "csp-logger.js",
      "lib",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# CSP Logger\n\nA basic service for logging [content security policy](https://developer.mozilla.org/en-US/docs/Security/CSP) violations.\n\nIt handles saving violation reports for you. You can save logs to any SQL database or to any appender that log4js is capable of - mainly log files with rotation features and configuration. Logging to console and intercepting in your own application is supported too.\n\n## Usage\n\nConfigure your CSP to report to the `/csp` route of this service. Incoming reports will be logged to your designated storage.\n\nThere are multiple ways to use csp-logger\n\n**As a comand line tool**\n\n```bash\nnpm install -g csp-logger\ncsp-logger -c config.json\n```\n\n**As a standalone app**\n\nClone this repository and run \n```bash\nnpm install .\nnode csp-logger.js -c config.json\n```\n\n**As a module in your application**\n\n```javascript\nvar cspLogger = require(\"csp-logger\")(\"config.json\"); //accepts a matching object too\n   \ncspLogger.interceptReport(function(report,req){\n    //this runs before the report is stored\n    //react to CSP reports or return a modified version \n\n});\n```\n\n## Configuration\n\nTo get an example configuration file run csp-logger with following arguments: `-c example.json --example`\nConfiguration is a json file containing the following:\n\n- `store` (String) - Choose a storage implementation from `lib/stores`, which currently gives you the choice of `sql`, `logger`, `console` or `nil`\n- `domainWhitelist` (Array of Strings) - A whitelist of domains that will have CSP exceptions logged.\n- `sourceBlacklist` (Array of Strings) - A list of sources to block from being recorded. \n\n### Store configurations:\n\n**logger**\n\n- `configuration` (String) - path to log4js configuration file. Logger name is `csp`\n\n**sql**\n\n- `dbDialect` (String) - Either `mysql`, `sqlite` or `postgres`.\n- `dbHost` (String) - SQL server hostname.\n- `dbName` (String) - Database name.\n- `dbPort` (Number) - Port number of SQL server.\n- `dbUsername` (String) - Username with write permissions for DB.\n- `dbPassword` (String) - Password.\n\n**console**\n\nJust logs with `console.warn`\n\n**nil**\n\nDoes nothing (useful when csp-logger is used as a module)\n\n## Module API\n\nThe module returns a function accepting 3 arguments:\n\n| configuration | required | Configure csp-logger - string path to configuration file or object matching the expected configuration |\n| server to use | optional | Bind it to the same port as your app - anything that can be passed as `server` to `express().listen(server)` |\n| testing | optional |  boolean stating if you want page throwing violations to be served at `/` for testing |\n\n```javascript\nvar cspLogger = require(\"csp-logger\")(\"config.json\", server, true); \n```\n\n### Intercepting violation reports\n\nAn initialized csp-logger instance exports two things:\n\n`cspLogger.Report`\n\nA report constructor. You can use it to base your implementation of report on it.\n\n`cspLogger.interceptReport`\n\nSets a callback that will be called before each report is stored. \nIf the callback returns a new object that implement `getLog` and `getRaw` methods - the new instance will be stored instead of the original report.\n\n\n```javascript\nvar cspLogger = require(\"csp-logger\")(\"config.json\");\n\nfunction MyReport(report, username){\n    this.report = report;\n    this.username = username\n}\n\nMyReport.prototype.getLog = function(){\n    var log = this.report.getLog();\n    log+=\"\\n username: \"+this.username;\n    return log;\n};\n\ncspLogger.interceptReport(function(report, req){\n    var username = getUsername(req);\n    var myReport = new MyReport(report, username);\n    return myReport;\n});\n```\n\nOverriding `getRaw` requires the output to match SQL schema, so all modifications should be done only to existing fields. `other` field (type: TEXT) is prepared for the purpose of extensions. \n\n## Testing your policies\n\nYou can try it out with any policies by running `node csp-logger.js -c yourconfig.json --test` as it serves `test/index.html` file on the root path alongside the `/csp` route.\n"
},
{
  "name": "partinfra-terraform",
  "files": {
    "/": [
      ".gitignore",
      "Jenkinsfile",
      "LICENSE",
      "README.md",
      "admin.tf",
      "backups.tf",
      "bitergia.tf",
      "confluence.tf",
      "consul.tf",
      "coss.tf",
      "db.tf",
      "discourse.tf",
      "distributions.tf",
      "dns",
      "iam.tf",
      "jenkins-public.tf",
      "logging.tf",
      "mesos-cluster.tf",
      "modules",
      "monitoring.tf",
      "mozillians.tf",
      "network.tf",
      "notifications.tf",
      "remo.tf",
      "storage.tf",
      "terraform.tf",
      "variables.tf",
      "vpn.tf"
    ]
  },
  "makefile": null,
  "readme": "# Participation Infrastructure\n## Terraform resources\n### Introduction\n\n``partinfra-terraform`` is a collection of resources and modules to manage the cloud infrastructure that power various sites related to mozilla community.\nThe code in this repository is authored and maintained by Mozilla engineers and a vibrant community of volunteer contributors.\n\nFor more information:\n\n* [Community Ops overview](https://wiki.mozilla.org/Community_Ops)\n* [Community Ops PaaS architecture](https://wiki.mozilla.org/Community_Ops/paas)\n* [Terraform documentation](https://www.terraform.io/docs/index.html)\n* Communication:\n  *  IRC: ``#communityit`` on irc.mozilla.org\n  *  Discourse: ``https://discourse.mozilla-community.org/c/community-ops``\n\nGet Involved!\n\n### Resources\n\n* ``mesos-cluster``\n  * Module that defines the infrastructure required for the community PaaS cluster.\n     * [AWS ELB](https://aws.amazon.com/elasticloadbalancing/) load balancer for regional community sites and ``*.mozilla.community`` apps\n     * [AWS EC2](https://aws.amazon.com/ec2/) configuration for ``mesos-master`` and ``mesos-slave`` nodes\n     * [AWS Autoscaling groups](https://aws.amazon.com/autoscaling/) for ``mesos-master`` and ``mesos-slave`` nodes\n     * Security group rules for the ``mesos-cluster`` network flow\n  * This acts as the base for our 2 mesos cluster tiers: ``production`` and ``staging``\n* ``admin``\n  * Deploy [AWS EC2](https://aws.amazon.com/ec2/) instance, security group rules, [AWS ELB](https://aws.amazon.com/elasticloadbalancing/) and SSL termination for admin node.\n* ``consul``\n  * Deploy shared [AWS VPC](https://aws.amazon.com/vpc/), security group rules and [autoscaling group](https://aws.amazon.com/autoscaling/) for our [consul](https://www.consul.io/) cluster\n* ``db``\n  * Deploy shared [AWS RDS](https://aws.amazon.com/rds/) (MySQL) instance, security group rules and [AWS Route53](https://aws.amazon.com/route53/) DNS entry for our generic MySQL instance.\n* ``network``\n  * Deploy staging, production and shared [AWS VPC](https://aws.amazon.com/vpc/) and configure the network flow required for the cluster needs.\n* ``terraform``\n  * Automation that stores [terraform state](https://www.terraform.io/docs/state/) in [AWS S3](https://aws.amazon.com/s3/).\n* ``vpn``\n  * Deploy [AWS EC2](https://aws.amazon.com/ec2/) instance and security group rules required for our VPN server.\n\n### Issues\n\nFor issue tracking we use bugzilla.mozilla.org. [Create a bug][1] on bugzilla.mozilla.org under ``Participation Infrastructure > Community Ops`` component.\n\n[1]: https://bugzilla.mozilla.org/enter_bug.cgi?product=Participation%20Infrastructure&component=Community%20Ops\n"
},
{
  "name": "charts",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "quantum"
    ]
  },
  "makefile": null,
  "readme": "charts\n======\nStatic pages for [charts.mozilla.org](http://charts.mozilla.org/) The summary\nof the this project is found on [the Mozilla wiki](https://wiki.mozilla.org/Auto-tools/Projects/Charts#Overview_of_charts.mozilla.org)\n\n\nPrerequisits\n------------\n\nYou should be able to reach the public cluster.  My people page has [a test page to confirm public cluster is accessible](http://people.mozilla.org/~klahnakoski/modevmetrics/Tutorial01-Minimum.html).\n\n\nSetup\n-----\n\nSimply clone from Github.\n\n    https://github.com/klahnakoski/MoDevMetrics.git\n\nYou can then open the files with your browser\n\n\nContribution\n------------\n\nThere are three main branches\n\n  * [master](https://github.com/mozilla/charts/tree/master) - deployed to production: charts.mozilla.org\n  * [allizom](https://github.com/mozilla/charts/tree/allizom) - deployed to staging: charts.paas.allizom.org\n  * [dev](https://github.com/mozilla/charts/tree/dev) - active development: **add you pull requests here**\n\nOther branches may exist which are working on new dashboards for other teams in the company.\n\n###Submitting a Patch\nAt a bare minimum:\n\n  * fork this repo\n  * craft your changes, and\n  * send a pull request\n\nExtra appreciation is granted if you also\n\n  * update the corresponding bugzilla bug with an attachment pointing to the\n  pull request\n\nUsing Github pull requests for code submission, and Bugzilla for issue\ntracking, is clunky:  So stick to the Github if you find this is a nasty\nbarrier to getting stuff done.  What's most important is you submit code!\nI will deal with the administration.\n\n"
},
{
  "name": "old-timer-extension",
  "files": {
    "/": [
      "README.md",
      "content_script.js",
      "manifest.json",
      "palette.svg",
      "popup.html",
      "popup.js",
      "script.js"
    ]
  },
  "makefile": null,
  "readme": "# \"Old-Timer\" sample extension\n\nThis is the sample extension built in the \"Build an Extension in Under 5 Minutes\" screencast.\n\n## Structure\n\n* **manifest.json** - defines metadata and functionality of the extension\n* **content_script.js** - a script injected into browser tabs\n* **popup.html** - the structure of the browser action's popup\n* **popup.js** - the functionality of the browser action's popup\n\n[Check out the Extension Workshop to learn more](https://extensionworkshop.mozilla.org/)"
},
{
  "name": "webcompat-crawls",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "README.md",
      "crawl-engineering",
      "crawl-prep",
      "crawls",
      "lists"
    ]
  },
  "makefile": null,
  "readme": "# Webcompat Crawls\n\nConfiguration and instructions used to crawls top sites using a specially instrumented version of Firefox gathering data for Webcompat analysis.\n\n## Generate the seed list via a series of pre-crawls\n\nSee [./crawl-prep/README.md](./crawl-prep/README.md).\n\n## Run an OpenWPM crawl in Google Cloud Platform\n\nSee [./crawl-engineering/gcp/README.md](./crawl-engineering/gcp/README.md).\n\n## Developer notes\n\nTo update the OpenWPM Crawler and crawl-prep submodules to the latest commits in the remotely tracked branches:\n\n```\ngit submodule update --remote\n```\n"
},
{
  "name": "mozilla-depends",
  "files": {
    "/": [
      ".gitignore",
      "README.md",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "# mozilla-depends\n\n## Requirements\n\n* _Node_ v9 or later (e.g. `nvm install v9 && nvm use v9`)\n* `npm` available in `$PATH`\n* `pipenv` with python 3.7\n* `graphviz` headers (e.g. on ubuntu or debian `sudo apt install libgraphviz-dev`)\n\n## Installation\n\n* ```hg clone --uncompressed https://hg.mozilla.org/mozilla-unified```\n* ```git clone git@github.com:mozilla/mozilla-depends.git```\n* ```cd mozilla-depends/utils/```\n* ```pipenv install -e .[dev]```\n* ```npm install .```\n\n## Usage\n\n```mozdep``` *must* be run from the _utils_ directory for finding retire binary.\nIt tries to be smart about finding the local mozilla-central tree.\nIf it is not smart enough, pass it ```--tree```.\n\n* ```pipenv run pytest -v```\n* ```pipenv run mozdep --tree ../../mozilla-unified/ --debug detect -c /tmp/out.csv```\n"
},
{
  "name": "SUMOBugs",
  "files": {
    "/": [
      "LICENSE"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "donate-wagtail-pontoon",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# donate-wagtail-pontoon\nTranslation files for the Wagtail-based Mozilla donations platform\n"
},
{
  "name": "js_mse_eme",
  "files": {
    "/": [
      ".gitmodules",
      "CONTRIBUTING",
      "LICENSE",
      "README",
      "event_listener.html",
      "harness",
      "lib",
      "logo.png",
      "main.html",
      "media",
      "style.css",
      "third_party"
    ]
  },
  "makefile": null,
  "readme": "js_mse_eme is an externally-published tool that is aimed to test the validity\nof a browser's HTML5 Media Source Extension and Encrypted Media Extension\nimplementations. As an added bonus, this tool is used by YouTube to certify\nhardware electronic devices (e.g. TVs, settop boxes, etc...).\n\nYear specific tests and tools are in the respective branches."
},
{
  "name": "sumobot",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "config.json.example",
      "index.js",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": "This Repo is for the [SuMo][1] IRC bot in the #SuMo channel \n\n[SuMo][1] stands for Support Mozilla\n\nIf you have any issues please post in the [Issues][2] section \n\nSuMo bot Dev irc channel is #sumobotdev on irc.mozilla.org\n\n  [1]: https://support.mozilla.org/\n  [2]:https://github.com/mozilla/sumobot/issues\n"
},
{
  "name": "discourse",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# discourse\nIssues repo for [Discourse](https://wiki.mozilla.org/ParticipationSystems/Discourse) roadmap within Mozilla\n"
},
{
  "name": "parsys-okr-dashboard",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "package.json",
      "preset.js",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# ParSys OKRs\n\nThis project tracks Mozilla Participation Systems OKRs.\n\n## Running it locally\n\nMake sure you have node and npm installed before using the following commands:\n\n```\n$ npm install\n$ npm start\n```\n\n## Deploy\n\n```\n$ npm run build\n$ ./node_modules/gh-pages/bin/gh-pages -d build -r git@github.com:<username/org>/<reponame>.git\n```\n\n## License\n\n[MPL-2.0](LICENSE)\n\nForked and heavily based on [this](https://github.com/mozilla-rpweb/okr-dashboard) and [this](https://github.com/MichaelKohler/OKRs).\n"
},
{
  "name": "lightbeam-we",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.json",
      ".gitattributes",
      ".gitignore",
      ".gitmodules",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "docs",
      "karma.conf.js",
      "package.json",
      "shavar-prod-lists",
      "src",
      "test"
    ],
    "/docs": [
      "images"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Lightbeam\nThis is the web extension version of the Firefox Lightbeam add-on for visualizing HTTP requests between websites in real time.\n\nThe Firefox Lightbeam extension by Mozilla is a key tool for Mozilla to educate the public about privacy.\n\n![lightbeam-screenshot](/docs/images/lightbeam.gif)\n\n## Quick Start\n\n### Clone the repository\n\n**Note** This repository uses a [submodule](https://github.com/mozilla-services/shavar-prod-lists) to allow some third party requests. To ensure the submodule is cloned along with this repository, use a modified `clone` command:\n`git clone --recursive https://github.com/mozilla/lightbeam-we.git`\n\n### Run the web extension\n\nThere are a couple ways to try out this web extension:\n\n1. Open Firefox and load `about:debugging` in the URL bar.\n    - Click the [Load Temporary Add-on](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Temporary_Installation_in_Firefox) button and select the `manifest.json` file within the directory of this repository.\n    - You should now see the Lightbeam icon on the top right bar of the browser.\n    - Click the Lightbeam icon to launch the web extension.\n\n2. Install the [web-ext](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext) tool, change into the `src` directory of this repository, and type `web-ext run`.\n    - This will launch Firefox and install the extension automatically.\n    - This tool gives you some additional development features such as [automatic reloading](https://developer.mozilla.org/en-US/Add-ons/WebExtensions/Getting_started_with_web-ext#Automatic_extension_reloading).\n\n## Development Guide\n\n### Download dependencies\nRun `npm run build`.\n\n### Update the submodule\nTo manually update the submodule at any time during development, run `git submodule update`.\n\n### Testing\nRun `npm run test` to check that everything is OK.\n\n* If you have installed `eslint` globally, you will have to install globally the following `eslint` plugins too:\n    - `eslint-plugin-json`\n    - `eslint-plugin-mocha`\n* Test suites include lint and unit testing. You can individually run lint or unit tests using the following commands:\n    * `npm run lint:eslint`\n    * `npm run test:karma`\n\nEslint is used for linting. Karma, Mocha & Chai are used for unit testing. Additionally the test suites are run on the Travis service providing continuous integration support.\n"
},
{
  "name": "eu2019-ad-transparency-report",
  "files": {
    "/": [
      ".circleci",
      ".gitignore",
      "LICENSE",
      "bin",
      "www"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "voice-corpus-tool",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bin",
      "requirements.txt",
      "soundstore",
      "voice.py"
    ]
  },
  "makefile": null,
  "readme": "# voice-corpus-tool\nA tool for creation, manipulation and maintenance of voice corpora\n\n## Installation\nThe tool requires Python packages `pydub` and `intervaltree`.\nYou can install them from the project root using the following command:\n```\n$ pip install -r requirements.txt\n```\n\nFor processing samples also the `sox` command line tool is required. You can install it using your normal package manager or retrieve it from [here](http://sox.sourceforge.net/).\n\n## Usage\nBasic principle of the voice corpus tool is to apply a series of \"commands\" to a virtual buffer of samples. \n\n### Illustrating example\nImagine you have a folder full of audio samples. The following example shows how to play a bunch of them.\n\n```\n$ ./voice.py add '/data/sample-00300*.mp3' skip 2 take 3 play\nAdded 10 samples to buffer.\nRemoved first 2 samples from buffer.\nTook 3 samples as new buffer.\nPlaying:\nFilename: \"/data/sample-003002.mp3\"\nTranscript: \"\"\nFilename: \"/data/sample-003003.mp3\"\nTranscript: \"\"\nFilename: \"/data/sample-003004.mp3\"\nTranscript: \"\"\nPlayed 3 samples.\n```\n\nThe first command __add__ requires one parameter. In our case we pass `'/data/sample-00300*.mp3'` in apostrophes to ensure the shell is not resolving the asterisk, but just forwards it to the tool which will do the wildcard processing instead.\nThis operation adds all wildcard-matching samples to the virtual buffer. To document this fact, it prints \"Added 10 samples to buffer.\".\n\nNow the second and third commands (__skip__ and __take__) and their respective output should explain themselves.\n\nFinally the command __play__ results in playing all remaining samples of the buffer. As they were directly added as files, there is no transcript associated with them. If samples were loaded from a voice corpus CSV file (like provided by the Common Voice project), each (voice) sample would feature its transcript. This transcript will then be kept associated to its sample throughout all further 1-to-1 processing of this sample.\n\nBe aware that the \"buffer\" is virtual in the sense of not loading any audio data into memory. Its purpose is just to assign operations to sequences of samples. Only final output commands like __write__ or __play__ and the command __augment__ result in actual sample processing (on a file by file basis).\n\nFor getting a complete list of supported commands just use the help command like this:\n\n```\n$ ./voice.py help\nA tool to apply a series of commands to a collection of samples.\nUsage: voice.py (command <arg1> <arg2> ... [-opt1 [<value>]] [-opt2 [<value>]] ...)*\n\nCommands:\n\n  help  \n\tDisplay help message\n\n  add <source> \n\tAdds samples to current buffer\n\tArguments:\n\t\tsource: string - Name of a named buffer or filename of a CSV file or WAV file (wildcards supported)\n\nBuffer operations:\n\n  shuffle  \n\tRandoimize order of the sample buffer\n\n  order  \n\tOrder samples in buffer by length\n\n  reverse  \n\tReverse order of samples in buffer\n\n  take <number> \n\tTake given number of samples from the beginning of the buffer as new buffer\n\tArguments:\n\t\tnumber: int - Number of samples\n\n  repeat <number> \n\tRepeat samples of current buffer <number> times as new buffer\n\tArguments:\n\t\tnumber: int - How often samples of the buffer should get repeated\n\n  skip <number> \n\tSkip given number of samples from the beginning of current buffer\n\tArguments:\n\t\tnumber: int - Number of samples\n\n  find <keyword> \n\tDrop all samples, whose transcription does not contain a keyword\n\tArguments:\n\t\tkeyword: string - Keyword to look for in transcriptions\n\n  tagged <tag> \n\tKeep only samples with a specific tag\n\tArguments:\n\t\ttag: string - Tag to look for\n\n  settag <tag> \n\tSets a tag on all samples of current buffer\n\tArguments:\n\t\ttag: string - Tag to set\n\n  clear  \n\tClears sample buffer\n\nNamed buffers:\n\n  set <name> [-percent <percent>]\n\tReplaces named buffer with portion of buffer\n\tArguments:\n\t\tname: string - Name of the named buffer\n\tOptions:\n\t\t-percent: int - Percentage of samples from the beginning of buffer. If omitted, complete buffer.\n\n  stash <name> [-percent <percent>]\n\tMoves buffer portion to named buffer. Moved samples will not remain in main buffer.\n\tArguments:\n\t\tname: string - Name of the named buffer\n\tOptions:\n\t\t-percent: int - Percentage of samples from the beginning of buffer. If omitted, complete buffer.\n\n  push <name> [-percent <percent>]\n\tAppends portion of buffer samples to named buffer\n\tArguments:\n\t\tname: string - Name of the named buffer\n\tOptions:\n\t\t-percent: int - Percentage of samples from the beginning of buffer. If omitted, complete buffer.\n\n  slice <name> <percent> \n\tMoves portion of named buffer to current buffer\n\tArguments:\n\t\tname: string - Name of the named buffer\n\t\tpercent: int - Percentage of samples from the beginning of named buffer\n\n  drop <name> \n\tDrops named buffer\n\tArguments:\n\t\tname: string - Name of the named buffer\n\nOutput:\n\n  print  \n\tPrints list of samples in current buffer\n\n  play  \n\tPlay samples of current buffer\n\n  pipe  \n\tPipe raw sample data of current buffer to stdout. Could be piped to \"aplay -r 44100 -c 2 -t raw -f s16\".\n\n  write <dir_name> [-just_csv]\n\tWrite samples of current buffer to disk\n\tArguments:\n\t\tdir_name: string - Path to the new sample directory. The directory and a file with the same name plus extension \".csv\" should not exist.\n\tOptions:\n\t\t-just_csv: bool - Prevents writing samples\n\n  hdf5 <alphabet_path> <hdf5_path> [-ninput <ninput>] [-ncontext <ncontext>]\n\tWrite samples to hdf5 MFCC feature DB that can be used by DeepSpeech\n\tArguments:\n\t\talphabet_path: string - Path to DeepSpeech alphabet file to use for transcript mapping\n\t\thdf5_path: string - Target path of hdf5 feature DB\n\tOptions:\n\t\t-ninput: int - Number of MFCC features (defaults to 26)\n\t\t-ncontext: int - Number of frames in context window (defaults to 9)\n\nEffects:\n\n  compr <kbit> \n\tDistortion by mp3 compression\n\tArguments:\n\t\tkbit: int - Virtual bandwidth in kBit/s\n\n  rate <rate> \n\tResampling to different sample rate\n\tArguments:\n\t\trate: int - Sample rate to apply\n\n  augment <source> [-times <times>] [-gain <gain>]\n\tAugment samples of current buffer with noise\n\tArguments:\n\t\tsource: string - CSV file with samples to augment onto current sample buffer\n\tOptions:\n\t\t-times: int - How often to apply the augmentation source to the sample buffer\n\t\t-gain: float - How much gain (in dB) to apply to augmentation audio before overlaying onto buffer samples\n```\n\n\n"
},
{
  "name": "bmo-find-dupes",
  "files": {
    "/": [
      ".glitch-assets",
      "client.js",
      "index.html",
      "shrinkwrap.yaml",
      "style.css"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "webcompat-addon-testbed",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "package.json",
      "public",
      "server.js"
    ]
  },
  "makefile": null,
  "readme": "# webcompat-addon-testbed\n\nThis repository contains a testbed application used for manually testing and verifying the [WebCompat System Addon](https://github.com/mozilla/webcompat-addon/). In addition to your automated test suite, we'd like to be able to run manual verifications when needed, and this application provides a ground for implementing these tests.\n\nFor more information, please check out the [System Addon repository](https://github.com/mozilla/webcompat-addon/) or [our wiki page on wiki.mozilla.org](https://wiki.mozilla.org/Compatibility/Go_Faster_Addon).\n\n## Code of Conduct\n\nPlease check our [Code of Conduct](/CODE_OF_CONDUCT.md) before contributing to this repository.\n\n## License\n\nMozilla Public License Version 2.0.\n"
},
{
  "name": "github-bugzilla-pr-linker",
  "files": {
    "/": [
      ".env-dist",
      ".flake8",
      ".gitignore",
      ".therapist.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Procfile",
      "README.md",
      "app",
      "constraints.txt",
      "debug-push.py",
      "dev-constraints.txt",
      "dev-requirements.txt",
      "pytest.ini",
      "requirements.txt",
      "runtime.txt",
      "sample-push-payload.json",
      "screenshot-github-webhook.png",
      "setup.py",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "What this is\n============\n\n[![Build Status](https://travis-ci.org/mozilla/github-bugzilla-pr-linker.svg?branch=master)](https://travis-ci.org/mozilla/github-bugzilla-pr-linker)\n[![Code style](https://img.shields.io/badge/Code%20style-black-000000.svg)](https://github.com/ambv/black)\n\nA webhook that can automatically create Bugzilla attachments\nwhen new GitHub Pull Requests are created.\n\nIt does this by looking at the git commit message of a newly created PR for a Bugzilla bug\nID in the form \"bug n\". Then if that bug can be found (and doesn't\nalready have a link) it creates an attachment which is a link that redirects\nto the Pull Request on GitHub.\n\nFor Mozilla projects\n====================\n\nThe code here is just the source code that any organization can use.\n\nThere is an instance of this installed on the Mozilla corporate Heroku account\nand it's hardwired to use `https://bugzilla.mozilla.org` and the account\nset up is exclusive to Mozilla projects using `bugzilla.mozilla.org`.\n\nTo use it in your project you need the password. See\n[the Mana documentation page](https://mana.mozilla.org/wiki/display/WebDev/GitHub+Bugzilla+PR+Linker).\n\nTo get insight into GitHub projects that use this project you can [search by user activity](https://bugzilla.mozilla.org/page.cgi?id=user_activity.html&action=run&from=-14d&who=pulgasaur%40mozilla.bugs)\nwhich gives you an idea of the projects.\n\nHow to use\n==========\n\nTo enable this you need to go into your GitHub project's **Settings** and\nclick **Webhooks**. The click the **Add webhook** button (top right corner).\n\nThere, type in the the following:\n\n  **Payload URL:** `https://github-bugzilla-pr-linker.herokuapp.com/postreceive`\n\n  **Content type:** `application/x-www-form-urlencoded`\n\n  **Secret:** [See Mozilla Mana page](https://mana.mozilla.org/wiki/display/WebDev/GitHub+Bugzilla+PR+Linker)\n\n  **Which events would you like to trigger this webhook?** Click the\n  `Let me select individual events.` option. Check the `Pull request`\n  checkbox and uncheck all others.\n\n![Screenshot](screenshot-github-webhook.png)\n\n\nHow to run locally\n==================\n\nFirst create a `.env` file:\n\n    cp .env-dist .env\n\nEdit that `.env` file with real stuff if you have it.\n\nCreate a Python 3 `virtualenv` and install the dependencies:\n\n    pip install -r requirements.txt\n\nNow start it:\n\n    FLASK_DEBUG=1 FLASK_APP=app.app flask run\n\nHow to run tests\n================\n\nInstall the dependencies for running tests:\n\n    pip install -r dev-requirements.txt\n\nRun the tests:\n\n    pip install -e .\n    FLASK_APP=app.app pytest\n\nHow to contribute\n=================\n\nAll Python code needs to be [Black](https://github.com/ambv/black) and this\nis enforced in TravisCI. The same is true for `flake8` .\n\nFor local development, it's best to use [`therapist`](https://pypi.org/project/therapist)\nwhich is already listed in `dev-requirements.txt`. To install a pre-commit\nhook that checks for `flake8` and `black` slip-ups. You install it like this:\n\n    pip install -r dev-requirements.txt\n    therapist install\n\nIf you're eager to test if all the linting is passing you can run:\n\n    therapist run\n\nWhich will check the files you have touched. This is basically what the\npre-commit installed does except its exits can preven the git commit.\n\nTo check *all* files run:\n\n    therapist run --use-tracked-files\n\nIf you don't have auto-matic Black formatting in your editor you can run:\n\n    therapist run --fix\n\nWhich will format all the files you have touched.\n\nHow to run with Docker\n======================\n\nTODO!\n\n\nLicense\n=======\n\n[MPL2](http://www.mozilla.org/MPL/2.0/)\n\n\nHeroku\n======\n\nAt the moment this Flask app is deployed on Heroku, using the\nMozilla corporate account, under the name\n[github-bugzilla-pr-linker](https://dashboard.heroku.com/apps/github-bugzilla-pr-linker).\n\n\nSentry\n======\n\nTo enable Sentry error reporting you simply need to set an environment\nvariable called `SENTRY_DSN` with the DSN string. Don't leave it as\nan empty string.\n"
},
{
  "name": "powerusage-android",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "Pipfile",
      "Pipfile.lock",
      "README.md",
      "pipenv.txt",
      "processors",
      "scripts",
      "setup.py",
      "tests",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "# powerusage-android\n\n<h2>Mozilla-driven battery and power-usage measurement tools for Android (7,8).</h2>\n\n\ud83d\udd25\ud83e\udd8a\u23f1\n\n======\n\n* **Test Plan:** [Android Power/Battery-Use](https://docs.google.com/document/d/1r1J_BZnE5l8nXoLVXVR1hUlEkzaPX2gx_ueZABkzi6g/edit) (Google Doc, WIP)\n* **Main bug:** [bug 1511350](https://bugzilla.mozilla.org/show_bug.cgi?id=1511350) - Test impact of dark mode on power usage\n\n[![license](https://img.shields.io/badge/license-MPL%202.0-blue.svg)](https://github.com/mozilla/powerusage-android/blob/master/LICENSE.txt)\n[![Build Status](https://travis-ci.org/mozilla/powerusage-android.svg?branch=master)](https://travis-ci.org/mozilla/powerusage-android)\n[![black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/ambv/black)\n\n## Preliminary Setup:\n1. Your **Moto G5** ([unlock](https://accounts.motorola.com/ssoauth/login?TARGET=https://motorola-global-portal.custhelp.com/cc/cas/sso/redirect/standalone%2Fbootloader%2Funlock-your-device-b), [specs](https://www.gsmarena.com/motorola_moto_g5-8454.php)) and/or **Pixel 2** [unlock](https://www.androidcentral.com/how-root-google-pixel-2)[specs](https://www.gsmarena.com/google_pixel_2-8733.php) phones should be *rooted* and *OEM/carrier-unlocked* (further [unlocking docs](https://docs.google.com/document/d/1XQLtvVM2U3h1jzzzpcGEDVOp4jMECsgLYJkhCfAwAnc/edit)\n2. [**Python 3.7.3**](https://www.python.org/downloads/release/python-373/) is installed and available in your system ```$PATH```\n3. You have [**git**](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git) installed and configured, also available in your system ```$PATH```\n4. [**adb**](https://www.xda-developers.com/quickly-install-adb/) is installed and available in your system ```$PATH``` (if ```adb devices``` doesn't spit out errors, you should be good to go)\n5. ```git clone``` your fork of https://github.com/mozilla/powerusage-android (read-only would be: ```git clone https://github.com/mozilla/powerusage-android```)\n6. ```cd powerusage-android```\n7. ```python3 setup.py develop```:\n```\n$ python3 setup.py develop\n```\n```\nrunning develop\nrunning egg_info\nwriting powerusage_android.egg-info/PKG-INFO\nwriting dependency_links to powerusage_android.egg-info/dependency_links.txt\nwriting top-level names to powerusage_android.egg-info/top_level.txt\nreading manifest file 'powerusage_android.egg-info/SOURCES.txt'\nwriting manifest file 'powerusage_android.egg-info/SOURCES.txt'\nrunning build_ext\nCreating /usr/local/lib/python3.7/site-packages/powerusage-android.egg-link (link to .)\n\nInstalled /Users/stephendonner/powerusage-android\nProcessing dependencies for powerusage-android==0.1.0\nFinished processing dependencies for powerusage-android==0.1.0\n```\n## Test-Environment (Device) Cleanup\n## Uninstall Browser Apps\n**Manually:** tap and hold until you can drag to the Uninstall option at the top right) all of your current Firefox-based browser apps (Fennec, GeckoView Example [GVE], Fenix, and Reference Browser [RefBrow]).\n\n## Install Browser APKs:\nI will prettify this later, but here are my (Stephen) build sources, for my Moto G5, running Android 7.x:\n* Fennec (Firefox 64.0.2): https://archive.mozilla.org/pub/mobile/releases/64.0.2/android-api-16/en-US/fennec-64.0.2.en-US.android-arm.apk\n* RefBrow: https://index.taskcluster.net/v1/task/project.mobile.reference-browser.signed-nightly.nightly.2019.4.9.latest/artifacts/public/target.arm.apk\n* Fenix: https://index.taskcluster.net/v1/task/project.mobile.fenix.signed-nightly.nightly.2019.4.9.latest/artifacts/public/target.arm.apk\n* GVE: https://taskcluster-artifacts.net/dzV5pl0SRz6IvbIDR1TEIA/0/public/build/geckoview_example.apk\n\n(Greg can add his here too if so desired)\n\n## Running a Test:\n1. For each color's power test, you must ```mkdir [unique name]``` with a convention of your choosing; ```black-bg```, ```white-bg```, etc.\n2. From the top-level (root) dir, run (e.g. substituting values, where appropriate):\n   ```$ ./scripts/blackbg-test.sh --output black-bg ```\n    Ensure that your custom dir exists prior to the above test run; ```results``` or similar, will do, for our purposes.\n3. You should now see output similar to https://gist.github.com/stephendonner/9c611a3dfc6d26c4f203bd06b38f688b:\n\n\nHere's a sample \"batch\" test of colors, called eye_of_sauron.py:\n\n```\n#!/bin/bash \n\nscripts/blackbg-test.sh black-bg fennec-64.0.2.en-US.android-arm.apk refbrow-target.arm.apk target.arm.apk geckoview_example.apk\nscripts/whitebg-test.sh white-bg fennec-64.0.2.en-US.android-arm.apk refbrow-target.arm.apk target.arm.apk geckoview_example.apk\nscripts/redbg-test.sh red-bg fennec-64.0.2.en-US.android-arm.apk refbrow-target.arm.apk target.arm.apk geckoview_example.apk\nscripts/greenbg-test.sh green-bg fennec-64.0.2.en-US.android-arm.apk refbrow-target.arm.apk target.arm.apk geckoview_example.apk\nscripts/bluebg-test.sh blue-bg fennec-64.0.2.en-US.android-arm.apk refbrow-target.arm.apk target.arm.apk geckoview_example.apk\n\n```\n\nNote: at the time of this doc update, ```greenbg-test.sh``` and ```bluebg-test.sh``` are not in-tree, and are used to test locally and here for illustrative purposes.\n\nWe'll soon be moving to Raptor proper, to hook into All the Things\u2122 we need for capabilities of all types.\n\n\n```\nRunning Android Pre/Post test. Running white background color test.\n\nMake sure you have no extra apps running in the background. Make sure that there is a wakelock app running(if going passed 30 minutes of testing). Charging is disabled before the test starts. It is enabled automatically when we reach the end of the test. Getting Phone Model... Is the model Moto_G__5 correct? Disabling charging... Old screen timeout: 12000000\n\nOn trial 0\n\nInstalling app... Attempting to start white test... Starting: Intent { act=android.intent.action.VIEW dat=data:text/html;base64,PGJvZHkgc3R5bGU9ImJhY2tncm91bmQtY29sb3I6d2hpdGUiPjwvYm9keT4= (forced wrap here for illustration)\ncmp=org.mozilla.reference.browser/.IntentReceiverActivity (has extras) }\n```\n\nfollowed by the magic and substance of the test/measurements, which are the (many) datapoints, of which we are currently focused on Charge-Counter values (**Charge counter: 267234**), as well as deltas, variance across scenarios, etc:\n\n```\nWaiting for a charge counter drop...\n\nTime elapsed waiting for drop: 35.473793029785156 seconds\nDrop detected, starting test\nStart time: 2019-04-10 23:06:34.454324\nStarting values:\nAC powered: false\nUSB powered: false\nWireless powered: false\nMOD powered: false\nMax charging current: 0\nMax charging voltage: 0\n**Charge counter: 2627234**\nstatus: 3\nhealth: 2\npresent: true\nlevel: 94\nscale: 100\nvoltage: 4242\ntemperature: 297\ntechnology: Li-ion\nmod level: -1\nmod status: 1\nmod flag: 0\nmod type: 0\ntimestamp: 1554937594.504281\nElapsed time (seconds): 1200.3231749534607\nEnd time: 2019-04-10 23:26:34.919007\nFinal values:\nAC powered: false\nUSB powered: false\nWireless powered: false\nMOD powered: false\nMax charging current: 0\nMax charging voltage: 0\n**Charge counter: 2549718**\nstatus: 3\nhealth: 2\npresent: true\nlevel: 93\nscale: 100\nvoltage: 4220\ntemperature: 315\ntechnology: Li-ion\nmod level: -1\nmod status: 1\nmod flag: 0\nmod type: 0\ntimestamp: 1554938794.918973\n\n**Charge counter used: 77516**\n**Percent used: 1**\n```\n"
},
{
  "name": "partinfra-playbooks",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "group_vars",
      "inventory",
      "roles",
      "site.yml"
    ]
  },
  "makefile": null,
  "readme": "# Participation Infrastructure\n## Ansible Playbooks\n### Introduction\n\n``partinfra-playbooks`` is a collection of Ansible playbooks to manage the infrastructure that powers various sites related to mozilla community. The code in this repository is authored and maintained by Mozilla engineers and a vibrant community of volunteer contributors.\n\nFor more information:\n\n* [Community Ops overview](https://wiki.mozilla.org/Community_Ops)\n* [Community Ops PaaS architecture](https://wiki.mozilla.org/Community_Ops/paas)\n* [Ansible documentation](https://docs.ansible.com/ansible/index.html)\n* Communication:\n  *  IRC: ``#communityit`` on irc.mozilla.org\n  *  Discourse: ``https://discourse.mozilla-community.org/c/community-ops``\n\nGet Involved!\n\n### Roles\n\n* Common\n* Consul\n  * Configures our [Consul](https://www.consul.io/) cluster used for service discovery and distributed key-value storage.\n  * [Consul](https://www.consul.io/) is also used to autoconfigure various templates using [consul-template](https://github.com/hashicorp/consul-template)\n* Jenkins\n* Mesos\n  * ``mesos-common``\n  * ``mesos-master``\n     * Configures the master nodes of our mesos cluster\n         * [Zookeeper](https://zookeeper.apache.org/) for leader election\n         * [Mesos master](https://open.mesosphere.com/reference/mesos-master/) for cluster resource management\n         * [Marathon](https://mesosphere.github.io/marathon/) for container orchestration\n  * ``mesos-slave``\n     * Configures the slave nodes of our mesos cluster\n         * Configures [docker](https://www.docker.com/) in all container hosts\n         * Configures [mesos slave](https://open.mesosphere.com/reference/mesos-slave/) that manages resource offers and task launching\n* OpenVPN\n  * ``openvpn-firewall``\n     * Configures ``iptables`` rules for our VPN traffic.\n\n### Ansible galaxy dependencies\n\n* [Stouts.OpenVPN](https://galaxy.ansible.com/Stouts/openvpn/)\n* [tersmitten.postfix](https://galaxy.ansible.com/tersmitten/postfix/)\n* [jnv.unattended-upgrades](https://galaxy.ansible.com/jnv/unattended-upgrades/)\n\n### Issues\n\nFor issue tracking we use bugzilla.mozilla.org. [Create a bug][1] on bugzilla.mozilla.org under ``Participation Infrastructure > Community Ops`` component.\n\n[1]: https://bugzilla.mozilla.org/enter_bug.cgi?product=Participation%20Infrastructure&component=Community%20Ops\n"
},
{
  "name": "geomodel",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# geomodel\n\nA re-implementation of a service designed to facilitate\n\n> IP geolocation for authentication events with MozDef\n\nbased on the [original geomodel](https://github.com/ameihm0912/geomodel).\n"
},
{
  "name": "fission-dashboard",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "LICENSE",
      "Procfile",
      "README.md",
      "bin",
      "docker-compose.yml",
      "fission",
      "mozdata.ini",
      "requirements.txt",
      "static"
    ]
  },
  "makefile": null,
  "readme": "# fission-dashboard\n\nDashboard for Fission project.\n\n## Setup\n\nInstall docker and docker-compose and then:\n```sh\ndocker-compose up --build\n```\nThen you can test in your browser: https://localhost:5000/.\n\n## Bugs\n\nhttps://github.com/mozilla/fission-dashboard/issues/new\n\n## Contact\n\nEmail: calixte@mozilla.com\n"
},
{
  "name": "emailmessage-rs",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Cargo.toml",
      "LICENSE",
      "README.md",
      "appveyor.yml",
      "examples",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Email Message library for Rust\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://opensource.org/licenses/MIT)\n[![Crates.io Package](https://img.shields.io/crates/v/emailmessage.svg)](https://crates.io/crates/emailmessage)\n[![Docs.rs API Documentation](https://docs.rs/emailmessage/badge.svg)](https://docs.rs/emailmessage)\n[![Travis-CI Build Status](https://travis-ci.org/katyo/emailmessage-rs.svg?branch=master)](https://travis-ci.org/katyo/emailmessage-rs)\n[![Appveyor Build status](https://ci.appveyor.com/api/projects/status/29im4ud4xb3r9hlv)](https://ci.appveyor.com/project/katyo/emailmessage-rs)\n\nThis project aims to provide a proper strongly typed way to build and parse emails.\n\n## Features\n\n* Typed headers using `hyperx::Header`\n* Support for headers with unicode values\n* Support for **MIME 1.0** multipart contents\n* Streaming messages to save memory usage\n* Email `Address`, `Mailbox` and `Mailboxes` types\n\n## Usage\n\n### Format email messages\n\n#### With string body\n\nThe easiest way how we can create email message with simple string\n(see [format\\_string.rs](examples/format_string.rs)).\n\n```rust\nextern crate emailmessage;\n\nuse emailmessage::Message;\n\nfn main() {\n    let m: Message<&str> = Message::builder()\n        .from(\"NoBody <nobody@domain.tld>\".parse().unwrap())\n        .reply_to(\"Yuin <yuin@domain.tld>\".parse().unwrap())\n        .to(\"Hei <hei@domain.tld>\".parse().unwrap())\n        .subject(\"Happy new year\")\n        .body(\"Be happy!\");\n\n    println!(\"{}\", m);\n}\n```\n\nRun this example:\n\n```\n$ cargo run --example format_string\n\nFrom: NoBody <nobody@domain.tld>\nReply-To: Yuin <yuin@domain.tld>\nTo: Hei <hei@domain.tld>\nSubject: Happy new year\n\nBe happy!\n```\n\nThe unicode header data will be encoded using _UTF8-Base64_ encoding.\n\n#### With mime body\n\n##### Single part\n\nThe more complex way is using MIME contents\n(see [format\\_mime.rs](examples/format_mime.rs)).\n\n```rust\nextern crate emailmessage;\n\nuse emailmessage::{header, Message, SinglePart};\n\nfn main() {\n    let m: Message<SinglePart<&str>> = Message::builder()\n        .from(\"NoBody <nobody@domain.tld>\".parse().unwrap())\n        .reply_to(\"Yuin <yuin@domain.tld>\".parse().unwrap())\n        .to(\"Hei <hei@domain.tld>\".parse().unwrap())\n        .subject(\"Happy new year\")\n        .mime_body(\n            SinglePart::builder()\n                .header(header::ContentType(\n                    \"text/plain; charset=utf8\".parse().unwrap(),\n                )).header(header::ContentTransferEncoding::QuotedPrintable)\n                .body(\"\u041f\u0440\u0438\u0432\u0435\u0442, \u043c\u0438\u0440!\"),\n        );\n\n    println!(\"{}\", m);\n}\n```\n\nThe body will be encoded using selected `Content-Transfer-Encoding`.\n\n```\n$ cargo run --example format_mime\n\nFrom: NoBody <nobody@domain.tld>\nReply-To: Yuin <yuin@domain.tld>\nTo: Hei <hei@domain.tld>\nSubject: Happy new year\nMIME-Version: 1.0\nContent-Type: text/plain; charset=utf8\nContent-Transfer-Encoding: quoted-printable\n\n=D0=9F=D1=80=D0=B8=D0=B2=D0=B5=D1=82, =D0=BC=D0=B8=D1=80!\n\n```\n\n##### Multiple parts\n\nAnd the more advanced way is a using multipart MIME contents\n(see [format\\_multipart.rs](examples/format_multipart.rs)).\n\n```rust\nextern crate emailmessage;\nuse emailmessage::{header, Message, MultiPart, SinglePart};\nfn main() {\n    let m: Message<MultiPart<&str>> = Message::builder()\n        .from(\"NoBody <nobody@domain.tld>\".parse().unwrap())\n        .reply_to(\"Yuin <yuin@domain.tld>\".parse().unwrap())\n        .to(\"Hei <hei@domain.tld>\".parse().unwrap())\n        .subject(\"Happy new year\")\n        .mime_body(\n            MultiPart::mixed()\n            .multipart(\n                MultiPart::alternative()\n                .singlepart(\n                    SinglePart::quoted_printable()\n                    .header(header::ContentType(\"text/plain; charset=utf8\".parse().unwrap()))\n                    .body(\"\u041f\u0440\u0438\u0432\u0435\u0442, \u043c\u0438\u0440!\")\n                )\n                .multipart(\n                    MultiPart::related()\n                    .singlepart(\n                        SinglePart::eight_bit()\n                        .header(header::ContentType(\"text/html; charset=utf8\".parse().unwrap()))\n                        .body(\"<p><b>Hello</b>, <i>world</i>! <img src=smile.png></p>\")\n                    )\n                    .singlepart(\n                        SinglePart::base64()\n                        .header(header::ContentType(\"image/png\".parse().unwrap()))\n                        .header(header::ContentDisposition {\n                            disposition: header::DispositionType::Inline,\n                            parameters: vec![],\n                        })\n                        .body(\"<smile-raw-image-data>\")\n                    )\n                )\n            )\n            .singlepart(\n                SinglePart::seven_bit()\n                .header(header::ContentType(\"text/plain; charset=utf8\".parse().unwrap()))\n                .header(header::ContentDisposition {\n                                 disposition: header::DispositionType::Attachment,\n                                 parameters: vec![\n                                     header::DispositionParam::Filename(\n                                         header::Charset::Ext(\"utf-8\".into()),\n                                         None, \"example.c\".as_bytes().into()\n                                     )\n                                 ]\n                             })\n                .body(\"int main() { return 0; }\")\n            )\n        );\n\n    println!(\"{}\", m);\n}\n```\n\n```\n$ cargo run --example format_multipart\n\nFrom: NoBody <nobody@domain.tld>\nReply-To: Yuin <yuin@domain.tld>\nTo: Hei <hei@domain.tld>\nSubject: Happy new year\nMIME-Version: 1.0\nContent-Type: multipart/mixed; boundary=\"RTxPCn9p31oAAAAAeQxtr1FbXr/i5vW1hFlH9oJqZRMWxRMK1QLjQ4OPqFk9R+0xUb/m\"\n\n--RTxPCn9p31oAAAAAeQxtr1FbXr/i5vW1hFlH9oJqZRMWxRMK1QLjQ4OPqFk9R+0xUb/m\nContent-Type: multipart/alternative; boundary=\"qW9QCn9p31oAAAAAodFBg1L1Qrraa5hEl0bDJ6kfJMUcRT2LLSWEoeyhSEbUBIqbjWqy\"\n\n--qW9QCn9p31oAAAAAodFBg1L1Qrraa5hEl0bDJ6kfJMUcRT2LLSWEoeyhSEbUBIqbjWqy\nContent-Transfer-Encoding: quoted-printable\nContent-Type: text/plain; charset=utf8\n\n=D0=9F=D1=80=D0=B8=D0=B2=D0=B5=D1=82, =D0=BC=D0=B8=D1=80!\n--qW9QCn9p31oAAAAAodFBg1L1Qrraa5hEl0bDJ6kfJMUcRT2LLSWEoeyhSEbUBIqbjWqy\nContent-Type: multipart/related; boundary=\"BV5RCn9p31oAAAAAUt42E9bYMDEAGCOWlxEz89Bv0qFA5Xsy6rOC3zRahMQ39IFZNnp8\"\n\n--BV5RCn9p31oAAAAAUt42E9bYMDEAGCOWlxEz89Bv0qFA5Xsy6rOC3zRahMQ39IFZNnp8\nContent-Transfer-Encoding: 8bit\nContent-Type: text/html; charset=utf8\n\n<p><b>Hello</b>, <i>world</i>! <img src=smile.png></p>\n--BV5RCn9p31oAAAAAUt42E9bYMDEAGCOWlxEz89Bv0qFA5Xsy6rOC3zRahMQ39IFZNnp8\nContent-Transfer-Encoding: base64\nContent-Type: image/png\nContent-Disposition: inline\n\nPHNtaWxlLXJhdy1pbWFnZS1kYXRhPg==\n--BV5RCn9p31oAAAAAUt42E9bYMDEAGCOWlxEz89Bv0qFA5Xsy6rOC3zRahMQ39IFZNnp8--\n--qW9QCn9p31oAAAAAodFBg1L1Qrraa5hEl0bDJ6kfJMUcRT2LLSWEoeyhSEbUBIqbjWqy--\n--RTxPCn9p31oAAAAAeQxtr1FbXr/i5vW1hFlH9oJqZRMWxRMK1QLjQ4OPqFk9R+0xUb/m\nContent-Transfer-Encoding: 7bit\nContent-Type: text/plain; charset=utf8\nContent-Disposition: attachment; filename=\"example.c\"\n\nint main() { return 0; }\n--RTxPCn9p31oAAAAAeQxtr1FbXr/i5vW1hFlH9oJqZRMWxRMK1QLjQ4OPqFk9R+0xUb/m--\n\n```\n\n#### Use streaming\n\nThe RAM-efficient way to formatting ans sending relatively big emails is a using streaming.\nFor example, you would like to send server access logs in attachments,\nHTML-documents with related media resources or PDFs.\n\nIn examples above we actually allocated formatted emails in memory,\nbut usually we don't want to do same for big emails which size measures in MBytes.\n\n##### Simple string\n\nThe simple example below shows actually sent chunks of streamed message\n(see [format\\_stream.rs](examples/format_stream.rs)).\n\n```rust\nextern crate emailmessage;\nextern crate futures;\nextern crate tokio;\n\nuse emailmessage::Message;\nuse futures::{Future, Stream};\nuse std::str::from_utf8;\nuse tokio::run;\n\nfn main() {\n    let m: Message = Message::builder()\n        .from(\"NoBody <nobody@domain.tld>\".parse().unwrap())\n        .reply_to(\"Yuin <yuin@domain.tld>\".parse().unwrap())\n        .to(\"Hei <hei@domain.tld>\".parse().unwrap())\n        .subject(\"Happy new year\")\n        .body(\"Be happy!\".into());\n\n    let f = m\n        .into_stream()\n        .map(|chunk| {\n            println!(\"CHUNK[[\\n{}]]\", from_utf8(&chunk).unwrap());\n            chunk\n        }).concat2()\n        .map(|message| {\n            println!(\"MESSAGE[[\\n{}]]\", from_utf8(&message).unwrap());\n        }).map_err(|error| {\n            eprintln!(\"ERROR: {}\", error);\n        });\n\n    run(f);\n}\n```\n\n```\n$ cargo run --example format_stream\n\nCHUNK[[\nFrom: NoBody <nobody@domain.tld>\nReply-To: Yuin <yuin@domain.tld>\nTo: Hei <hei@domain.tld>\nSubject: Happy new year\n]]\nCHUNK[[\n\nBe happy!]]\n```\n\nIn real world app we may do some buffering of stream to prevent too short and too long sendings.\n\n##### Multipart data\n\n(see [format\\_stream\\_multipart.rs](examples/format_stream_multipart.rs))\n\n```rust\nextern crate emailmessage;\nextern crate futures;\nextern crate tokio;\nuse emailmessage::{header, Message, MultiPart, SinglePart};\nuse futures::{Future, Stream};\nuse std::str::from_utf8;\nuse tokio::run;\nfn main() {\n    let b: MultiPart = MultiPart::mixed()\n        .multipart(\n            MultiPart::alternative()\n                .singlepart(\n                    SinglePart::quoted_printable()\n                        .header(header::ContentType(\n                            \"text/plain; charset=utf8\".parse().unwrap(),\n                        )).body(\"\u041f\u0440\u0438\u0432\u0435\u0442, \u043c\u0438\u0440!\".into()),\n                ).multipart(\n                    MultiPart::related()\n                        .singlepart(\n                            SinglePart::eight_bit()\n                                .header(header::ContentType(\n                                    \"text/html; charset=utf8\".parse().unwrap(),\n                                )).body(\n                                    \"<p><b>Hello</b>, <i>world</i>! <img src=smile.png></p>\".into(),\n                                ),\n                        ).singlepart(\n                            SinglePart::base64()\n                                .header(header::ContentType(\"image/png\".parse().unwrap()))\n                                .header(header::ContentDisposition {\n                                    disposition: header::DispositionType::Inline,\n                                    parameters: vec![],\n                                }).body(\"<smile-raw-image-data>\".into()),\n                        ),\n                ),\n        ).singlepart(\n            SinglePart::seven_bit()\n                .header(header::ContentType(\n                    \"text/plain; charset=utf8\".parse().unwrap(),\n                )).header(header::ContentDisposition {\n                    disposition: header::DispositionType::Attachment,\n                    parameters: vec![header::DispositionParam::Filename(\n                        header::Charset::Ext(\"utf-8\".into()),\n                        None,\n                        \"example.c\".as_bytes().into(),\n                    )],\n                }).body(\"int main() { return 0; }\".into()),\n        );\n\n    let m = Message::builder()\n        .from(\"NoBody <nobody@domain.tld>\".parse().unwrap())\n        .reply_to(\"Yuin <yuin@domain.tld>\".parse().unwrap())\n        .to(\"Hei <hei@domain.tld>\".parse().unwrap())\n        .subject(\"Happy new year\")\n        .mime_body(b.into_stream());\n\n    let f = m\n        .into_stream()\n        .map(|chunk| {\n            println!(\"CHUNK[[\\n{}]]\", from_utf8(&chunk).unwrap());\n            chunk\n        }).concat2()\n        .map(|message| {\n            println!(\"MESSSAGE[[\\n{}]]\", from_utf8(&message).unwrap());\n        }).map_err(|error| {\n            eprintln!(\"ERROR: {:?}\", error);\n        });\n\n    run(f);\n}\n```\n\n```\n$ cargo run --example format_stream_multipart\n\nCHUNK[[\nFrom: NoBody <nobody@domain.tld>\nReply-To: Yuin <yuin@domain.tld>\nTo: Hei <hei@domain.tld>\nSubject: Happy new year\nMIME-Version: 1.0\n]]\nCHUNK[[\nContent-Type: multipart/mixed; boundary=\"1S8dCMR/31oAAAAApHRNMETjK2uRsQs4mVVFKVNujcqnm8FHOXWvqARiaYy9ZmnpQ7uQ\"\n]]\nCHUNK[[\n--1S8dCMR/31oAAAAApHRNMETjK2uRsQs4mVVFKVNujcqnm8FHOXWvqARiaYy9ZmnpQ7uQ\n]]\nCHUNK[[\n--1S8dCMR/31oAAAAApHRNMETjK2uRsQs4mVVFKVNujcqnm8FHOXWvqARiaYy9ZmnpQ7uQ\n]]\nCHUNK[[\nContent-Type: multipart/alternative; boundary=\"TCMeCMR/31oAAAAAmf7KBuXt4qRk2RnBJCj8YJNdwm2dsadXxjOlC74hlb1tO6U/SqXY\"\n]]\nCHUNK[[\n--TCMeCMR/31oAAAAAmf7KBuXt4qRk2RnBJCj8YJNdwm2dsadXxjOlC74hlb1tO6U/SqXY\n]]\nCHUNK[[\n--TCMeCMR/31oAAAAAmf7KBuXt4qRk2RnBJCj8YJNdwm2dsadXxjOlC74hlb1tO6U/SqXY\n]]\nCHUNK[[\nContent-Transfer-Encoding: quoted-printable\nContent-Type: text/plain; charset=utf8\n]]\nCHUNK[[\n=D0=9F=D1=80=D0=B8=D0=B2=D0=B5=D1=82, =D0=BC=D0=B8=D1=80!]]\nCHUNK[[\n]]\nCHUNK[[\n--TCMeCMR/31oAAAAAmf7KBuXt4qRk2RnBJCj8YJNdwm2dsadXxjOlC74hlb1tO6U/SqXY\n]]\nCHUNK[[\nContent-Type: multipart/related; boundary=\"YsgeCMR/31oAAAAAanzeyu/dFJGjfzDxpsAOLhRB0RfSw+DXefQybZxGq6HIBEzotZ5Y\"\n]]\nCHUNK[[\n--YsgeCMR/31oAAAAAanzeyu/dFJGjfzDxpsAOLhRB0RfSw+DXefQybZxGq6HIBEzotZ5Y\n]]\nCHUNK[[\n--YsgeCMR/31oAAAAAanzeyu/dFJGjfzDxpsAOLhRB0RfSw+DXefQybZxGq6HIBEzotZ5Y\n]]\nCHUNK[[\nContent-Transfer-Encoding: 8bit\nContent-Type: text/html; charset=utf8\n]]\nCHUNK[[\n<p><b>Hello</b>, <i>world</i>! <img src=smile.png></p>]]\nCHUNK[[\n]]\nCHUNK[[\n--YsgeCMR/31oAAAAAanzeyu/dFJGjfzDxpsAOLhRB0RfSw+DXefQybZxGq6HIBEzotZ5Y\n]]\nCHUNK[[\nContent-Transfer-Encoding: base64\nContent-Type: image/png\nContent-Disposition: inline\n\n]]\nCHUNK[[\nPHNtaWxlLXJhdy1pbWFnZS1kYXRhPg==]]\nCHUNK[[\n\n]]\nCHUNK[[\n--YsgeCMR/31oAAAAAanzeyu/dFJGjfzDxpsAOLhRB0RfSw+DXefQybZxGq6HIBEzotZ5Y--\n]]\nCHUNK[[\n--TCMeCMR/31oAAAAAmf7KBuXt4qRk2RnBJCj8YJNdwm2dsadXxjOlC74hlb1tO6U/SqXY--\n]]\nCHUNK[[\n--1S8dCMR/31oAAAAApHRNMETjK2uRsQs4mVVFKVNujcqnm8FHOXWvqARiaYy9ZmnpQ7uQ\n]]\nCHUNK[[\nContent-Transfer-Encoding: 7bit\nContent-Type: text/plain; charset=utf8\nContent-Disposition: attachment; filename=\"example.c\"\n\n]]\nCHUNK[[\nint main() { return 0; }]]\nCHUNK[[\n\n]]\nCHUNK[[\n--1S8dCMR/31oAAAAApHRNMETjK2uRsQs4mVVFKVNujcqnm8FHOXWvqARiaYy9ZmnpQ7uQ--\n]]\n...\n```\n"
},
{
  "name": "discourse-mozilla-letter-avatar",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "plugin.rb",
      "zilla-slab"
    ]
  },
  "makefile": null,
  "readme": "# mozilla-letter-avatar\n\nChanges letter avatars to be more Mozilla\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n"
},
{
  "name": "webxr",
  "files": {
    "/": [
      ".gitignore",
      ".pr-preview.json",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE.md",
      "Makefile",
      "README.md",
      "designdocs",
      "explainer.md",
      "favicon-32x32.png",
      "favicon-96x96.png",
      "favicon.ico",
      "hit-testing-explainer.md",
      "images",
      "index.bs",
      "input-explainer.md",
      "package.json",
      "spatial-tracking-explainer.md",
      "style.css",
      "w3c.json"
    ]
  },
  "makefile": ".PHONY: all index.html\n\nall: index.html\n\nindex.html: index.bs\n\tcurl https://api.csswg.org/bikeshed/ -F file=@index.bs -F output=err\n\tcurl https://api.csswg.org/bikeshed/ -F file=@index.bs -F force=1 > index.html | tee\n",
  "readme": "# WebXR Device API Specification\n\n[![Build Status](https://travis-ci.org/immersive-web/webxr.svg?branch=master)](https://travis-ci.org/immersive-web/webxr)\n\nThe WebXR device API is for accessing virtual reality (VR) and augmented reality (AR) devices, including sensors and head-mounted displays on the Web. \n\n|    |             Headset Devices             | Handheld Device e.g. Phone |\n|----|:---------------------------------------:|:--------------------------:|\n| VR | VR Devices, previously handled by WebVR | Magic Window Behaviour     |\n| AR | Mixed Reality Headsets                  | Phone AR                   |\n\nThe [WebXR Device API Specification][1] is the repository of the [Immersive Web Working Group][17].\n\n## Taking Part\n\n1. Read the [code of conduct][18]\n2. See if your issue is being discussed in the [issues][8], or if your idea is being discussed in the [proposals repo][19].\n3. We will be publishing the minutes from the bi-weekly calls.\n4. You can also join the working group to participate in these discussions.\n\n## Specifications\n\n* [WebXR Device API Specification][1]: Main specification for JavaScript API for accessing VR and AR devices, including sensors and head-mounted displays.\n* [Legacy WebVR API Specification][2]: Legacy WebVR API 1.1 specification for JavaScript API for accessing VR displays. Development of the WebVR API has halted in favor of being replaced the WebXR Device API. Several browsers will continue to support this version of the API in the meantime.\n* [Gamepad API Specification][5]: Introduces a low-level JS API interface for accessing gamepad devices.\n* [Legacy Gamepad Extensions API Specification][6]: Extends the Gamepad API to enable access to more advanced device capabilities.\n\n\n## Relevant Links\n\n* [Immersive Web Community Group][3]\n* [WebXR Device API Specification][1]\n* [Immersive Web Early Adopters Guide][16]\n* [Legacy WebVR API Specification][2]\n* [Immersive Web Working Group Charter][4]\n\n\n## Communication\n\n* [Immersive Web Working Group][17]\n* [Immersive Web Community Group][3]\n* [GitHub issues list: `webxr`][8]\n* [`public-immersive-web` mailing list][20]\n* [Legacy `public-webvr` mailing list archive][7]\n\n## Maintainers\n\nTo generate the spec document (`index.html`) from the `index.bs` [Bikeshed][10] document:\n\n```sh\nmake\n```\n\n\n## Tests\n\nFor normative changes, a corresponding\n[web-platform-tests][11] PR is highly appreciated. Typically,\nboth PRs will be merged at the same time. Note that a test change that contradicts the spec should\nnot be merged before the corresponding spec change. If testing is not practical, please explain why\nand if appropriate [file a web-platform-tests issue][12]\nto follow up later. Add the `type:untestable` or `type:missing-coverage` label as appropriate.\n\n\n## License\n\nPer the [`LICENSE.md`](LICENSE.md) file:\n\n> All documents in this Repository are licensed by contributors under the  [W3C Software and Document License](https://www.w3.org/Consortium/Legal/copyright-software).\n\n<!-- Links -->\n[1]: https://immersive-web.github.io/webxr/\n[2]: https://immersive-web.github.io/webvr/\n[3]: https://www.w3.org/community/webvr/\n[4]: https://www.w3.org/2018/09/immersive-web-wg-charter.html\n[5]: https://w3c.github.io/gamepad/\n[6]: https://w3c.github.io/gamepad/extensions.html\n[7]: https://lists.w3.org/Archives/Public/public-webvr/\n[8]: https://github.com/immersive-web/webxr/issues\n[10]: https://github.com/tabatkins/bikeshed\n[11]: https://github.com/web-platform-tests/wpt\n[12]: https://github.com/web-platform-tests/wpt/issues/new\n[13]: http://www.w3.org/Consortium/Legal/2015/copyright-software-and-document\n[14]: https://www.w3.org/community/about/agreements/cla/\n[15]: https://www.w3.org/Consortium/Legal/2008/03-bsd-license.html\n[16]: https://immersive-web.github.io/webxr-reference/\n[17]: https://w3.org/immersive-web\n[18]: https://immersive-web.github.io/homepage/code-of-conduct.html\n[19]: https://github.com/immersive-web/proposals\n[20]: https://lists.w3.org/Archives/Public/public-immersive-web/\n"
},
{
  "name": "gcp-infra",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "img"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla | GCP Infrastructure\n\n---\n\n# Introduction\n\nSeveral departments within Mozilla have interest in running workloads on the Google Compute Platform (GCP). This document describes how the GCP accounts are setup and is based on the POC run with the Bedrock team (mozilla.org). This design is meant to limit the potential _blast radius_ of a single \"super admin\" compromise in GSuite by separating GCP from GSuite.  \n\n## Goals\n\n* The GCP environment should have 100% of regular users go through SSO to login.\n* The GCP environment should have consolidated billing.\n* Resources in the account should only be editable by delegated admins per cost center.\n* Super Admins should still have an organization \"break glass\" access to perform incident response (security & operations alike).\n\n## Technical Implementation\n\nMozilla currently uses GSuite (as in Drive, Docs, etc.) for several organizations.  The requirements above require an additional domain _gcp.infra.mozilla.com_ in order to ensure complete GSuite separation of super admins.  This was setup as a standalone GSuite domain with a single mailbox leveraging the _new_ Google Cloud Identity free tier for additional users.  \n\n## URLs of Interest\n\nThese are the primary URls for Google Cloud Management. In both cases unless signing in as \"super-admin\" it will be best\nto use the provider initiated signon URL.  As of today June 27, 2018 the provider initiated url is:\n\n```\nhttps://auth.mozilla.auth0.com/samlp/uYFDijsgXulJ040Os6VJLRxf0GG30OmC\n```\n\n> Note this url requires a `relayState` parameter. See relayState section below for more information on Google SAML flows.\n\n* GSuite Admin Console  https://admin.google.com\n* Google Cloud Admin https://cloud.google.com\n\n> Point and click\n```\nhttps://auth.mozilla.auth0.com/samlp/uYFDijsgXulJ040Os6VJLRxf0GG30OmC?RelayState=https://console.cloud.google.com/\n\n```\n\n# How to setup the GCP environment\n\n### Notes on GSuite vs GCP\nA single mailbox GSuite domain exists for *gcp.infra.mozilla.com*. Due to the nature of the way the admin console works in GSuite Enterprise, things can be confusing. In order to create a GSuite domain that contains only a single licensed user two things need to happen: Turn off auto-licensing and enable Google Cloud Identity (Free Tier).\n\n## How-to setup GSuite & GCP\n\n### 1 - Disabling Auto Licensing\n\nIn the Gsuite Admin Console, navigate to billing. Select \"G Suite Basic\" and ensure Auto-Licensing is set to \"Off for Everyone\".\n\n![](img/howto_1.png)\n\n### 2 - Enable Google Cloud Identity (Free Tier)\n\nIn the Gsuite Admin Console, navigate to billing again. Enable \"Google Cloud Identity (Free Tier)\".\n\n> Note that you may need to request a limit increase by calling Google. The default number of cloud identity seats is ~ 100 at the time of writing.  \n\n![](img/howto_2.png)\n\nThis allow us (as admin) to add users to GSuite without them receiving licenses to use GSuite products. They are strictly principals that can be mapped to an SSO user using SAML Claims.\n\n## User provisioning in GCP\n\n### Create your first user manually (Cloud Users vs GSuite Mailboxes)\n\n> Note, this is an optional step if you just want to test that users work\n> \nIn order to create a user without a license start in the GSuite Admin Console and navigate to \"Directory => Users\". Within users any user can be created so long as the username matches the Mozilla LDAP username (usually it's also their Mozilla email). As a standard we put the mozilla email as the users secondary email at present for all manually created users (e.g. jdoe@mozilla.com).\n\n![](img/howto_3.png)\n\n## Automatic user provisioning with Mozilla IAM (SSO)\n\n> Note, this is the recommended way to setup provisioning.\n\nUsers can and should be automatically provisioned and de-provisioned by the SSO setup. Google cannot currently do this automatically for environments that do not solely rely on Active Directory. In other words, SCIM is not supported.\n\nThe alternative is to write a driver that will call Google's API in order to care for the provisioning and deprovisioning directly.\n\nThis is the Mozilla IAM driver: https://github.com/mozilla-iam/gsuite_cloud_users_driver\n\nFollow the setup instructions from the repository to set the driver up. Once setup, users will be automatically add and removed as needed.\n\n## Integration for SAML Based Authentication\n\n### With Auth0\n#### SAML Claim Mapping configuration\nWhen using Auth0 as SSO/Access Provider you should use the following access map. It indicates which SAML claims Google understands and maps the Auth0 values to these claims. *An additional mapping is done with a rule in addition to this configuration (see below).*\n\n> Note that the Application callback URL (ACS) in this case is:\n> https://www.google.com/a/gcp.infra.mozilla.com/acs and should also be reflected in the configuration as \"callback uri\" when using Auth0\n\n\n```\n{\n  \"audience\": \"https://www.google.com/a/gcp.infra.mozilla.com/acs\",\n  \"mappings\": {\n    \"nickname\": \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/name\"\n  },\n  \"createUpnClaim\": false,\n  \"passthroughClaimsWithNoMapping\": false,\n  \"mapUnknownClaimsAsIs\": false,\n  \"mapIdentities\": false,\n  \"signatureAlgorithm\": \"rsa-sha256\",\n  \"digestAlgorithm\": \"sha256\",\n  \"lifetimeInSeconds\": 86400,\n  \"signResponse\": false,\n  \"nameIdentifierFormat\": \"urn:oasis:names:tc:SAML:2.0:nameid-format:email\",\n  \"nameIdentifierProbes\": [\n    \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\"\n  ],\n  \"authnContextClassRef\": \"urn:oasis:names:tc:SAML:2.0:ac:classes:unspecified\"\n}\n```\n\n> Tip: The audience changes per GSuite domain.\n\nOn the Google side the SAML setup points to the IDP initiated URL and receives the SAML certificate from Auth0.\n\n![](img/howto_4.png)\n\n#### SAML Claim mapping rule\n\nDue to the fact users are signing into the GSuite domain `mozilla.com` (or other Mozilla staff domains) using the SSO flow we have to remap additional claims to make the users look like they belong to another domain `gcp.infra.mozilla.com`. This can be done with Auth0 rules. See the sample rule below:\n\n``` nodejs\nfunction (user, context, callback) {\n  if (context.clientID !== 'uYFDijsgXulJ040Os6VJLRxf0GG30OmC') // GSuite `client_id` (mozilla.com)\n    return callback(null, user, context);\n\n  // Replace all known-staff domains\n  user.myemail = user.email.replace(\"mozilla.com\", \"gcp.infra.mozilla.com\").replace(\"mozillafoundation.org\", \"gcp.infra.mozilla.com\").replace(\"getpocket.com\", \"gcp.infra.mozilla.com\");\n\n  context.samlConfiguration = context.samlConfiguration || {};\n\n  context.samlConfiguration.mappings = {\n     \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/nameidentifier\":     \"myemail\",\n     \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/emailaddress\":       \"myemail\",\n     \"http://schemas.xmlsoap.org/ws/2005/05/identity/claims/email\":       \"myemail\",\n  };\n  context.samlConfiguration.nameIdentifierFormat = \"urn:oasis:names:tc:SAML:2.0:nameid-format:email\";\n\n  callback(null, user, context);\n}\n\n```\n\n> Note that for potential non-staff user accounts, since they would sign-in into a regular Google account, this rule is not necessary. That said, any user that is not part of the `gcp.infra.mozilla.com` domain would still need to be remapped to have default accesses set. Please contact the Mozilla IAM team for such a use case.\n\n### Google RelayState Parameters and SSO\n\nGoogle services require the use of the `?RelayState=` GET parameter when signing in. This allows Google's SAML variant to understand what service you're signing into. It will be added to the requested URL automatically, but if you contruct an URL manually this will need to be added for things to work.\n\nThe relay state for GCP is `?RelayState=https://console.cloud.google.com/`\n\n## Billing and Billing Associations\n\nBilling in GCP can be setup at the project level _or_ at the organization level. Anyone with access to the billing account\ncan associate it with a project. As it would be dangerous to allow everyone to associate projects with a central credit card, you can only do this by request to a GCP Billing Admin.\n\n> XXX Where should the request actually go? Link? Email? Issue?\n\n### Feature wish list\n\nThere are a number of features that should exist but don't yet. Here they are in user story form in case Google solves these in the future:\n\n* As an admin I should be able to associate a billing account with a folder. The billing should inherit to new resources and projects in the folder. \n* As a developer I can get an event _similar to cloudwatch_ events for types of user signin.\n* As an admin I can set a policy to opportunistically provision users on signin with no resources or access.\n* As an admin I can map SAML group claims to specific roles in the GCP console.\n* As an admin I can assign a role at the ORG level as the default role for all users allowing recursion of the folder and tree structure.\n\n> XXX Stuff below is yet to write down\n> \n## GCP Console Org Setup\n\n### Code that doesn't exist yet\n\n## User Roles and Role Bindings\n\n### Minimum Roles Per User\n\n## Break Glass Credentials for the super-admin account\n\nIn case of problem, such as the SSO setup malfunctioning, you may need to access the GSuite super-admin.\nThis is a local Google user which does not use SSO and have full access to the complete GSuite and GCP setup. It's an extremely sensitive account.\n\nIn order to access the GSuite super-admin you need to break the glass: i.e. cause a loud action that will cause the security teams to verify your actions were legitimate.\n\n### Requirements\n\n- The credentials must never be accessible by a single person (Shamir Secret Sharing is used to require multiple parties to be present when the account is used)\n- Using the credentials must send an alert to Mozilla's SIEM (MozDef)\n- The encrypted credentials must be stored in Mozilla Infosec's safes\n- The credentials must follow [Mozilla's Security Principles](https://infosec.mozilla.org/fundamentals/security_principles.html) (i.e. be strong and use 2FA)\n\n### Being part of the quorum with access to super-admin\n\nIf you believe you should be part of the quorum with access to the super-admin, you have to:\n\n- Be part of Mozilla Operational or Security Staff\n- Have a few use cases in mind\n- Have a PGP key that is signed and verified by existing members of the quorum\n- Request being added (with your PGP key and use cases) at XXX\n\n> XXX Which online storage is used?\n> XXX Add request contact or method\n> \n\n### How credentials are stored encrypted\n\n- Run this on a safe machine\n```\n# Ensure you have sss_share and GnuPG installed: https://github.com/azet/sss_share and https://www.gnupg.org\n# Ensure you have the signed, verified keys for all quorum members\n\n$ ./share member@mozilla.com, member2@mozilla.com, .. <<< credentials.txt\n$ srm credentials.txt\n```\n- Distribute the resulting encrypte files to each member\n\n### How to break the glass!\n\n- Decrypt your part of the credentials `gpg --decrypt ...`\n- Ask another member to decrypt their part of the credentials. They will verify your request is legitimate and ask you why this is necessary.\n- Inform [Mozilla Enterprise Information Security](https://bugzilla.mozilla.org/enter_bug.cgi?product=Enterprise+Information+Security&component=General) that you're going to use the credentials\n- Use credentials\n- Revoke credentials and re-create them when done\n\n## Incident Response in GCP Flow\n\n"
},
{
  "name": "iris_old",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Pipfile",
      "README.md",
      "_config.yml",
      "bootstrap",
      "config.ini",
      "docs",
      "iris",
      "setup.py",
      "tools"
    ],
    "/docs": [
      "annotated.html",
      "annotated_dup.js",
      "bc_s.png",
      "bdwn.png",
      "classes.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences_1_1_privacy-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences_1_1_privacy.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences_1_1_privacy.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences_1_1_privacy_1_1_exceptions-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences_1_1_about_preferences_1_1_privacy_1_1_exceptions.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1bookmarks_1_1_bookmarks.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1bookmarks_1_1_bookmarks_1_1_star_dialog-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1bookmarks_1_1_bookmarks_1_1_star_dialog.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking_1_1_content_blocking-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking_1_1_content_blocking.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking__tour_1_1_content_blocking_tour-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking__tour_1_1_content_blocking_tour.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1docker_1_1_docker-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1docker_1_1_docker.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__dialog_1_1_download_dialog-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__dialog_1_1_download_dialog.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_about_downloads-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_about_downloads.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_download_state-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_download_state.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_49df6c5e3ffc8da301732d77f74cc91d.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_context_menu-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_context_menu.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_panel-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_panel.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_panel.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager_1_1_download_manager_1_1_downloads_panel_1_1_download_details.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1find__toolbar_1_1_find_toolbar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1find__toolbar_1_1_find_toolbar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1hamburger_1_1_hamburger_menu-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1hamburger_1_1_hamburger_menu.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_c_lear_recent_history-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_c_lear_recent_history.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_c_lear_recent_history.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_c_lear_recent_history_1_1_time_range-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_c_lear_recent_history_1_1_time_range.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_forget_last-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_forget_last.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_history_menu-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_history_menu.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_recently_closed_tabs-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_recently_closed_tabs.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_recently_closed_windows-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1history_1_1_history_1_1_recently_closed_windows.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_download_library-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_download_library.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_import_and_backup-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_import_and_backup.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_import_and_backup.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_import_and_backup_1_1_restore-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_import_and_backup_1_1_restore.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_organize-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_organize.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views_1_1_show_columns-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views_1_1_show_columns.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views_1_1_sort-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library_1_1_library_1_1_views_1_1_sort.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu_1_1_bookmarks_option-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu_1_1_bookmarks_option.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu_1_1_bookmarks_option.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu_1_1_bookmarks_option_1_1_bookmarking_tools-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_library_menu_1_1_bookmarks_option_1_1_bookmarking_tools.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_sidebar_bookmarks-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_sidebar_bookmarks.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_sidebar_bookmarks.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_sidebar_bookmarks_1_1_bookmarks_toolbar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu_1_1_sidebar_bookmarks_1_1_bookmarks_toolbar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1location__bar_1_1_location_bar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1location__bar_1_1_location_bar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1menu__bar_1_1_menu_bar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1menu__bar_1_1_menu_bar_1_1_edit-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1menu__bar_1_1_menu_bar_1_1_edit.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1nav__bar_1_1_nav_bar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1nav__bar_1_1_nav_bar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1private__window_1_1_private_window-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1private__window_1_1_private_window.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_bookmarks_sidebar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_bookmarks_sidebar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar.js",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar_1_1_timeline-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar_1_1_timeline.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar_1_1_view_by-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_history_sidebar_1_1_view_by.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_sidebar_header-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_sidebar_header.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_synced_tabs_sidebar-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar_1_1_sidebar_1_1_synced_tabs_sidebar.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1site__information__panel_1_1_site_information_panel-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1site__information__panel_1_1_site_information_panel.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1tabs_1_1_tabs-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1tabs_1_1_tabs.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1utils_1_1_utils-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1utils_1_1_utils.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1window__controls_1_1_auxiliary_window-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1window__controls_1_1_auxiliary_window.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1window__controls_1_1_main_window-members.html",
      "classiris_1_1api_1_1core_1_1firefox__ui_1_1window__controls_1_1_main_window.html",
      "classiris_1_1api_1_1core_1_1key_1_1___iris_key-members.html",
      "classiris_1_1api_1_1core_1_1key_1_1___iris_key.html",
      "classiris_1_1api_1_1core_1_1key_1_1___iris_key.js",
      "classiris_1_1api_1_1core_1_1key_1_1_key-members.html",
      "classiris_1_1api_1_1core_1_1key_1_1_key.html",
      "classiris_1_1api_1_1core_1_1key_1_1_key_modifier-members.html",
      "classiris_1_1api_1_1core_1_1key_1_1_key_modifier.html",
      "classiris_1_1api_1_1core_1_1local__web_1_1_local_web-members.html",
      "classiris_1_1api_1_1core_1_1local__web_1_1_local_web.html",
      "classiris_1_1api_1_1core_1_1location_1_1_location-members.html",
      "classiris_1_1api_1_1core_1_1location_1_1_location.html",
      "classiris_1_1api_1_1core_1_1location_1_1_location.js",
      "classiris_1_1api_1_1core_1_1match_1_1_match-members.html",
      "classiris_1_1api_1_1core_1_1match_1_1_match.html",
      "classiris_1_1api_1_1core_1_1match_1_1_match.js",
      "classiris_1_1api_1_1core_1_1mouse_1_1_mouse-members.html",
      "classiris_1_1api_1_1core_1_1mouse_1_1_mouse.html",
      "classiris_1_1api_1_1core_1_1pattern_1_1_pattern-members.html",
      "classiris_1_1api_1_1core_1_1pattern_1_1_pattern.html",
      "classiris_1_1api_1_1core_1_1pattern_1_1_pattern.js",
      "classiris_1_1api_1_1core_1_1platform_1_1_platform-members.html",
      "classiris_1_1api_1_1core_1_1platform_1_1_platform.html",
      "classiris_1_1api_1_1core_1_1region_1_1_region-members.html",
      "classiris_1_1api_1_1core_1_1region_1_1_region.html",
      "classiris_1_1api_1_1core_1_1region_1_1_region.js",
      "classiris_1_1api_1_1core_1_1settings_1_1___iris_settings-members.html",
      "classiris_1_1api_1_1core_1_1settings_1_1___iris_settings.html",
      "classiris_1_1api_1_1core_1_1settings_1_1___iris_settings.js",
      "classiris_1_1api_1_1helpers_1_1customize__utils_1_1_customize_page-members.html",
      "classiris_1_1api_1_1helpers_1_1customize__utils_1_1_customize_page.html",
      "classiris_1_1api_1_1helpers_1_1download__manager__utils_1_1_download_files-members.html",
      "classiris_1_1api_1_1helpers_1_1download__manager__utils_1_1_download_files.html",
      "classiris_1_1api_1_1helpers_1_1general_1_1_option-members.html",
      "classiris_1_1api_1_1helpers_1_1general_1_1_option.html",
      "classiris_1_1api_1_1helpers_1_1general_1_1_right_click_location_bar-members.html",
      "classiris_1_1api_1_1helpers_1_1general_1_1_right_click_location_bar.html",
      "classiris_1_1api_1_1helpers_1_1general_1_1_zoom_type-members.html",
      "classiris_1_1api_1_1helpers_1_1general_1_1_zoom_type.html",
      "classiris_1_1api_1_1helpers_1_1test__utils_1_1_step-members.html",
      "classiris_1_1api_1_1helpers_1_1test__utils_1_1_step.html",
      "classiris_1_1api_1_1helpers_1_1test__utils_1_1_step.js",
      "closed.png",
      "dir_0188a3e6da905bc60aceb35bf790b8c9.html",
      "dir_0b64732ec2a8084ec86124ca12c5adba.html",
      "dir_61c4cffad3e0c8138b97d700b4f44498.html",
      "dir_856073a159dac1331a7198300ccc1fdf.html",
      "doc.png",
      "doxygen.css",
      "doxygen.png",
      "dynsections.js",
      "folderclosed.png",
      "folderopen.png",
      "functions.html",
      "functions_a.html",
      "functions_b.html",
      "functions_c.html",
      "functions_d.html",
      "functions_dup.js",
      "functions_e.html",
      "functions_f.html",
      "functions_func.html",
      "functions_g.html",
      "functions_h.html",
      "functions_i.html",
      "functions_j.html",
      "functions_k.html",
      "functions_l.html",
      "functions_m.html",
      "functions_n.html",
      "functions_o.html",
      "functions_p.html",
      "functions_q.html",
      "functions_r.html",
      "functions_s.html",
      "functions_t.html",
      "functions_u.html",
      "functions_v.html",
      "functions_vars.html",
      "functions_vars.js",
      "functions_vars_b.html",
      "functions_vars_c.html",
      "functions_vars_d.html",
      "functions_vars_e.html",
      "functions_vars_f.html",
      "functions_vars_g.html",
      "functions_vars_h.html",
      "functions_vars_i.html",
      "functions_vars_j.html",
      "functions_vars_k.html",
      "functions_vars_l.html",
      "functions_vars_m.html",
      "functions_vars_n.html",
      "functions_vars_o.html",
      "functions_vars_p.html",
      "functions_vars_q.html",
      "functions_vars_r.html",
      "functions_vars_s.html",
      "functions_vars_t.html",
      "functions_vars_u.html",
      "functions_vars_v.html",
      "functions_vars_w.html",
      "functions_vars_x.html",
      "functions_vars_y.html",
      "functions_vars_z.html",
      "functions_w.html",
      "functions_x.html",
      "functions_y.html",
      "functions_z.html",
      "hierarchy.html",
      "hierarchy.js",
      "index.html",
      "jquery.js",
      "menu.js",
      "menudata.js",
      "namespaceiris.html",
      "namespaceiris.js",
      "namespaceiris_1_1api.html",
      "namespaceiris_1_1api.js",
      "namespaceiris_1_1api_1_1core.html",
      "namespaceiris_1_1api_1_1core.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1about__preferences.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1bookmarks.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1bookmarks.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking__tour.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1content__blocking__tour.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1docker.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1docker.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1download__dialog.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1download__dialog.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1download__manager.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1find__toolbar.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1find__toolbar.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1hamburger.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1hamburger.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1history.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1history.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1library.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1library.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1library__menu.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1location__bar.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1location__bar.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1menu__bar.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1menu__bar.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1nav__bar.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1nav__bar.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1private__window.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1private__window.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1sidebar.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1site__information__panel.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1site__information__panel.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1tabs.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1tabs.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1utils.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1utils.js",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1window__controls.html",
      "namespaceiris_1_1api_1_1core_1_1firefox__ui_1_1window__controls.js",
      "namespaceiris_1_1api_1_1core_1_1key.html",
      "namespaceiris_1_1api_1_1core_1_1key.js",
      "namespaceiris_1_1api_1_1core_1_1local__web.html",
      "namespaceiris_1_1api_1_1core_1_1local__web.js",
      "namespaceiris_1_1api_1_1core_1_1location.html",
      "namespaceiris_1_1api_1_1core_1_1location.js",
      "namespaceiris_1_1api_1_1core_1_1match.html",
      "namespaceiris_1_1api_1_1core_1_1match.js",
      "namespaceiris_1_1api_1_1core_1_1mouse.html",
      "namespaceiris_1_1api_1_1core_1_1mouse.js",
      "namespaceiris_1_1api_1_1core_1_1pattern.html",
      "namespaceiris_1_1api_1_1core_1_1pattern.js",
      "namespaceiris_1_1api_1_1core_1_1platform.html",
      "namespaceiris_1_1api_1_1core_1_1platform.js",
      "namespaceiris_1_1api_1_1core_1_1region.html",
      "namespaceiris_1_1api_1_1core_1_1region.js",
      "namespaceiris_1_1api_1_1core_1_1settings.html",
      "namespaceiris_1_1api_1_1core_1_1settings.js",
      "namespaceiris_1_1api_1_1helpers.html",
      "namespaceiris_1_1api_1_1helpers.js",
      "namespaceiris_1_1api_1_1helpers_1_1customize__utils.html",
      "namespaceiris_1_1api_1_1helpers_1_1customize__utils.js",
      "namespaceiris_1_1api_1_1helpers_1_1download__manager__utils.html",
      "namespaceiris_1_1api_1_1helpers_1_1download__manager__utils.js",
      "namespaceiris_1_1api_1_1helpers_1_1general.html",
      "namespaceiris_1_1api_1_1helpers_1_1general.js",
      "namespaceiris_1_1api_1_1helpers_1_1history__utils.html",
      "namespaceiris_1_1api_1_1helpers_1_1keyboard__shortcuts.html",
      "namespaceiris_1_1api_1_1helpers_1_1results.html",
      "namespaceiris_1_1api_1_1helpers_1_1test__utils.html",
      "namespaceiris_1_1api_1_1helpers_1_1test__utils.js",
      "namespaceiris_1_1test__dependencies.html",
      "namespacemembers.html",
      "namespacemembers_func.html",
      "namespacemembers_vars.html",
      "nav_f.png",
      "nav_g.png",
      "nav_h.png",
      "navtree.css",
      "navtree.js",
      "navtreedata.js",
      "navtreeindex0.js",
      "navtreeindex1.js",
      "open.png",
      "resize.js",
      "search",
      "splitbar.png",
      "sync_off.png",
      "sync_on.png",
      "tab_a.png",
      "tab_b.png",
      "tab_h.png",
      "tab_s.png",
      "tabs.css"
    ]
  },
  "makefile": null,
  "readme": "# Welcome to Mozilla Iris!\n\nNOTE: This repo is an archive of the original Iris 1.0 project, and is no longer being maintained.\n\nPlease refer to these active projects instead:\n\n* [Iris](https://github.com/mozilla/iris) - a visual automation platform for the desktop, written in Python 3.\n* [Iris for Firefox](https://github.com/mozilla/iris_firefox) - tests written specifically for automation of the Firefox browser.\n"
},
{
  "name": "dye-score",
  "files": {
    "/": [
      ".editorconfig",
      ".github",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.rst",
      "CONTRIBUTING.rst",
      "HISTORY.rst",
      "LICENSE",
      "MANIFEST.in",
      "Makefile",
      "README.rst",
      "docs",
      "dye_score",
      "requirements_dev.txt",
      "setup.cfg",
      "setup.py",
      "tests"
    ],
    "/docs": [
      "Makefile",
      "api.rst",
      "conf.py",
      "contributing.rst",
      "index.rst",
      "installation.rst",
      "make.bat",
      "usage.ipynb"
    ],
    "/.github": [
      "ISSUE_TEMPLATE.md"
    ]
  },
  "makefile": ".PHONY: clean clean-test clean-pyc clean-build docs help\n.DEFAULT_GOAL := help\n\ndefine BROWSER_PYSCRIPT\nimport os, webbrowser, sys\n\nfrom urllib.request import pathname2url\n\nwebbrowser.open(\"file://\" + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\n\ndefine PRINT_HELP_PYSCRIPT\nimport re, sys\n\nfor line in sys.stdin:\n\tmatch = re.match(r'^([a-zA-Z_-]+):.*?## (.*)$$', line)\n\tif match:\n\t\ttarget, help = match.groups()\n\t\tprint(\"%-20s %s\" % (target, help))\nendef\nexport PRINT_HELP_PYSCRIPT\n\nBROWSER := python -c \"$$BROWSER_PYSCRIPT\"\n\nhelp:\n\t@python -c \"$$PRINT_HELP_PYSCRIPT\" < $(MAKEFILE_LIST)\n\nclean: clean-build clean-pyc clean-test ## remove all build, test, coverage and Python artifacts\n\nclean-build: ## remove build artifacts\n\trm -fr build/\n\trm -fr dist/\n\trm -fr .eggs/\n\tfind . -name '*.egg-info' -exec rm -fr {} +\n\tfind . -name '*.egg' -exec rm -f {} +\n\nclean-pyc: ## remove Python file artifacts\n\tfind . -name '*.pyc' -exec rm -f {} +\n\tfind . -name '*.pyo' -exec rm -f {} +\n\tfind . -name '*~' -exec rm -f {} +\n\tfind . -name '__pycache__' -exec rm -fr {} +\n\nclean-test: ## remove test and coverage artifacts\n\trm -f .coverage\n\trm -fr htmlcov/\n\trm -fr .pytest_cache\n\nlint: ## check style with flake8\n\tflake8 dye_score tests\n\ntest: ## run tests quickly with the default Python\n\tpy.test\n\ncoverage: ## check code coverage quickly with the default Python\n\tcoverage run --source dye_score -m pytest\n\tcoverage report -m\n\tcoverage html\n\t$(BROWSER) htmlcov/index.html\n\ndocs: ## generate Sphinx HTML documentation, including API docs\n\t$(MAKE) -C docs clean\n\t$(MAKE) -C docs html\n\t$(BROWSER) docs/_build/html/index.html\n\nservedocs: docs ## compile the docs watching for changes\n\twatchmedo shell-command -p '*.rst;*.py;*.ipynb' -c '$(MAKE) -C docs html' -R -D .\n\nrelease: dist ## package and upload a release\n\ttwine upload dist/*\n\ndist: clean ## builds source and wheel package\n\tpython setup.py sdist\n\tpython setup.py bdist_wheel\n\tls -l dist\n\ninstall: clean ## install the package to the active Python's site-packages\n\tpython setup.py install\n",
  "readme": "=========\nDye Score\n=========\n\n.. image:: https://readthedocs.org/projects/dyescore/badge/?version=latest\n    :target: https://dyescore.readthedocs.io/en/latest/?badge=latest\n    :alt: Documentation Status\n.. image:: https://travis-ci.org/mozilla/dye-score.svg?branch=master\n    :target: https://travis-ci.org/mozilla/dye-score\n\n\nUtilities to build the dye-score metric from OpenWPM_ javascript call data.\n\n\n* GitHub repo: https://github.com/mozilla/dye-score/\n* Documentation: https://dyescore.readthedocs.io/\n* Free software: Mozilla Public License\n\nQuickstart\n----------\n\nInstall dye score: ``conda install dye-score -c conda-forge`` or ``pip install dye-score``\n\nReview usage notebook: https://github.com/mozilla/dye-score/blob/master/docs/usage.ipynb\n\n.. _OpenWPM: https://github.com/mozilla/openwpm\n"
},
{
  "name": "discourse-mozilla-theme-dark",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "discourse-category-categorization",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "assets",
      "plugin.rb",
      "screenshot.png"
    ]
  },
  "makefile": null,
  "readme": "# category-categorization\n*Discourse plugin which enables categorization of categories*\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Installation\n\nFollow the Discourse [Install a Plugin](https://meta.discourse.org/t/install-a-plugin/19157) guide.\n\n## Usage\n\nA categorization can be defined in the category settings modal:\n\n![category settings modal](screenshot.png)\n\nIt'll then appear in `/categories.json` and `/c/:id/show.json` as the `categorization` property.\n\n## Licence\n\n[MPL 2.0](https://www.mozilla.org/MPL/2.0/)\n"
},
{
  "name": "discourse-sidekiq-monitor",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "app",
      "config",
      "jobs",
      "lib",
      "plugin.rb",
      "spec"
    ]
  },
  "makefile": null,
  "readme": "# sidekiq-monitor\n\n*Exposes an endpoint to show the current status of sidekiq*\n\n[![Build Status](https://travis-ci.org/mozilla/discourse-sidekiq-monitor.svg?branch=master)](https://travis-ci.org/mozilla/discourse-sidekiq-monitor)\n [![Coverage Status](https://coveralls.io/repos/github/mozilla/discourse-sidekiq-monitor/badge.svg?branch=master)](https://coveralls.io/github/mozilla/discourse-sidekiq-monitor?branch=master)\n\n## Bug reports\n\nBug reports should be filed [by following the process described here](https://discourse.mozilla.org/t/where-do-i-file-bug-reports-about-discourse/32078).\n\n## Usage\n\nSet the API key in the `sidekiq_monitor_key` setting.\n\nThen use that key in the X-Sidekiq-Monitor-Key header, to query `/sidekiq_monitor/status.json`:\n\n```\ncurl -H 'X-Sidekiq-Monitor-Key: 1234' localhost:3000/sidekiq_monitor/status.json\n```\n\nIf the key is invalid, the endpoint will return a 404 status.\n\nIf the key is valid, the endpoint will return a 200 status and:\n\n- `running: true`, if sidekiq is running\n- `running: false`, if sidekiq isn't running\n"
},
{
  "name": "events-issues",
  "files": {
    "/": [
      ".github",
      "CODE_OF_CONDUCT.md"
    ],
    "/.github": [
      "ISSUE_TEMPLATE"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "sumo-mt",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "WikiData.py",
      "WikiParser.py",
      "WikiSettings.py",
      "mediawiki.py",
      "orig_input.txt",
      "runTests.sh",
      "samples",
      "settings_example",
      "test_wikiparser.py",
      "test_wikitranslation.py"
    ]
  },
  "makefile": null,
  "readme": "## Description\n\nThis script uses Google Cloud Translate API to translate mediwiki markup files.\n\n## Requirements\n\n### Python version\n\n```\n$ python --version\nPython 2.7.5\n```\n\n### Dependencies\n\n```\n$ pip install --upgrade google-cloud-translate\n$ pip install configparser\n```\n\n### Google Cloud Translate set-up\n[Set-up Google Cloud translate](https://cloud.google.com/translate/docs/quickstart-v3)\n- Create a project.\n- Enable Translation API.\n- Create a service account with Translate permissions.\n- Download the Google Cloud private key as ``credentials.json``, say, in this same folder.\n\n### Settings\n\nRename ``settings_example`` into ``settings``.\n\n## Usage\n\nUsage examples:\n\nYou don't need to specify a language we can just default to the language that is specified in your settings file.\n```\n$ ./mediawiki.py --input orig_input.txt\n```\n\nSpecify a specific single language.\n```\n$ ./mediawiki.py --input orig_input.txt --lang es\n$ ./mediawiki.py --input orig_input.txt --lang ru\n```\n\nSpecify multiple languages for a single file.\n```\n$ ./mediawiki.py --input orig_input.txt --lang 'es,ru'\n```\n\nOr you can specify a weird language and the error message shall tell you what are supported.\n```\n$ ./mediawiki.py --input orig_input.txt --lang unknown\n```\n\nFor multiple files in a single directory and we want to generate multiple output languages.\n```\n$ ./mediawiki.py --indir myinputdirectory/ --lang 'ru,es'\n```\n\nIf you want to send the translated files to a different directory you can specify an outdir (note that you don't need the slash on the end of the directory names).\n```\n$ ./mediawiki.py --indir my-input-directory/ --lang 'ru,es' --outdir my-output-directory/\n```\n\nNote that if the output directory does not already exist, then it is created.\n\nIf you want to have your settings file named something other than the default value of ``settings`` then you can provide this.\n```\n$ ./mediawiki.py --indir my-input-directory/ --lang 'ru,es' --outdir my-output-directory/ --settings custom-settings-filename.txt\n```\n\n\n## Other notes\n\n### Overview\nTranslate an input mediawiki file of Spanish and generate an output mediawiki file of English.\norig_input.txt -> script -> orgi_output.txt\n\n### Inputs\n- Input input-filename - Input mediawiki file.\n- Language of output file (default language is specified in ``settings`` file)\n- Language of output files can be a list of languages eg 'ru,es' would be for Russian and Spanish.\n- Input directory - so need to get a list of all files in that directory and then parse each one of them.\n- Output directory - place all the translated files into the output directory.\n- Input settings file - provides custom default languages and location of Google App API credentials JSON file.\n\n### Outputs\n- output file with the name of the file \"myfile-es.txt' if the input is \"myfile.txt\", for a target language of es.\n- status\n  - success (zero) or\n  - failure (non-zero)\n\n\n## Design\n\n### Control Flow\n\n1. Open and read input file.\n1. Parse input file into a data structure.\n1. Process each line one at a time.\n1. For each line replace special text sequences with a symbol as we may want to translate these separately.\n1. Send requests to Cloud Translation to perform the language conversion.\n1. Create and write to output file.\n\n#### Error conditions\n- Cannot find input file.\n- Empty input file.\n- Format of input file not valid according to mediawiki.\n- Unable to send requests to Cloud Translation.\n- Unable to create output file.\n\n#### Data structure(s)\n\n##### List of objects\n- Object - for each line in the input file.\n  - Original Line - Original line of text from the input file, in English, say.\n  - Translated Line - The final translated line of text into the requested output language.\n  - Line number - Line number of the input file.\n  - Sequence Line - After special sequences of interest within the original line have been replaced with a special squences so that we don't want to translate these.\n  - Sequences - List of the unique sequences in the current line, we may or may not want to translate individually.\n  - Empty Line - Boolean true or false so that we don't ask Google to translate an empty string.\n\n- Object - for each unique sequence for a given line.\n  - sequence - This is a special sequence that looks like 123-456, say.\n  - original - This is the original string before any translations.\n  - translate - Boolean true or false if we would like to translate the sting or leave it in the original language.\n\n\n### Detailed Design\n\n#### Control Flow\n1. Start with the parsing of the input arguments to verify them.\n1. Parse over the input file.\n1. Look at one line at a time.\n1. Look for specific patterns of interest in the input file and if they are special then remove them from the line and replace them with a unique tag.\n1. Then send the remaining line to Google Cloud Translate API.\n1. Each of the special unique tags replace them with the original content.\n1. OR some of the special unique tags we need to still translate them but just a bit of their content.\n1. write the line to the output file.\n\n#### Data Flow\n- Need to add more details here.\n\n### Test-cases\n\nRun the local script called runTest.sh.\n\n```\n$ ./runTests.sh\n```\n\nThis local script uses the package pytest in order to run a suite of unit tests.\n\n#### Run a test suite\n\nOr to run the test suite in verbose mode, for a suite of tests, you can say,\n\n```\n$ pytest -v test_wikiparser.py\n```\n\n### Run a single test-case\n\nOr to run just a single testcase called test_filepath_directive, from the test suite TestWikiParser, in verbose mode, you can say,\n\n```\n$ pytest -v test_wikiparser.py::TestWikiParser::test_filepath_directive\n```\n\n"
},
{
  "name": "CampaignsforMDM",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "issue_template"
    ]
  },
  "makefile": null,
  "readme": "# Campaigns for Mission Driven Mozillians\n\nThis is a repository that cordinates all the work related to Campaigns for Mission Driven Mozillians. Once a campaign is live it will show up on the [Activate website](http://activate.mozilla.community/). \n\nCampaigns are time-bound activities for [Mission Driven Mozillians](https://wiki.mozilla.org/Innovation/Projects/Mission-Driven_Mozillians_Strategy) that advance Mozilla's mission through one of the high-value contribution categories:  \n\n### Localization\nCampaigns that increase the number, success, or impact of localization.\n\n### SuMo\nCampaigns that increase the impact of volunteers work supporting Mozilla's users.\n\n### Documenting\nCampaigns that drive new approaches or bring new volume and impact to writing documentation for MDN etc.  \n\n### Testing \nCampaigns that bring energy or volume to testing websites or products i.e. [Firefox Quantum Sprint](https://firefoxsprint.mozilla.community/).\n\n### Evangelism (Product)\nCampaigns that raise awareness, create new skills, or increase contributions to Mozilla's product or technology. i.e. [Rain of Rust](https://blog.mozillaindia.org/1932).\n\n### Evangelism (Mission)\nCampaigns that drive awareness and impact of one of the 5 [Internet Health Issue Areas](https://www.mozilla.org/en-US/internet-health/): Privacy & Security, Openess, Decentralization, Digital Inclusion, Web Literacy. i.e. [Aadhaar Take Action Campaign](https://foundation.mozilla.org/campaigns/aadhaar/take-action/) or the [January Privacy Month](https://wiki.mozilla.org/India/task_force/Policy_and_Advocacy/January_Privacy_Month_Campaign) .\n\n\n## Campaigns Pipeline\nIf you're interested to find  find upcoming, ongoing, and past campaigns and even submit ideas for new campaigns, you can check the [pipeline project](https://github.com/mozilla/CampaignsforMDM/projects/1)\n"
},
{
  "name": "multipreffer",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ]
  },
  "makefile": null,
  "readme": "# Multipreffer\n\n- Allows defining a set of prefs per SHIELD branch, and sets them at install and cleans them up upon uninstall.\n- Pref values are set on the default branch. User values are preserved.\n\nThe prefs to be set should be defined in src/variations.json, following this scheme:\n\n```\n{\n  \"branch 1\": { // Name of branch\n    \"weight\": 1, // Weight determines relative chance of getting assigned to this branch\n    \"prefs\": { // Prefs and values to set upon install\n      \"pref1\": \"string1\",\n      \"pref2\": true,\n      \"pref3\": 99\n    }\n  },\n\n  [...]\n}\n```\n\n# [WIP] Process to develop a multipreffer-based study\n\nTODO: streamline, automate.\n\n1. Make a copy of the repo\n2. Update metadata in `src/manifest.json` and `package.json`\n3. Update `abort` and `branch_name` prefs\n4. Define branches and target prefs/values in `src/variations.json`\n4. `npm install`\n5. `npm run build`\n6. `npm run test`\n\nIf the tests pass, should be good to go! Build is in `dist/` - upload to the study bug for signing.\n"
},
{
  "name": "cryptomining-retention-study",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ]
  },
  "makefile": null,
  "readme": "# Cryptomining Protections Retention Study\n\nThis addon is a Shield study based on https://github.com/nhnt11/multipreffer.\n\nSee https://bugzilla.mozilla.org/show_bug.cgi?id=1533778 for details on the study.\n"
},
{
  "name": "task-defect-enhancement",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "index.html",
      "public"
    ]
  },
  "makefile": null,
  "readme": "# task-defect-feature\nIs it a task, a defect, or a feature?\n"
},
{
  "name": "repo-templates",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Github Repository Templates\nThis repository houses the default template required for Mozilla Github repositories.   \n\n## Getting Started\nThis repository is currently a work in progress, but all files present under 'templates' represent the currently recommended version for these files.\n\n## Updates\nPlease subscribe to this repo, to ensure your templates are up to date.\n\n## Contributing\n\nPleaser refer to CONTRIBUTING.md\n"
},
{
  "name": "submissionapi",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "safebrowsing.py",
      "safebrowsing.yml",
      "urllist.txt"
    ]
  },
  "makefile": null,
  "readme": "# submissionapi\nA Submission API client\n"
},
{
  "name": "cropd",
  "files": {
    "/": [
      ".gitignore",
      ".prettierrc",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "demo",
      "lerna.json",
      "package.json",
      "packages",
      "yarn.lock"
    ]
  },
  "makefile": null,
  "readme": "# cropd - simple image cropper ![npm bundle size](https://img.shields.io/bundlephobia/minzip/cropd.svg)\n\n## Installation\n\n```bash\nnpm install --save cropd\n```\n\nor\n\n```bash\nyarn add cropd\n```\n\n## Usage\n\n```javascript\nimport crop from 'cropd';\n\nconst cropper = crop(targetElement, {\n  src: 'someimageurl'\n});\n\n// When you want to extract the cropped region\ncropper.toDataURL();\n```\n\n## Development\n\n```bash\nyarn start\n```\n\n## TODO\n\n- wrappers for:\n  - Vue\n  - React\n  - Svelte\n- finish Lerna setup\n"
},
{
  "name": "mozreport",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "mozreport",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "# mozreport [![Build Status](https://travis-ci.org/mozilla/mozreport.svg?branch=master)](https://travis-ci.org/mozilla/mozreport) [![PyPI](https://img.shields.io/pypi/v/mozreport.svg)](https://pypi.org/project/mozreport/)\n\nmozreport is a CLI tool that intends to help streamline\nthe process of preparing an experiment report.\n\n## Installing mozreport\n\nHomebrew users can install mozreport with\n`brew install mozilla/mozreport/mozreport`.\n\nYou can also install mozreport using `pip3 install mozreport`.\n(You might consider using [pipsi](https://github.com/mitsuhiko/pipsi).)\n\nMozreport requires Python 3.6+.\n\n## Using mozreport\n\n```\n$ mozreport --help\n\nUsage: mozreport [OPTIONS] COMMAND [ARGS]...\n\n  Mozreport helps you write experiment reports.\n\n  The workflow looks like:\n\n  * `mozreport setup` the first time you use Mozreport\n\n  * `mozreport new` to declare a new experiment and generate an analysis\n  script\n\n  * `mozreport submit` to run an analysis script on Databricks\n\n  * `mozreport fetch` to download the result\n\n  * `mozreport report` to set up a report template\n\n  The local configuration directory is /Users/tsmith/Library/Application Support/mozreport.\n```\n\n## What's a template?\n\nA report template is any collection of code that operates on a file named `summary.sqlite3`\nin the current working directory,\nand renders a report.\nTo add a template,\nadd a folder to the `mozreport/templates` folder in this repository,\nor the `templates` folder inside your local configuration directory\n(see the bottom of `mozreport --help`).\n\nYou may wish to adopt the convention of including a script named `build.py`\nthat performs the necessary steps to render the report.\n\n## Hacking on mozreport\n\nTo run unit tests only:\n\n`tox -- -m \"not integration\"`\n\nTo run all tests, including integration tests that hit our live Databricks account:\n\n* Run `mozreport setup` once\n* `tox`\n"
},
{
  "name": "selenium",
  "files": {
    "/": [
      "CHANGES.md",
      "LICENSE",
      "NOTICE",
      "README.md",
      "chrome.js",
      "edge.js",
      "example",
      "firefox",
      "http",
      "ie.js",
      "index.js",
      "io",
      "lib",
      "net",
      "node_modules",
      "opera.js",
      "package-lock.json",
      "package.json",
      "phantomjs.js",
      "proxy.js",
      "remote",
      "safari.js",
      "test",
      "testing"
    ]
  },
  "makefile": null,
  "readme": "# selenium-webdriver\n\nSelenium is a browser automation library. Most often used for testing\nweb-applications, Selenium may be used for any task that requires automating\ninteraction with the browser.\n\n## Installation\n\nSelenium may be installed via npm with\n\n    npm install selenium-webdriver\n\nYou will need to download additional components to work with each of the major\nbrowsers. The drivers for Chrome, Firefox, PhantomJS, Opera, and\nMicrosoft's IE and Edge web browsers are all standalone executables that should\nbe placed on your system [PATH]. Apple's safaridriver is shipped with\nSafari 10 for OS X El Capitan and macOS Sierra. You will need to enable Remote\nAutomation in the Develop menu of Safari 10 before testing.\n\n\n| Browser           | Component                          |\n| ----------------- | ---------------------------------- |\n| Chrome            | [chromedriver(.exe)][chrome]       |\n| Internet Explorer | [IEDriverServer.exe][release]      |\n| Edge              | [MicrosoftWebDriver.msi][edge]     |\n| Firefox           | [geckodriver(.exe)][geckodriver]   |\n| PhantomJS         | [phantomjs(.exe)][phantomjs]       |\n| Opera             | [operadriver(.exe)][opera]         |\n| Safari            | [safaridriver]                     |\n\n## Usage\n\nThe sample below and others are included in the `example` directory. You may\nalso find the tests for selenium-webdriver informative.\n\n    const {Builder, By, Key, until} = require('selenium-webdriver');\n\n    let driver = new Builder()\n        .forBrowser('firefox')\n        .build();\n\n    driver.get('http://www.google.com/ncr');\n    driver.findElement(By.name('q')).sendKeys('webdriver', Key.RETURN);\n    driver.wait(until.titleIs('webdriver - Google Search'), 1000);\n    driver.quit();\n\n### Using the Builder API\n\nThe `Builder` class is your one-stop shop for configuring new WebDriver\ninstances. Rather than clutter your code with branches for the various browsers,\nthe builder lets you set all options in one flow. When you call\n`Builder#build()`, all options irrelevant to the selected browser are dropped:\n\n    var webdriver = require('selenium-webdriver'),\n        chrome = require('selenium-webdriver/chrome'),\n        firefox = require('selenium-webdriver/firefox');\n\n    var driver = new webdriver.Builder()\n        .forBrowser('firefox')\n        .setChromeOptions(/* ... */)\n        .setFirefoxOptions(/* ... */)\n        .build();\n\nWhy would you want to configure options irrelevant to the target browser? The\n`Builder`'s API defines your _default_ configuration. You can change the target\nbrowser at runtime through the `SELENIUM_BROWSER` environment variable. For\nexample, the `example/google_search.js` script is configured to run against\nFirefox. You can run the example against other browsers just by changing the\nruntime environment\n\n    # cd node_modules/selenium-webdriver\n    node example/google_search\n    SELENIUM_BROWSER=chrome node example/google_search\n    SELENIUM_BROWSER=safari node example/google_search\n\n### The Standalone Selenium Server\n\nThe standalone Selenium Server acts as a proxy between your script and the\nbrowser-specific drivers. The server may be used when running locally, but it's\nnot recommend as it introduces an extra hop for each request and will slow\nthings down. The server is required, however, to use a browser on a remote host\n(most browser drivers, like the IEDriverServer, do not accept remote\nconnections).\n\nTo use the Selenium Server, you will need to install the\n[JDK](http://www.oracle.com/technetwork/java/javase/downloads/index.html) and\ndownload the latest server from [Selenium][release]. Once downloaded, run the\nserver with\n\n    java -jar selenium-server-standalone-2.45.0.jar\n\nYou may configure your tests to run against a remote server through the Builder\nAPI:\n\n    var driver = new webdriver.Builder()\n        .forBrowser('firefox')\n        .usingServer('http://localhost:4444/wd/hub')\n        .build();\n\nOr change the Builder's configuration at runtime with the `SELENIUM_REMOTE_URL`\nenvironment variable:\n\n    SELENIUM_REMOTE_URL=\"http://localhost:4444/wd/hub\" node script.js\n\nYou can experiment with these options using the `example/google_search.js`\nscript provided with `selenium-webdriver`.\n\n## Documentation\n\nAPI documentation is available online from the [Selenium project][api].\nAdditional resources include\n\n- the #selenium channel on freenode IRC\n- the [selenium-users@googlegroups.com][users] list\n- [SeleniumHQ](http://www.seleniumhq.org/docs/) documentation\n\n## Contributing\n\nContributions are accepted either through [GitHub][gh] pull requests or patches\nvia the [Selenium issue tracker][issues]. You must sign our\n[Contributor License Agreement][cla] before your changes will be accepted.\n\n## Node Support Policy\n\nEach version of selenium-webdriver will support the latest _semver-minor_\nversion of the [LTS] and stable Node releases. All _semver-major_ &\n_semver-minor_ versions between the LTS and stable release will have \"best\neffort\" support. Following a Selenium release, any _semver-minor_ Node releases\nwill also have \"best effort\" support. Releases older than the latest LTS,\n_semver-major_ releases, and all unstable release branches (e.g. \"v.Next\")\nare considered strictly unsupported.\n\nFor example, suppose the current LTS and stable releases are v6.9.5 and v7.5.0,\nrespectively. Then a Selenium release would have the following support levels:\n\n| Version | Support       |\n| ------- | ------------- |\n| <= 6.8  | _unsupported_ |\n| 6.9     | supported     |\n| 7.0-4   | best effort   |\n| 7.5     | supported     |\n| >= 7.5  | best effort   |\n| v.Next  | _unsupported_ |\n\n### Support Level Definitions\n\n- _supported:_ A selenium-webdriver release will be API compatible with the\n    platform API, without the use of runtime flags.\n\n- _best effort:_ Bugs will be investigated as time permits. API compatibility is\n    only guaranteed where required by a _supported_ release. This effectively\n    means the adoption of new JS features, such as ES2015 modules, will depend\n    on what is supported in Node's LTS.\n\n- _unsupported:_ Bug submissions will be closed as will-not-fix and API\n    compatibility is not guaranteed.\n\n### Projected Support Schedule\n\nIf Node releases a new [LTS] each October and a new major version every 6\nmonths, the support window for selenium-webdriver will be roughly:\n\n| Date      | LTS  | Stable |\n| --------- | ---: | -----: |\n| (current) |  6.9 |    7.5 |\n| 2017-04   |  6.0 |    8.0 |\n| 2017-10   |  8.0 |    9.0 |\n| 2018-04   |  8.0 |   10.0 |\n| 2018-10   | 10.0 |   11.0 |\n\n## Issues\n\nPlease report any issues using the [Selenium issue tracker][issues]. When using\nthe issue tracker\n\n- __Do__ include a detailed description of the problem.\n- __Do__ include a link to a [gist](http://gist.github.com/) with any\n    interesting stack traces/logs (you may also attach these directly to the bug\n    report).\n- __Do__ include a [reduced test case][reduction]. Reporting \"unable to find\n    element on the page\" is _not_ a valid report - there's nothing for us to\n    look into. Expect your bug report to be closed if you do not provide enough\n    information for us to investigate.\n- __Do not__ use the issue tracker to submit basic help requests. All help\n    inquiries should be directed to the [user forum][users] or #selenium IRC\n    channel.\n- __Do not__ post empty \"I see this too\" or \"Any updates?\" comments. These\n    provide no additional information and clutter the log.\n- __Do not__ report regressions on closed bugs as they are not actively\n    monitored for updates (especially bugs that are >6 months old). Please open a\n    new issue and reference the original bug in your report.\n\n## License\n\nLicensed to the Software Freedom Conservancy (SFC) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The SFC licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n\n[LTS]: https://github.com/nodejs/LTS\n[PATH]: http://en.wikipedia.org/wiki/PATH_%28variable%29\n[api]: http://seleniumhq.github.io/selenium/docs/api/javascript/module/selenium-webdriver/\n[cla]: http://goo.gl/qC50R\n[chrome]: http://chromedriver.storage.googleapis.com/index.html\n[gh]: https://github.com/SeleniumHQ/selenium/\n[issues]: https://github.com/SeleniumHQ/selenium/issues\n[opera]: https://github.com/operasoftware/operachromiumdriver/releases\n[phantomjs]: http://phantomjs.org/\n[edge]: http://go.microsoft.com/fwlink/?LinkId=619687\n[geckodriver]: https://github.com/mozilla/geckodriver/releases/\n[reduction]: http://www.webkit.org/quality/reduction.html\n[release]: http://selenium-release.storage.googleapis.com/index.html\n[users]: https://groups.google.com/forum/#!forum/selenium-users\n[safaridriver]: https://developer.apple.com/library/prerelease/content/releasenotes/General/WhatsNewInSafari/Articles/Safari_10_0.html#//apple_ref/doc/uid/TP40014305-CH11-DontLinkElementID_28\n"
},
{
  "name": "global-sprint",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gemfile",
      "Gemfile.lock",
      "LICENSE",
      "README.md",
      "_articles",
      "_config.yml",
      "_data",
      "_includes",
      "_layouts",
      "contents.html",
      "css",
      "img",
      "index.md",
      "issue-template-email-es.md",
      "issue-template-email.md",
      "issue-template.md",
      "js",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Global Sprint 2018\n\n[![Join the chat at https://gitter.im/mozilla/global-sprint-2018](https://badges.gitter.im/mozilla/global-sprint-2017.svg)](https://gitter.im/mozilla/global-sprint-2017?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nMozilla\u2019s [Global Sprint](https://mozilla.github.io/global-sprint/) is a fun, fast-paced and two-day collaborative event to hack and build projects for a healthy Internet. A diverse network of scientists, educators, artists, engineers and others come together in person and online to innovate in the open.\n\nhttps://mozilla.github.io/global-sprint/\n\nThis repo holds content for the website.\n\n## Getting Started\n\nYou'll need to install [Jekyll](https://jekyllrb.com/), [Ruby](https://www.ruby-lang.org/en/) and [Bundler](http://bundler.io/) to run this site locally.\n\n1. `bundle install`\n2. `bundle exec jekyll serve`\n3. Open [http://localhost:4000/global-sprint](http://localhost:4000/global-sprint) in your favourite browser!\n\n## Contributing\n\nThanks for your interest in contributing to the #mozsprint website! There are many ways to contribute. To get started, take a look at [CONTRIBUTING.md](CONTRIBUTING.md).\n\n## Participation Guidelines\n\nThis project adheres to a [Mozilla's Particpation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/). By participating, you are expected to uphold this code. Please report unacceptable behavior to zannah [at] mozillafoundation.org.\n\n## #mozsprint\n\nJoin us at the [Mozilla Global Sprint](http://mozilla.github.io/global-sprint/) May 10-11, 2018! We'll be gathering in-person at sites around the world and online to collaborate and learn from each other. [Get your #mozsprint tickets now](http://mozilla.github.io/global-sprint/)!\n\n![Global Sprint](https://cloud.githubusercontent.com/assets/617994/24632585/b2b07dcc-1892-11e7-91cf-f9e473187cf7.png)\n"
},
{
  "name": "fingerprinting-cryptomining-retention-study",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ]
  },
  "makefile": null,
  "readme": "# Fingerprinting and Cryptomining Protections Retention Study\n\nThis addon is a Shield study based on https://github.com/nhnt11/multipreffer.\n\nSee https://bugzilla.mozilla.org/show_bug.cgi?id=1533778 for details on the study.\n"
},
{
  "name": "fingerprinting-retention-study",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ]
  },
  "makefile": null,
  "readme": "# Fingerprinting Protections Retention Study\n\nThis addon is a Shield study based on https://github.com/nhnt11/multipreffer.\n\nSee https://bugzilla.mozilla.org/show_bug.cgi?id=1533778 for details on the study.\n"
},
{
  "name": "webcompat-weekly-diagnosis",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# webcompat-weekly-diagnosis\nA place to document the diagnosis work done by the Mozilla @webcompat team\n"
},
{
  "name": "basket-example",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "basket-client.js",
      "basket-example.css",
      "index.html"
    ]
  },
  "makefile": null,
  "readme": "# Basket Example Code\n\nSample code for newsletter subscriptions via basket.mozilla.org. See it in action at https://mozilla.github.io/basket-example/.\n\n## Tips\n\n* Use \"success@example.com\" as the email to have the request return successfully but not actually record a subscription,\n  and \"failure@example.com\" to always return failure.\n* You may also send \"country\" and \"lang\" parameters to indicate the user's country and language respectively\n  (see the [forms on mozilla.org](https://www.mozilla.org/en-US/newsletter/) for examples)\n* You may also send a \"fmt\" parameter with the value \"H\" or \"T\" for \"HTML\" and \"Text\", which will indicate the user's\n  preferred email format. HTML is the default.\n"
},
{
  "name": "pipstrap",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.rst",
      "pipstrap.py"
    ]
  },
  "makefile": null,
  "readme": "========\nPipstrap\n========\n\nPipstrap is a small script that acts as a trust root for installing pip 8 or\ngreater.\n\nEmbed this in your project, and your VCS checkout is all you have to trust. In\na post-`peep <https://pypi.python.org/pypi/peep/>`_ era, this lets you claw\nyour way to a `hash-checking version of pip\n<https://pip.readthedocs.org/en/stable/reference/pip_install/#hash-checking-\nmode>`_, with which you can install the rest of your dependencies safely. All\nit assumes is Python 2.6 or better and *some* trustworthy version of pip\nalready installed. If anything goes wrong, it will exit with a non-zero status\ncode.\n\nInvoke it like this::\n\n    my-virtual-env/bin/python my-project/pipstrap.py\n\nAt that point, pinned, hash-checked versions of pip, setuptools, and wheel will\nbe installed. (If your existing version of pip was already new enough, pipstrap\nwill have exited happily without doing anything.)\n\n.. note::\n\n    ``pip`` is invoked as a module of the python executable that was used to\n    run pipstrap, so everything will be installed into that executable's\n    environment.\n\nHow do I trust this?\n====================\n\nAn astute question!\n\nPipstrap is short; read it. Validate its embedded hashes by downloading from\nvarious locations (to defeat local MITMs), checking the PGP signature on pip,\ncomparing with version-control checkouts of all three packages, and talking\nwith friends. Check it into your project, sign your commits, and verify\nsignatures on deploy.\n\nAlso, if you trust me and my key-management practices, you can check any of the\nrelease tags against my GPG signature.\n\nWhy?\n====\n\n* get-pip.py is yuckily large to embed, and it hits the network to fetch the\n  latest pip, setuptools, and wheel, leaving the question of their genuineness\n  up to HTTPS. (Its embedded pip is used only to download those new versions.)\n* Continuing to embed peep just to bootstrap pip 8 is unwieldy, with an extra\n  requirements file and a longer-than-necessary script. Plus, now that I've got\n  hash-checking into pip, I don't want to continue maintaining a script that\n  breaks whenever pip's private APIs change.\n\nFor how long?\n=============\n\nWhen your OS packages pip 8 or you otherwise get a copy of pip 8 you trust onto\nyour servers, you can dispense with pipstrap.\n\nWhat about firewalls?\n=====================\n\nTo use pipstrap from a machine that cannot access https://pypi.python.org/, set\nthe ``PIP_INDEX_URL`` environment variable to point to a PyPI mirror, either\npublic or internal. (See `the pip documentation on this option\n<https://pip.pypa.io/en/stable/reference/pip_wheel/#cmdoption-i>`_ for\ndetails.) Pipstrap needs a ``packages`` directory which is a copy of the one on\nPyPI proper, and it will look for it in 2 places:\n\n1. If ``PIP_INDEX_URL`` ends with \"/simple\" (as it often does in the real world\n   to satisfy pip), pipstrap will look at ``${PIP_INDEX_URL}/../packages``. In\n   other words, ``packages`` is expected to sit right next to ``simple``.\n2. Otherwise, it will look at ``${PIP_INDEX_URL}/packages``.\n\nTrust in the mirror is not necessary, as SHA-256 hash-checking suffices to\nprove authenticity. Also, pipstrap doesn't care whether ``PIP_INDEX_URL`` has a\ntrailing slash.\n\nVersion History\n===============\n\n2.0\n  * Execute pip as a module from the python executable used to call pipstrap,\n    instead of resolving pip from the ``PATH``.\n\n1.5.1\n  * Revert our 2-phase installation procedure, which was causing setuptools not\n    to be upgraded on Debian Wheezy. We now install a modern pip and setuptools\n    all in one pip invocation. (The problem the 2-phase install was meant to\n    solve has been fixed in recent versions of Ubuntu 16.04, and we don't know\n    of any other distros having the problem.)\n\n1.5\n  * Update to setuptools 29.0.1, the newest version that doesn't drop support\n    for any Python versions. This allows use of the ``python_requires`` keyword\n    arg.\n\n1.4\n  * Add support for PyPI mirrors.\n  * Do the installation in 2 phases: first pip; then argparse, wheel, and\n    setuptools. This dodges an obscure bug:\n    https://github.com/certbot/certbot/issues/4938. (bmw)\n\n1.3\n  * Update pip to 9.0.1 so we can support manylinux1 wheels.\n  * Restore Python 2.6 compatibility.\n\n1.2\n  * Don't do anything if the pip version is already new enough.\n  * Disable the pip cache to avoid ownership warnings, which don't apply since\n    we're passing in files and not using the cache.\n  * Fix a bytes/string mismatch under Python 3.\n\n1.1.1\n  * Under Python 2.6 don't pass the CalledProcessError exception the ``output``\n    kwarg, which hasn't been invented yet.\n\n1.1\n  * Support Python 2.6. I feel so dirty, but Let's Encrypt needs it.\n  * Update to pip 8.0.3, wheel 0.29, and setuptools 20.2.2.\n\n1.0.1\n  * Make flake8-compliant so you can embed it without having to make exceptions\n    for it.\n\n1.0\n  * Initial release. Before I signed the release, I verified all 3 embedded\n    hashes from various network locations and the pip one against dstufft's GPG\n    signature.\n"
},
{
  "name": "spiderflunky",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "MANIFEST.in",
      "README.rst",
      "setup.py",
      "spiderflunky"
    ]
  },
  "makefile": null,
  "readme": "============\nspiderflunky\n============\n\nSpiderflunky is a proof-of-concept JS static analysis tool based on Mozilla's\nSpiderMonkey runtime. It uses SpiderMonkey's ``Reflect.parse`` to do the\nparsing and then takes it from there with a flattened-out form of alias\nanalysis. Ultimately, it will develop into a JS plugin for DXR\n(http://dxr.allizom.org/).\n\n\nGetting Started\n===============\n\nBuild SpiderMonkey's ``js`` executable. Check out mozilla-central, and then\nbuild the interpreter::\n\n    hg clone https://hg.mozilla.org/mozilla-central/ mozilla-central\n    cd mozilla-central\n    cd js/src\n    autoconf213  # must be 2.13\n    ./configure\n    make\n    sudo cp dist/bin/js /usr/local/bin/\n\nAll you can really do so far is run the tests. Enjoy doing that! ::\n\n    python setup.py test\n"
},
{
  "name": "fxc2",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Makefile",
      "README.md",
      "dll",
      "fxc2.cpp"
    ]
  },
  "makefile": "x86 :\n\ti686-w64-mingw32-clang++ -static fxc2.cpp -ofxc2.exe\nx64 :\n\tx86_64-w64-mingw32-clang++ -static fxc2.cpp -ofxc2.exe\n",
  "readme": "# fxc2\nA wine-runnable version of Microsofts Shader Compiler fxc\n"
},
{
  "name": "build-tooltool",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "Tooltool client was moved to https://github.com/mozilla/release-services/tree/master/src/tooltool/client\n"
},
{
  "name": "federated-learning-v2-study-addon",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "LICENSE",
      "README.md",
      "dist",
      "docs",
      "karma.conf.js",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "schemas",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# Federated Learning v2 - Study Add-On\n\n[![CircleCI badge](https://img.shields.io/circleci/project/github/mozilla/federated-learning-v2-study-addon/master.svg?label=CircleCI)](https://circleci.com/gh/mozilla/federated-learning-v2-study-addon/)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/federated-learning-v2-study-addon/badge.svg)](https://coveralls.io/github/mozilla/federated-learning-v2-study-addon)\n\nFederated Learning is a subarea of machine learning where the training process is distributed among many users.\nInstead of sharing their data, users only have to provide weight updates to the server.\n\nThis is the second draft of a Firefox add-on study that implements the client-side part of a Federated Learning system.\nEvery time users perform searches in the awesome bar, the model's predictions are compared to the actual user behaviour and [frecency](https://developer.mozilla.org/en-US/docs/Mozilla/Tech/Places/Frecency_algorithm) weight updates are computed.\nThese updates are collected using Telemetry.\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Analyzing data\n\nTelemetry pings are loaded into S3 and re:dash. Sample query:\n\n* [All pings](https://sql.telemetry.mozilla.org/queries/61520/source)\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n## References\n\n### Version 2\n\n* [Experimenter](https://experimenter.services.mozilla.com/experiments/federated-learning-v2/)\n* [Bugzilla](https://bugzilla.mozilla.org/show_bug.cgi?id=1532217)\n\n### Version 1\n\n* [Blog post](https://florian.github.io/federated-learning/) explaining the concepts behind federated learning\n* [Bugzilla](https://bugzilla.mozilla.org/show_bug.cgi?id=1462102)\n* [Federated learning simulations](https://github.com/florian/federated-learning)\n"
},
{
  "name": "rust-size",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Cargo.lock",
      "Cargo.toml",
      "src"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "vautomator-standalone",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "Dockerfile",
      "LICENSE",
      "Makefile",
      "Readme.md",
      "docker-compose.yml",
      "lib",
      "requirements.txt",
      "run.py",
      "setup.cfg",
      "setup.py",
      "tests",
      "tox.ini",
      "vendor"
    ]
  },
  "makefile": "ROOT_DIR\t:= $(shell dirname $(realpath $(lastword $(MAKEFILE_LIST))))\nTARGET\t\t:= \n\nall:\n\t@echo 'Available make targets:'\n\t@grep '^[^#[:space:]^\\.PHONY.*].*:' Makefile\n\n.PHONY: build\nbuild: Dockerfile docker-compose.yml\n\tdocker-compose build vautomator\n\n.PHONY: force-build\nforce-build: Dockerfile docker-compose.yml\n\tdocker-compose build --no-cache vautomator\n\n.PHONY: scan\nscan:\n\tdocker run -v ${PWD}/results:/app/results -it vautomator:latest ./run.py $(TARGET)\n\n.PHONY: test\ntest:\n\tpython -m pytest tests/\n\n.PHONY: flake8\nflake8:\n\tflake8 lib/*py\n\tflake8 tests/*py\n\n.PHONY: test-tox\ntest-tox:\n\ttox\n\n.PHONY: clean\nclean:\n\trm -rf results\n\trm -rf .tox\n\trm -rf .eggs\n\trm -rf .pytest_cache\n\tfind . -name __pycache__ -type d -exec rm -rf {}\\;\n\trm -rf vautomator.egg-info\n",
  "readme": null
},
{
  "name": "CrashesVSHoursAndURI",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# CrashesVSHoursAndURI"
},
{
  "name": "firefox-code-quality",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "css",
      "deps",
      "index.html",
      "js",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Code Quality\nArchitectural measures of complexity for revisions in mozilla-central.\n\nThe repository includes the now fully-automated workflow for running the code-quality analyses on revisions in mozilla-central.  More documentation to follow.\n\n### Quick-start guide\n\nTo run the complete set of analyses on the latest revision in mozilla-central and update the interface, run the following script:\n\n```\nscripts/analyzeMozillaCentral.sh \n```\n\n![Code Quality](https://dl.dropboxusercontent.com/u/20109708/code-quality.png \"Code Quality\")\n\n### Modifying things\n* ``data/modules.txt``: contains the set of directories that constitute modules (the current ones may not be accurate)\n* ``data/filter.txt``: contains the set of files and directories that we omit from the analysis\n* ``getSource.py``: contains the path to the codebase that we'll be analyzing\n\n### How the script works\nThe script (``analyzeMozillaCentral.sh``) takes approximately 30 minutes to complete and runs twice a day. It performs the following tasks:\n\n1. Pulls the latest revision from mozilla-central (``getSource.py``)\n2. Performs static analysis on the codebase to get LOC, cyclomatic complexity and dependency data (``generateProjectMetrics.py`` and ``generateProjectMetricsFunctionLevel.py``)\n3. Generates a hash table from the dependency data (``extractFilesAndDeps.py``)\n4. Gets dependencies, propagation cost and highly-interconnected files data (`generateDepMetrics.py`)\n5. Writes the entire set of data to be graphed to ``metrics_out/full_metrics-all.csv`` (``addToFullMetrics.py``)\n\nThe script then goes through the above steps for each of the modules in ``data/modules.txt``. Once the script terminates, the respective directories under ``scripts`` will be populated, allowing you to both view the data in the dashboard at ``index.html`` and make use of the dependencies endpoint.\n\n### Analyzing older revisions\n\nYou can generate metrics for older revisions by running ``analyzeMozillaCentralHistorical.sh 2016-01-01`` where the first parameter is the date you're interested in. At present, the script pulls one of potentially several revisions for that date, runs the analysis on it, and writes the metrics out to files as you would expect.\n\n### Dependencies endpoint\n\nTo get the set of files that depend on some arbitrary file in the latest revision (fan-in) or the files that that file depends on (fan-out), you can call the following endpoint--the URL is temporary and will change once [this bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1224318) is resolved:\n\n``http://almossawi.com:3003/deps/filename=xpcom:glue:nsINIParser.cpp``\n\nThe response is a JSON object like this:\n\n```javascript\n{\n  \"file\": \"xpcom/glue/nsINIParser.cpp\",\n  \"fanIn\": [\n    \"toolkit/system/androidproxy/nsAndroidSystemProxySettings.cpp\",\n    \"toolkit/xre/EventTracer.cpp\",\n    \"toolkit/xre/nsAppRunner.h\",\n    \"toolkit/xre/nsXREDirProvider.h\",\n    \"webapprt/prefs.js\",\n    \"widget/BasicEvents.h\",\n    \"xpcom/ds/nsINIParserImpl.h\",\n    \"xulrunner/app/xulrunner.js\",\n    \"xulrunner/tools/redit/redit.cpp\"\n  ],\n  \"fanOut\": [\n    \"accessible/atk/AccessibleWrap.h\",\n    \"xpcom/base/nsErrorService.cpp\",\n    \"xpcom/glue/nsCategoryCache.cpp\",\n    \"xpcom/glue/nsDeque.cpp\",\n    \"xpcom/glue/nsISupportsImpl.cpp\"\n  ]\n}\n```\n\nIf a filename cannot be found, the resulting JSON object will look like this:\n\n```javascript\n{\n  error: 'File name missing or does not exist in the codebase, usage: https://metrics.mozilla.com/code-quality/dep/?filename=xpcom/glue/nsINIParser.cpp'\n}\n```\n\n### Function-level metrics endpoint\n\nTo get function-level metrics for a given file, you can call the following endpoint--the URL is temporary and will change once [this bug](https://bugzilla.mozilla.org/show_bug.cgi?id=1224318) is resolved:\n\n``http://almossawi.com:3003/functions/filename=accessible:jsat:EventManager.jsm``\n\nThe response is a JSON object like this:\n\n```javascript\n{\n  \"file\": \"accessible/jsat/EventManager.jsm\",\n  \"functions\": [\n    {\n      \"name\": \"handleAccEvent\",\n      \"loc\": \"84\",\n      \"loc_code\": \"76\",\n      \"mccabe\": \"15\"\n    },\n    {\n      \"name\": \"handleEvent\",\n      \"loc\": \"35\",\n      \"loc_code\": \"33\",\n      \"mccabe\": \"9\"\n    },\n    {\n      \"name\": \"start\",\n      \"loc\": \"22\",\n      \"loc_code\": \"18\",\n      \"mccabe\": \"3\"\n    },\n    {\n      \"name\": \"stop\",\n      \"loc\": \"17\",\n      \"loc_code\": \"16\",\n      \"mccabe\": \"3\"\n    },\n    {\n      \"name\": \"EventManager\",\n      \"loc\": \"12\",\n      \"loc_code\": \"12\",\n      \"mccabe\": \"1\"\n    }\n  ]\n}\n```\n\nIf a filename cannot be found, the resulting JSON object will look like this:\n\n```javascript\n{\n  error: 'File name missing or does not exist in the codebase, usage: http://almossawi.com:3003/functions/filename=accessible:jsat:EventManager.jsm'\n}\n```\n\nIt's currently not possible to get call-graphs and dependencies for functions, seeing as the units that Understand [generates dependencies for are classes and files](https://scitools.com/documents/manuals/python/understand.html#Ent-depends).\n\n### Requirements\n\n* [Scitools' Understand](http://scitools.com)\n* [Python 3](https://www.python.org/) \n* [NumPy](http://www.numpy.org) and [SciPy](http://www.scipy.org/)\n* [Node.js](https://nodejs.org/en/) for the dependencies endpoint\n \n\n### Demo\n[https://metrics.mozilla.com/code-quality](https://metrics.mozilla.com/code-quality)\n\n### Blog post\n[https://blog.mozilla.org/metrics/2015/12/01/measuring-code-quality](https://blog.mozilla.org/metrics/2015/12/01/measuring-code-quality)\n"
},
{
  "name": "webcompat-team-okrs",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# webcompat-team-okrs\nThese are quarterly team level OKR projects for the WebCompat Tools team\n"
},
{
  "name": "email-tabs",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "addon",
      "docs",
      "index.html",
      "package-lock.json",
      "package.json"
    ],
    "/docs": [
      "acceptance.md",
      "data-review-request.md",
      "faq.md",
      "metrics.md"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Email Tabs\n\nThis is an experimental extension for Firefox that composes a Gmail email with information from a bunch of tabs in it.\n\nNote that only Gmail is supported, because there's no general standard for composing HTML emails.\n\nThere is a user-focused [Frequently Asked Questions](./docs/faq.md) as well.\n\n## Installing\n\n[**Install add-on from AMO**](https://addons.mozilla.org/en-US/firefox/addon/email-tabs/)\n\nThat will install the latest version of the add-on built from the [production branch](https://github.com/mozilla/email-tabs/tree/production).\n\n### Using the add-on\n\nOnce you've installed the add-on you'll see an icon in your toolbar: ![icon](https://raw.githubusercontent.com/mozilla/email-tabs/master/addon/emailtabs.svg)\n\nIf you click on the icon you'll be able to select one or more of your open tabs. After you've selected tabs, an email composition tab will open up and we'll put in links to each of the pages, along with the page title, and a screenshot. If you have selected some text then that text selection will also be included.\n\n## Developing\n\nTo install and test out:\n\n```sh\ngit clone https://github.com/mozilla/email-tabs.git\ncd email-tabs\nnpm install\nnpm start\n```\n\nYou must login to gmail.com before sending an email.\n\nIf you are developing, note that the `.jsx` file will not trigger a reload on its own. To enable this reloading, in a separate terminal window run:\n\n```sh\nnpm run watch\n```\n\nIf you want to view the template in browser (easier for style tweaks):\n```sh\nnpm run preview-templates\n```\n\n### Code layout\n\nThe popup UI is in [addon/popup.jsx](./addon/popup.jsx).\n\nThe email templates are in [addon/emailTemplates.jsx](./addon/emailTemplates.jsx).\n\nThe content script [addon/capture-data.js](./addon/capture-data.js) is loaded into any tabs being sent, and captures the screenshot and some metadata.\n\nThe content script [addon/set-html-mail.js](./addon/set-html-email.js) is loaded into the Gmail compose window, and effectively pastes in the HTML.\n\nOverall things are managed with the [addon/background.js](./addon/background.js) script.\n\n### Contact & Contribution\n\nYou can email us at [team-email-tabs@mozilla.com](mailto:team-email-tabs@mozilla.com).\n\nIRC is the best way to communicate, via `#testpilot` on irc.mozilla.org (you can use [this link](https://kiwiirc.com/nextclient/irc.mozilla.org/testpilot) for chat access via the web if you do not otherwise use IRC). You might want to ping `ianbicking` or `JSON_voorhees`.\n\nWe label some of our bugs with [good first issue](https://github.com/mozilla/email-tabs/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22).\n"
},
{
  "name": "firefox-browser-architecture",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "README.md",
      "_config.yml",
      "_layouts",
      "assets",
      "experiments",
      "newsletter",
      "newsletters.xml",
      "text"
    ]
  },
  "makefile": null,
  "readme": "\n# Firefox Browser Architecture\n\n## Mission\n\nChange Mozilla. Investigate big technical challenges and produce engineering programs to address them.\n\n\n## Our Conclusions\n\nThis is a list of our findings that we're reasonably happy with so far.\n\n* [Documenting our output](text/0001-documenting-output.md) looks at how we\u2019re going to communicate with the rest of Mozilla.\n* [Extracting Necko](text/0002-extracting-necko.md) considers whether it's feasible or worthwhile to extract Necko \u2014 Gecko's C++ networking library \u2014 for use as a standalone component.\n* [Problems with XUL](text/0003-problems-with-xul.md) aims to list the different kinds of problems that exist with XUL.\n* [XBL and Web Components](text/0004-xbl-web-components.md) compares some old Mozilla technology (XBL) with modern Web Components.\n* [Problems with XBL](text/0005-problems-with-xbl.md) aims to list the different kinds of problems that exist with XBL.\n* [Architecture Reviews](text/0006-architecture-review-process.md) are healthy and we proposed a process for healthy reviews.\n* [XBL Design Review packet](text/0007-xbl-design-review-packet.md) is the packet that we prepared for the architectural design review for XBL removal.\n* [Roadmap Review: Sync and Storage](text/0008-sync-and-storage-review-packet.md) establishes that storage and syncing of user data is a pillar of the Firefox ecosystem, warranting holistic and long-term attention, and outlines where we\u2019d like to end up and some ideas for how to get there.\n* [JavaScript Type Safety Systems](text/0009-type-safety-systems.md) are some conclusions of an investigation into the use of JavaScript type safety systems.\n* [Firefox Data Stores Documentation](text/0010-firefox-data-stores.md) documents the existing data stores across all current Firefox platforms.\n* [Fluent in Prefs Design Review](text/0011-fluent-in-prefs-design-review.md) describes the lightweight design review for Fluent in Prefs.\n* [A brief analysis of JSON file-backed storage](text/0012-jsonfile.md) outlines some of the pros and cons of using a flat file (particularly via `JSONFile.jsm`) to store data in Firefox.\n* [Process Isolation in Firefox](text/0012-process-isolation-in-firefox.md) is a WIP evaluation of how far we can push process isolation to improve security and stability.\n* [IPC Security Models and Status](text/0013-ipc-security-models-and-status.md) is an audit of our current IPC status.\n* [XUL Overlay Removal Review Packet](text/0014-xul-overlay-removal-review-packet.md) is the packet that we prepared for the architectural design review for XUL Overlay removal.\n* [Design Review: Key-Value Storage](text/0015-rkv.md) proposes the introduction of a fast, cross-platform key-value store for Mozilla products.\n* [XULStore Using rkv \u2013 Proof of Concept](text/0016-xulstore-rkv-poc.md) describes a proof-of-concept implementation of XULStore that uses [rkv](https://github.com/mozilla/rkv).\n* [LMDB vs. LevelDB](text/0017-lmdb-vs-leveldb.md) compares the [Lightning Memory-mapped Database](https://symas.com/lmdb/) (LMDB) key-value storage engine to the [LevelDB](https://github.com/google/leveldb) key-value storage engine.\n\n## Posts\n\nWe typically send our newsletters to [firefox-dev](https://www.mozilla.org/en-US/about/forums/#firefox-dev).\n\n* [Browser Architecture Update](newsletter/_posts/2017-07-27-browser-architecture-update.md). See also [mailing-list-post](https://groups.google.com/forum/#!topic/firefox-dev/ueRILL2ppac).\n* [Browser Architecture Newsletter #2](newsletter/_posts/2017-08-24-browser-architecture-newsletter-2.md). See also [mailing-list-post](https://groups.google.com/forum/#!topic/firefox-dev/Rc2w2a9e8HQ).\n* [Browser Architecture Newsletter #3](newsletter/_posts/2017-09-22-browser-architecture-newsletter-3.md). See also [mailing-list-post](https://groups.google.com/forum/#!topic/firefox-dev/p9rTlfFUXlQ).\n* [Browser Architecture Newsletter #4](newsletter/_posts/2017-10-19-browser-architecture-newsletter-4.md). See also [mailing-list-post](https://groups.google.com/forum/#!topic/firefox-dev/CLFtj8qUSv8).\n* [Browser Architecture Newsletter #5](newsletter/_posts/2017-11-29-browser-architecture-newsletter-5.md). See also [mailing-list-post](https://groups.google.com/forum/#!topic/firefox-dev/XKp3EthdJ60).\n\n## Explorations and Experiments\n\nTo support our conclusions we occasionally perform explorations and experiments. The first exploration is designed to support the notion that we can create a sync and storage layer in Rust that we can deploy to Desktop, Android and iOS.\n\n* [Deploying a Rust library on iOS](experiments/2017-09-06-rust-on-ios.md). A short tutorial describing how to build and deploy a Rust library for use inside an iOS app.\n* [Deploying a Rust library on Android](experiments/2017-09-21-rust-on-android.md). A short tutorial describing how to build and deploy a Rust library for use inside an Android app.\n\n<!-- This is a copy of this text in CODE_OF_CONDUCT.md -->\n\n# Community Participation Guidelines\n\nThis repository is governed by Mozilla's code of conduct and etiquette guidelines. \nFor more details, please read the\n[Mozilla Community Participation Guidelines](https://www.mozilla.org/about/governance/policies/participation/). \n\n## How to Report\nFor more information on how to report violations of the Community Participation Guidelines, please read our '[How to Report](https://www.mozilla.org/about/governance/policies/participation/reporting/)' page.\n"
},
{
  "name": "etp-search-volume-study",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ]
  },
  "makefile": null,
  "readme": "# ETP Search Volume Study\n\nThis addon is a Shield study based on https://github.com/mozilla/cookie-restrictions-strict-list-study.\n\nSee https://bugzilla.mozilla.org/show_bug.cgi?id=1532678 for details on the study.\n"
},
{
  "name": "shield-studies-addon-template",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "docs",
      "karma.conf.js",
      "package-lock.json",
      "package.json",
      "run-firefox.js",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TEMPLATE.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# Shield Studies Add-On Template\n\n[![CircleCI badge](https://img.shields.io/circleci/project/github/mozilla/shield-studies-addon-template/master.svg?label=CircleCI)](https://circleci.com/gh/mozilla/shield-studies-addon-template/)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/shield-studies-addon-template/badge.svg)](https://coveralls.io/github/mozilla/shield-studies-addon-template)\n\nThis repository is intended as an example repository containing templates and good\npractices for creating an [Add-On Experiment](https://mana.mozilla.org/wiki/display/FIREFOX/Pref-Flip+and+Add-On+Experiments) for Firefox.\n\nSee [TEMPLATE.md](./docs/TEMPLATE.md) for more details on how to use the template.\nNote: Remove or adapt this text after you have cloned/merged the template to your own study add-on repo.\n\n## Seeing the add-on in action\n\nSee [TESTPLAN.md](./docs/TESTPLAN.md) for more details on how to get the add-on installed and tested.\n\n## Data Collected / Telemetry Pings\n\nSee [TELEMETRY.md](./docs/TELEMETRY.md) for more details on what pings are sent by this add-on.\n\n## Analyzing data\n\nTelemetry pings are loaded into S3 and re:dash. Sample query:\n\n* [All pings](https://sql.telemetry.mozilla.org/queries/{#your-id}/source#table)\n\n(OR, if Pioneer, use the below instead)\n\nTelemetry pings are loaded into the encrypted Pioneer pipeline.\n\n## Improving this add-on\n\nSee [DEV.md](./docs/DEV.md) for more details on how to work with this add-on as a developer.\n\n## References\n\n* [Experimenter](https://experimenter.services.mozilla.com/experiments/TODO/)\n* [Bugzilla](https://bugzilla.mozilla.org/show_bug.cgi?id=TODO)\n"
},
{
  "name": "protocol-sketch-plugin",
  "files": {
    "/": [
      ".appcast.xml",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "appcast.xml",
      "assets",
      "mozilla.sketchplugin",
      "package-lock.json",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Styles\n\nAdd Mozilla branded text styles, layer styles, and fills to any Sketch file.\n\n## Installation\n\n- Download the [latest release](../../releases) of the Mozilla plugin\n- Double-click the zip file \u201cmozilla.sketchplugin.zip\u201d\n- Open up \u201cmozilla.sketchplugin\u201d\n\n\n## Instructions\n\n"
},
{
  "name": "fxmonitor-remotesettings-updater",
  "files": {
    "/": [
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "package-lock.json",
      "package.json",
      "updatebreaches.js"
    ]
  },
  "makefile": null,
  "readme": "# fxmonitor-remotesettings-updater\nScript to add new breaches from HIBP to the fxmonitor-breaches collection in Remote Settings\n\n## Usage\nThe script *dry-runs* by default and simply dumps the new breaches to be added to stdout.\nTo push new breaches to Kinto, set the following environment variables:\n```\nPUSH_TO_KINTO=1 # Can be any non-empty value\nKINTO_USERNAME=<kinto account username>\nKINTO_PASSWORD=<kinto account password>\n```\nTo run:\n```\nnode updatebreaches.js\n```\nNote: VPN access is required to access Kinto server.\n\nThe script exits with `0` for success and `1` if there were any errors.\n"
},
{
  "name": "outofdate-notifications-system-addon",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "addon"
    ]
  },
  "makefile": null,
  "readme": "# outofdate-notifications-system-addon\nThis system addon notifies users that they are out of date via a doorhanger\nnotification in the browser.\n\n## Telemetry\nThis addon sends telemetry pings when it starts, when the doorhanger is shown,\nand when the button in the doorhanger linking to the download page is clicked.\nA maximum of three pings (one for each event) is sent per browser session.\n\nThe structure of these pings is a JSON object fitting the [common ping format](https://gecko.readthedocs.io/en/latest/toolkit/components/telemetry/telemetry/common-ping.html)\nwith the ping type set to \"outofdate-notifications-system-addon\" and payload\ndata in this form:\n\n    {\n      event: \"started\"|\"shown\"|\"clicked\"\n    }\n\nThe event can be one of the following:\n\n    \"started\": Indicates when the addon has started up.\n    \"shown\"  : Indicates when the doorhanger notification has been shown.\n    \"clicked\": Indicates when the button linking to the download page has been\n               clicked.\n\nThe optional environment data from the common ping format is not included, but\nthe optional client ID is included, so that these pings can be correlated with\nother telemetry.\n\nThese pings will not be sent if telemetry is disabled.\n\n## Build, Test & Release\nRefer to https://wiki.mozilla.org/Firefox/Go_Faster/Releasing_an_add-on_mechanics for the latest instructions.\n"
},
{
  "name": "nss-taskcluster",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Procfile",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "popup-urls",
  "files": {
    "/": [
      "LICENSE",
      "README.rst",
      "urls.csv"
    ]
  },
  "makefile": null,
  "readme": "==========================================\nMozilla's Crowdsourced In-Page Pop-Up URLs\n==========================================\n\nFor a period of about 11 months\u2014March 2018 to January 2019\u2014Mozilla asked\nvolunteers to report URLs that showed in-page pop-ups. Using a `custom add-on\n<https://addons.mozilla.org/en-US/firefox/addon/in-page-pop-up-reporter/>`_,\nFirefox users contributed over 12,000 of these, which we now make available to\nyou. Thanks to all who participated! Watch this space for applications of\nthis research.\n\nThanks\n======\n\n* The University of Toronto team who scoured the data to expunge any\n  accidentally submitted personally identifiable information: Patrick Wu,\n  Ashley Sheng, Kristen Malik, and Douglas McDermott.\n* Ehsan Akhgari, Erik Rose, and Mike Hoye, Mozilla staff mentors of the above\n"
},
{
  "name": "apps-design",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "README.md",
      "_config.yml",
      "_includes",
      "_layouts",
      "_posts",
      "images",
      "index.html",
      "main.css"
    ]
  },
  "makefile": null,
  "readme": "apps-design\n===========\n\nDesign Guidelines for Open Web Apps\n"
},
{
  "name": "galaxy",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENCE",
      "README.md",
      "directory",
      "gulpfile.js",
      "images",
      "package.json",
      "src"
    ]
  },
  "makefile": null,
  "readme": "![galaxy logo](images/logo.png?raw=true)\n\nTo infinity and beyond.\n\n> __Note:__ This project is not ready for prime time. Not an official Mozilla project. Pre-alpha everything. Anything and everything at your own risk.\n\n\n## Installation\n\nTo install the dependencies:\n\n    npm install\n\n\n## Development\n\nTo run the local web server:\n\n    npm run-script dev\n\n\n## Deployment\n\nTo build the directory for production:\n\n    npm run-script build_directory\n\nAlternatively, via gulp:\n\n    gulp build\n\n\n## Components\n\n### Directory\n\nThe games directory (v0) lives at `/directory/`.\n\n### Catalogue\n\nThe game centre lobby (v1) lives at `/catalogue/`.\n"
},
{
  "name": "nss-fuzzing-corpus",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "certDN",
      "dtls-client-no_fuzzer_mode",
      "dtls-client",
      "dtls-server-no_fuzzer_mode",
      "dtls-server",
      "mpi-add",
      "mpi-addmod",
      "mpi-div",
      "mpi-expmod",
      "mpi-invmod",
      "mpi-mod",
      "mpi-mulmod",
      "mpi-sqr",
      "mpi-sqrmod",
      "mpi-sub",
      "mpi-submod",
      "pkcs8",
      "quickder",
      "tls-client-no_fuzzer_mode",
      "tls-client",
      "tls-server-no_fuzzer_mode",
      "tls-server"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "webowonder-demos",
  "files": {
    "/": [
      ".htaccess",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "demos",
      "shared"
    ]
  },
  "makefile": null,
  "readme": "Please use this private github repo to share your demo work.\n\nCreate a directory under demos. In that directory put all your demo's files.\n\nMake all your references (images, CSS, JavaScript) relative so that this code can be deployed anywhere and your demo will work properly.\n\nRequired files\n* demo.html\n* screenshot.jpg (458x230)\n\nOtherwise, You can lay things out however you want under your directory.\n\nBe sure to git pull occasionally as we will all work together to get the demos ready for launch.\n\nThanks!"
},
{
  "name": "janus-eventhandler-sqlite",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "Cargo.toml",
      "LICENSE",
      "Makefile",
      "README.md",
      "rustfmt.toml",
      "src"
    ]
  },
  "makefile": "PREFIX = /opt/janus/lib/janus/events\nTARGET = target/release/libjanus_eventhandler_sqlite.so\n\ninstall:\n\tcargo build --release\n\tcargo test --release\n\tmkdir -p $(DESTDIR)$(PREFIX)\n\tcp $(TARGET) $(DESTDIR)$(PREFIX)\n\n.PHONY: install\n",
  "readme": "# janus-eventhandler-sqlite\n\n[![Build Status](https://travis-ci.org/mozilla/janus-eventhandler-sqlite.svg?branch=master)](https://travis-ci.org/mozilla/janus-eventhandler-sqlite)\n\nA simple [Janus][] [event handler][Janus event handler] to record events in a SQLite database on disk.\n\n## Configuration\n\nThis event handler will read janus.eventhandler.sqlite.cfg from the Janus config directory, if present. Like other Janus configuration, the config file should be in INI format. The following options are configurable in the `general` section of the config file:\n\n- `enabled = yes|no`: Whether this event handler does any work at all. Default `yes`.\n- `db_path = /path/to/sqlite/db`: The path to the SQLite DB in which events will be written. The database will be created and initialized if it's not already present. Defaults to `events.db`.\n- `events = 65535`: A 32-bit integer bitmask defining which events will be logged. See the [Janus source][Janus event definitions] for valid event types. Defaults to all events.\n\n## Dependencies\n\n```\n$ sudo apt install libjansson-dev libsqlite3-dev\n```\n\n## Building\n\n```\n$ cargo build [--release]\n```\n\n## Testing\n\n```\n$ cargo test\n```\n\n## Installing\n\nInstall the library output by the build process (e.g. ./target/release/libjanus_eventhandler_sqlite.so) into the Janus event handlers\ndirectory (e.g. /usr/lib/janus/events). By default, event handlers may not be enabled in your Janus install; check your janus.cfg to make sure `broadcast=yes` is set in the `events` section. (If you are doing this for the first time, you might also want to double-check to make sure that there aren't other event handlers installed that you don't need.) Restart Janus to activate.\n\n[Janus]: https://janus.conf.meetecho.com/\n[Janus event handler]: https://janus.conf.meetecho.com/docs/group__eventhandlerapi.html\n[Janus event definitions]: https://github.com/meetecho/janus-gateway/blob/master/events/eventhandler.h#L128\n"
},
{
  "name": "minijanus.js",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bundle.js",
      "minijanus.js",
      "package-lock.json",
      "package.json",
      "tests.js"
    ]
  },
  "makefile": null,
  "readme": "# minijanus.js\n\n[![npm](https://img.shields.io/npm/v/minijanus.svg)](https://www.npmjs.com/package/minijanus)\n[![Build Status](https://travis-ci.org/mozilla/minijanus.js.svg?branch=master)](https://travis-ci.org/mozilla/minijanus.js)\n\nA super-simplistic and -minimal wrapper for talking to the [Janus signalling API][api-docs]. Developed for use with\nJanus as a web game networking backend via [janus-plugin-sfu][], but fundamentally plugin-agnostic. Designed to\nprovide useful possible abstractions while still providing the maximum possible control over `RTCPeerConnection`\nconfiguration and precise plugin signalling flow.\n\nIf you want a batteries-included wrapper, you should use the one distributed by the Janus developers --\n[janus.js][]. This one is different in a few ways:\n\n1. It doesn't try to maintain compatibility with older browsers very hard; the use case is modern browsers only.\n2. It's very small and straightforward, so it may serve as a useful reference client for people who want to better\n   understand the signalling API.\n3. It gives you control over most of the configuration and usage of the `RTCPeerConnection` directly, whereas janus.js\n   wraps and manages the connection for you.\n\nIf you want a similar but moderately more featureful wrapper, check out [minnie-janus][].\n\n[api-docs]: https://janus.conf.meetecho.com/docs/rest.html\n[janus.js]: https://github.com/meetecho/janus-gateway/blob/master/html/janus.js\n[janus-plugin-sfu]: https://github.com/mquander/janus-plugin-sfu\n[minnie-janus]: https://github.com/michaelfranzl/minnie-janus\n\n## Example\n\nRequire `minijanus` in Node, or link to bundle.js in a browser. Then:\n\n```javascript\nvar ws = new WebSocket(\"ws://localhost:8188\", \"janus-protocol\");\nvar session = new JanusPluginSession(ws.send.bind(ws));\nvar handle = new JanusPluginHandle(session);\nvar conn = new RTCPeerConnection({});\n\nws.addEventListener(\"message\", ev => session.receive(JSON.parse(ev.data)));\nws.addEventListener(\"open\", _ => {\n  session.create()\n    .then(_ => handle.attach(\"janus.plugin.sfu\"))\n    .then(_ => {\n      conn.addEventListener(\"icecandidate\", ev => {\n        handle.sendTrickle(ev.candidate || null).catch(e => console.error(\"Error trickling ICE: \", e));\n      });\n      conn.addEventListener(\"negotiationneeded\", _ => {\n        var offer = conn.createOffer();\n        var local = offer.then(o => conn.setLocalDescription(o));\n        var remote = offer.then(j => handle.sendJsep(j)).then(r => conn.setRemoteDescription(r.jsep));\n        Promise.all([local, remote]).catch(e => console.error(\"Error negotiating offer: \", e));\n      });\n      var unreliableCh = conn.createDataChannel(\"unreliable\", { ordered: false, maxRetransmits: 0 });\n      var reliableCh = conn.createDataChannel(\"reliable\", { ordered: true });\n      navigator.mediaDevices.getUserMedia({ audio: true })\n        .then(m => m.getTracks().forEach(t => conn.addTrack(t, m)))\n        .catch(e => console.error(\"Error acquiring media: \", e));\n      return new Promise(resolve => handle.on(\"webrtcup\", resolve));\n    })\n    .then(_ => { console.info(\"Connected to Janus: \", conn); })\n    .catch(e => { console.error(\"Error connecting to Janus: \", e); });\n});\n```\n\n(Note that this example code first negotiates only the data channels, and then renegotiates afterward when the\nmicrophone permission is provided. Only recent versions of Janus support renegotiation. If you didn't want this, you\nwould instead wait to create the connection until the microphone permission was granted.)\n\n## Building\n\nTo generate bundle.js:\n\n```\n$ npm run build\n```\n\n## Testing\n\n```\n$ npm test\n```\n"
},
{
  "name": "parapara",
  "files": {
    "/": [
      ".gitattributes",
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "Gruntfile.js",
      "README",
      "docs",
      "editor",
      "package.json",
      "shared",
      "tests",
      "wall"
    ],
    "/docs": [
      "api.md",
      "creating-video-previews.txt",
      "server-setup.txt",
      "streak-setup.txt",
      "wall-api.txt"
    ]
  },
  "makefile": null,
  "readme": "Parapara Animation\n\nParapara Animation is a workshop to produce a collaborative animation by drawing\npictures on mobile devices such as smart phones or tablets. The objective is not\nonly to know how animation is produced but also to experience the open-ness of\ncreating & sharing on the web.\n\nCurrently the project consists of two parts:\n\n1. An (client-side) editor for creating individual animations to be added to the\n   master animation.\n2. A \"wall\" feature (server-side) which collects and displays the individual\n   animations in one master animation.\n\nLICENSE\n\nThis code is subject to the terms of the Mozilla Public License, v. 2.0\n(http://mozilla.org/MPL/2.0/)\n"
},
{
  "name": "qbrt",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.js",
      ".gitattributes",
      ".gitignore",
      ".taskcluster.yml",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "application.ini",
      "bin",
      "chrome.manifest",
      "chrome",
      "components",
      "defaults",
      "devtools.manifest",
      "examples",
      "launcher.bat",
      "launcher.sh",
      "modules",
      "npm-shrinkwrap.json",
      "package.json",
      "shell",
      "test"
    ]
  },
  "makefile": null,
  "readme": "![Activity Level](https://img.shields.io/badge/status-active-green.svg)\n![Stability](https://img.shields.io/badge/stability-unstable-red.svg)\n![Travis Status](https://travis-ci.org/mozilla/qbrt.svg?branch=master)\n[![Taskcluster Status](https://github.taskcluster.net/v1/repository/mozilla/qbrt/master/badge.svg)](https://github.taskcluster.net/v1/repository/mozilla/qbrt/master/latest)\n[![Greenkeeper Status](https://badges.greenkeeper.io/mozilla/qbrt.svg)](https://greenkeeper.io/)\n\nqbrt: CLI to a Gecko desktop app runtime\n===\n\nqbrt is a command-line interface to a Gecko desktop app runtime.\nIt's designed to simplify the process of building and testing desktop apps\nusing Gecko.\n\n# Usage\n\nInstall it via npm:\n\n```bash\nnpm install -g qbrt\n```\n\nInstalling it also installs a Gecko runtime (currently a nightly build\nof Firefox, but in the future it could be a stable build of Firefox\nor a custom Gecko runtime). Its simplest use is then to invoke the *run*\ncommand with a URL:\n\n```bash\nqbrt run https://eggtimer.org/\n```\n\nWhich will start a process and load the URL into a native window:\n\n<img width=\"695\" alt=\"screen shot 2017-03-15 at 01 16 28\" src=\"https://cloud.githubusercontent.com/assets/305455/23939844/c5f39068-091f-11e7-8335-d1ba27fed6f3.png\">\n\nURLs loaded in this way don't have privileged access to the system.\nThey're treated as web content, not application chrome.\n\nTo load a desktop app with system privileges, point qbrt at a local directory\ncontaining a package.json file and main script:\n\n```bash\nqbrt run path/to/my/app/\n```\n\nFor an example, clone qbrt's repo and try its example/ app, which will start\na process and load the app into a privileged context, giving it access\nto Gecko's APIs for opening windows and loading web content along with system\nintegration APIs for file manipulation, networking, process management, etc.:\n\n```bash\ngit clone https://github.com/mozilla/qbrt.git\nqbrt run qbrt/example/\n```\n\n(Another good example is\nthe [shell app](https://github.com/mozilla/qbrt/tree/master/shell)\nthat qbrt uses to load URLs.)\n\nTo package an app for distribution, invoke the *package* command,\nwhich creates a platform-specific package containing both your app's resources\nand the Gecko runtime:\n\n```bash\nqbrt package path/to/my/app/\n```\n\n# Caveats\n\nWhile qbrt itself is written in Node.js, it doesn't provide Node.js APIs\nto apps. Unprivileged URLs have access to Web APIs, and privileged apps\nalso have access to Gecko's APIs.\n\nqbrt doesn't yet support runtime version management (i.e. being able to specify\nwhich version of Gecko to use, and to switch between them). At the time\nyou install it, it downloads the latest nightly build of Firefox.\n(You can update that nightly build by reinstalling qbrt.)\n\nThe packaging support is primitive. qbrt creates a shell script (batch script\non Windows) to launch your app, and it packages your app using\na platform-specific format (ZIP on Windows, DMG on Mac, and tar/gzip on Linux).\nBut it doesn't set icons nor most other package meta-data, and it doesn't create\nauto-installers nor support signing the package.\n\nIn general, qbrt is immature and unstable! It's appropriate for testing,\nbut it isn't yet mature and stable enough for you to ship apps with it.\n\n# Contributing\n\nContributions of all kinds are welcome! As are all contributors. We only ask\nthat you treat other contributors with care and respect and observe Mozilla's\n[Community Participation Guidelines](https://www.mozilla.org/en-US/about/governance/policies/participation/).\n"
},
{
  "name": "admin_for_mozilla",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# admin_for_mozilla\nNeed something changed in github? See https://wiki.mozilla.org/Github\n"
},
{
  "name": "application-services-bug-mirror",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Application Services Bug Mirror Repo\n\nThis is a little experiment in mirroring app-services-related issues\nfrom other github repos/projects/orgs, so that we can have a consistent\nway of viewing them in out github-based work planning flow at:\n\n  https://waffle.io/mozilla/application-services\n\nRight now it's just a collection of issues that have been mirror by hand.\nIf we decide we like it, we could invest in some automation to mirror\nissues over automatically, similar to what's uesd in\n[fxa-bugzilla-mirror](https://github.com/mozilla/fxa-bugzilla-mirror/).\nWe could also decide that having a separate repo for this doesn't really\nmake sense, and just put the issues directly on the main\n[application-services](https://github.com/mozilla/application-services)\nrepo.\n"
},
{
  "name": "Mozilla-GitHub-Standards",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "check_CoC.py",
      "client.py",
      "poetry.lock",
      "pyproject.lock",
      "pyproject.toml"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Code of Conduct support\n\nThe scripts in this repository help repositories to be in compliance\nwith the [Mozilla GitHub Standards][mgs].\n\n## check_CoC.py\n\nThis script checks repositories for compliance with the [Code of\nConduct][coc](CoC) requirement. It does the following, from an account\nthat should be dedicated to this purpose.\n\nThe process is:\n- check a repository for a file of the correct name and contents,\n- open an issue if it is missing or incorrect\n- if the CoC is missing:\n    - fork the repository (using the hash of the full name as the fork\n      name - to avoid name space collisions)\n    - commit the CoC file to the fork\n    - create a pull request back to the source for the addition, linked\n      to the issue.\n\nThe script can be rerun -- it will not create additional issues or PRs.\n\n\n[mgs]: https://wiki.mozilla.org/GitHub/Repository_Requirements\n[coc]: https://wiki.mozilla.org/GitHub/Repository_Requirements#Code_Of_Conduct\n"
},
{
  "name": "pioneer-opt-in",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "analysis",
      "bin",
      "build-includes.txt",
      "manifest.json"
    ]
  },
  "makefile": null,
  "readme": "# Pioneer Opt-in Add-on\n\nThis add-on is used as a marker for enrollment in the Pioneer program.\n\n## Build Instructions\n\n1. Run `bin/make-xpi.sh`.\n\n## License\n\npioneer-opt-in is licensed under the MPLv2. See the `LICENSE` file for details.\n"
},
{
  "name": "pioneer-enrollment-study",
  "files": {
    "/": [
      ".circleci",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bin",
      "extension",
      "package.json",
      "yarn.lock"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# Pioneer Enrollment Study\nThe purpose of this extension is to test various prompts for enrolling users in [Pioneer][].\n\n[Pioneer]: https://medium.com/firefox-context-graph/make-firefox-better-with-pioneer-10c82d0f9301\n\n## Eligibility\n- Users are ineligible for the study if they already have the Pioneer add-on installed.\n\n## User Flow\n1. 5 minutes after installation, the user will be prompted to enroll in Pioneer.\n2. If the user does not enroll, they will be prompted again 2 days after the first prompt.\n3. If the user still does not enroll, the study will remove itself 1 day after the second prompt.\n\n### Notes\n- If a user enrolls in Pioneer, either via the prompt or unprompted, they will no longer be prompted and the study will remove itself 1 day after they enrolled.\n- The user may visit `about:pioneer` at any time and enroll in the program if the study is still installed. Once the user has enrolled, `about:pioneer` will not show enrollment buttons. If the study is removed, `about:pioneer` will cease to function.\n\n## Treatment Branches\n1. `popunder` - The user is not directly prompted; a tab with `about:pioneer` is opened in the current active window but is not focused.\n2. `notification` - The user is shown a Heartbeat-style notification bar with a button that opens `about:pioneer` in a new tab.\n3. `notificationAndPopunder` - A tab with `about:pioneer` is opened in the current active window but is not focused. In addition, a Heartbeat-style notification bar is shown that focuses the tab when clicked.\n4. `notificationOldStudyPage` - The user is shown a Heartbeat-style notification bar with a button that opens the AMO-hosted Pioneer enrollment page.\n\n## Testing Preferences\nThe following preferences can be set to customize the study behavior for testing purposes.\n\n<dl>\n  <dt><code>extensions.pioneer-enrollment-study.treatment</code></dt>\n  <dd>The treatment to use. Set this to a value from the Treatment Branches section to force the add-on to show you that treatment. You must set this preference before installing the study (default: random).</dd>\n\n  <dt><code>extensions.pioneer-enrollment-study.updateTimerInterval</code></dt>\n  <dd>The interval for checking if the user should be prompted in minutes. (default: <code>43200</code>, 12 hours).</dd>\n\n  <dt><code>extensions.pioneer-enrollment-study.firstPromptDelay</code></dt>\n  <dd>The delay between installation and the first prompt being shown in milliseconds (default: <code>300000</code>, 5 minutes).</dd>\n\n  <dt><code>extensions.pioneer-enrollment-study.secondPromptDelay</code></dt>\n  <dd>The delay between the first prompt being shown and the second prompt being shown in milliseconds (default: <code>169200000</code>, or 47 hours).</dd>\n\n  <dt><code>extensions.pioneer-enrollment-study.studyEndDelay</code></dt>\n  <dd>The delay between the second prompt being shown and the study end in milliseconds (default: <code>86396400</code>, or 23 hours).</dd>\n\n  <dt><code>extensions.pioneer-enrollment-study.studyEnrolledEndDelay</code></dt>\n  <dd>The delay between enrollment and the study end in milliseconds (default: <code>86396400</code>, or 23 hours).</dd>\n</dl>\n\nDue to timer variations, for testing purposes it's recommended to set `updateTimerInterval` to `1` and the rest of the delays to `180000`. The timers are not exact and may vary by a few seconds/minutes before triggering.\n\n## Telemetry\nAlong with standard Shield study pings from the [study utils][], pings are sent under the `shield-study-addon` type during the study when certain events happen. The pings are shaped like so:\n\n```json\n{\n  \"version\": 3,\n  \"study_name\": \"pioneer-enrollment-study\",\n  \"branch\": \"notificationOldStudyPage\",\n  \"addon_version\": \"1.0.1\",\n  \"shield_version\": \"4.0.0\",\n  \"type\": \"shield-study-addon\",\n  \"data\": {\n    \"attributes\": {\n      \"event\": \"enrolled\"\n    }\n  },\n  \"testing\": false\n}\n```\n\nThe `data.attributes.event` key contains an identifier for the type of event that was triggered:\n\n[study utils]: https://github.com/mozilla/shield-studies-addon-utils/\n\n### `prompted`\nSent when the user is prompted to enroll with a notification or popunder. Pings of this type contain an extra key, `promptType`, that describes which type of prompt triggered the event:\n\n<dl>\n  <dt><code>first-prompt</code></dt>\n  <dd>The initial prompt soon after installation.</dd>\n  <dt><code>second-prompt</code></dt>\n  <dd>The second prompt shown if the user did not enroll after the first prompt.</dd>\n</dl>\n\nExample:\n\n```json\n{\n  \"version\": 3,\n  \"study_name\": \"pioneer-enrollment-study\",\n  \"branch\": \"notificationOldStudyPage\",\n  \"addon_version\": \"1.0.1\",\n  \"shield_version\": \"4.0.0\",\n  \"type\": \"shield-study-addon\",\n  \"data\": {\n    \"attributes\": {\n      \"event\": \"prompted\",\n      \"promptType\": \"first-prompt\"\n    }\n  },\n  \"testing\": false\n}\n```\n\n### `enrolled` / `enrolled-via-study`\nSent when the user enrolls in Pioneer. The `enrolled-via-study` event is sent only when the user enrolls from the `about:pioneer` page included in the study. The `enrolled` event is sent whenever the enrollment add-on is installed, even if it occurs outside of `about:pioneer`.\n\nNote that this means that enrollments from `about:pioneer` result in __two events: both `enrolled` and `enrolled-via-study`__. Also note that __enrollments from the `notificationOldStudyPage` do not have an `enrolled-via-study` event__.\n\nExample:\n\n```json\n{\n  \"version\": 3,\n  \"study_name\": \"pioneer-enrollment-study\",\n  \"branch\": \"notificationOldStudyPage\",\n  \"addon_version\": \"1.0.1\",\n  \"shield_version\": \"4.0.0\",\n  \"type\": \"shield-study-addon\",\n  \"data\": {\n    \"attributes\": {\n      \"event\": \"enrolled-via-study\"\n    }\n  },\n  \"testing\": false\n}\n```\n\n### `engagedPrompt`\nSent when the user clicks the button on the Heartbeat-style prompts to either focus or open `about:pioneer`.\n\nExample:\n\n```json\n{\n  \"version\": 3,\n  \"study_name\": \"pioneer-enrollment-study\",\n  \"branch\": \"notificationOldStudyPage\",\n  \"addon_version\": \"1.0.1\",\n  \"shield_version\": \"4.0.0\",\n  \"type\": \"shield-study-addon\",\n  \"data\": {\n    \"attributes\": {\n      \"event\": \"engagedPrompt\"\n    }\n  },\n  \"testing\": false\n}\n```\n"
},
{
  "name": "pioneer-utils",
  "files": {
    "/": [
      ".circleci",
      ".eslintrc.js",
      ".gitignore",
      ".npmignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "analysis",
      "package-lock.json",
      "package.json",
      "src",
      "tsconfig.json",
      "tslint.json",
      "webpack.config.js"
    ],
    "/.circleci": [
      "config.yml"
    ]
  },
  "makefile": null,
  "readme": "# pioneer-utils\n\n:warning: This repository is deprecated.\nPlease use <https://github.com/mozilla/shield-studies-addon-utils> instead.\n\n<old-readme>\n\n## Old readme\n\nWrite add-ons for use with Pioneer more easily.\n\n### Get the utils\n\n* [source code](https://github.com/mozilla/pioneer-utils)\n* [npm package](https://www.npmjs.com/package/pioneer-utils)\n\n</old-readme>\n"
},
{
  "name": "pioneer-study-online-news-2",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      ".htmllintrc",
      ".stylelintrc",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "TESTPLAN.md",
      "bin",
      "extension",
      "package-lock.json",
      "package.json",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "# pioneer-study-online-news-2\nSecond generation online news study\n"
},
{
  "name": "pioneer-study-online-news",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "TESTPLAN.md",
      "addon.json",
      "bin",
      "extension",
      "package-lock.json",
      "package.json",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "# Pioneer Study: Online News\nA [Pioneer][] study looking at online news consumption.\n\n[Pioneer]: https://support.mozilla.org/en-US/kb/about-firefox-pioneer\n\n## Build\nTo build the study add-on:\n\n1. Run `make-xpi.sh`:\n\n   ```sh\n   $ ./bin/make-xpi.sh\n   ```\n\nThis will create a `pioneer-study-online-news.xpi` file in the repo root.\n\n## License\nPioneer Study: Online News is licensed under the Mozilla Public License Version\n2.0. See the `LICENSE` file for details.\n"
},
{
  "name": "pioneer-study-nothing-addon",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "addon.json",
      "bin",
      "package-lock.json",
      "package.json",
      "src",
      "templates"
    ]
  },
  "makefile": null,
  "readme": "# pioneer-study-nothing-addon\nA simple, nothing burger pioneer study addon\n"
},
{
  "name": "L10nBudgetExperiment",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# L10nBudgetExperiment\nRepository for L10n Budget Experiment\n"
},
{
  "name": "MDM-Portal",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# MDM-Portal-\nThis repo is where we'll do project management for the MDM portal project.\n\nProject Goal: Create a centralized site that allows staff and contributors to discover and access Mozillian individuals, communities and effectively connect to high-impact contribution opportunities. \n\nThis is part of the mission-driven mozillians project work for 2019 that you can read about on discourse [here](https://discourse.mozilla.org/t/mission-driven-mozillians-2019-update/35430).\n"
},
{
  "name": "community-development",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "README.md",
      "releseanotes.md"
    ]
  },
  "makefile": null,
  "readme": "# Community Development Team Planning Repo\n\nThis repository is used for tracking [issues](https://github.com/mozilla/community-development/issues) directly related to the work of the Community Development. For more information on how to use this Repo please read [this](https://discourse.mozilla-community.org/t/coordinating-participation-on-github/6638).\n\nIf you'd like to propose new work or get in touch with the team, head over to discourse here: https://discourse.mozilla-community.org/c/participation\n\nLooking for Reps Projects? Visit the Reps Repo https://github.com/mozilla/reps\n"
},
{
  "name": "fxa-jwtool",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "index.js",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "# fxa-jwtool\n\nA module for creating and verifying JWTs used by Firefox Accounts.\n\n## Example\n\n```js\nvar JWTool = require('fxa-jwtool')\n\nvar secretKey = JWTool.JWK.fromFile(\n  'priv.pem',\n  {\n    jku: 'https://api.accounts.firefox.com/.well-known/public-keys',\n    kid: 'dev-1'\n  }\n)\n\nvar encodedJWT = secretKey.sign({ sub: 'hello world' })\n\nvar trustedJKUs = [\n  'https://api.accounts.firefox.com/.well-known/public-keys'\n]\n\nvar jwtool = new JWTool(trustedJKUs)\n\nvar message = jwtool.verify(encodedJWT)\n\nconsole.log(message) // { sub: \"hello world\" }\n\n```\n"
},
{
  "name": "infosec-risk-management",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "Principles",
      "README.md",
      "RRA",
      "risk-register"
    ]
  },
  "makefile": null,
  "readme": "# Risk Management\n\nThe Risk Management product at Mozilla is set of lightweight processes and concise, high-quality and public documentation that we use to prioritize work and keep Mozilla safe as a whole. It spans across multiple teams (Legal, Firefox Security, Enterprise Information Security)  and was leveraged to run the 2017 Trust & Safety program (mandated by our CEO).\n\nTheir quote:\n  > The Risk Management services allows Mozilla to take smart risks - i.e. the ability to take the best choices we can\n\n\nSee also (*Staff Confidential access required*): https://docs.google.com/document/d/1-L5xd2LfztrEWPLCQq6ng_ENcXPs19odtndUDOI24Vg/edit#\n\n\nLooking for more? Our risk assessment documentation is located at https://infosec.mozilla.org/#risk-assessment\n\nYou can also find stand-alone API glue at these locations:\n- https://github.com/gdestuynder/rra2json\n- https://github.com/gdestuynder/rra3json\n"
},
{
  "name": "ping-centre",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.json",
      ".gitignore",
      ".npmignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "example.js",
      "package.json",
      "src",
      "test",
      "webpack.addon.config.js",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# ping-centre\n\n[![Build Status](https://travis-ci.org/mozilla/ping-centre.svg?branch=master)](https://travis-ci.org/mozilla/ping-centre)\n[![Coverage Status](https://coveralls.io/repos/github/mozilla/ping-centre/badge.svg?branch=master)](https://coveralls.io/github/mozilla/ping-centre?branch=master)\n\nA client for easily collecting events and metrics.\n\n# Install\n```sh\n$ npm install ping-centre\n```\n\n# Usage\n\n## Node\n\n1. `npm install ping-centre --save` to get the ping-centre dependencies\n2. Import as follows: `const PingCentre = require(\"ping-centre\");`\n\n## Firefox Addon\n\n1. Run npm run bundle to output bundles to `dist/`\n2. Place the `ping-centre.addon.min.js` bundle in the addon directory\n3. Import as follows: `const PingCentre = require(\"./ping-centre.addon.min\");`\n\n## Browser\n\n1. Run npm run bundle to output bundles to `dist/`\n2. Place the `ping-centre.min.js` bundle in your project directory\n3. Import as follows: `<script type=\"text/javascript\" src=\"ping-centre.min.js\" charset=\"utf-8\"></script>`\n4. `PingCentre` will be available as a global\n\nFrom there, the following code can be executed in any environment:\n\n```js\n// create a ping-centre object\nconst pc = new PingCentre(\"some_topic_foo\", \"some_cient_id_123\");\n\n// create the payload\nconst payload = {\n  \"event_type\": \"ping_session\"\n};\n\n// send the payload asynchronously\npc.sendPing(payload);\n\n// validate the payload asynchronously\npc.validate(payload);\n```\n\nWhen testing your app with Ping Centre, your data will be sent to a staging server by default.\nTo send your data to a production server, set the `NODE_ENV` environment variable to `production`.\n\n# Overview\n\nPing-centre consists of three main parts: the clients, the data pipeline, and the dashboard.\n\nThe clients are responsible for collecting the events and forwarding\nthem to [Onyx][Onyx Homepage] - the entrance of the data pipeline. Besides Onyx, the\ndata pipeline employes a [Disco][Disco Homepage] cluster to run the ETL jobs, which\nin turn persist the outcome to AWS Redshift. Through [re:dash dashboard][Re:dash Dashboard],\nthe user can access the data warehouse, slice and dice the datasets via SQL queries.\n\nBehind the scenes, a ping-centre client is simply a wrapper around the HTTP POST request.\nTherefore, it could be implemented in any programming language. And this repo implements\nit in Javascript.\n\n## Topics\n\nAs ease-of-use is the primary goal of the client, the client user does *not* need to\nspecify the telemetry destination, i.e. the endpoint of the Onyx. Instead, the user\njust specifies the topic of the payload. In fact, Onyx merely exposes a single endpoint and\nmultiplexes all the topics onto that endpoint. The ETL task runner [Infernyx][Infernyx Homepage]\nwill demultiplex the inputs and process each topic separately.\n\n## Schemas\n\nFor each topic, the user is going to provide a schema to describe the associated payload.\nAs the reference of table schema in Redshift, this schema could also be used by the\nETL jobs to conduct the data extraction, cleaning, and transforming.\n\n\nWe use [joi-browser][Joi-browser Homepage] to define the schemas for the Javascript client. By convention, all\nschemas are saved in the `schemas` directory with the same name of the topics. In each schema,\nthe user specifies following attributes in the schema for each topic:\n\n* Field name\n* Field modifiers\n  - type\n  - required or optional\n  - length if applicable\n  - enum values, e.g. ['click', 'search', 'delete']\n  - see [Joi][Joi Homepage] for more details\n* Other ETL requirements are attached as comments if applicable\n\nHere is an example:\n\n```js\nconst Joi = require(\"joi-browser\");\n\nconst schema = Joi.object().keys({\n    // a required string field with no more than 128 characters\n    client_id: Joi.string().max(128).required(),\n    // a required javascript timestamp with milliseconds\n    received_at: Joi.date().timestamp().required(),\n    // an required enum string field\n    event: Joi.any().valid(['add', 'delete', 'search']).required(),\n    // an optional positive integer field\n    value: Joi.number().integer().positive().optional(),\n}).options({allowUnknown: true});  // allow other non-specified fields\n\n/*\n * ETL processing\n *\n * 1. Truncate the milliseconds of the 'received_at', e.g. 147743323232 -> 147743323\n * 2. Rename the 'value' field to 'latency' in the database\n * 3. Capitalize the 'event' field\n */\n\nmodule.exports = schema;\n```\n\n[Onyx Homepage]: https://github.com/mozilla/onyx\n[Disco Homepage]: http://discoproject.org/\n[Re:dash Dashboard]: https://sql.telemetry.mozilla.org/\n[Infernyx Homepage]: https://github.com/tspurway/infernyx\n[Joi Homepage]: https://github.com/hapijs/joi\n[Joi-browser Homepage]: https://github.com/jeffbski/joi-browser\n"
},
{
  "name": "inventory",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      ".travis.yml",
      "CHANGES.txt",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "MozInvAuthorization",
      "README.md",
      "__init__.py",
      "adapters",
      "api",
      "api_v1",
      "api_v2",
      "api_v3",
      "apps",
      "base",
      "bulk_action",
      "core",
      "decommission",
      "decorators",
      "dhcp",
      "docs",
      "lib",
      "libs",
      "locale",
      "manage.py",
      "mcsv",
      "media",
      "middleware",
      "migrate_dns",
      "migrations",
      "misc",
      "mozdns",
      "oncall",
      "reports",
      "requirements",
      "reversion_compare",
      "scripts",
      "settings",
      "slurpee",
      "static",
      "systems",
      "templates",
      "truth",
      "urls.py",
      "user_systems",
      "vendor",
      "vendor-local",
      "wsgi"
    ],
    "/docs": [
      "Makefile",
      "_build",
      "_static",
      "_templates",
      "build-github.zsh",
      "conf.py",
      "index.rst",
      "rage.rst"
    ]
  },
  "makefile": null,
  "readme": "[![Build Status](https://travis-ci.org/mozilla/inventory.svg?branch=master)](https://travis-ci.org/mozilla/inventory)\n\nInventory keeps inventory of all the things.\n\nAlso checkout [Invtool][0], a command line interface to Inventory.\n\n[0]:https://github.com/uberj/inv-tool\n"
},
{
  "name": "CookieRestrictionsBreakageStudy",
  "files": {
    "/": [
      ".babelrc",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "docs",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ]
  },
  "makefile": null,
  "readme": "# Cookie Restrictions Breakage Shield Study\n\nThis repository is a [Shield Study](https://wiki.mozilla.org/Firefox/Shield/Shield_Studies) based on the [Shield Studies Add-on Template](https://github.com/mozilla/shield-studies-addon-template). \n\n### About This Add-on\n\nBy default, cookies and site data can be set by all websites.  We would like to change this default to instead block cookies and site data when they are accessed or set by third parties that are identified as trackers by Disconnect\u2019s Tracking Protection List.\n\nCookie Restrictions make it difficult for trackers to track users across different websites.  A third party resource identified as a tracker on a first party page will not get to read any cookies or site data from their site stored in Firefox.  They will not get to set any cookies or site data, and document.cookie will not return any data.  If the third party is opened with a window.open in a new tab that the user interacts with, the third party will get exempted from the restriction on the previous tab.  If the user opens Control Center and \u201cDisables protection for this site\u201d, the first party website will get whitelisted and all trackers within it will get full cookie and site data access when embedded by that whitelisted first party.\n\n## Development\n\nYou must run the study with [Firefox\nNightly](https://www.mozilla.org/en-US/firefox/channel/desktop/#nightly)\n\nSee [Getting\nStarted](https://github.com/mozilla/CookieRestrictionsShield/blob/master/docs/DEV.md#getting-started) for instructions to install, run, lint, and build the add-on.\n\nYou should be able to `npm start -- -f Nightly`\n\n### Tests\n\nWe currently have functional tests, you can find them under `test/functional/`.\nPlease test your new code and make sure to run the tests before committing.\n\nTo run the tests, use\n\n```shell\nnpm run build\nnpm run test\n```\n\nNote that you have to re-run `npm run build` when making changes to study code because the tests use a bundled version of the add-on.\n\n## Running Variations\n\nFirst, be sure you go through the [Development](#Development) steps and are able\nto `npm start -- -f Nightly`\n\nTo run a specific variation, you will pass the variation name to the `start`\ncommand with `--pref`.\n\n### Variations\n\nThere are a 3 variations to study features and heuristics:\n\n  * `Control`\n  * `ThirdPartyTrackingBasic`\n  * `ThirdPartyTrackingStrict`\n  * `AllThirdPartyCookies`\n\nYou can run a specific variation like so:\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions-breakage_shield_mozilla_org.test.variationName=ThirdPartyTrackingBasic\n```\n\n## User Scenarios\n\nIn all variations:\n <!-- TODO Describe behaviour here of popups and panels -->\n* The shield in the top left of the url bar should never show up.\n* When opening the control panel, the Content Blocking section should not exist\n* Telemetry Behaviour:\n\n\n### Control\nIn a Control [variation](#variations):\n\n  * There are no differences for Control branches from the behaviours described for all variations\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions-breakage_shield_mozilla_org.\ntest.variationName=Control\n```\n\n### Third Party Tracking Basic\nIn a Third Party Tracking Basic [variation](#variations):\n\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions-breakage_shield_mozilla_org.\ntest.variationName=ThirdPartyTrackingBasic\n```\n\n### Third Party Tracking Strict\nIn a Third Party Tracking Strict [variation](#variations):\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions-breakage_shield_mozilla_org.test.variationName=ThirdPartyTrackingStrict\n```\n### All Third Party Cookies\nIn a All Third Party Cookies [variation](#variations):\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions-breakage_shield_mozilla_org.test.variationName=AllThirdPartyCookies\n```\n\n### Testing Guide\n\nIn combination with the above instructions, add the pref `shieldStudy.logLevel=all` to the command to see extra logging. The logging will show the contents of the Telemetry ping, and the variation.\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions-breakage_shield_mozilla_org.test.variationName=ThirdPartyTrackingBasic --pref=shieldStudy.logLevel=all\n```\n\n### After Study Survey\n"
},
{
  "name": "FastBlockShield",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "docs",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# Fast Block Shield Study\n\nThis repository is a [Shield Study](https://wiki.mozilla.org/Firefox/Shield/Shield_Studies) based on the [Shield Studies Add-on Template](https://github.com/mozilla/shield-studies-addon-template). \n\n### About This Add-on\n\nThe sole focus of the Fastblock feature is to restrict the loading of trackers. It monitors trackers waiting for the first byte of data since the start of navigation of the current tab's top level document. If this is not received within a timeout, the request is canceled. If any bytes are received, the timeout is stopped. In some of the experimental branches, a few tracker requests are whitelisted, and do not have this monitoring. These include resources known to cause breakage, such essential audio/video, and commenting platforms.\n\n## Development\n\nYou must run the study with [Firefox\nNightly](https://www.mozilla.org/en-US/firefox/channel/desktop/#nightly)\n\nSee [Getting\nStarted](https://github.com/mozilla/FastBlockShield/blob/master/docs/DEV.md#getting-started) for instructions to install, run, lint, and build the add-on.\n\nYou should be able to `npm start -- -f Nightly`\n\n### Tests\n\nWe currently have functional tests, you can find them under `test/functional/`.\nPlease test your new code and make sure to run the tests before committing.\n\nTo run the tests, use\n\n```shell\nnpm run build\nnpm run test:func\n```\n\nNote that you have to re-run `npm run build` when making changes to study code because the tests use a bundled version of the add-on.\n\n## Running Variations\n\nFirst, be sure you go through the [Development](#Development) steps and are able\nto `npm start -- -f Nightly`\n\nTo run a specific variation, you will pass the variation name to the `start`\ncommand with `--pref`.\n\n### Variations\n\nThere are a number of variations to study features and heuristics:\n\n  * `Control`\n    * 4 control branches - denoted by `[0-4]`\n  * Tracking Protection - denoted by `TP`\n  * Fastblock - denoted by `FB`\n    * 2 timeouts - denoted by `[2|5]`\n    * 4 separate block-lists - denoted by `L[0-4]`\n\nAll the variations are listed in\n[`variations.js`](https://github.com/mozilla/FastBlockShield/blob/master/src/variations.js).\nYou can run any of them like so:\n\n```\nnpm start -- --pref=extensions.fastblock_shield_mozilla_org.test.variationName=FB2L0\n```\n\n## User Scenarios\n\nIn all variations:\n\n  * Nothing different should happen in Private Browsing or Safe Mode operation.\n  * Nothing different should happen on a page without trackers.\n  * No telemetry will be sent on a page without trackers.\n  * Panel Behaviour:\n    * If the user refreshes a page that has trackers on it, they have a chance of being shown\n      a panel notification: \"Did you reload this page to resolve loading issues?\". This chance is 100% by the 6th refresh.\n    * If the panel is ignored it will not show up again on the next refreshes. Once the user\n      navigates, on the next refresh there is once again a chance the panel will show up. And the\n      chance that it might show up on the same etld+1 is once again possible.\n    * If \"yes\" or \"no\" is clicked on the panel, it will never show up again for that etld+1.\n    * The panel should not dismiss until interacted with, or until the user navigates or refreshes\n      the page\n  * Telemetry Behaviour:\n    * Telemetry will be sent upon page unload.\n\n### Control\nIn a Control [variation](#variations):\n\n  * There are no differences for Control branches from the behaviours described for all variations\n\n```\nnpm start -- --pref=extensions.fastblock_shield_mozilla_org.test.variationName=ControlL0\n```\n\n### Tracking Protection\n\n```\nnpm start -- --pref=extensions.fastblock_shield_mozilla_org.test.variationName=TP\n```\n\nIn a Tracking Protection [variation](#variations):\n\n  * The user should see the \"How Tracking Protection works\" onboarding experience\n    when they first visit a site with trackers detected.\n  * The \"Content Blocking\" panel should show \"Trackers: Blocked\",\n    \"Slow-loading Trackers: Add blocking...\", and \"Disable Blocking for This\n    Site\"\n\n### Fastblock\n\n```\nnpm start -- --pref=extensions.fastblock_shield_mozilla_org.test.variationName=FB2L0\n```\n\nIn a Fastblock [variation](#variations):\n\n  * The user will not receive any Fastblock onboarding\n  * The \"Content Blocking\" panel should show \"Slow-loading Trackers: Blocked\",\n    \"Trackers: Add blocking...\", and \"Disable Blocking for This Site\"\n\n### Testing Guide\n\nIn combination with the above instructions, add the pref `shieldStudy.logLevel=all` to the command to see extra logging. The logging will show the contents of the Telemetry ping, and the variation.\n\n```\nnpm start -- --pref=extensions.fastblock_shield_mozilla_org.test.variationName=TP --pref=shieldStudy.logLevel=all\n```\n\n### Websites to test\n\nYou can find a good a assortment of test sites with trackers on the [Tracking Protection Wiki Page](https://wiki.mozilla.org/Security/Tracking_protection#QA). These pages were designed to simply and reliably load one or two tracking resources for testing.\n\nHere is a [test page](https://mozilla.github.io/FastBlockShield/) that causes various Javascript Errors when buttons are clicked. The page also contains a GA tracker, resulting in a telemetry ping. The errors should be reported in the telemetry ping.\n\nOf course there is a large variety of sites on the internet that employ trackers and cause errors. This study should generally work the same for all of them, though there may be specific exceptions. In general please be aware of the sensitivity of FastBlock to network speed and that sites can also intermittently differ in how they load trackers or throw errors.\n"
},
{
  "name": "CookieRestrictionsShield",
  "files": {
    "/": [
      ".babelrc",
      ".circleci",
      ".eslintignore",
      ".eslintrc.js",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "data-review.txt",
      "dist",
      "docs",
      "package-lock.json",
      "package.json",
      "src",
      "test",
      "web-ext-config.js"
    ],
    "/docs": [
      "DEV.md",
      "TELEMETRY.md",
      "TESTPLAN.md",
      "WINDOWS_SETUP.md"
    ],
    "/.circleci": [
      "config.yml",
      "reports"
    ]
  },
  "makefile": null,
  "readme": "# Cookie Restrictions Shield Study\n\nThis repository is a [Shield Study](https://wiki.mozilla.org/Firefox/Shield/Shield_Studies) based on the [Shield Studies Add-on Template](https://github.com/mozilla/shield-studies-addon-template). \n\n### About This Add-on\n\nBy default, cookies and site data can be set by all websites.  We would like to change this default to instead block cookies and site data when they are accessed or set by third parties that are identified as trackers by Disconnect\u2019s Tracking Protection List.\n\nCookie Restrictions make it difficult for trackers to track users across different websites.  A third party resource identified as a tracker on a first party page will not get to read any cookies or site data from their site stored in Firefox.  They will not get to set any cookies or site data, and document.cookie will not return any data.  If the third party is opened with a window.open in a new tab that the user interacts with, the third party will get exempted from the restriction on the previous tab.  If the user opens Control Center and \u201cDisables protection for this site\u201d, the first party website will get whitelisted and all trackers within it will get full cookie and site data access when embedded by that whitelisted first party.\n\n## Development\n\nYou must run the study with [Firefox\nNightly](https://www.mozilla.org/en-US/firefox/channel/desktop/#nightly)\n\nSee [Getting\nStarted](https://github.com/mozilla/CookieRestrictionsShield/blob/master/docs/DEV.md#getting-started) for instructions to install, run, lint, and build the add-on.\n\nYou should be able to `npm start -- -f Nightly`\n\n### Tests\n\nWe currently have functional tests, you can find them under `test/functional/`.\nPlease test your new code and make sure to run the tests before committing.\n\nTo run the tests, use\n\n```shell\nnpm run build\nnpm run test:func\n```\n\nNote that you have to re-run `npm run build` when making changes to study code because the tests use a bundled version of the add-on.\n\n## Running Variations\n\nFirst, be sure you go through the [Development](#Development) steps and are able\nto `npm start -- -f Nightly`\n\nTo run a specific variation, you will pass the variation name to the `start`\ncommand with `--pref`.\n\n### Variations\n\nThere are a 2 variations to study features and heuristics:\n\n  * `Control`\n  * `CookiesBlocked`\n\nYou can run a specific variation like so:\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions_shield_mozilla_org.test.variationName=CookiesBlocked\n```\n\n## User Scenarios\n\nIn both variations:\n\n* Nothing different should happen in Private Browsing or Safe Mode operation.\n* Panel Behaviour:\n  * If the user refreshes a page they have a chance of being shown\n    a panel notification: \"Did you reload this page to resolve loading issues?\". This chance is 100% by the 6th refresh.\n  * If the panel is ignored it will not show up again on the next refreshes. Once the user\n    navigates, on the next refresh there is once again a chance the panel will show up. And the\n    chance that it might show up on the same etld+1 is once again possible.\n  * If \"yes\" or \"no\" is clicked on the panel, it will never show up again for that etld+1.\n  * The panel should not dismiss until interacted with, or until the user navigates or refreshes\n    the page\n  * If \"yes\" is clicked we will add an exception for this page for content blocking\n* Telemetry Behaviour:\n  * Telemetry will be sent upon page unload.\n\n\n### Control\nIn a Control [variation](#variations):\n\n  * There are no differences for Control branches from the behaviours described for all variations\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions_shield_mozilla_org.\ntest.variationName=Control\n```\n\n### Cookie Restrictions\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions_shield_mozilla_org.\ntest.variationName=CookiesBlocked\n```\n\nIn a Cookies Blocked [variation](#variations):\n\n* The user should see the \"How Tracking Protection works\" onboarding experience\n  when they first visit a site with trackers detected.\n* The \"Content Blocking\" panel should show \"Trackers: Blocked\",\n  \"Slow-loading Trackers: Add blocking...\", and \"Disable Blocking for This\n  Site\"\n\n### All Third Party Cookies Blocked \n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions_shield_mozilla_org.test.variationName=AllThirdPartyCookiesBlocked\n```\n\nIn a AllThirdPartyCookiesBlocked [variation](#variations):\n\n  * Behaviour should be the same as CookiesBlocked, but with stricter cookie blocking.\n\n### Testing Guide\n\nIn combination with the above instructions, add the pref `shieldStudy.logLevel=all` to the command to see extra logging. The logging will show the contents of the Telemetry ping, and the variation.\n\n```shell\nnpm start -- -f Nightly --pref=extensions.cookie-restrictions_shield_mozilla_org.test.variationName=CookiesBlocked --pref=shieldStudy.logLevel=all\n```\n\n### After Study Survey\n\nThere is a breadcrumb pref at `extensions.cookie-restrictions.wasEnabled`\na string containing the variation name is the value of the pref. Heartbeat will be used to target and survey users."
},
{
  "name": "amo-loadtest",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "README.md",
      "add-ons",
      "data",
      "docker-compose-master.yml",
      "docker-compose-worker.yml",
      "docker-compose.yml",
      "docker-configs",
      "locustfile.py",
      "requirements.txt",
      "scripts"
    ]
  },
  "makefile": null,
  "readme": "This is an attempt to run some load tests on\n[addons.mozilla.org](https://addons.mozilla.org/) ([Olympia](https://github.com/mozilla/olympia)).\nThe short term goal is to use this to fix write performance issues.\nWe'll see what happens after that.\n\nThe tests run using [Locust](http://locust.io/) but everything is managed with\n[docker-compose](https://docs.docker.com/compose/).\n\n\n## Run load tests\n\nGenerate some users for a load testing session.\nRun this from the [Olympia](https://github.com/mozilla/olympia) repo\non the site that you want to load test. For example, to test things out locally,\ngenerate this from your local Olympia repo::\n\n    ./manage.py gen_loadtest_users\n\nMove the `loadtest-users.txt` file to the `data` directory of the load test\nsource code repository.\n\n## Run a local test\n\nRun master and workers against your local Olympia docker container (by IP\naddress, which may be different for you).\n\n    SITE_UNDER_TEST=http://$(docker-machine ip default) \\\n        MASTER_HOST=$(docker-machine ip default) \\\n        docker-compose up -d\n\nOpen the Locust dashboard at `open http://$(docker-machine ip default):8089/`.\n\n## Run load tests from AWS\n\nGenerate a `loadtest-users.txt` file on the server for the website under test\nsimilar to how it's documented above.\n\nLaunch a new Ubuntu EC2 instance and provision it like this:\n\n    sudo apt-get update\n    sudo apt-get install -y git-core\n    git clone https://github.com/mozilla/amo-loadtest.git\n    cd amo-loadtest\n    sudo ./scripts/provision-ec2.sh\n\nFinally, push your `loadtest-users.txt` file up to the\n`amo-loadtest/data` directory.\n\n### Start a master\n\nBoot up a master EC2 instance, provision it as described above,\nand run this to start a master container:\n\n    cd src/amo-loadtest\n    sudo SITE_UNDER_TEST=https://addons.allizom.org \\\n        docker-compose -f docker-compose-master.yml up -d\n\nBe sure this EC2 instance can accept inbound TCP connections from 8089 (the\ndashboard) and 5557-5558 (worker connections).\n\nFind the IP of your master and check the dashboard. It will be\nat something like http://ec2-N-N-N-N.us-west-2.compute.amazonaws.com:8089/\n\n### Start a worker\n\nYou can start as many workers as you want. For each EC2 instance you start, you\nneed to begin by provisioning it as described above.\nRun this command to start a worker container:\n\n    cd src/amo-loadtest\n    sudo SITE_UNDER_TEST=https://addons.allizom.org \\\n        MASTER_HOST=ec2-N-N-N-N.us-west-2.compute.amazonaws.com \\\n        docker-compose -f docker-compose-worker.yml up -d\n\nYou'll notice that the `$MASTER_HOST` var is set to the publicly accessible DNS\nhost that your master is running on.\n\n### Updating EC2 Workers\n\nDue to how the containers are set up and how EC2 IPs change, you need to rebuild\n*and* update the containers after restarting an EC2 instance. It would look\nsomething like this on a worker instance:\n\n    sudo SITE_UNDER_TEST=https://addons.allizom.org \\\n        MASTER_HOST=ec2-N-N-N-N.us-west-2.compute.amazonaws.com \\\n        docker-compose -f docker-compose-worker.yml build\n    sudo SITE_UNDER_TEST=https://addons.allizom.org \\\n        MASTER_HOST=ec2-N-N-N-N.us-west-2.compute.amazonaws.com \\\n        docker-compose -f docker-compose-worker.yml up -d\n\n## Results!\n\n- AMO tests on stage\n  - [2016-01-11](https://docs.google.com/spreadsheets/d/17y8MqnLgf5LG6wlQ6SVEljcQl3FuNaDt3Wr8zo4ERP8/edit#gid=331334299)\n  - [2016-01-12](https://docs.google.com/spreadsheets/d/1l-8AXxhjEV1QT9Kl1raB6B76u3MU5ira927OY7mcy5Y/edit#gid=2013401068)\n  - [2016-01-13](https://docs.google.com/spreadsheets/d/1sSjGnjJMNxgOTBROyDrRd93MFgMxx0hNk9dMZXsp_is/edit#gid=0)\n  - [2016-01-15](https://docs.google.com/spreadsheets/d/189JT9yxABnWjwI83xUcU5eYVjoV2lkamTFB2eu2uG-A/edit#gid=2090299171)\n"
},
{
  "name": "vaani.iot",
  "files": {
    "/": [
      ".gitmodules",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "images",
      "openhab",
      "openhab-core",
      "openhab-distro",
      "openhab2-addons",
      "smarthome"
    ]
  },
  "makefile": null,
  "readme": "# Project Vaani (No Longer Under Active Development)\n  \n**Vaani.IoT** is at an early stage of development. Thus, despite our deep desire for external contributions, we are not quite ready to receive them yet.\n\nHowever, when that moment arrives, and we are ready for external contributions, we will announce it far and wide, adding blog posts and instructions on getting started with **Vaani.IoT**.\n\n<p align=\"center\">\n  \n  <img src=\"https://raw.githubusercontent.com/mozilla/vaani.iot/master/images/Vaani.IoT.jpg\" alt=\"Vaani.IoT\"/>\n</p>\n"
},
{
  "name": "vaani.raspberrypi",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "bootstrap.sh",
      "clean.sh",
      "client",
      "cross-build.sh",
      "cross-chroot-script.sh",
      "cross-chroot.sh",
      "cross-compile-prepare.sh",
      "cross-compile.sh",
      "cross-finalize-image.sh",
      "cross-prepare-image.sh",
      "install-prereqs.sh",
      "setup.sh",
      "setup",
      "updater"
    ]
  },
  "makefile": null,
  "readme": "# Vaani for the Raspberry Pi\nInstalls and maintains a Vaani Appliance for the Raspberry Pi\n\n## Prerequisites\n\n- Raspberry Pi 3(!) board (plus power support)\n- Recent Raspbian image\n- Micro SD card of at least 4GB size\n- Ethernet patch cable (plus USB or Thunderbolt adapter if needed)\n\n## Creating the image\n\n### Preparing the SD card\n\nInsert the SD card into your card reader.\nFor the sake of simplicity you should choose a\nrather small one, if you want to redistribute the final image.\nShrinking the SD card image after this installation process is\ncomplicated and tedious.\n\nYou can download the Raspbian image from [here][].\nUncompress the archive with your preferred unarchiver\n(on Mac this should work automatically).\nThen flash the raw image to the SD card using your preferred SD card flashing\ntool or ```dd```. E.g. on Mac you can use\n[Apple Pi Baker][http://www.tweaking4all.com/hardware/raspberry-pi/macosx-apple-pi-baker/]\nfor this\n([download link][http://www.tweaking4all.com/?wpfb_dl=94 ]).\nIf the SD card is ready, you should now put it into your Pi.\n\n### Connect to the Pi\n\nConnect the Pi to your local network or directly to your computer using a\npatch cable (and an adapter if needed).\nBoot it up by connecting it to the power supply.\n\nTime to SSH into the Pi: The Raspberry Pi's SSH is already running -\nits mDNS name is ```raspberry.local```.\nPrepare your computer or network accordingly.\n\nE.g. if you want to connect it directly to you Mac, you can execute the\nfollowing:\n```sh\nsudo /usr/libexec/bootpd -D -d -i en0\n```\n\nIf everything is set up, you can finally call\n```sh\nssh -l pi raspberrypi.local\n```\nto connect (password: raspberry).\n\nIf the Pi is not connected to your home network yet, you somehow have to give it\ninternet access. This is how you could for example connect it to some\nwifi access point:\n```sh\nsudo iwconfig wlan0 essid \"access point name\" key \"your secret access point key\"\nsudo dhclient wlan0\n```\nFor open wifi networks you can leave out the key part.\nIn some cases you first have to do\n```sudo iwdown wlan0``` and/or ```sudo iwup wlan0```.\n\nFinally you can start the setup process by:\n```sh\nwget https://raw.githubusercontent.com/mozilla/vaani.raspberrypi/master/bootstrap.sh\nsudo bash bootstrap.sh\n```\nThis will take quite a while. Once finished, you can reboot the Pi and/or save\nand distribute an image of the SD card.\n"
},
{
  "name": "vaani.assistant",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "app.js",
      "index.js",
      "lib",
      "package.json",
      "resources"
    ]
  },
  "makefile": null,
  "readme": "# vaani.assistant\nThe vaani voice assistant\n"
},
{
  "name": "git-auto-updater",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "bin",
      "lib",
      "package.json"
    ]
  },
  "makefile": null,
  "readme": null
},
{
  "name": "cvtools",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "checklen.py",
      "cvgen.py",
      "cvread.py"
    ]
  },
  "makefile": null,
  "readme": "# cvtools\nTools for creation and maintenance of Common Voice corpus and data\n"
},
{
  "name": "www-moz-works",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "Dockerfile",
      "README.md",
      "bin",
      "circle.yml",
      "html"
    ]
  },
  "makefile": null,
  "readme": "This is www.moz.works.\n"
},
{
  "name": "mozplatformqa-jenkins-jobs",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# mozplatform-jenkins-jobs\nJobs for the Platform QA Jenkins instance.\n\nThis repository contains the jobs for the Platform QA Jenkins instance. Note that the\nmaster branch will not contain any content.\n\nThis repo should be configured as a submodule for mozplatform-jenkins-config with the\ndirectory name 'jobs'. Each branch is a Jenkins instance. For instance, 'pf-platform'\nis in Mountain View, and 'home' is at Syd Polk's house in Austin.\n\n"
},
{
  "name": "rust-tooltool",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "fetch.sh",
      "manifest.tt",
      "repack.sh"
    ]
  },
  "makefile": null,
  "readme": "# Tooltool upload tracking for rust\n\nCollection of manifests and scripts for maintaining tooltool\npackages of the rust compiler.\n"
},
{
  "name": "aom",
  "files": {
    "/": [
      ".clang-format",
      ".gitattributes",
      ".gitignore",
      ".mailmap",
      "AUTHORS",
      "CHANGELOG",
      "CMakeLists.txt",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "PATENTS",
      "README",
      "README.md",
      "aom",
      "aom_dsp",
      "aom_mem",
      "aom_ports",
      "aom_scale",
      "aom_util",
      "aomdec.c",
      "aomenc.c",
      "aomenc.h",
      "aomstats.c",
      "aomstats.h",
      "args.c",
      "args.h",
      "av1",
      "build",
      "codereview.settings",
      "configure",
      "docs.cmake",
      "docs.mk",
      "examples.mk",
      "examples",
      "ivfdec.c",
      "ivfdec.h",
      "ivfenc.c",
      "ivfenc.h",
      "keywords.dox",
      "libs.doxy_template",
      "libs.mk",
      "mainpage.dox",
      "md5_utils.c",
      "md5_utils.h",
      "rate_hist.c",
      "rate_hist.h",
      "solution.mk",
      "test",
      "third_party",
      "tools.mk",
      "tools",
      "tools_common.c",
      "tools_common.h",
      "usage.dox",
      "usage_cx.dox",
      "usage_dx.dox",
      "video_common.h",
      "video_reader.c",
      "video_reader.h",
      "video_writer.c",
      "video_writer.h",
      "warnings.c",
      "warnings.h",
      "webmdec.cc",
      "webmdec.h",
      "webmenc.cc",
      "webmenc.h",
      "y4menc.c",
      "y4menc.h",
      "y4minput.c",
      "y4minput.h"
    ]
  },
  "makefile": null,
  "readme": "# AV1 Codec Library\n\n## Building the library and applications\n\n### Prerequisites\n\n 1. [CMake](https://cmake.org) version 3.5 or higher.\n 2. [Git](https://git-scm.com/).\n 3. [Perl](https://www.perl.org/).\n 4. For x86 targets, [yasm](http://yasm.tortall.net/), which is preferred, or a\n    recent version of [nasm](http://www.nasm.us/).\n 5. Building the documentation requires [doxygen](http://doxygen.org).\n 6. Building the unit tests requires [Python](https://www.python.org/).\n\n### Basic build\n\nCMake replaces the configure step typical of many projects. Running CMake will\nproduce configuration and build files for the currently selected CMake\ngenerator. For most systems the default generator is Unix Makefiles. The basic\nform of a makefile build is the following:\n\n    $ cmake path/to/aom\n    $ make\n\nThe above will generate a makefile build that produces the AV1 library and\napplications for the current host system after the make step completes\nsuccessfully. The compiler chosen varies by host platform, but a general rule\napplies: On systems where cc and c++ are present in $PATH at the time CMake is\nrun the generated build will use cc and c++ by default.\n\n### Configuration options\n\nThe AV1 codec library has a great many configuration options. These come in two\nvarieties:\n\n 1. Build system configuration options. These have the form `ENABLE_FEATURE`.\n 2. AV1 codec configuration options. These have the form `CONFIG_FEATURE`.\n\nBoth types of options are set at the time CMake is run. The following example\nenables ccache and disables high bit depth:\n\n    $ cmake path/to/aom -DENABLE_CCACHE=1 -DCONFIG_HIGHBITDEPTH=0\n    $ make\n\nThe available configuration options are too numerous to list here. Build system\nconfiguration options can be found at the top of the CMakeLists.txt file found\nin the root of the AV1 repository, and AV1 codec configuration options can\ncurrently be found in the file `build/cmake/aom_config_defaults.cmake`.\n\n### Dylib builds\n\nA dylib (shared object) build of the AV1 codec library can be enabled via the\nCMake built in variable BUILD\\_SHARED\\_LIBS:\n\n    $ cmake path/to/aom -D BUILD_SHARED_LIBS=1\n    $ make\n\nThis is currently only supported on non-Windows targets.\n\n### Cross compiling\n\nFor the purposes of building the AV1 codec and applications and relative to the\nscope of this guide, all builds for architectures differing from the native host\narchitecture will be considered cross compiles. The AV1 CMake build handles\ncross compiling via the use of toolchain files included in the AV1 repository.\nThe toolchain files available at the time of this writing are:\n\n - arm64-ios.cmake\n - arm64-linux-gcc.cmake\n - armv7-ios.cmake\n - armv7-linux-gcc.cmake\n - armv7s-ios.cmake\n - mips32-linux-gcc.cmake\n - mips64-linux-gcc.cmake\n - x86-ios-simulator.cmake\n - x86-linux.cmake\n - x86-macos.cmake\n - x86\\_64-ios-simulator.cmake\n\nThe following example demonstrates use of the x86-macos.cmake toolchain file on\na x86\\_64 MacOS host:\n\n    $ cmake path/to/aom \\\n      -DCMAKE_TOOLCHAIN_FILE=path/to/aom/build/cmake/toolchains/x86-macos.cmake\n    $ make\n\nTo build for an unlisted target creation of a new toolchain file is the best\nsolution. The existing toolchain files can be used a starting point for a new\ntoolchain file since each one exposes the basic requirements for toolchain files\nas used in the AV1 codec build.\n\nAs a temporary work around an unoptimized AV1 configuration that builds only C\nand C++ sources can be produced using the following commands:\n\n    $ cmake path/to/aom -DAOM_TARGET_CPU=generic\n    $ make\n\nIn addition to the above it's important to note that the toolchain files\nsuffixed with gcc behave differently than the others. These toolchain files\nattempt to obey the $CROSS environment variable.\n\n### Microsoft Visual Studio builds\n\nBuilding the AV1 codec library in Microsoft Visual Studio is supported. The\nfollowing example demonstrates generating projects and a solution for the\nMicrosoft IDE:\n\n    # This does not require a bash shell; command.exe is fine.\n    $ cmake path/to/aom -G \"Visual Studio 15 2017\"\n\n### Xcode builds\n\nBuilding the AV1 codec library in Xcode is supported. The following example\ndemonstrates generating an Xcode project:\n\n    $ cmake path/to/aom -G Xcode\n\n\n## Testing the AV1 codec\n\n### Testing basics\n\nCurrently there are two types of tests in the AV1 codec repository:\n\n 1. Unit tests.\n 2. Example tests.\n\nThe unit tests can be run at build time:\n\n    # Before running the make command the LIBAOM_TEST_DATA_PATH environment\n    # variable should be set to avoid downloading the test files to the\n    # cmake build configuration directory.\n    $ cmake path/to/aom\n    # Note: The AV1 CMake build creates many test targets. Running make\n    # with multiple jobs will speed up the test run significantly.\n    $ make runtests\n\nThe example tests require a bash shell and can be run in the following manner:\n\n    # See the note above about LIBAOM_TEST_DATA_PATH above.\n    $ cmake path/to/aom\n    $ make\n    # It's best to build the testdata target using many make jobs.\n    # Running it like this will verify and download (if necessary)\n    # one at a time, which takes a while.\n    $ make testdata\n    $ path/to/aom/test/examples.sh --bin-path examples\n\n### IDE hosted tests\n\nBy default the generated projects files created by CMake will not include the\nruntests and testdata rules when generating for IDEs like Microsoft Visual\nStudio and Xcode. This is done to avoid intolerably long build cycles in the\nIDEs-- IDE behavior is to build all targets when selecting the build project\noptions in MSVS and Xcode. To enable the test rules in IDEs the\n`ENABLE_IDE_TEST_HOSTING` variable must be enabled at CMake generation time:\n\n    # This example uses Xcode. To get a list of the generators\n    # available, run cmake with the -G argument missing its\n    # value.\n    $ cmake path/to/aom -DENABLE_IDE_TEST_HOSTING=1 -G Xcode\n\n### Downloading the test data\n\nThe fastest and easiest way to obtain the test data is to use CMake to generate\na build using the Unix Makefiles generator, and then to build only the testdata\nrule:\n\n    $ cmake path/to/aom -G \"Unix Makefiles\"\n    # 28 is used because there are 28 test files as of this writing.\n    $ make -j28 testdata\n\nThe above make command will only download and verify the test data.\n\n\n## Coding style\n\nThe coding style used by this project is enforced with clang-format using the\nconfiguration contained in the .clang-format file in the root of the repository.\n\nBefore pushing changes for review you can format your code with:\n\n    # Apply clang-format to modified .c, .h and .cc files\n    $ clang-format -i --style=file \\\n      $(git diff --name-only --diff-filter=ACMR '*.[hc]' '*.cc')\n\nCheck the .clang-format file for the version used to generate it if there is any\ndifference between your local formatting and the review system.\n\nSee also: http://clang.llvm.org/docs/ClangFormat.html\n\n## Support\n\nThis library is an open source project supported by its community. Please\nplease email aomediacodec@jointdevelopment.kavi.com for help.\n\n## Bug reports\n\nBug reports can be filed in the Alliance for Open Media\n[issue tracker](https://bugs.chromium.org/p/aomedia/issues/list).\n"
},
{
  "name": "overscripted-explorer",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "environment.yaml",
      "server_prep.md",
      "text_search"
    ]
  },
  "makefile": null,
  "readme": "# overscripted-explorer\nExplorer for the OverScripted dataset\n"
},
{
  "name": "blogdotmozilladotorg",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# blogdotmozilladotorg\nProject to manage work requests for blog.mozilla.org (Wordpress/WP engine, etc.)\n"
},
{
  "name": "meao",
  "files": {
    "/": [
      ".editorconfig",
      ".gitignore",
      "404.html",
      "CODE_OF_CONDUCT.md",
      "Gemfile",
      "Gemfile.lock",
      "LICENSE.md",
      "README.md",
      "_config.yml",
      "_drafts",
      "_includes",
      "_layouts",
      "_posts",
      "about.md",
      "atom.xml",
      "docker-run.sh",
      "index.html",
      "public",
      "utils"
    ]
  },
  "makefile": null,
  "readme": "# MEAO\n\nThis is the source code repository for the\n[The Marketing Engineering And Operations Team Blog](https://mozilla.github.io/meao/).\n\n## Run with Docker\n\n1. Run the Docker container:\n\n```\n./docker-run.sh\n```\n\nThis will take about a minute to start on macOS.\n\nThe site (with drafts) is available at http://localhost:4000/meao/\n\n## Run with Ruby\n\n1. Install [Bundler](http://bundler.io/):\n\n```\ngem install bundler\n```\n\n2. Install dependencies:\n\n```\nbundle install --path vendor/bundle\n```\n\n3. Serve the site with or without drafts enabled:\n\n```\nbundle exec jekyll serve\nbundle exec jekyll serve --drafts\n```\n\nThe site is available at http://127.0.0.1:4000/meao/\n\n## Posts and Drafts\n\n[Posts](https://jekyllrb.com/docs/posts/) appear in the ``_posts`` folder and\nhave a date in the filename, such as\n``_posts/2017-07-04-happy-birthday-america.md``.  These are displayed in time\norder, and posts with future dates are not published.\n\n[Drafts](https://jekyllrb.com/docs/drafts/) appear in the ``_drafts`` folder\nand do not have a date in the filename, such as\n``_drafts/my-cool-blog-post.md``. They are useful for authoring a post, and\nthen moving it to the ``_posts`` folder when you are ready for a PR.  Drafts\nmust be enabled in ``jekyll`` to view drafts.\n"
},
{
  "name": "parsepatch",
  "files": {
    "/": [
      ".coveragerc",
      ".flake8",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "MANIFEST.in",
      "README.md",
      "VERSION",
      "parsepatch",
      "requirements.txt",
      "setup.py",
      "test-requirements.txt",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# parsepatch\n> Library to parse patches in an efficient manner\n\n[![Build Status](https://api.travis-ci.org/mozilla/libmozdata.svg?branch=master)](https://travis-ci.org/mozilla/parsepatch)\n[![codecov.io](https://img.shields.io/codecov/c/github/mozilla/libmozdata/master.svg)](https://codecov.io/github/mozilla/parsepatch?branch=master)\n\n## Setup\n\nInstall the prerequisites via `pip`:\n```sh\nsudo pip install -r requirements.txt\n```\n\n## Running tests\n\nInstall test prerequisites via `pip`:\n```sh\nsudo pip install -r test-requirements.txt\n```\n\nRun tests:\n```sh\ncoverage run --source=parsepatch -m unittest discover tests/\n```\n\n## Bugs\n\nhttps://github.com/mozilla/parsepatch/issues/new\n\n## Contact\n\nEmail: release-mgmt@mozilla.com\n"
},
{
  "name": "aframe-xr",
  "files": {
    "/": [
      ".babelrc",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "examples",
      "index.html",
      "package-lock.json",
      "package.json",
      "src",
      "vendor",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# aframe-xr\n\n[![Version](http://img.shields.io/npm/v/aframe-xr.svg?style=flat-square)](https://npmjs.org/package/aframe-xr)\n[![License](http://img.shields.io/npm/l/aframe-xr.svg?style=flat-square)](https://npmjs.org/package/aframe-xr)\n\nSystem &amp; components to build [WebXR](https://github.com/mozilla/webxr-api) experiences with [A-frame](https://github.com/aframevr/aframe)\n\n## Running the examples\n\nVisit [this URL with all the examples](https://mozilla.github.io/aframe-xr/) or:\n\n<a href=\"https://docs.npmjs.com/getting-started/installing-node\">Install npm</a> and then run the following:\n\n```\n$ npm install\n$ npm start\n```\n\n## Supported browsers\n\n### AR\n\n  - ARKit: Mozilla's [ARKit based iOS app](https://github.com/mozilla/webxr-ios)\n  - ARCore: Google's [WebARonARCore Android app](https://github.com/google-ar/WebARonARCore)\n\n### VR\n\n  - Daydream: [Chrome for Android](https://webvr.rocks/chrome_for_android)\n  - Gear VR: [Oculus Browser](https://webvr.rocks/oculus_browser)\n  - HTC Vive / Oculus Rift: [Firefox](https://webvr.rocks/firefox)\n  - Windows Mixed Reality: [Microsoft Edge](https://webvr.rocks/microsoft_edge)\n\n## Configuration\n\n```html\n<a-scene>\n  <a-entity xr=\"ar: true; vr: false; magicWindow: false\"></a-entity>\n  <!-- ... -->\n</a-scene>\n```\n\n## Documentation\n\n### xr System\n\n| Property                        | Default | Description                                          |\n|---------------------------------|---------|------------------------------------------------------|\n| arAutostart                     | true    | Start AR if is the unique display available          |\n| arLightEstimate                 | false   | Modify lights intensity with the light estimation    |\n\n### xr Component\n\n| Property                        | Default | Description                                          |\n|---------------------------------|---------|------------------------------------------------------|\n| ar                              | true    | If the entity is visible on AR mode                  |\n| magicWindow                     | true    | If the entity is visible on magic window mode        |\n| vr                              | true    | If the entity is visible on VR mode                  |\n\n### ar-mode-ui Component\n\nBased on the [vr-mode-ui](https://github.com/aframevr/aframe/blob/v0.7.0/src/components/scene/vr-mode-ui.js) component\n\n| Property                        | Default | Description                                          |\n|---------------------------------|---------|------------------------------------------------------|\n| enabled                         | true    | Whether or not to display UI related to entering AR. |\n\n## Usage\n\n### Browser\nInclude A-Frame (for now, we are using master version - soon an official published version), followed by `three.xr.js` &amp; `aframe-xr`:\n```html\n<script src=\"aframe-master.js\"></script>\n<script src=\"three.xr.js\"></script>\n<script src='aframe-xr.js'></script>\n```\n\n#### npm\n\nInstall via npm:\n\n```bash\nnpm install aframe-xr\n```\n\nThen require and use.\n\n```js\nrequire('aframe');\nrequire('aframe-xr');\n```\n\nUntil A-Frame 0.8.0 is released, make sure to reference the master version of A-Frame in `package.json`:\n\n```json\n \"dependencies\": {\n    \"aframe\": \"github:aframevr/aframe#master\"\n  }\n ```\n \n Or reference the A-Frame included with `aframe-xr` directly:\n \n ```js\nrequire('aframe-xr/vendor/aframe-master.js');\nrequire('aframe-xr');\n```\n"
},
{
  "name": "three.xr.js",
  "files": {
    "/": [
      ".babelrc",
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "LICENSE",
      "README.md",
      "dist",
      "examples",
      "index.html",
      "package-lock.json",
      "package.json",
      "src",
      "vendor",
      "webpack.config.js"
    ]
  },
  "makefile": null,
  "readme": "# three.xr.js\n\n[![Version](http://img.shields.io/npm/v/three.xr.js.svg?style=flat-square)](https://npmjs.org/package/three.xr.js)\n[![License](http://img.shields.io/npm/l/three.xr.js.svg?style=flat-square)](https://npmjs.org/package/three.xr.js)\n\nLibrary to build [WebXR](https://github.com/mozilla/webxr-api) experiences with [three.js](https://github.com/mrdoob/three.js)\n\n## Running the examples\n<a href=\"https://docs.npmjs.com/getting-started/installing-node\">Install npm</a> and then run the following:\n\n```\n$ npm install\n$ npm start\n```\n\n## Supported browsers\n\n### AR\n\n  - ARKit: Mozilla's [ARKit based iOS app](https://github.com/mozilla/webxr-ios)\n  - ARCore: Google's [WebARonARCore Android app](https://github.com/google-ar/WebARonARCore)\n\n### VR\n\n  - Daydream: [Chrome for Android](https://webvr.rocks/chrome_for_android)\n  - Gear VR: [Oculus Browser](https://webvr.rocks/oculus_browser)\n  - HTC Vive / Oculus Rift: [Firefox](https://webvr.rocks/firefox)\n  - Windows Mixed Reality: [Microsoft Edge](https://webvr.rocks/microsoft_edge)\n\n## Usage\n\nInclude three.xr.js after THREE.js:\n```html\n<script src='three.js'></script>\n<script src='three.xr.js'></script>\n```\n\nIn your application code you can do:\n```js\nTHREE.WebXRUtils.getDisplays().then(init);\n\nfunction init(displays) {\n  container = document.createElement( 'div' );\n  document.body.appendChild( container );\n\n  scene = new THREE.Scene();\n  camera = new THREE.PerspectiveCamera();\n  scene.add( camera );\n\n  renderer = new THREE.WebGLRenderer( { alpha: true } );\n  renderer.autoClear = false;\n  container.appendChild( renderer.domElement );\n\n  // Add custom code here\n\n  window.addEventListener( 'resize', onWindowResize, false );\n  onWindowResize();\n\n  // Set XR options\n  var options = {\n    // Flag to start AR if is the unique display available.\n    AR_AUTOSTART: false, // Default: true\n  }\n  // Init WebXR\n  renderer.xr = new THREE.WebXRManager(options, displays, renderer, camera, scene, update);\n\n  // Listen when a session is started or stopped\n  renderer.xr.addEventListener('sessionStarted', sessionStarted);\n  renderer.xr.addEventListener('sessionEnded', sessionEnded);\n\n  // Auto start if only has one AR display supported\n  if(!renderer.xr.autoStarted){\n    // Add as many buttons as there are displays that can be started\n    addEnterButtons(displays);\n  }\n\n  renderer.animate(render);\n}\n\nfunction sessionStarted(data) {\n  activeRealityType = data.session.realityType;\n  // We can show or hide elements depending on the active reality type\n  // ar, magicWindow, vr\n}\n\nfunction sessionEnded(data) {\n  activeRealityType = 'magicWindow';\n  // We can show or hide elements depending on the active reality type\n  // ar, magicWindow, vr\n}\n\nfunction addEnterButtons(displays) {\n  for (var i = 0; i < displays.length; i++) {\n    var display = displays[i];\n    if(display.supportedRealities.vr){\n      // Add ENTER VR button\n      // and to call enterVR on 'click' event\n    }\n    if(display.supportedRealities.ar){\n      // Add ENTER AR button\n      // and to call enterVR on 'click' event\n    }\n  }\n}\n\nfunction enterAR(){\n  renderer.xr.startSession(display, 'ar', true);\n}\n\nfunction exitAR(){\n  renderer.xr.endSession();\n}\n\nfunction enterVR(){\n  renderer.xr.startPresenting();\n}\n\n// To detect and exitVR\nwindow.addEventListener('vrdisplaypresentchange', (evt) => {\n  // Polyfill places cameraActivateddisplay inside the detail property\n  var display = evt.display || evt.detail.display;\n  if (!display.isPresenting) {\n    // Exiting VR.\n    renderer.xr.endSession();\n  }\n});\n\nfunction onWindowResize() {\n  camera.aspect = window.innerWidth / window.innerHeight;\n  camera.updateProjectionMatrix();\n}\n\n// Called once per frame, before render, to give the app a chance to update this.scene\nfunction update(frame) {\n  render();\n}\n\nfunction render() {\n  // We can different commands depending on the active reality type\n  // ar, magicWindow, vr\n  switch (activeRealityType) {\n    case 'ar':\n    case 'magicWindow':\n    case 'vr':\n      \n      break;\n  } \n\n  // Only renderer.render out of renderer.xr if the session is not active\n  if(!renderer.xr.sessionActive){\n    renderer.setSize( window.innerWidth, window.innerHeight );\n    renderer.render(this.scene, this.camera);\n  }\n}\n\n```"
},
{
  "name": "speaktome-node",
  "files": {
    "/": [
      ".eslintrc.yml",
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "README.md",
      "index.js",
      "package-lock.json",
      "package.json",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "# Speak To Me - Mozilla Speech Recognition API\n\n[![Version](http://img.shields.io/npm/v/speaktome-node.svg?style=flat-square)](https://npmjs.org/package/speaktome-node)\n[![License](http://img.shields.io/npm/l/speaktome-node.svg?style=flat-square)](https://npmjs.org/package/speaktome-node)\n\nNode.js module for SpeakToMe, Mozilla's Speech-to-text REST API.\n\nSupports recording of audio on local system, encoding and sending the recording to Mozilla's service for processing, and retrieval of results.\n\n## Installation\n\nSupport for recording from system or USB mics is through the [`mic` package](https://www.npmjs.com/package/mic), which depends on installation of OS-specific recording utilities:\n\nWindows and macOS require SOX and Linux requires ALSA tools.\n\n* Windows: Download and install [SOX from the website](http://sox.sourceforge.net/)\n* Linux: ```sudo apt-get install alsa-base alsa-utils```\n* macOS: ```brew install sox```\n\nInstall via npm:\n\n```bash\nnpm install speaktome-node\n```\n\n## Usage\n\n```js\nconst speech = require('speaktome-node');\n\nspeech.record().then(results => {\n\n  // Results is an array of objects containing\n  // `text` and `confidence` properties:\n  //\n  // [\n  //   { confidence: \"0.8090\", text: \"TEST\" }\n  // ]\n  console.log(results);\n\n}).catch(console.error);\n```\n\n## Development Notes\n\nInstall opus-tools for command line utilities to test Opus encoding/decoding/playback.\n\n```bash\nbrew intall opus-tools\n```\n\nCommand for encoding raw sound files recorded from system microphone to Opus:\n\n```bash\nopusenc --raw --raw-rate 16000 --raw-chan 1 recording.raw recording.opus\n```\n"
},
{
  "name": "OSSN",
  "files": {
    "/": [
      "ISSUE_TEMPLATE.md",
      "OSSNQ1.png",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "<h2> Mozilla Open Source Student Network Working Repo </h2>\n\n<p>We\u2019re fueling the open movement by creating a network of clubs on university campuses in the United States who learn about and contribute to open source. <br> You can find more information about this program at http://opensource.mozilla.community</p>\nThis repository serves as a project management tool for the development of the program.\n\n<h3>Team Members</h3>\n\n- Lucy Harris\n- Christos Bacharakis\n- Rina Tambo Jensen\n- Arielle Kilroy\n- Megan Branson\n- Tasos Katsoulas\n\n\n<h3> Project's goals for 2018 & Q1 </h3>\n<img src=\"OSSNQ1.png\"></img>\n"
},
{
  "name": "firefox-data-store-docs",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "image_0.png",
      "image_1.png",
      "image_10.png",
      "image_11.png",
      "image_12.png",
      "image_13.png",
      "image_14.png",
      "image_15.png",
      "image_16.png",
      "image_2.png",
      "image_3.png",
      "image_4.png",
      "image_5.png",
      "image_6.png",
      "image_7.png",
      "image_8.png",
      "image_9.png"
    ]
  },
  "makefile": null,
  "readme": "# Firefox Data Stores\n\n# TL;DR\n\n* Firefox desktop has 45 separate data stores. Firefox for iOS has 5, and Firefox for Android has 2, in addition to stores shared with desktop.\n\n* Only 9 shared domain objects can be found on all 3 platforms. One of those, favicons, is present but not synced at all. \n\n* Firefox desktop has an entire SQLite database, `storage.sqlite`, that contains no tables. It is used entirely to reference the schema version value and uses that value to create the version number for the Storage directory.\n\n* Firefox desktop stores over 1200 preferences in a single, synchronously accessed file. This file is read from and written to from all over the application. Each individual write will cause the entire file to be written to disk at a later date.\n\n* Firefox for desktop utilises 10 different data storage formats. These are\n\n  1. SQLite\n  2. IndexedDB\n  3. Extensionless character-delimited files (AlternateServices, SiteSecurityServices, SecurityPreloadState)\n  4. JSON\n  5. .ini\n  6. .txt\n  7. .dat\n  8. .rdf\n  9. .xml\n  10. .js (prefs, sessionstore)\n\n* Three data stores contain only one item of information.\n\n  1. `times.json`\n  2. `datareporting/state.json`\n  3. `healthreport/state.json`\n\n* Of the data fields stored by desktop, only 8% of them are available to sync. Of data that is synced, desktop syncs 95% of fields, Android 90% and iOS 86%.\n\n# Desktop Data Stores\n\n## SQLite Stores\n\n### `places.sqlite`\n\nPlaces is where the vast majority of synced data is stored. This database closely resembles the database structures on iOS and Android. The only data types that are synced that are not contained within places, are logins, form fields, add-ons (only desktop), prefs (only desktop), clients (synthetic), open tabs, and commands (synthetic, transient).\n\n![image alt text](image_0.png)\n\n### `content-prefs.sqlite`\n\nStores arbitrary data, or \"preferences\", associated with specific domains, or web \"content\".\n\n![image alt text](image_1.png)\n\n### `cookies.sqlite`\n\n![image alt text](image_2.png)\n\n### `favicons.sqlite`\n\nStores blob data for favicons for quick retrieval. This database is `ATTACH`ed to `places.sqlite`.\n\n![image alt text](image_3.png)\n\n### `formhistory.sqlite`\n\nStores data entered into forms.\n\n![image alt text](image_4.png)\n\n### `kinto.sqlite`\n\nBacking store for the blocklists code in the client. These lists\nrepresent addons, certificates, graphics drivers, and plugins which we\nshould be careful not to trust, as well as \"pinned\" certificates we\nshould trust. This local store is updated from a server periodically.\nIt has the unfortunate name of \"kinto\" because this synchronization is\nperformed using the Kinto server and client libraries. Because Kinto\nis an object store, the SQL schema is not especially helpful.\n\n![image alt text](image_5.png)\n\n### `storage-sync.sqlite`\n\nAnother Kinto-based SQLite store, this one supports our implementation\nof the `browser.storage.sync` API. This is just the local store; the\nsynchronization happens with a server \"in the cloud\", of course. See\nhttps://wiki.mozilla.org/CloudServices/Sync/ExtensionStorage_Design_Doc\nfor more information on how extension-storage information is stored in\nKinto.\n\n![image alt text](image_5.png)\n\n### `permissions.sqlite`\n\nStores host, API, and clipboard permissions. See also `content-prefs.sqlite`.\n\n![image alt text](image_6.png)\n\n### `signons.sqlite`\n\nThe login manager has two backing store implementations: SQLite, which is used by Android, and a delayed-flush JSON file, used by desktop as of Bug 853549 in 2014. Android uses the SQLite implementation to support simultaneous access from Gecko and Android-side Sync code.\n\n![image alt text](image_7.png)\n\n### `storage.sqlite`\n\n`storage.sqlite` is a completely empty database. It is opened and the schema version number checked and that version number is used as the version number for the storage directory. The storage directory is the directory that contains all of the data store files. \n\n### `webappsstore.sqlite`\n\nBacking store for DOMStorage.\n\n![image alt text](image_8.png)\n\n## Key-Value Stores\n\n#### AlternateServices\n\nRFC 7838 Alternative Services. \"Alt-Svc allows separation of transport routing from the origin host without using a proxy.\"\n\n#### SiteSecurityServices, SecurityPreloadState\n\nHSTS & HPKP storage.\n\n#### `secmod.db`\n\nThe \"legacy\" security module database.\n\n#### `key3.db`\n\nContains the key that is used to encrypt the *Firefox* passwords stored in `logins.json`. \"Legacy\", according to documentation. Android apparently sometimes uses `key4.db`, which is a SQLite replacement.\n\n####  `cert7.db`\n\n\"Legacy\" Network Security Services DB.\n\n#### GMP/PBGP\n\n?\n\n#### Abouthome\n\nThe about:home snippet service is a simple, highly-cached content management service. It is intended to assemble and deliver content snippets to the about:home page in Firefox.\n\n#### AppProjects\n\nData store for local apps.\n\n#### devtools-async-storage\n\nSimple key-value store for persisting values in devtools\n\n#### About:reader\n\nStores cached article information for the about:reader page.\n\n![image alt text](image_9.png)\n\n## JSON file stores\n\n### **s****essionstore****.js/recovery.js**\n\nStores all browsing session state data for session restoration. \n\nWhile Firefox is running, your live session is stored in recovery.js**.  **sessionstore.js is created at shutdown. \n\n```json\n{\n  \"windows\": [\n    {\n      \"tabs\": [\n        {\n          \"entries\": [\n            {\n              \"url\": string,\n              \"title\": string,\n              \"cacheKey\": long,\n              \"ID\": int,\n              \"docshellID\": int,\n              \"referrer\": string,\n              \"docIdentifier\": int,\n              \"structuredCloneState\": string,\n              \"structuredCloneVersion\": int,\n              \"children\": [\n                {\n                  \"url\": string,\n                  \"ID\": int,\n                  \"docshellID\": int,\n                  \"owner_b64\": string,\n                  \"docIdentifier\": int\n                },\n              ]\n            }\n          ],\n          \"lastAccessed\": long,\n          \"hidden\": bool,\n          \"attributes\": {\n          },\n          \"image\": string,\n          \"index\": int,\n          \"storage\": {\n            \"<url>\": {\n              \"ScribeTransport\": string\n            }\n          },\n          \"formdata\": {\n            \"xpath\": {\n              \"<path>\": string\n            },\n            \"id\": {\n              \"age-gate-year\": {\n                \"selectedIndex\": int,\n                \"value\": string\n              },\n              \"age-gate-month\": {\n                \"selectedIndex\": int,\n                \"value\": string\n              },\n              \"age-gate-day\": {\n                \"selectedIndex\": int,\n                \"value\": string\n              }\n            },\n            \"url\": string\n          },\n          \"scroll\": {\n            \"children\": [\n              {\n                \"scroll\": stringint\n              }\n            ]\n          },\n        },\n      \"selected\": int,\n      \"_closedTabs\": [\n        {\n          \"state\": {\n            \"entries\": [\n              {\n                  \"url\": string,\n                  \"title\": string,\n                  \"cacheKey\": long,\n                  \"ID\": int,\n                  \"docshellID\": int,\n                  \"referrer\": string,\n                  \"docIdentifier\": int,\n                  \"structuredCloneState\": string,\n                  \"structuredCloneVersion\": int,\n                  \"children\": [\n                    {\n                      \"url\": string,\n                      \"ID\": int,\n                      \"docshellID\": int,\n                      \"owner_b64\": string,\n                      \"docIdentifier\": int\n                    },\n                ]\n              }\n            ],\n            \"lastAccessed\": long,\n            \"userTypedValue\": string,\n            \"userTypedClear\": int,\n            \"hidden\": bool,\n            \"attributes\": {\n            },\n            \"image\": string,\n            \"index\": int,\n            \"pageStyle\": {\n              \"pageStyle\": string\n            },\n            \"scroll\": {\n              \"scroll\": string\n            }\n          },\n          \"title\": string,\n          \"image\": string,\n          \"pos\": int,\n          \"closedAt\": long\n        }\n      ],\n      \"busy\": bool,\n      \"width\": string,\n      \"height\": string,\n      \"screenX\": string,\n      \"screenY\": string,\n      \"sizemode\": string\n    }\n  ],\n  \"selectedWindow\": int,\n  \"_closedWindows\": [\n      {\n        tabs: [\n          { \n            entries: [\n              {\n                url: string, \n                triggeringPrincipal_base64: string, \n                title: string \n              }\n            ] \n          }\n        ],\n        selected: 2,\n        title: \"mozilla.org\",\n        _closedTabs: []\n      },\n  ],\n  \"session\": {\n    \"lastUpdate\": long,\n    \"startTime\": long,\n    \"recentCrashes\": int\n  },\n  \"scratchpads\": [ string ],\n  \"global\": {\n    \n  }\n}\n```\n\n### addons.json\n\nCompatible addons for this browser version.\n\n### containers.json\n\nDetails of the different containers currently in use.\n\n### xulstore.json\n\nStores information about toolbars, window positioning, and other interface elements in Firefox 34 and above.\n\n### `bookmarkbackups/*.jsonlz4`\n\nLocal backup of bookmarks.\n\n### `logins.json`\n\nContains all the stored username and password information for login manager. See also `signons.sqlite`.\n\n### `handlers.json`\n\nDetails of the different services available and how to connect to them.\n\n![image alt text](image_10.png)\n\n### `search.json`\n\nContains information about all of the search engines supported by the browser. This is used by all other search UI to retrieve query and configuration information about a specific search engine. \n\n### `signedInUser.json`\n\nContains information about the currently signed in Firefox Accounts user. This file is written by both desktop and Android codebases, but the code that does so is entirely different.\n\n### `extensions.json`\n\nContains information about installed web extensions.\n\n### `times.json`\n\nWritten by whichever code first creates the profile \u2014 Gecko or Firefox for Android \u2014 in order to set a lower bound on profile creation date for analytics. Originally supported Firefox Health Report. JSON to allow for additional fields to be added \u2014 e.g., to record \"Refresh Firefox\" events.\n\n### `datareporting/state.json`\n\nContains the client ID used to identify this installation for datareporting.\n\n### `healthreport/state.json`\n\nContains the client ID used to identify this installation for Firefox Health Report.\n\n### `sessionCheckpoints.json`\n\nRecords the profile state after every notification.\n\n![image alt text](image_11.png)\n\n## File Based Stores.\n\n### `compatibility.ini`\n\nContains a snapshot of browser version information for compatibility comparison.\n\n### `revocations.txt`\n\nContains a list of revoked certificates.\n\n### `persdict.dat`\n\nContains a list of words that have been added to the browser\u2019s dictionary.\n\n### `cert-override.txt`\n\nContains user certificate exceptions.\n\n### `serviceworker.txt`\n\nContains information about installed service workers.\n\n### `pluginreg.dat`\n\nContains a list of installed plugins.\n\n![image alt text](image_12.png)\n\n## XML file stores.\n\n### `mimeTypes.rdf`\n\nContains associations between MIME types and application/save preferences. RDF/XML.8\n\n### `blocklist.xml`\n\nContains a list of addons, plugins, certificates, and graphics hardware and drivers that Firefox should exclude. Previously (?) stored in `blocklist-{addons,gfx,plugins}.json`.\n\n![image alt text](image_13.png)\n\n### Prefs\n\nThere are over 1000 preferences collected and stored in desktop Firefox. You can see a list of the preference keys in Appendix 1.\n\n# Android Data Stores\n\n### `browser.db`, `signedInUser.json`\n\nSee also discussion of `signedInUser.json`, `times.json`, and `signons.sqlite`, above.\n\n![image alt text](image_14.png)\n\n### Prefs\n\nAndroid uses both Gecko's libpref store (`prefs.js`) and several Android `SharedPreferences` files, which are key-value pairs written to XML files with concurrency support provided by the Android frameworks.\n\n# iOS Data Stores\n\n### `browser.db`\n\n![image alt text](image_15.png)\n\n### `logins.db`, `ReadingList.db`, `signedInUser.json`\n\n`metadata.db` was created but never used.\n\n![image alt text](image_16.png)\n\n### Prefs\n* `applicationDidRequestUserNotificationPermissionPrefKey`\n* `backoff.storage`\n* `baseTimestamp`\n* `blockPopups`\n* `bookmarkvalidationattempt`\n* `clearprivatedata.toggles`\n* `CompactTabLayout`\n* `currentClient`\n* `feature_switches.<feature_id>`\n* `flowID`\n* `HomePageButtonIsInMenuPrefKey`\n* `HomePageURLPref`\n* `ids`\n* `initialPingSent`\n* `IntroViewControllerSeen`\n* `KeyDefaultHomePageURL`\n* `lastClientUpload`\n* `lastFetched`\n* `lastModified`\n* `lastRemoteTabSyncTime`\n* `lastSyncFinishTime`\n* `lastTabsUpload`\n* `latestAppVersion`\n* `MailToOption`\n* `NewTabPrefKey`\n* `nextOffset`\n* `NightModeButtonIsInMenuPrefKey`\n* `NightModeStatus`\n* `NoImageModeButtonIsInMenuPrefKey`\n* `NoImageModeStatus`\n* `offsetNewer`\n* `PrefKeyModel`\n* `PrefKeyPingCount`\n* `PrefKeyProfileDate`\n* `previousClients`\n* `readermode.style`\n* `recentlyClosedTabs`\n* `repair.aborted`\n* `repair.finished`\n* `repair.need-new-client`\n* `repair.sent`\n* `repair.sent-again`\n* `repairs.bookmark`\n* `saveLogins`\n* `search.disabledEngineNames`\n* `search.orderedEngineNames`\n* `search.suggestions.show`\n* `search.suggestions.showOptIn`\n* `settings.allowThirdPartyKeyboards`\n* `settings.closePrivateTabs`\n* `settings.sendUsageData`\n* `state`\n* `sync.scratchpad`\n* `timestamp`\n* `topSites.deletedSuggestedSites`\n* `topSitesCacheIsValid`\n* `topSitesCacheSize`\n\n\u2026 plus an array of preferences to manage Sync state. These are all written to `UserDefaults`, which is the iOS equivalent of the Android `SharedPreferences` system. These end up in `.plist` files on disk.\n\n# Firefox Sync storage format 5\n\nSee also [http://docs.services.mozilla.com/sync/objectformats.html](http://docs.services.mozilla.com/sync/objectformats.html) and [https://mozilla-services.readthedocs.io/en/latest/storage/apis-1.5.html](https://mozilla-services.readthedocs.io/en/latest/storage/apis-1.5.html) .\n\nFirefox Sync is a record-oriented system: the current state (and only the current state) of the connected devices' opted-in data is present as encrypted JSON objects on the server, keyed by a GUID. Data types correspond to collections on the server: e.g., login records are stored as objects in the passwords collection. Changes are observed by seeing a change in modified timestamp. Changes are retrieved by downloading the entirety of each modified record. New local changes are applied by uploading complete new records to the relevant collection.\n\nSee the [datatypes](https://docs.google.com/spreadsheets/d/1k9_K7Dc3q2h3SDV0vwjTgJou-ndza6WuobyJ1bbemtc/edit?ts=5977ab9d#gid=1269587388) companion document to see an overview of which data is synced between which platforms.\n\n## meta/global\n\nControl structure for engine selection and versioning. One per account.\n\n```json\n{\n\"engines\" : {\n\"bookmarks\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"forms\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"tabs\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"clients\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"addons\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"passwords\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"history\" : {\n\"version\" : int,\n\"syncID\" : string\n},\n\"prefs\" : {\n\"version\" : int,\n\"syncID\" : string\n}\n},\n\"syncID\" : string,\n\"declined\" : [\nstring\n],\n\"storageVersion\" : int\n}\n```\n\n## Keys\n\nOne object per account. Written to crypto/keys.\n```json\n{\n\"default\" : [ string ],\n\"collection\" : string,\n\"id\" : string,\n\"collections\" : {\n\"string\": string\n}\n}\n```\n\n## Client\n\nOne per client. Expires after three weeks. Commands are sent to a client by writing them to *its* client record.\n```json\n{\n\"commands\" : [ {\n\"command\" : string,\n\"args\" : [ string ]\n} ],\n\"protocols\" : [ string ],\n\"application\" : string,\n\"appPackage\" : string,\n\"id\" : string,\n\"os\" : string,\n\"name\" : string,\n\"fxaDeviceId\" : string,\n\"version\" : string,\n\"formfactor\" : string,\n\"type\" : string,\n\"device\" : string\n}\n```\n## Forms\n\nOne per form entry.\n```json\n{\n\"name\" : string,\n\"value\" : string\n}\n```\n\n## Tabs\n\nOne per client. All tabs for a device are written in a single array. The entire record is reuploaded when the set of open tabs changes.\n```json\n{\n\"id\" : string,\n\"clientName\" : string,\n\"tabs\" : [\n{\n\"lastUsed\" : string/decimalstring/integer,\n\"icon\" : string,\n\"title\" : string,\n\"urlHistory\" : [ string ]\n}\n]\n}\n```\n\n## Passwords\n\nOne per login entry.\n```json\n{\n\"httpRealm\" : string,\n\"passwordField\" : string,\n\"id\" : string,\n\"password\" : string,\n\"timeCreated\" : ulong,\n\"formSubmitURL\" : string,\n\"timeLastUsed\" : ulong,\n\"timePasswordChanged\" : ulong,\n\"usernameField\" : string,\n\"hostname\" : string,\n\"username\" : string,\n\"timesUsed\" : ulong\n}\n```\n\n## Bookmark\n\nOne per bookmark or folder.\n```json\n{\n\"id\" : string,\n\"parentid\" : string,\n\"parentName\" : string,\n\"title\" : string,\n\"bmkUri\" : string,\n\"siteUri\" : string,\n\"feedUri\" : string,\n\"type\" : string,\n\"loadInSidebar\" : bool,\n\"pos\" : string,\n\"tags\" : [ string ]\n\"children\" : [ string ]\n}\n```\n\n## History\n\nOne per URL, containing a truncated array of visits (currently limited to 20). No way of representing the deletion of a visit.\n```json\n{\n\"title\" : string,\n\"histUri\" : string,\n\"id\" : string,\n\"visits\" : [\n{\n\"date\" : integer,\n\"type\" : integer\n}\n]\n}\n```\n\n## Prefs\n\nOne object per *application* (e.g., Firefox).\n```json\n{\n\"value\" :[{\n\"name\" : string,\n\"value\" : <type>,\n}]\n}\n```\n\n## Addons\n```json\n{\n\"addonID\" : string,\n\"applicationID\" : string,\n\"enabled\" : bool,\n\"source\" : string,\n}\n```\n\n# Appendices\n\n## Appendix 1 - Desktop Prefs\n\n* `accessibility.accessfu.keyboard_echo`\n* `accessibility.accessfu.notify_output`\n* `accessibility.accessfu.quicknav_index`\n* `accessibility.accessfu.quicknav_modes`\n* `accessibility.accessfu.skip_empty_images`\n* `Accessibility.accessfu.utterance`\n* `accessibility.accesskeycausesactivation`\n* `accessibility.AOM.enabled`\n* `Accessibility.blockautorefresh`\n* `Accessibility.browsewithcaret`\n* `accessibility.browsewithcaret_shortcut.enabled`\n* `accessibility.delay_plugin_time`\n* `accessibility.delay_plugins`\n* `accessibility.force_disabled`\n* `accessibility.lastLoadDate`\n* `accessibility.loadedInLastSession`\n* `accessibility.mouse_focuses_formcontrol`\n* `Accessibility.screenreader`\n* `accessibility.tabfocus_applies_to_xul`\n* `accessibility.typeaheadfind`\n* `Accessibility.typeaheadfind.autostart`\n* `Accessibility.typeaheadfind.casesensitive`\n* `Accessibility.typeaheadfind.enablesound`\n* `Accessibility.typeaheadfind.enabletimeout`\n* `accessibility.typeaheadfind.flashBar`\n* `accessibility.typeaheadfind.linksonly`\n* `accessibility.typeaheadfind.matchesCountLimit`\n* `accessibility.typeaheadfind.prefillWithSelection`\n* `accessibility.typeaheadfind.soundURL`\n* `accessibility.typeaheadfind.startlinksonly`\n* `accessibility.typeaheadfind.timeout`\n* `accessibility.usebrailledisplay`\n* `accessibility.usetexttospeach`\n* `accessibility.warn_on_browsewithcaret`\n* `advanced.mailftp`\n* `alerts.showFavicons`\n* `app.feedback.baseURL`\n* `app.productInfo.baseURL`\n* `app.releaseNotesURL`\n* `app.support.baseURL`\n* `app.support.e10sAccessibilityUrl`\n* `app.update.altwindowtype`\n* `app.update.auto`\n* `app.update.backgroundMaxErrors`\n* `app.update.badgeWaitTime`\n* `app.update.cancelations.osx`\n* `app.update.cancelled`\n* `app.update.channel`\n* `app.update.checkInstallTime`\n* `app.update.checkInstallTime.days`\n* `app.update.disable_button.showUpdateHistory`\n* `app.update.doorhanger`\n* `app.update.download.attempts`\n* `app.update.download.backgroundInterval`\n* `app.update.download.promptMaxAttempts`\n* `app.update.elevate.attempts`\n* `app.update.elevate.never`\n* `app.update.elevation.promptMaxAttempts`\n* `app.update.enabled`\n* `app.update.idletime`\n* `app.update.interval`\n* `app.update.lastUpdateTime.\\*`\n* `app.update.link.updateAvailableWhatsNew`\n* `app.update.link.updateManualWhatsNew`\n* `app.update.log`\n* `app.update.postupdate`\n* `app.update.promptWaitTime`\n* `app.update.service.enabled`\n* `app.update.silent`\n* `app.update.staging.enabled`\n* `app.update.timerFirstInterval`\n* `app.update.timerMinimumDelay`\n* `app.update.url`\n* `app.update.url.details`\n* `app.update.url.manual`\n* `application.use_ns_plugin_finder`\n* `apz.allow_checkerboarding`\n* `apz.allow_immediate_handoff`\n* `apz.allow_zooming`\n* `apz.axis_lock.breakout_angle`\n* `apz.axis_lock.breakout_threshold`\n* `apz.axis_lock.direct_pan_angle`\n* `apz.axis_lock.lock_angle`\n* `apz.axis_lock.mode`\n* `apz.content_response_timeout`\n* `apz.danger_zone_x`\n* `apz.danger_zone_y`\n* `apz.disable_for_scroll_linked_effects`\n* `apz.displayport_expiry_ms`\n* `apz.drag.enabled`\n* `apz.drag.initial.enabled`\n* `apz.enlarge_displayport_when_clipped`\n* `apz.fling_accel_base_mult`\n* `apz.fling_accel_interval_ms`\n* `apz.fling_accel_min_velocity`\n* `apz.fling_accel_supplemental_mult`\n* `apz.fling_curve_function_x1`\n* `apz.fling_curve_function_x2`\n* `apz.fling_curve_function_y1`\n* `apz.fling_curve_function_y2`\n* `apz.fling_curve_threshold_inches_per_ms`\n* `apz.fling_friction`\n* `apz.fling_min_velocity_threshold`\n* `apz.fling_stop_on_tap_threshold`\n* `apz.fling_stopped_threshold`\n* `apz.frame_delay.enabled`\n* `apz.keyboard.enabled`\n* `apz.max_velocity_inches_per_ms`\n* `apz.max_velocity_queue_size`\n* `apz.min_skate_speed`\n* `apz.minimap.enabled`\n* `apz.minimap.visbility.enabled`\n* `apz.one_touch_pinch.enabled`\n* `apz.overscroll.enabled`\n* `apz.overscroll.min_pan_distance_ratio`\n* `apz.overscroll.spring_friction`\n* `apz.overscroll.spring_stiffness`\n* `apz.overscroll.stop_distance_threshold`\n* `apz.overscroll.stop_velocity_threshold`\n* `apz.overscroll.stretch_factor`\n* `apz.paint_skipping.enabled`\n* `apz.peek_messages.enabled`\n* `apz.popups.enabled`\n* `apz.printtree`\n* `apz.record_checkerboarding`\n* `apz.scale_repaint_delay_ms`\n* `apz.test.logging_enabled`\n* `apz.touch_move_tolerance`\n* `apz.touch_start_tolerance`\n* `apz.velocity_bias`\n* `apz.velocity_relevance_time_ms`\n* `apz.x_skate_highmem_adjust`\n* `apz.x_skate_size_multipler`\n* `apz.x_stationary_size_multiplier`\n* `apz.y_skate_highmem_adjust`\n* `apz.y_skate_size_multiplier`\n* `apz.y_stationary_size_multiplier`\n* `apz.zoom_animation_duration_ms`\n* `beacon.enabled`\n* `bidi.browser.ui`\n* `bidi.direction`\n* `bidi.edit.caret_movement_style`\n* `bidi.edit.delete_immediately`\n* `bidi.numeral`\n* `bidi.texttype`\n* `bookmarks.initialized.pref`\n* `breakpad.reportURL`\n* `browser.aboutHomeSnippets.updateUrl`\n* `browser.active_color`\n* `browser.addon-watch.ignore`\n* `browser.altClickSave`\n* `browser.anchor_color`\n* `browser.audioFeeds.handler`\n* `browser.audioFeeds.handler.default`\n* `browser.audioFeeds.handlers.application`\n* `browser.audioFeeds.handlers.webservice`\n* `browser.autofocus`\n* `browser.backspace_action`\n* `browser.bookmarks.added_static_root`\n* `browser.bookmarks.autoExportHTML`\n* `browser.bookmarks.editDialog.firstEditField`\n* `browser.bookmarks.max_backups`\n* `browser.bookmarks.restore_default_bookmarks`\n* `browser.bookmarks.showMobileBookmarks`\n* `browser.bookmarks.showRecentlyBookmarked`\n* `browser.cache.auto_delete_cache_version`\n* `browser.cache.check_doc_frequency`\n* `browser.cache.compression_level`\n* `browser.cache.disk.{enable,capacity,filesystem_reported, free_space_hard_limit,free_space_soft_limit,hashstats_reported,max_chunks_memory_usage,max_entry_size,max_priority_chunks_memory_usage,metadata_memory_limit,preload_chunk_count,smart_size\\*}`\n* `browser.cache.desk_cache_ssl`\n* `browser.cache.frecency_experiment`\n* `browser.cache.frecency_half_life_hours`\n* `browser.cache.max_shutdown_io_lag`\n* `browser.cache.memory.{enable,max_entry_size}`\n* `browser.cache.offline.capacity`\n* `browser.cache.offline.enable`\n* `browser.cache.use_new_backend`\n* `browser.cache.use_new_backend_temp`\n* `browser.casting.enabled`\n* `browser.chromeURL`\n* `browser.chrome.favicons`\n* `browser.chrome.image_icons.max_size`\n* `browser.chrome.site_icons`\n* `browser.chrome.toolbar_style`\n* `browser.chrome.toolbar_tips`\n* `browser.contentHandlers.types.0.title`\n* `browser.contentHandlers.types.0.type`\n* `browser.contentHandlers.types.0.uri`\n* `browser.contentHandlers.types.1.title`\n* `browser.contentHandlers.types.1.type`\n* `browser.contentHandlers.types.1.uri`\n* `browser.contentHandlers.types.2.title`\n* `browser.contentHandlers.types.2.type`\n* `browser.contentHandlers.types.2.uri`\n* `browser.contentHandlers.types.3.title`\n* `browser.contentHandlers.types.3.type`\n* `browser.contentHandlers.types.3.uri`\n* `browser.contentHandlers.types.4.title`\n* `browser.contentHandlers.types.4.type`\n* `browser.contentHandlers.types.4.uri`\n* `browser.contentHandlers.types.5.title`\n* `browser.contentHandlers.types.5.type`\n* `browser.contentHandlers.types.5.uri`\n* `browser.crashReports.unsubmittedCheck.autoSubmit`\n* `browser.crashReports.unsubmittedCheck.chancesUntilSuppress`\n* `browser.crashReports.unsubmittedCheck.enabled`\n* `browser.crashReports.unsubmittedCheck.lastShownDate`\n* `browser.ctrlTab.disallowForScreenReaders`\n* `browser.ctrlTab.previews`\n* `browser.customizemode.tip0.learnMoreUrl`\n* `browser.customizemode.tip0.shown`\n* `browser.defaultbrowser.notificationbar`\n* `browser.dictionaries.download.url`\n* `browser.display.background_color`\n* `browser.display.document_color_use`\n* `browser.display.focus_background_color`\n* `browser.display.focus_ring_on_anything`\n* `browser.display.focus_ring_style`\n* `browser.display.focus_text_color`\n* `browser.display.force_inline_alttext`\n* `browser.display.foreground_color`\n* `browser.display.normal_lineheight_calc_control`\n* `browser.display.show_image_placeholders`\n* `browser.display.show_loading_image_placeholder`\n* `browser.display.use_document_fonts`\n* `browser.display.use_focus_colors`\n* `browser.display.use_system_colors`\n* `browser.devedition.showCustomizeButton`\n* `browser.devedition.theme.enabled`\n* `browser.dictionaries.download.url`\n* `browser.disableResetPrompt`\n* `browser.display.document_color_use`\n* `browser.displayedE10SNotice`\n* `browser.displayedE10SPrompt.1`\n* `browser.dom.window.dump.enabled`\n* `browser.download.animateNotificatons`\n* `browser.download.folderList`\n* `browser.download.forbid_open_with`\n* `browser.download.hide_plugins_without_extensions`\n* `browser.download.importedFromSqlite`\n* `browser.download.lastDir`\n* `browser.download.loglevel`\n* `browser.download.manager.addToRecentDocs`\n* `browser.download.manager.resumeOnWakeDelay`\n* `browser.download.manager.showWhenStarting`\n* `browser.download.panel.firstSessionCompleted`\n* `browser.download.panel.shown`\n* `browser.download.progressDnldDialog.keepAlive`\n* `browser.download.saveLinkAsFilenameTimeout`\n* `browser.download.save_converter_index`\n* `browser.download.show_plugins_in_list`\n* `browser.download.useDownloadDir`\n* `browser.eme.ui.enabled`\n* `browser.eme.ui.firstContentShown`\n* `browser.enable_automatic_image_resizing`\n* `browser.enable_click_image_resizing`\n* `browser.esedbreader.loglevel`\n* `browser.EULA.version`\n* `browser.EULA.<version>.accepted`\n* `browser.feeds.handler`\n* `browser.feeds.handler.default`\n* `browser.feeds.handlers.application`\n* `browser.feeds.handlers.webservice`\n* `browser.feeds.showFirstRunUI`\n* `browser.fixup.alternate.enabled`\n* `browser.fixup.alternate.prefix`\n* `browser.fixup.alternate.suffix`\n* `browser.fixup.dns_first_for_single_words`\n* `browser.fixup.domainwhitelist.localhost`\n* `browser.fixup.hide_user_pass`\n* `browser.flash-protected-mode-flip.done`\n* `browser.flash-protected-mode-flip.enable`\n* `browser.formfill.adgedWeight`\n* `browser.formfill.boundaryWeight`\n* `browser.formfill.bucketSize`\n* `browser.formfill.debug`\n* `browser.formfill.enable`\n* `browser.formfill.expire_days`\n* `browser.formfill.maxTimeGroupings`\n* `browser.formfill.prefixWeight`\n* `browser.formfill.timeGroupingSize`\n* `browser.fullscreen.animateUp`\n* `browser.fullscreen.autohide`\n* `browser.geolocation.warning.infoURL`\n* `browser.gesture.pinch.in`\n* `browser.gesture.pinch.in.shift`\n* `browser.gesture.pinch.latched`\n* `browser.gesture.pinch.out`\n* `browser.gesture.pinch.out.shift`\n* `browser.gesture.pinch.threshold`\n* `browser.gesture.swipe.down`\n* `browser.gesture.swipe.left`\n* `browser.gesture.swipe.right`\n* `browser.gesture.swipe.up`\n* `browser.gesture.tap`\n* `browser.gesture.twist.end`\n* `browser.gesture.twist.latched`\n* `browser.gesture.twist.left`\n* `browser.gesture.twist.right`\n* `browser.gesture.twist.threshold`\n* `browser.hangNotification.waitPeriod`\n* `browser.helperApps.alwaysAsk.force`\n* `browser.helperApps.deleteTempFileOnExit`\n* `browser.helperApps.neverAsk.openFile`\n* `browser.helperApps.neverAsk.saveToDisk`\n* `browser.hiddenWindowChromeURL`\n* `browser.history.maxStateObjectSize`\n* `browser.laterrun.bookkeeping.profileCreationTime`\n* `browser.laterrun.bookkeeping.sessionCount`\n* `browser.laterrun.enabled`\n* `browser.laterrun.pages.<slug>`\n* `browser.link.open_newwindow`\n* `browser.link.open_newwindow.disabled_in_fullscreen`\n* `browser.link.open_newwindow.override.external`\n* `browser.link.open_newwindow.restriction`\n* `browser.menu.showCharacterEncoding`\n* `browser.meta_refresh_when_inactive.disabled`\n* `browser.migrate.automigrate.daysToOfferUndo`\n* `browser.migrate.automigrate.enabled`\n* `browser.migrate.automigrate.inpage.ui.enabled`\n* `browser.migrate.automigrate.ui.enabled`\n* `browser.migrate.chrome.history.limit`\n* `browser.migrate.chrome.history.maxAgeInDays`\n* `browser.migrated-sync-button`\n* `browser.migration.version`\n* `browser.newtab.preload`\n* `browser.newtabpage.activity-stream.default.sites`\n* `browser.newtabpage.activity-stream.enabled`\n* `browser.newtabpage.activity-stream.feeds.localization`\n* `browser.newtabpage.activity-stream.feeds.migration`\n* `browser.newtabpage.activity-stream.feeds.newtabinit`\n* `browser.newtabpage.activity-stream.feeds.places`\n* `browser.newtabpage.activity-stream.feeds.prefs`\n* `browser.newtabpage.activity-stream.feeds.section.topstories`\n* `browser.newtabpage.activity-stream.feeds.section.topstories.options`\n* `browser.newtabpage.activity-stream.feeds.snippets`\n* `browser.newtabpage.activity-stream.feeds.systemtick`\n* `browser.newtabpage.activity-stream.feeds.telemetry`\n* `browser.newtabpage.activity-stream.feeds.topsites`\n* `browser.newtabpage.activity-stream.feeds.migrationExpire`\n* `browser.newtabpage.activity-stream.feeds.migrationLastShownDate`\n* `browser.newtabpage.activity-stream.feeds.migrationRemainingDays`\n* `browser.newtabpage.activity-stream.feeds.showSearch`\n* `browser.newtabpage.activity-stream.feeds.showTopSites`\n* `browser.newtabpage.activity-stream.feeds.telemetry`\n* `browser.newtabpage.activity-stream.feeds.telemetry.log`\n* `browser.newtabpage.activity-stream.feeds.telemetry.ping.endpoint`\n* `browser.newtabpage.blocked`\n* `browser.newtabpage.columns`\n* `browser.newtabpage.compact`\n* `browser.newtabpage.directory.source`\n* `browser.newtabpage.enabled`\n* `browser.newtabpage.enhanced`\n* `browser.newtabpage.introShown`\n* `browser.newtabpage.rows`\n* `browser.newtabpage.storageVersion`\n* `browser.newtabpage.thumbnailPlaceholder`\n* `browser.newtabpage.updateIntroShown`\n* `browser.offline`\n* `browser.offline-apps.notify`\n* `browser.onboarding.enabled`\n* `browser.onboarding.hidden`\n* `browser.onboarding.newtour`\n* `browser.onboarding.notification.finished`\n* `browser.onboarding.notification.lastPrompted`\n* `browser.onboarding.notification.max-life-time-per-tour-ms`\n* `browser.onboarding.notification.max-prompt-count-per-tour`\n* `browser.onboarding.notification.mute-duration-on-first-session-ms`\n* `browser.onboarding.seen-tourset-version`\n* `browser.onboarding.tour-type`\n* `browser.onboarding.tour.onboarding-tour-*.completed`\n* `browser.onboarding.tourset-version`\n* `browser.onboarding.updatetour`\n* `browser.overlink-delay`\n* `browser.pageActions.persistedActions`\n* `browser.pagethumbnails.storage_version`\n* `browser.panorama.*`\n* `browser.photon.structure.enabled`\n* `browser.places.importBookmarksHTML`\n* `browser.places.smartBookmarksVersion`\n* `browser.places.useAsyncTransactions`\n* `browser.popups.showPopupBlocker`\n* `browser.preferences.advanced.selectedTabIndex`\n* `browser.preferences.defaultPerformanceSettings.enabled`\n* `browser.preferences.instantApply`\n* `browser.preferences.offlineGroup.enabled`\n* `browser.preferences.search`\n* `browser.preferences.useOldOrganization`\n* `browser.privatebrowsing.autostart`\n* `browser.reader.deletectedFirstArticle`\n* `browser.rights.<version>.shown`\n* `browser.rights.override`\n* `browser.rights.version`\n* `browser.safebrowsing.allowOverride`\n* `browser.safebrowsing.blockedURIs.enabled`\n* `browser.safebrowsing.downloads.enabled`\n* `browser.safebrowsing.downloads.remote.block_dangerous`\n* `browser.safebrowsing.downloads.remote.block_dangerous_host`\n* `browser.safebrowsing.downloads.remote.block_potentially_unwanted`\n* `browser.safebrowsing.downloads.remote.enabled`\n* `browser.safebrowsing.downloads.remote.timeout_ms`\n* `browser.safebrowsing.downloads.remote.url`\n* `browser.safebrowsing.enabled`\n* `browser.safebrowsing.id`\n* `browser.safebrowsing.malware.enabled`\n* `browser.safebrowsing.phishing.enabled`\n* `browser.safebrowsing.provider.{provider}.advisoryName`\n* `browser.safebrowsing.provider.{provider}.advistoryURL`\n* `browser.safebrowsing.provider.{provider}.gethashURL`\n* `browser.safebrowsing.provider.{provider}.lastupdatetime`\n* `browser.safebrowsing.provider.{provider}.lists`\n* `browser.safebrowsing.provider.{provider}.pver`\n* `browser.safebrowsing.provider.{provider}.reportMalwareMistakeURL`\n* `browser.safebrowsing.provider.{provider}.reportPhishMistakeURL`\n* `browser.safebrowsing.provider.{provider}.reportURL`\n* `browser.safebrowsing.provider.{provider}.updateURL`\n* `browser.safebrowsing.reportPhishURL`\n* `browser.search.context.loadInBackground`\n* `browser.search.countryCode`\n* `browser.search.defaultenginename`\n* `browser.search.defaultenginename.US`\n* `browser.search.geoip.timeout`\n* `browser.search.geoip.url`\n* `browser.search.geoSpecificDefaults`\n* `browser.search.geoSpecificDefaults.url`\n* `browser.search.hiddenOneOffs`\n* `browser.search.log`\n* `browser.search.openintab`\n* `browser.search.order.1`\n* `browser.search.order.2`\n* `browser.search.order.3`\n* `browser.search.order.US.1`\n* `browser.search.order.US.2`\n* `browser.search.order.US.3`\n* `browser.search.region`\n* `browser.search.reset.enabled`\n* `browser.search.reset.whitelist`\n* `browser.search.searchEnginesURL`\n* `browser.search.selectedEngine`\n* `browser.search.suggest.enabled`\n* `browser.search.widget.inNavBar`\n* `browser.search.update`\n* `browser.search.update.interval`\n* `browser.search.update.log`\n* `browser.search.widget.inNavBar`\n* `browser.selfsupport.enabled`\n* `browser.send_pings`\n* `browser.send_pings.max_per_link`\n* `browser.send_pings.require_same_hist`\n* `browser.sessionhistory.max_entries`\n* `browser.sessionhistory.max_total_viewers`\n* `browser.sessionstore.cleanup.forget_closed_after`\n* `browser.sessionstore.debug`\n* `browser.sessionstore.debug.no_auto_updates`\n* `browser.sessionstore.dom_storage_limit`\n* `browser.sessionstore.idleDelay`\n* `browser.sessionstore.interval`\n* `browser.sessionstore.interval.idle`\n* `browser.sessionstore.max_resumed_crashes`\n* `browser.sessionstore.max_serialize_back`\n* `browser.sessionstore.max_serialize_forward`\n* `browser.sessionstore.max_tabs_undo`\n* `browser.sessionstore.max_windows_undo`\n* `browser.sessionstore.privacy_level`\n* `browser.sessionstore.restore_hidden_tabs`\n* `browser.sessionstore.restore_on_demand`\n* `browser.sessionstore.restore_pinned_tabs_on_demand`\n* `browser.sessionstore.restore_tabs_lazily`\n* `browser.sessionstore.resume_from_crash`\n* `browser.sessionstore.resume_session_once`\n* `browser.sessionstore.upgradeBackup.latestBuildID`\n* `browser.sessionstore.upgradeBackup.maxUpgradeBackups`\n* `browser.shell.checkDefaultBrowser`\n* `browser.shell.defaultBrowserCheckCount`\n* `browser.shell.didSkipDefaultBrowserCheckOnFirstRun`\n* `browser.shell.mostRecentDateSetAsDefault`\n* `browser.shell.shortcutFavicons`\n* `browser.shell.skipDefaultBrowserCheckOnFirstRun`\n* `browser.showMenubar`\n* `browser.showPersonalToolbar`\n* `browser.showQuitWarning`\n* `browser.slowStartup.averageTime`\n* `browser.slowStartup.maxSamples`\n* `browser.slowStartup.notificationDisabled`\n* `browser.slowStartup.samples`\n* `browser.slowStartup.timeThreshold`\n* `browser.snapshots.limit`\n* `browser.ssl_override_behavior`\n* `browser.startup.firstrunSkipsHomepage`\n* `browser.startup.homepage`\n* `browser.startup.homepage_override.buildID`\n* `browser.startup.homepage_override.mstone`\n* `browser.startup.page`\n* `browser.stopReloadAnimation.enabled`\n* `browser.storageManager.enabled`\n* `browser.storageManager.pressureNotification.minIntervalMS`\n* `browser.storageManager.pressureNotification.usageThresholdGB`\n* `browser.suppress_first_window_animation`\n* `browser.syncPromoViewsLeftMap`\n* `browser.tabs.closeWindowWithLastTab`\n* `browser.tabs.crashReporting.email`\n* `browser.tabs.crashReporting.emailMe`\n* `browser.tabs.crashReporting.includeURL`\n* `browser.tabs.crashReporting.requestEmail`\n* `browser.tabs.crashReporting.sendReport`\n* `browser.tabs.delayHidingAudioPlayingIconMS`\n* `browser.tabs.drawInTitlebar`\n* `browser.tabs.insertRelatedAfterCurrent`\n* `browser.tabs.loadBookmarksInBackground`\n* `browser.tabs.loadDivertedInBackground`\n* `browser.tabs.loadInBackground`\n* `browser.tabs.maxOpenBeforeWarn`\n* `browser.tabs.opentabfor.middleclick`\n* `browser.tabs.remote.allowLinkedWebInFileUriProcess`\n* `browser.tabs.remote.autostart`\n* `browser.tabs.remote.autostart.1`\n* `browser.tabs.remote.autostart.2`\n* `browser.tabs.remote.desktopbehavior`\n* `browser.tabs.remote.force-enable`\n* `browser.tabs.remote.separateFileUriProcess`\n* `browser.tabs.restorebutton`\n* `browser.tabs.selectOwnerOnClose`\n* `browser.tabs.showAudioPlayingIcon`\n* `browser.tabs.tabClipWidth`\n* `browser.tabs.warnOnClose`\n* `browser.tabs.warnOnCloseOtherTabs`\n* `browser.tabs.warnOnOpen`\n* `browser.taskbar.previews.cachetime`\n* `browser.taskbar.previews.enable`\n* `browser.taskbar.previews.max`\n* `browser.taskbar.lists.enabled`\n* `browser.taskbar.lists.frequent.enabled`\n* `browser.taskbar.lists.maxListItemCount`\n* `browser.taskbar.lists.recent.enabled`\n* `browser.taskbar.lists.refreshInSeconds`\n* `browser.taskbar.lists.tasks.enabled`\n* `browser.toolbarbuttons.introduced.pocket-button`\n* `browser.touchmode.auto`\n* `browser.translation.detectLanguage`\n* `browser.translation.engine`\n* `browser.translation.neverForLanguages`\n* `browser.translation.ui.show`\n* `browser.triple_click_selects_paragraph`\n* `browser.uiCustomization.debug`\n* `browser.uiCustomization.state`\n* `browser.uidensity`\n* `browser.uitour.enabled`\n* `browser.uitour.loglevel`\n* `browser.uitour.requireSecure`\n* `browser.uitour.surveyDuration`\n* `browser.uitour.themeOrigin`\n* `browser.uitour.url`\n* `browser.underline_anchors`\n* `browser.urlbar.autocomplete.enabled`\n* `browser.urlbar.autoFill`\n* `browser.urlbar.autoFill.typed`\n* `browser.urlbar.clickSelectsAll`\n* `browser.urlbar.daysBeforeHidingSuggestionsPrompt`\n* `browser.urlbar.decodeURLsOnCopy`\n* `browser.altClickSave`\n* `browser.urlbar.default.behavior`\n* `browser.urlbar.delay`\n* `browser.urlbar.doubleClickSelectsAll`\n* `browser.urlbar.filter.javascript`\n* `browser.urlbar.formatting.enabled`\n* `browser.urlbar.lastSuggestionsPromptDate`\n* `browser.urlbar.matchBehavior`\n* `browser.urlbar.maxCharsForSearchSuggestions`\n* `browser.urlbar.maxRichResults`\n* `browser.urlbar.oneOffSearches`\n* `browser.urlbar.speculativeConnect.enable`\n* `browser.urlbar.suggest.bookmark`\n* `browser.urlbar.suggest.history`\n* `browser.urlbar.suggest.history.onlyTyped`\n* `browser.urlbar.suggest.openpage`\n* `browser.urlbar.suggest.searches`\n* `browser.urlbar.timesBeforeHidingSuggestionsHint`\n* `browser.urlbar.usepreloadedtopurls.enabled`\n* `browser.urlbar.usepreloadedtopurls.expire_days`\n* `browser.urlbar.userMadeSearchSuggestionsChoice`\n* `browser.videoFeeds.handler`\n* `browser.videoFeeds.handler.default`\n* `browser.videoFeeds.handlers.application`\n* `browser.videoFeeds.handlers.webservice`\n* `browser.visited_color`\n* `browser.warnOnQuit`\n* `browser.xul.error_pages.enabled`\n* `browser.xul.error_pages.expert_bad_cert`\n* `browser.zoom.full`\n* `browser.zoom.siteSpecific`\n* `browser.zoom.updateBackgroundTabs`\n* `camera.control.face_detection.enabled`\n* `canvas.capturestream.enabled`\n* `canvas.customfocusring.enabled`\n* `canvas.filters.enabled`\n* `canvas.focusring.enabled`\n* `canvas.hitregions.enabled`\n* `canvas.image.cache.limit`\n* `canvas.imagebitmap_extensions.enabled`\n* `canvas.path.enabled`\n* `capability.policy.policynames`\n* `capability.policy.<policy-name>.checkloaduri.enabled`\n* `capability.policy.<policy-name>.sites`\n* `captivedetect.canonicalContent`\n* `captivedetect.canonicalURL`\n* `captivedetect.maxRetryCount`\n* `captivedetect.maxWaitingTime`\n* `captivedetect.pollingTime`\n* `chrome.override_package.<*>`\n* `clipboard.autocopy`\n* `clipboard.plainTextOnly`\n* `content.sink.pending_event_mode`\n* `converter.html2txt.always_include_ruby`\n* `converter.html2txt.header_strategy`\n* `converter.html2txt.structs`\n* `datareporting.healthreport.abount.reportURI`\n* `datareporting.healthreport.infoURL`\n* `datareporting.healthreport.lastDataSubmissionFailureTime`\n* `datareporting.healthreport.lastDataSubmissionRequestedTime`\n* `datareporting.healthreport.lastDataSubmissionSuccessfulTime`\n* `datareporting.healthreport.nextDataSubmissionTime`\n* `datareporting.healthreport.service.firstRun`\n* `datareporting.healthreport.uploadEnabled`\n* `datareporting.policy.currentPolicyVersion`\n* `datareporting.policy.dataSubmissionEnabled`\n* `datareporting.policy.dataSubmissionPolicyNotifiedTime`\n* `datareporting.policy.firstRunTime`\n* `datareporting.policy.firstRunURL`\n* `datareporting.policy.minimumPolicyVersion`\n* `datareporting.sessions.current.activeTicks`\n* `datareporting.sessions.current.clean`\n* `datareporting.sessions.current.firstPaint`\n* `datareporting.sessions.current.main`\n* `datareporting.sessions.current.sessionRestored`\n* `datareporting.sessions.current.startTime`\n* `datareporting.sessions.current.totalTime`\n* `datareporting.sessions.currentIndex`\n* `datareporting.sessions.previous.<index>`\n* `datareporting.sessions.prunedIndex`\n* `device.sensors.enabled`\n* `device.storage.enabled`\n* `devtools.debugger.log`\n* `devtools.theme`\n* `distribution.id`\n* `distribution.testing.loadFromProfile`\n* `distribution.version`\n* `distribution.about`\n* `distribution.<id>.bookmarksProcessed`\n* `dom.debug.propagate_gesture_events_through_content`\n* `dom.disable_open_during_load`\n* `dom.disable_window_flip`\n* `dom.disable_window_move_resize`\n* `dom.disable_window_open_feature.location`\n* `dom.disable_window_status_change`\n* `dom.keyboardevent.dispatch_during_composition`\n* `dom.ipc.cpow.timeout`\n* `dom.ipc.cpows.allow-cpows-in-compat-addons`\n* `dom.ipc.cpows.forbid-cpows-in-compat-addons`\n* `dom.ipc.cpows.forbid-unsafe-from-browser`\n* `dom.ipc.plugins.flash.disable-protected-mode`\n* `dom.ipc.plugins.nativeCursorSupport`\n* `dom.ipc.plugins.sandbox-level.defaut`\n* `dom.ipc.plugins.sandbox-level.flash`\n* `dom.ipc.plugins.sandbox-level.flash`\n* `dom.ipc.processHangMonitor`\n* `dom.ipc.processPrelaunch.enabled`\n* `dom.ipc.plugins.reportCrashURL`\n* `dom.ipc.reportProcessHangs`\n* `dom.ipc.shims.enabledWarnings`\n* `dom.mozBrowserFramesEnabled`\n* `dom.push.enabled`\n* `dom.serviceWorkers.enabled`\n* `dom.serviceWorkers.openWindow.enabled`\n* `dom.w3c_touch_events.enabled`\n* `experiments.activeExperiment`\n* `experiments.enabled`\n* `experiments.manifest.fetchIntervalSeconds`\n* `experiments.manifest.uri`\n* `experiments.supported`\n* `extensions.addon-sdk.useBundledSDK`\n* `extensions.allow-non-mpc-extensions`\n* `extensions.autoDisableScopes`\n* `extensions.checkCompatibility.temporaryThemeOverride_minAppVersionURL`\n* `extensions.dss.switchPending`\n* `extensions.e10sBlocksEnabling`\n* `extensions.e10sMultiBlocksEnabling`\n* `extensions.formautofill.addresses.enabled`\n* `extensions.formautofill.experimental`\n* `extensions.formautofill.firstTimeUse`\n* `extensions.formautofill.heuristics.enabled`\n* `extensions.formautofill.loglevel`\n* `extensions.geckoProfiler.acceptedExtensionIds`\n* `extensions.geckoProfiler.getSymbolRules`\n* `extensions.geckoProfiler.symbols.url`\n* `extensions.getAddons.cache.enabled`\n* `extensions.getAddons.get.url`\n* `extensions.getAddons.getWithPerformance.url`\n* `extensions.getAddons.link.url`\n* `extensions.getAddons.maxResults`\n* `extensions.getAddons.recommended.url`\n* `extensions.getAddons.search.browseURL`\n* `extensions.getAddons.search.url`\n* `extensions.getAddons.themes.browseURL`\n* `extensions.hotfix.cert.checkAttributes`\n* `extensions.hotfix.certs.1.sha1Fingerprint`\n* `extensions.hotfix.certs.2.sha1Fingerprint`\n* `extensions.hotfix.id`\n* `extensions.interposition.enabled`\n* `extensions.interposition.prefetching`\n* `extensions.legacy.exceptions`\n* `extensions.logging.enabled`\n* `extensions.minCompatibleAppVersion`\n* `extensions.modules.<id>.path`\n* `extensions.pocket.enabled`\n* `extensions.screenshots.disabled`\n* `extensions.screenshots.system-disabled`\n* `extensions.startupScanScopes`\n* `extensions.strictCompatibility`\n* `extensions.systemAddon.update.url`\n* `extensions.ui.ignoreUnsigned`\n* `extensions.update.autoUpdateDefault`\n* `extensions.update.background.url`\n* `extensions.update.enabled`\n* `extensions.update.interval`\n* `extensions.update.url`\n* `extensions.webapi.testing`\n* `extensions.webcompat-reporter.enabled`\n* `extensions.webextensions.base-content-security-polic`\n* `extensions.webextensions.default-content-security-policy`\n* `extensions.webextensions.remote`\n* `extensions.webextensions.themes.enabled`\n* `extensions.webextensions.themes.icons.buttons`\n* `extensions.webextOptionalPermissionPrompts`\n* `extensions.webextPermissionPrompts`\n* `extensions.webservice.discover`\n* `extensions.<id>.name`\n* `extensions.<id>.description`\n* `font.language.group`\n* `full-screen-api.enabled`\n* `full-screen-api.warning.delay`\n* `full-screen-api.warning.timeout`\n* `gecko.handlerService.defaultHandlersVersion`\n* `gecko.handlerService.schemes.irc.0.name`\n* `gecko.handlerService.schemes.irc.0.uriTemplate`\n* `gecko.handlerService.schemes.irc.1.name`\n* `gecko.handlerService.schemes.irc.1.uriTemplate`\n* `gecko.handlerService.schemes.irc.2.name`\n* `gecko.handlerService.schemes.irc.2.uriTemplate`\n* `gecko.handlerService.schemes.irc.3.name`\n* `gecko.handlerService.schemes.irc.3.uriTemplate`\n* `gecko.handlerService.schemes.ircs.0.name`\n* `gecko.handlerService.schemes.ircs.0.uriTemplate`\n* `gecko.handlerService.schemes.ircs.1.name`\n* `gecko.handlerService.schemes.ircs.1.uriTemplate`\n* `gecko.handlerService.schemes.ircs.2.name`\n* `gecko.handlerService.schemes.ircs.2.uriTemplate`\n* `gecko.handlerService.schemes.ircs.3.name`\n* `gecko.handlerService.schemes.ircs.3.uriTemplate`\n* `gecko.handlerService.schemes.mailto.0.name`\n* `gecko.handlerService.schemes.mailto.0.uriTemplate`\n* `gecko.handlerService.schemes.mailto.1.name`\n* `gecko.handlerService.schemes.mailto.1.uriTemplate`\n* `gecko.handlerService.schemes.mailto.2.name`\n* `gecko.handlerService.schemes.mailto.2.uriTemplate`\n* `gecko.handlerService.schemes.mailto.3.name`\n* `gecko.handlerService.schemes.mailto.3.uriTemplate`\n* `gecko.handlerService.schemes.webcal.0.name`\n* `gecko.handlerService.schemes.webcal.0.uriTemplate`\n* `gecko.handlerService.schemes.webcal.1.name`\n* `gecko.handlerService.schemes.webcal.1.uriTemplate`\n* `gecko.handlerService.schemes.webcal.2.name`\n* `gecko.handlerService.schemes.webcal.2.uriTemplate`\n* `gecko.handlerService.schemes.webcal.3.name`\n* `gecko.handlerService.schemes.webcal.3.uriTemplate`\n* `general.autoScroll`\n* `general.skins.selectedSkin`\n* `general.smoothScroll`\n* `general.useragent.locale`\n* `general.warnOnAboutConfig`\n* `geo.provider.ms-windows-location`\n* `geo.provider.use_corelocation`\n* `geo.provider.use_gpsd`\n* `geo.wifi.uri`\n* `gfx.blacklist.suggested-driver-version`\n* `gfx.blacklist.direct2d`\n* `gfx.blacklist.layers.direct3d9`\n* `gfx.blacklist.layers.direct3d10`\n* `gfx.blacklist.layers.direct3d10-1`\n* `gfx.blacklist.layers.direct3d11`\n* `gfx.blacklist.direct3d11angle`\n* `gfx.blacklist.hardwarevideodecoding`\n* `gfx.blacklist.layers.opengl`\n* `gfx.blacklist.webgl.opengl`\n* `gfx.blacklist.webgl.angle`\n* `gfx.blacklist.webgl.msaa`\n* `gfx.blacklist.stagefright`\n* `gfx.blacklist.webrtc.hw.acceleration`\n* `gfx.blacklist.webrtc.hw.acceleration.encode`\n* `gfx.blacklist.webrtc.hw.acceleration.decode`\n* `gfx.blacklist.canvas2d.acceleration`\n* `gfx.blacklist.webgl2`\n* `gfx.blacklist.layers.advanced`\n* `gfx.blacklist.d3d11.keyed.mutex`\n* `gfx.blacklist.<feature>.failureid`\n* `identity.fxaccounts.contextParam`\n* `identity.fxaccounts.migrateToDevEdition`\n* `identity.fxaccounts.remote.force_auth.uri`\n* `identity.fxaccounts.remote.oauth.uri`\n* `identity.fxaccounts.remote.profile.uri`\n* `identity.fxaccounts.remote.signin.uri`\n* `identity.fxaccounts.remote.signup.uri`\n* `identity.fxaccounts.remote.webchannel.uri`\n* `identity.fxaccounts.settings.devices.uri`\n* `identity.fxaccounts.settings.uri`\n* `identity.mobilepromo.android`\n* `identity.mobilepromo.ios`\n* `identity.sync.tokenserver.uri`\n* `idle.lastDailyNotification`\n* `image.http.accept`\n* `image.mem.max_decoded_image_kb`\n* `intl.charset.detector`\n* `intl.uidirection`\n* `javascript.enabled`\n* `javascript.options.showInConsole`\n* `keyword.enabled`\n* `layout.css.devPixelsPerPx`\n* `layout.show_previous_page`\n* `layout.spellcheckDefault`\n* `lightweightThemes.getMoreURL`\n* `lightweightThemes.recommendedThemes`\n* `lightweightThemes.selectedThemeID`\n* `lightweightThemes.update.enabled`\n* `media.eme.enabled`\n* `media.eme.vp9-in-mp4.enabled`\n* `media.gmp-provider.enabled`\n* `media.gmp.trial-create.enabled`\n* `media.gmp-widevinecdm.enabled`\n* `media.gmp-widevinecdm.visible`\n* `media.webspeech.synth.enabled`\n* `middlemouse.contentLoadURL`\n* `mousewheel.with_alt.action`\n* `mousewheel.with_control.action`\n* `mousewheel.with_control.action.override_x`\n* `mousewheel.with_meta.action`\n* `mousewheel.with_meta.action.override_x`\n* `mousewheel.with_shift.action`\n* `mousewheel.with_win.action`\n* `mozilla.widget.disable-native-theme`\n* `network.captive-portal-service.enabled`\n* `network.disable.ipc.security`\n* `network.manage-offline-status`\n* `network.cookie.cookieBehavior`\n* `network.cookie.lifetimePolicy`\n* `network.protocol-handler.expose-all`\n* `network.protocol-handler.expose.mailto`\n* `network.protocol-handler.expose.news`\n* `network.protocol-handler.expose.nntp`\n* `network.protocol-handler.expose.snews`\n* `network.protocol-handler.external.mailto`\n* `network.protocol-handler.external.ms-windows-store`\n* `network.protocol-handler.external.news`\n* `network.protocol-handler.external.nntp`\n* `network.protocol-handler.external.snews`\n* `network.protocol-handler.warn-external.mailto`\n* `network.protocol-handler.warn-external.ms-windows-store`\n* `network.protocol-handler.warn-external.news`\n* `network.protocol-handler.warn-external.nntp`\n* `network.protocol-handler.warn-external.snews`\n* `network.proxy.share_proxy_settings`\n* `nglayout.debug.crossing_event_dumping`\n* `nglayout.enable_drag_images`\n* `nglayout.debug.event_dumping`\n* `nglayout.debug.invalidate_dumping`\n* `nglayout.debug.motion_event_dumping`\n* `nglayout.debug.paint_dumping`\n* `nglayout.debug.paint_flashing`\n* `pdfium.enabled`\n* `pdfjs.disabled`\n* `pdfjs.firstRun`\n* `pdfjs.previousHandler.alwaysAskBeforeHandling`\n* `pdfjs.previousHandler.preferredAction`\n* `permissions.default.image`\n* `permissions.manager.defaultsUrl`\n* `places.frecency.bookmarkVisitBonus`\n* `places.frecency.defaultBucketWeight`\n* `places.frecency.defaultVisitBonus`\n* `places.frecency.downloadVisitBonus`\n* `places.frecency.embedVisitBonus`\n* `places.frecency.firstBucketCutoff`\n* `places.frecency.firstBucketWeight`\n* `places.frecency.fourthBucketCutoff`\n* `places.frecency.fourthBucketWeight`\n* `places.frecency.framedLinkVisitBonus`\n* `places.frecency.linkVisitBonus`\n* `places.frecency.numVisits`\n* `places.frecency.permRedirectVisitBonus`\n* `places.frecency.redirectSourceVisitBonus`\n* `places.frecency.reloadVisitBonus`\n* `places.frecency.secondBucketCutoff`\n* `places.frecency.secondBucketWeight`\n* `places.frecency.tempRedirectVisitBonus`\n* `places.frecency.thirdBucketCutoff`\n* `places.frecency.thirdBucketWeight`\n* `places.frecency.typedVisitBonus`\n* `places.frecency.unvisitedBookmarkBonus`\n* `places.frecency.unvisitedTypedBonus`\n* `places.history.enabled`\n* `plain_text.wrap_long_lines`\n* `plugin.defaultXpi.state`\n* `plugin.default.state`\n* `plugin.state.flash`\n* `plugin.state.java`\n* `plugins.click_to_play`\n* `plugins.favorfallback.mode`\n* `plugins.favorfallback.rules`\n* `plugins.flashBlock.enabled`\n* `plugins.remember_infobar_dismissal`\n* `plugins.show_infobar`\n* `plugins.testmode`\n* `pointer-lock-api.warning.timeout`\n* `print.use_simplify_page`\n* `print.printer_<name>.print_margin_top`\n* `print.printer_<name>.print_margin_left`\n* `print.printer_<name>.print_margin_bottom`\n* `print.printer_<name>.print_margin_right`\n* `print.printer_<name>.print_edge_top`\n* `print.printer_<name>.print_edge_left`\n* `print.printer_<name>.print_edge_bottom`\n* `print.printer_<name>.print_edge_right`\n* `print.printer_<name>.print_unwriteable_margin_top`\n* `print.printer_<name>.print_unwriteable_margin_left`\n* `print.printer_<name>.print_unwriteable_margin_bottom`\n* `print.printer_<name>.print_unwriteable_margin_right`\n* `print.printer_<name>.print_evenpages`\n* `print.printer_<name>.print_oddpages`\n* `print.printer_<name>.print_headerleft`\n* `print.printer_<name>.print_headercenter`\n* `print.printer_<name>.print_headerright`\n* `print.printer_<name>.print_footercenter`\n* `print.printer_<name>.print_footerright`\n* `print.printer_<name>.print_reversed`\n* `print.printer_<name>.print_in_color`\n* `print.printer_<name>.print_paper_name`\n* `print.printer_<name>.print_paper_data`\n* `print.printer_<name>.print_paper_size_unit`\n* `print.printer_<name>.print_paper_width`\n* `print.printer_<name>.print_paper_height`\n* `print.printer_<name>.print_orientation`\n* `print.printer_<name>.print_printer`\n* `print.printer_<name>.print_to_file`\n* `print.printer_<name>.print_to_filename`\n* `print.printer_<name>.print_bgcolor`\n* `print.printer_<name>.print_bgimages`\n* `print.printer_<name>.print_shrink_to_fit`\n* `print.printer_<name>.print_scaling`\n* `print.printer_<name>.print_resolution`\n* `print.printer_<name>.print_duplex`\n* `privacy.clearOnShutdown.cache`\n* `privacy.clearOnShutdown.cookies`\n* `privacy.clearOnShutdown.downloads`\n* `privacy.clearOnShutdown.formdata`\n* `privacy.clearOnShutdown.history`\n* `privacy.clearOnShutdown.offlineApps`\n* `privacy.clearOnShutdown.openWindows`\n* `privacy.clearOnShutdown.sessions`\n* `privacy.clearOnShutdown.siteSettings`\n* `privacy.cpd.cache`\n* `privacy.cpd.cookies`\n* `privacy.cpd.downloads`\n* `privacy.cpd.formdata`\n* `privacy.cpd.history`\n* `privacy.cpd.offlineApps`\n* `privacy.cpd.openWindows`\n* `privacy.cpd.passwords`\n* `privacy.cpd.sessions`\n* `privacy.cpd.siteSettings`\n* `privacy.donottrackheader.enabled`\n* `privacy.donottrackheader.value`\n* `privacy.firstparty.isolate`\n* `privacy.firstparty.isolate.restrict_opener_access`\n* `privacy.history.custom`\n* `privacy.item.cookies`\n* `privacy.panicButton.enabled`\n* `privacy.permissionPrompts.showCloseButton`\n* `privacy.popups.policy`\n* `privacy.popups.showBrowserMessage`\n* `privacy.popups.usecustom`\n* `privacy.resistFingerprinting`\n* `privacy.sanitize.migrateFx3Prefs`\n* `privacy.sanitize.sanitizeOnShutdown`\n* `privacy.sanitize.timeSpan`\n* `privacy.temporary_permission_expire_time_ms`\n* `privacy.trackingprotection.introCount`\n* `privacy.trackingprotection.introURL`\n* `privacy.trackingprotection.ui.enabled`\n* `privacy.usercontext.about_newtab_segregation.enabled`\n* `privacy.userContext.enabled`\n* `privacy.userContext.longPressBehavior\"`\n* `privacy.userContext.ui.enabled`\n* `prompts.tab_modal.enabled`\n* `reader.errors.includeURLs`\n* `reader.parse-node-limit`\n* `security.allow_chrome_frames_inside_content`\n* `security.alternate_certificate_error_page`\n* `security.cert_pinning.enforcement_level`\n* `security.csp.enable`\n* `security.dialog_enable_delay`\n* `security.fileuri.strict_origin_policy`\n* `security.insecure_field_warning.contextual.enabled`\n* `security.insecure_password.ui.enabled`\n* `security.mixed_content.block_active_content`\n* `security.sandbox.content.level`\n* `security.sandbox.content.read_path_whitelist`\n* `security.sandbox.content.syscall_whitelist`\n* `security.sandbox.content.tempDirSuffix`\n* `security.sandbox.content.write_path_whitelist`\n* `security.sandbox.gpu.level`\n* `security.sandbox.logging.enabled`\n* `security.sandbox.windows.log.stackTraceDepth`\n* `security.view-source.reachable-from-inner-protocol`\n* `services.sync.autoconnectDelay`\n* `services.sync.username`\n* `services.sync.clients.devices.<desktop/mobile>`\n* `services.sync.prefs.sync.accessibility.blockautorefresh`\n* `services.sync.prefs.sync.accessibility.browsewithcaret`\n* `services.sync.prefs.sync.accessibility.typeaheadfind`\n* `services.sync.prefs.sync.accessibility.typeaheadfind.linksonly`\n* `services.sync.prefs.sync.addons.ignoreUserEnabledChanges`\n* `services.sync.prefs.sync.browser.ctrlTab.previews`\n* `services.sync.prefs.sync.browser.download.useDownloadDir`\n* `services.sync.prefs.sync.browser.formfill.enable`\n* `services.sync.prefs.sync.browser.link.open_newwindow`\n* `services.sync.prefs.sync.browser.newtabpage.enabled`\n* `services.sync.prefs.sync.browser.newtabpage.enhanced`\n* `services.sync.prefs.sync.browser.newtabpage.pinned`\n* `services.sync.prefs.sync.browser.offline-apps.notify`\n* `services.sync.prefs.sync.browser.safebrowsing.phishing.enabled`\n* `services.sync.prefs.sync.browser.safebrowsing.malware.enabled`\n* `services.sync.prefs.sync.browser.search.update`\n* `services.sync.prefs.sync.browser.sessionstore.restore_on_demand`\n* `services.sync.prefs.sync.browser.startup.homepage`\n* `services.sync.prefs.sync.browser.startup.page`\n* `services.sync.prefs.sync.browser.tabs.loadInBackground`\n* `services.sync.prefs.sync.browser.tabs.warnOnClose`\n* `services.sync.prefs.sync.browser.tabs.warnOnOpen`\n* `services.sync.prefs.sync.browser.urlbar.autocomplete.enabled`\n* `services.sync.prefs.sync.browser.urlbar.maxRichResults`\n* `services.sync.prefs.sync.browser.urlbar.suggest.bookmark`\n* `services.sync.prefs.sync.browser.urlbar.suggest.history`\n* `services.sync.prefs.sync.browser.urlbar.suggest.history.onlyTyped`\n* `services.sync.prefs.sync.browser.urlbar.suggest.openpage`\n* `services.sync.prefs.sync.browser.urlbar.suggest.searches`\n* `services.sync.prefs.sync.dom.disable_open_during_load`\n* `services.sync.prefs.sync.dom.disable_window_flip`\n* `services.sync.prefs.sync.dom.disable_window_move_resize`\n* `services.sync.prefs.sync.dom.event.contextmenu.enabled`\n* `services.sync.prefs.sync.extensions.personas.current`\n* `services.sync.prefs.sync.extensions.update.enabled`\n* `services.sync.prefs.sync.intl.accept_languages`\n* `services.sync.prefs.sync.layout.spellcheckDefault`\n* `services.sync.prefs.sync.lightweightThemes.selectedThemeID\"`\n* `services.sync.prefs.sync.lightweightThemes.usedThemes`\n* `services.sync.prefs.sync.network.cookie.cookieBehavior`\n* `services.sync.prefs.sync.network.cookie.lifetimePolicy`\n* `services.sync.prefs.sync.network.cookie.lifetime.days`\n* `services.sync.prefs.sync.network.cookie.thirdparty.sessionOnly`\n* `services.sync.prefs.sync.permissions.default.image`\n* `services.sync.prefs.sync.pref.advanced.images.disable_button.view_image`\n* `services.sync.prefs.sync.pref.advanced.javascript.disable_button.advanced`\n* `services.sync.prefs.sync.pref.downloads.disable_button.edit_actions`\n* `services.sync.prefs.sync.pref.privacy.disable_button.cookie_exceptions`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.cache`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.cookies`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.downloads`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.formdata`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.history`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.offlineApps`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.sessions`\n* `services.sync.prefs.sync.privacy.clearOnShutdown.siteSettings`\n* `services.sync.prefs.sync.privacy.donottrackheader.enabled`\n* `services.sync.prefs.sync.privacy.sanitize.sanitizeOnShutdown`\n* `services.sync.prefs.sync.privacy.trackingprotection.enabled`\n* `services.sync.prefs.sync.privacy.trackingprotection.pbmode.enabled`\n* `services.sync.prefs.sync.security.OCSP.enabled`\n* `services.sync.prefs.sync.security.OCSP.require`\n* `services.sync.prefs.sync.security.default_personal_cert`\n* `services.sync.prefs.sync.security.tls.version.min`\n* `services.sync.prefs.sync.security.tls.version.max`\n* `services.sync.prefs.sync.services.sync.syncedTabs.showRemoteIcons`\n* `services.sync.prefs.sync.signon.rememberSignons`\n* `services.sync.prefs.sync.spellchecker.dictionary`\n* `services.sync.prefs.sync.xpinstall.whitelist.required`\n* `services.sync.syncedTabs.showRemoteIcons`\n* `sidebar.position_start`\n* `signed.applets.codebase_principal_support`\n* `signon.schemeUpgrades`\n* `social.directories`\n* `social.share.activationPanelEnabled`\n* `social.shareDirectory`\n* `startup.homepage_override_url`\n* `startup.homepage_welcome_url`\n* `startup.homepage_welcome_url.additional`\n* `storage.nfs_filesystem`\n* `storage.vacuum.last.<filename>`\n* `storage.vacuum.last.index`\n* `toolkit.cosmeticAnimations.enabled`\n* `toolkit.crashreporter.infoURL`\n* `toolkit.datacollection.infoURL`\n* `toolkit.pageThumbs.minHeight`\n* `toolkit.pageThumbs.minWidth`\n* `toolkit.startup.max_resumed_crashes`\n* `toolkit.storage.pageSize`\n* `toolkit.storage.synchronous`\n* `toolkit.telemetry.archive.enabled`\n* `toolkit.telemetry.enabled`\n* `toolkit.telemetry.newProfilePing.enabled`\n* `toolkit.telemetry.shutdownPingSender.enabled`\n* `ui.click_hold_context_menus.delay`\n* `ui.caretBlinkTime`\n* `ui.caretWidth`\n* `ui.caretVisibleWithSelection`\n* `ui.submenuDelay`\n* `ui.dragThresholdX`\n* `ui.dragThresholdY`\n* `ui.useAccessibilityTheme`\n* `ui.menusCanOverlapOSBar`\n* `ui.useOverlayScrollbars`\n* `ui.scrollbarDisplayOnMouseMove`\n* `ui.scrollbarFadeBeginDelay`\n* `ui.scrollbarFadeDuration`\n* `ui.showHideScrollbars`\n* `ui.skipNavigatingDisabledMenuItem`\n* `ui.treeOpenDelay`\n* `ui.treeCloseDelay`\n* `ui.treeLazyScrollDelay`\n* `ui.treeScrollDelay`\n* `ui.treeScrollLinesMax`\n* `accessibility.tabfocus`\n* `ui.alertNotificationOrigin`\n* `ui.scrollToClick`\n* `ui.IMERawInputUnderlineStyle`\n* `ui.IMESelectedRawTextUnderlineStyle`\n* `ui.IMEConvertedTextUnderlineStyle`\n* `ui.IMESelectedConvertedTextUnderlineStyle`\n* `ui.SpellCheckerUnderlineStyle`\n* `ui.scrollbarButtonAutoRepeatBehavior`\n* `ui.tooltipDelay`\n* `ui.physicalHomeButton`\n* `ui.contextMenuOffsetVertical`\n* `ui.contextMenuOffsetHorizontal`\n* `ui.windowBackground`\n* `ui.windowForeground`\n* `ui.widgetBackground`\n* `ui.widgetForeground`\n* `ui.widgetSelectBackground`\n* `ui.widgetSelectForeground`\n* `ui.widget3DHighlight`\n* `ui.widget3DShadow`\n* `ui.textBackground`\n* `ui.textForeground`\n* `ui.textSelectBackground`\n* `ui.textSelectForeground`\n* `ui.textSelectForegroundCustom`\n* `ui.textSelectBackgroundDisabled`\n* `ui.textSelectBackgroundAttention`\n* `ui.textHighlightBackground`\n* `ui.textHighlightForeground`\n* `ui.IMERawInputBackground`\n* `ui.IMERawInputForeground`\n* `ui.IMERawInputUnderline`\n* `ui.IMESelectedRawTextBackground`\n* `ui.IMESelectedRawTextForeground`\n* `ui.IMESelectedRawTextUnderlin`\n* `ui.IMEConvertedTextBackground`\n* `ui.IMEConvertedTextForeground`\n* `ui.IMEConvertedTextUnderline`\n* `ui.IMESelectedConvertedTextBackground`\n* `ui.IMESelectedConvertedTextForeground`\n* `ui.IMESelectedConvertedTextUnderline`\n* `ui.SpellCheckerUnderline`\n* `ui.activeborder`\n* `ui.activecaption`\n* `ui.appworkspace`\n* `ui.background`\n* `ui.buttonface`\n* `ui.buttonhighlight`\n* `ui.buttonshadow`\n* `ui.buttontext`\n* `ui.captiontext`\n* `ui.graytext`\n* `ui.highlight`\n* `ui.highlighttext`\n* `ui.inactiveborder`\n* `ui.inactivecaption`\n* `ui.inactivecaptiontext\"`\n* `ui.infobackground`\n* `ui.infotext`\n* `ui.menu`\n* `ui.menutext`\n* `ui.scrollbar`\n* `ui.threeddarkshadow`\n* `ui.threedface`\n* `ui.threedhighlight`\n* `ui.threedlightshadow`\n* `ui.threedshadow`\n* `ui.window`\n* `ui.windowframe`\n* `ui.windowtext`\n* `ui.-moz-buttondefault`\n* `ui.-moz-field`\n* `ui.-moz-fieldtext`\n* `ui.-moz-dialog`\n* `ui.-moz-dialogtext`\n* `ui.-moz-dragtargetzone`\n* `ui.-moz-cellhighlight`\n* `ui.-moz_cellhighlighttext`\n* `ui.-moz-html-cellhighlight`\n* `ui.-moz-html-cellhighlighttext`\n* `ui.-moz-buttonhoverface`\n* `ui.-moz_buttonhovertext`\n* `ui.-moz_menuhover`\n* `ui.-moz_menuhovertext`\n* `ui.-moz_menubartext`\n* `ui.-moz_menubarhovertext`\n* `ui.-moz_eventreerow`\n* `ui.-moz_oddtreerow`\n* `ui.-moz-mac-buttonactivetext`\n* `ui.-moz_mac_chrome_active`\n* `ui.-moz_mac_chrome_inactive`\n* `ui.-moz-mac-defaultbuttontext`\n* `ui.-moz-mac-focusring`\n* `ui.-moz-mac-menuselect`\n* `ui.-moz-mac-menushadow`\n* `ui.-moz-mac-menutextdisable`\n* `ui.-moz-mac-menutextselect`\n* `ui.-moz_mac_disabledtoolbartext`\n* `ui.-moz-mac-secondaryhighlight`\n* `ui.-moz-win-mediatext`\n* `ui.-moz-win-communicationstext`\n* `ui.-moz-nativehyperlinktext`\n* `ui.-moz-comboboxtext`\n* `ui.-moz-combobox`\n* `ui.key.accelKey`\n* `ui.key.chromeAccess`\n* `ui.key.contentAccess`\n* `ui.key.generalAccessKey`\n* `ui.key.menuAccessKey`\n* `ui.key.menuAccessKeyFocuses`\n* `urlclassifier.malwareTable`\n* `urlclassifier.phishTable`\n* `view_source.tab`\n* `webchannel.allowObject.urlWhitelist`\n* `xpinstall.customConfirmationUI`\n* `xpinstall.enabled`\n* `xpinstall.signatures.required`\n* `xpinstall.signatures.devInfoURL`\n* `xpinstall.whitelist.required`\n\n\n## Appendix 2 - Synced Datatypes visualisation\n\n[https://docs.google.com/spreadsheets/d/1k9_K7Dc3q2h3SDV0vwjTgJou-ndza6WuobyJ1bbemtc/edit?ts=5977ab9d#gid=1269587388](https://docs.google.com/spreadsheets/d/1k9_K7Dc3q2h3SDV0vwjTgJou-ndza6WuobyJ1bbemtc/edit?ts=5977ab9d#gid=1269587388)\n\n\n"
},
{
  "name": "participation-metrics-org",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# participation-metrics-org\n## Planning repo for participation metrics\n![Participate](https://wiki.mozilla.org/images/d/d0/ParticipationSmall.png)\n\nThis repository is used for tracking [issues](https://github.com/mozilla/participation-metrics-org/issues) and feedback related to the work of the [Participation/Contribution Metrics Project](https://wiki.mozilla.org/ParticipationSystems/Participation_Metrics#Contribution_metrics:).\n\nThe aim has been to implement a tool that aggregates contribution/participation metrics from a [variety of data sources](https://docs.google.com/document/d/1cO6l80lJugqaQh--FrdOt0MQ3N07mGgsK_CHRlQnB0c/edit#) across Mozilla into one place. This is the first step in getting to a set of team-level and executive dashboards on contribution/participation.\n\nThe goals are to:\n\na) Create a single place where this type of data can be accessed easily, rather than needing to go into multiple locations\n\nb) Have a tool that is useful for both operational realities (e.g. finding actionable insights, setting up tests), and also for tracking overall performance at a group or executive level\n\nc) Be able to run analysis across different data sources, removing duplicates and asking about staff/non-staff contribution. Basically getting more sophisticated about tracking contributor \"journeys\" and impact through Mozilla\n"
},
{
  "name": "firefox-test-engineering",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Makefile",
      "README.rst",
      "conf.py",
      "guide",
      "images",
      "index.rst",
      "reference",
      "requirements.txt",
      "resources"
    ]
  },
  "makefile": "# Makefile for Sphinx documentation\n#\n\n# You can set these variables from the command line.\nSPHINXOPTS    =\nSPHINXBUILD   = sphinx-build\nPAPER         =\nBUILDDIR      = _build\n\n# Internal variables.\nPAPEROPT_a4     = -D latex_paper_size=a4\nPAPEROPT_letter = -D latex_paper_size=letter\nALLSPHINXOPTS   = -d $(BUILDDIR)/doctrees $(PAPEROPT_$(PAPER)) $(SPHINXOPTS) .\n\n.PHONY: help clean html dirhtml singlehtml pickle json htmlhelp qthelp devhelp epub latex latexpdf text man changes linkcheck doctest livehtml\n\nhelp:\n\t@echo \"Please use \\`make <target>' where <target> is one of\"\n\t@echo \"  html       to make standalone HTML files\"\n\t@echo \"  dirhtml    to make HTML files named index.html in directories\"\n\t@echo \"  singlehtml to make a single large HTML file\"\n\t@echo \"  pickle     to make pickle files\"\n\t@echo \"  json       to make JSON files\"\n\t@echo \"  htmlhelp   to make HTML files and a HTML help project\"\n\t@echo \"  qthelp     to make HTML files and a qthelp project\"\n\t@echo \"  devhelp    to make HTML files and a Devhelp project\"\n\t@echo \"  epub       to make an epub\"\n\t@echo \"  latex      to make LaTeX files, you can set PAPER=a4 or PAPER=letter\"\n\t@echo \"  latexpdf   to make LaTeX files and run them through pdflatex\"\n\t@echo \"  text       to make text files\"\n\t@echo \"  man        to make manual pages\"\n\t@echo \"  changes    to make an overview of all changed/added/deprecated items\"\n\t@echo \"  linkcheck  to check all external links for integrity\"\n\t@echo \"  doctest    to run all doctests embedded in the documentation (if enabled)\"\n\nclean:\n\t-rm -rf $(BUILDDIR)/*\n\nhtml:\n\t$(SPHINXBUILD) -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html\n\t@echo\n\t@echo \"Build finished. The HTML pages are in $(BUILDDIR)/html.\"\n\ndirhtml:\n\t$(SPHINXBUILD) -b dirhtml $(ALLSPHINXOPTS) $(BUILDDIR)/dirhtml\n\t@echo\n\t@echo \"Build finished. The HTML pages are in $(BUILDDIR)/dirhtml.\"\n\nsinglehtml:\n\t$(SPHINXBUILD) -b singlehtml $(ALLSPHINXOPTS) $(BUILDDIR)/singlehtml\n\t@echo\n\t@echo \"Build finished. The HTML page is in $(BUILDDIR)/singlehtml.\"\n\npickle:\n\t$(SPHINXBUILD) -b pickle $(ALLSPHINXOPTS) $(BUILDDIR)/pickle\n\t@echo\n\t@echo \"Build finished; now you can process the pickle files.\"\n\njson:\n\t$(SPHINXBUILD) -b json $(ALLSPHINXOPTS) $(BUILDDIR)/json\n\t@echo\n\t@echo \"Build finished; now you can process the JSON files.\"\n\nhtmlhelp:\n\t$(SPHINXBUILD) -b htmlhelp $(ALLSPHINXOPTS) $(BUILDDIR)/htmlhelp\n\t@echo\n\t@echo \"Build finished; now you can run HTML Help Workshop with the\" \\\n\t      \".hhp project file in $(BUILDDIR)/htmlhelp.\"\n\nqthelp:\n\t$(SPHINXBUILD) -b qthelp $(ALLSPHINXOPTS) $(BUILDDIR)/qthelp\n\t@echo\n\t@echo \"Build finished; now you can run \"qcollectiongenerator\" with the\" \\\n\t      \".qhcp project file in $(BUILDDIR)/qthelp, like this:\"\n\t@echo \"# qcollectiongenerator $(BUILDDIR)/qthelp/WebdevBootcamp.qhcp\"\n\t@echo \"To view the help file:\"\n\t@echo \"# assistant -collectionFile $(BUILDDIR)/qthelp/WebdevBootcamp.qhc\"\n\ndevhelp:\n\t$(SPHINXBUILD) -b devhelp $(ALLSPHINXOPTS) $(BUILDDIR)/devhelp\n\t@echo\n\t@echo \"Build finished.\"\n\t@echo \"To view the help file:\"\n\t@echo \"# mkdir -p $$HOME/.local/share/devhelp/WebdevBootcamp\"\n\t@echo \"# ln -s $(BUILDDIR)/devhelp $$HOME/.local/share/devhelp/WebdevBootcamp\"\n\t@echo \"# devhelp\"\n\nepub:\n\t$(SPHINXBUILD) -b epub $(ALLSPHINXOPTS) $(BUILDDIR)/epub\n\t@echo\n\t@echo \"Build finished. The epub file is in $(BUILDDIR)/epub.\"\n\nlatex:\n\t$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex\n\t@echo\n\t@echo \"Build finished; the LaTeX files are in $(BUILDDIR)/latex.\"\n\t@echo \"Run \\`make' in that directory to run these through (pdf)latex\" \\\n\t      \"(use \\`make latexpdf' here to do that automatically).\"\n\nlatexpdf:\n\t$(SPHINXBUILD) -b latex $(ALLSPHINXOPTS) $(BUILDDIR)/latex\n\t@echo \"Running LaTeX files through pdflatex...\"\n\tmake -C $(BUILDDIR)/latex all-pdf\n\t@echo \"pdflatex finished; the PDF files are in $(BUILDDIR)/latex.\"\n\ntext:\n\t$(SPHINXBUILD) -b text $(ALLSPHINXOPTS) $(BUILDDIR)/text\n\t@echo\n\t@echo \"Build finished. The text files are in $(BUILDDIR)/text.\"\n\nman:\n\t$(SPHINXBUILD) -b man $(ALLSPHINXOPTS) $(BUILDDIR)/man\n\t@echo\n\t@echo \"Build finished. The manual pages are in $(BUILDDIR)/man.\"\n\nchanges:\n\t$(SPHINXBUILD) -b changes $(ALLSPHINXOPTS) $(BUILDDIR)/changes\n\t@echo\n\t@echo \"The overview file is in $(BUILDDIR)/changes.\"\n\nlinkcheck:\n\t$(SPHINXBUILD) -b linkcheck $(ALLSPHINXOPTS) $(BUILDDIR)/linkcheck\n\t@echo\n\t@echo \"Link check complete; look for any errors in the above output \" \\\n\t      \"or in $(BUILDDIR)/linkcheck/output.txt.\"\n\ndoctest:\n\t$(SPHINXBUILD) -b doctest $(ALLSPHINXOPTS) $(BUILDDIR)/doctest\n\t@echo \"Testing of doctests in the sources finished, look at the \" \\\n\t      \"results in $(BUILDDIR)/doctest/output.txt.\"\n\nlivehtml:\n\tsphinx-autobuild -b html $(ALLSPHINXOPTS) $(BUILDDIR)/html\n",
  "readme": "########################\nFirefox Test Engineering\n########################\nA tutorial on some of the basic things you'd want to know when\ncontributing to `Firefox Test Engineering`_ at Mozilla.\n\nThis can be read on the `Firefox Test Engineering Read the Docs <http://firefox-test-engineering.readthedocs.io/en/latest/>`_ website.\n\nThis document is based off of the excellent `Webdev Bootcamp`_ documentation.\n\nPatches are always welcome! If you'd like to contribute, fork and make a pull\nrequest.\n\n.. _`Firefox Test Engineering`: https://wiki.mozilla.org/TestEngineering\n.. _`Webdev Bootcamp`: https://mozweb.readthedocs.io/\n\n****************\nBuilding locally\n****************\nThe instructions below assume you have Python and `pip`_ installed. It is also\nstrongly recommended that you create and activate a `virtualenv`_ first.\n\nIf you'd like to build locally:\n\n.. code-block:: sh\n\n   pip install -r requirements.txt\n   make html\n\nThe resulting docs can be located under the ``_build/html`` directory.\n\nYou can also run ``make livehtml`` to launch a webserver on\nhttp://127.0.0.1:8000 that auto-rebuilds the documentation when any files are\nchanged.\n\n.. _pip: https://pip.pypa.io/\n.. _virtualenv: https://virtualenv.pypa.io/\n\n*********\nLicensing\n*********\nFeel free to fork this repo or adapt its work for your own needs. All work\nis licensed under a `Creative Commons Attribution`_.\n\n.. _`Creative Commons Attribution`: https://creativecommons.org/licenses/by/4.0/\n"
},
{
  "name": "web-lit-training",
  "files": {
    "/": [
      "LICENSE",
      "README.md",
      "bucket-list",
      "business-front-page",
      "content.md",
      "index.html",
      "resume",
      "template-assets"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Web Literacy Training\n## Sharing Your Work with Github!\n\nThis repo includes three projects for participants in Mozilla's Web Literacy Training to choose from, in order to practice some of the skills learned during the training.\n\nParticipants can fork this repo and then choose a project template to edit on their forked version.\n\n## Fork this repo:\n\nClick the \"Fork\" button in the top right of this page\n\n![fork](https://cloud.githubusercontent.com/assets/8389648/15445397/a15a3cdc-1eb2-11e6-93eb-42fc3d57f1e9.png)\n\n## Edit this README\n\nMake sure you're on your own forked version of this repo (you'll know it's your own version because your username will appear as part of the path in the top left).\n\n![yourname](https://cloud.githubusercontent.com/assets/8389648/15445412/0195acee-1eb3-11e6-8366-7c33197307a8.png)\n\nClick the README.md file\n\n![readme](https://cloud.githubusercontent.com/assets/8389648/15445431/54b48ddc-1eb3-11e6-86e3-af5df22f7c34.png)\n\nClick the pencil icon to edit\n\n![pencil](https://cloud.githubusercontent.com/assets/8389648/15445468/f0910442-1eb3-11e6-9f52-fefcc1c63031.png)\n\nAdd a line to the README explaining that you've created this repo as part of a web literacy training. Check out this [markdown cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) and experiment with different styles you can apply using markdown. Have fun!\n\n## Save and publish your changes\n\nTo save and publish your changes, scroll down and complete the information in the \"Commit changes\" box. Add a note about what you changed. This is an important part of version control\u2014it allows other people (and even your future self!) to know what changes you made and why. \n\nFor today, you can commit directly to the gh-pages branch, so you can see your changes immediately. If you were working collaboratively on a project with others, you would likely create a new branch and submit a pull request for someone else to review.\n\n![commit](https://cloud.githubusercontent.com/assets/8389648/15445566/b827b73e-1eb5-11e6-9a14-b4a1dd2e6b8d.png)\n"
},
{
  "name": "project_haiku.iot",
  "files": {
    "/": [
      ".gitignore",
      "Bluetooth",
      "ESP8266",
      "LICENSE",
      "Prototype",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# Mozilla Project Haiku\n\n### &#x1F534; Project Status - On Hold\n\nLearn about the Project Haiku (former Project Smart Home) on the Mozilla wiki\n* [https://wiki.mozilla.org/Connected_Devices/Projects/Project_Haiku](https://wiki.mozilla.org/Connected_Devices/Projects/Project_Haiku)\n\nFollow our progress in our Github wiki\n* [https://github.com/mozilla/project_haiku.iot/wiki](https://github.com/mozilla/project_haiku.iot/wiki)\n* Github won't notify you when this wiki is updated, but you can [subscribe via RSS](https://github.com/mozilla/project_haiku.iot/wiki.atom).\n\nLinks to related repos\n* The status API service used by Project Haiku and its experiments    \nhttps://github.com/mozilla/project_haiku_status_api.iot    \n* Reports and visualizations of data collected from Project Haiku user studies    \nhttps://github.com/mozilla/project_haiku_dataviz.iot    \n* Signalling server to connect pairs of WebRTC clients\nhttps://github.com/mozilla/project_haiku_webrtc_signaling.iot\n"
},
{
  "name": "mwos-letsencrypt-2015",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "Makefile",
      "README.md",
      "config",
      "example",
      "ngx_http_acme_lib.h",
      "ngx_http_acme_module.c",
      "ngx_http_acme_module.h"
    ]
  },
  "makefile": "ROOT_DIR := $(strip $(shell dirname $(realpath $(lastword $(MAKEFILE_LIST)))))\n\nSRC_VERSION := nginx-1.11.0\nSRC_LINK := \"http://nginx.org/download/$(SRC_VERSION).tar.gz\"\n\nSRC_PATH := $(ROOT_DIR)/$(SRC_VERSION)\nRUN_PATH := $(ROOT_DIR)/run\n\nMODULE_SRC := $(ROOT_DIR)/ngx_http_acme_module.c $(ROOT_DIR)/ngx_http_acme_module.h $(ROOT_DIR)/ngx_http_acme_lib.h\nMODULE_CFG := $(ROOT_DIR)/config\n\nEXAMPLE_DIR := $(ROOT_DIR)/example\nEXAMPLE_CONFIG := $(EXAMPLE_DIR)/nginx.conf\n\nRUN_CONF_DIR := $(RUN_PATH)/conf\nRUN_CONFIG := $(RUN_CONF_DIR)/nginx.conf\nRUN_BIN := $(RUN_PATH)/sbin/nginx\nPID_FILE := $(RUN_PATH)/logs/nginx.pid\n\nSRC_MKFILE := $(SRC_PATH)/Makefile\nSRC_BIN := $(SRC_PATH)/objs/nginx\n\nCONFIGURE_OPTS := --prefix=\"$(RUN_PATH)\" --with-http_ssl_module --add-module=\"$(ROOT_DIR)\"\n\n# For debug output\nCONFIGURE_OPTS := $(CONFIGURE_OPTS) --with-debug\n\n# Dev variables\nEXAMPLE_CERT := $(EXAMPLE_DIR)/cert.pem\nEXAMPLE_CERT_KEY := $(EXAMPLE_DIR)/cert-key.pem\nACME_DIR := $(RUN_CONF_DIR)/acme\nACME_SERVER_NAME := ledev2.kbauer.at\nACME_CERT_DIR := $(ACME_DIR)/live/$(ACME_SERVER_NAME)\nACME_ACC_DIR := $(ACME_DIR)/accounts\n\n.PHONY: default source configure build install run \\\n\tclean kill reinstall clean-install clean-all \\\n\trun reconfigure clean-build clean-source\n\n#\n# Phony targets\n#\n\ndefault: build\n\nall: source install\n\nsource: clean-source\n\tmkdir -p \"$(SRC_PATH)\"\n\tcurl $(SRC_LINK) | tar xz\n\nconfigure: $(SRC_MKFILE)\n\nbuild: $(SRC_BIN)\n\ninstall: $(RUN_BIN)\n\nrun:\n\t@test -f \"$(RUN_BIN)\" || (echo \"You have to run 'make install' first\"; exit 2)\n\t@test ! -f \"$(PID_FILE)\" || (echo \"Error: NginX is already running\"; exit 2)\n\t\"$(RUN_BIN)\"\n\nkill:\n\ttest -f \"$(PID_FILE)\" && kill `cat \"$(PID_FILE)\"` || echo \"Warning: NginX isn't running\"\n\nclean-install:\n\trm -rf \"$(RUN_PATH)\"\n\nreinstall: clean-install install\n\nclean-build:\n\t$(MAKE) -C \"$(SRC_PATH)\" clean 2>/dev/null || true\n\nreconfigure: clean-build configure\n\nrebuild: clean-build build\n\nclean: clean-install clean-build\n\nclean-source:\n\trm -rf $(SRC_PATH)\n\nclean-all: clean clean-source\n\n#\n# File targets\n#\n\n$(SRC_MKFILE): $(MODULE_CFG)\n\t@test -d $(SRC_PATH) || (echo \"You have to run 'make source' first to download the Nginx source code\"; exit 2)\n\tcd \"$(SRC_PATH)\"; ./configure $(CONFIGURE_OPTS)\n\n$(SRC_BIN): $(SRC_MKFILE) $(MODULE_SRC)\n\t$(MAKE) -C \"$(SRC_PATH)\"\n\n$(RUN_BIN): $(SRC_BIN)\n\t$(MAKE) -C \"$(SRC_PATH)\" install\n\t# Install example files\n\tcp \"$(EXAMPLE_CONFIG)\" \"$(RUN_CONFIG)\"\n\tmkdir -p \"$(ACME_CERT_DIR)\"\n\tmkdir -p \"$(ACME_ACC_DIR)\"\n",
  "readme": "# Let's Encrypt module for Nginx\n\n## Introduction\n\n[TODO]\n\n## Dependencies\n\n### Makefile dependencies\n\nTo build this module you need all the tools installed which you would also need to\nbuild the Nginx server alone. In addition you need `curl` if you download the source code\nof Nginx using `make source` (recommended).\n\n### Libraries\n\nThe module uses libcurl for making the HTTP calls to the ACME server and libjansson for\nparsing and creating the JSON strings according to the ACME protocol.\n\n## Installation\n\n  1. Download the NginX source code using:\n\n        make source\n\n  2. Configure, build and install the server with the module:\n\n        make install\n\n      With this step the server is compiled and installed in the `./run` directory.\n      Don't worry, nothing is installed on your system outside this directory.\n      \n  3. Run the server:\n  \n        make run\n\n    You can later stop the server with:\n    \n        make kill\n        \n## Configuration\n\n  * The build process and the directories can be configured in the first few lines of\n    the [Makefile](Makefile).\n\n  * The server will run with a copy of the configuration file [example/nginx.conf](example/nginx.conf)\n"
},
{
  "name": "sumo-projects",
  "files": {
    "/": [
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "# sumo-projects\nThe shared repository for documenting and tracking all collaborative SUMO projects\n"
},
{
  "name": "connected-devices-experiments",
  "files": {
    "/": [
      "README.md",
      "_repo_tools",
      "experiments",
      "investigations",
      "reviews"
    ]
  },
  "makefile": null,
  "readme": "# Connected Devices Experiments and Investigations\nThis is a place to publish experiments and investigations from the Connected Devices team.\n\n### Content types\n* *Reviews* of smart home products and/or your experiences with those products\n* *Experiments* you've tried. For example, if you built an Arduino to do X, you can use this space to publish your instructions so others can replicate your experiment.\n* *Investigations*. Things you're actively looking into that you want to share with the CD team and the world.\n* Other stuff you want to add!\n\n### How to contribute\n[Create an issue][1] for your experiment, investigation, review, etc.\n\nLabel it. Add more labels if you think we need them - see below!\n\nContinue the conversation - see other people's issues and comment.\n\n<img src=\"_repo_tools/img/repo_labels.png\" align=\"left\" height=\"300\" border=\"1\" alt=\"labels used for repo\">  \n[1]: https://github.com/mozilla/connected-devices-experiments/issues\n"
},
{
  "name": "ChangeDetector",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md",
      "analysis",
      "app.py",
      "pyLibrary",
      "requirements.txt",
      "rpackageinstalls.sh",
      "tests"
    ]
  },
  "makefile": null,
  "readme": "ChangeDetector\n==============\n\nService to markup data series regarding outliers and mean-shifts\n\nOverview\n--------\n\nIt is early in the project.   The objective is to find mean-shifts in our performance data, with a focus on significantly reducing false positives.   This will be achieved by accepting large confidence intervals, or demanding numerous samples.\n\n\nPart 1\n------\n\nMake a service we can start using now in our other projects. This service will also help evaluate potential unit tests to add to the suite.\n\nPart 2\n------\n\nImplement existing change detection procedures.   Add some historical data that should be detected.\n\nPart 3\n------\n\nIterate\n\nPart 4\n------\n\nIntegrate with alert databases.    "
},
{
  "name": "mozfest-program",
  "files": {
    "/": [
      "README.md",
      "assets",
      "mozfest_pathways_explanation.md"
    ]
  },
  "makefile": null,
  "readme": "# Mozfest 2015 Program on Github!\n\nHead to this repository's issues, where the Mozfest program data is being reviewed and curated. Questions on how to use this tool or ideas to improve it? Email festival@mozilla.org.\n\n## Overview \n\nAll the Mozfest program data is in the [\"Issues\"](https://github.com/mozilla/mozfest-program/issues) section of this Github repository. This is a brief tutorial to help you, member of the Mozfest program team, get started with Github Issues.\n\n### Spaces\n\nEach space is represented as a Github [\"milestone\"](https://github.com/mozilla/mozfest-program/milestones). Each milestone includes description of its respective space. \n\n<img src=\"assets/img/tutorial-spaces.png\">\n\nClick \"(more)\" in Issues > Milestones to see the details of a space. \n\n<img src=\"assets/img/tutorial-spaces-details.png\">\n\nYou can edit the description of your space at any time. \n\nYou can click on the name of a space to see all the associated pathways and sessions. You can modify sessions to be part of your space by adding them to the appropriate \"milestone.\"\n\n### Pathways\n\nEach pathway is represented as a Github [\"label.\"](https://github.com/mozilla/mozfest-program/labels). You can add new labels, change the color and change the name:   \n\n<img src=\"assets/img/tutorial-labels.png\">\n\nLabels can be applied to sessions (i.e. individual Github issues) to indicate that the session is part of that pathway. You can then search for and see all the sessions that are part of that pathway. There's not limit to the number of labels you can apply to a session. That's great -- we love things that are cross-listed. \n\n<img src=\"assets/img/tutorial-pathway-list.png\">\n\nIdeally, each pathway also has what's called a \"tracking issue\" that provides more information about the pathway. \n\n<img src=\"assets/img/tutorial-pathway-details.png\">\n\n\n### Sessions\n\nEach session is a [Github Issue](https://github.com/mozilla/mozfest-program/issues). They can be easily labeled to be part of a pathway. You can also comment on them, edit them, and \"close\" them if they are not a good fit for Mozfest. \n\n<img src=\"assets/img/tutorial-spaces.png\">\n\n## Why we're using Github Issues for Mozfest\n\nThis year we're using Github to manage, review and curate sessions for Mozfest. This tool will allow us to better collaborate and coordinate across the program, as well as provide a record for how the program came together. \n\nIt is new for us to use Github Issues in this way, but based on some other events who've used it effectively, we think it's a great solution for Mozfest. \n \n\nWe are here to help in case you get stuck or need help at anytime. Don't worry -- you can't break anything! \n\n## Who is using Github Issues for Mozfest\n\nFor Mozfest, we are inviting any Space Wrangler, Pathway Finder or core Mozfest team member to use Github Issues. \n\nIf you are not in one of these roles but are interested to be active in Github Issues with us, please email festival@mozilla.org. Due to the sometimes sensitive nature of our conversations on this platform, please be aware that an invitation to participate is not guaranteed. \n\nIf you are using Github Issues, please be advised that your comments will be recorded and may be viewed publicly at sometime. We will not publish anyone's personal information, and we ask you to be respectful in your comments, both towards the people who submitted sessions and to your fellow program team members.\n\n## Getting started with Github Issues\n\nGithub Issues is a powerful way to collaborate on Github. It was originally designed as an \"issue tracker\" or project management tool for editing code. However, its simple interface allows us to adapt it for Mozfest's program curation.\n\nTo get started, you'll first have to [create a Github account.](https://github.com/join)\n\nOnce you've created your account, send your Github user name to festival@mozilla.org. You will then be notified that you have access to the Mozfest repository.\n\n## Learning how to use Github Issues\n\nThe Mozfest team will offer support for you to use Github Issues in the following ways: \n\n* **This tutorial!** Is there anything missing or unclear? Let us know how to improve it by emailing: festival@mozilla.org\n* **Orientations on the Mozfest program call.** During our regular weekly calls, we will walk the group through the major features and workflows in Github Issues. We'll answer questions and troubleshoot. \n* **1:1 Conversations.** We'll also offer timeslots for anyone interested in having a more in-depth conversation about how to use Github Issues effectively. In addition, we'll be available for any urgent Github questions via email (festival@mozilla.org), Twitter (#mozfest) or on Skype.\n\n## How to review sessions\n\nBe mindful about what you comment. People who are submitting a session are putting themselves out there. We are invested in ensuring they have a good experience with us, regardless whether their session is accepted or not. Let's be respective and constructive. \n\nAlso note: This is a public Github repo. If you need to say something sensitive about a session or a facilitator, you can use the sessions@ mailing list.\n\n* 1. Read through the sessions. You can search for key words. \n* 2. Signal your interest in the sessions you want to take on: \n** A. Apply a pathway label to indicate you want to include it in your pathway. A session can have multiple labels. \n** B. Apply your space milestone to all your pathways and sessions that you want  included within your space. Note: in Github, a session can only be assigned to one space milestone. \n** C. CC yourself in the comment section of a session if you want to be notified about changes to this session and otherwise be involved in deciding to keep or tweak it. \n\n* 3. Accepting and declining sessions.\n** **How to accept a session:** Add a pathway label and/or space milestone. This indicates you are taking responsibility for the session's inclusion and success at Mozfest. The issue will remain open.\n** **How to suggest declining a session:** Any session issue that is unlabeled and without a space milestone will be perceived as not accepted for this year's festival. \n\nAfter September 30 if the session remains unlabeled and without a space milestone, the issue will be closed by the Mozfest team. The session facilitator will be notified by the program team that their session has not be accepted to Mozfest.\n\n## Improving this process\n\nIdeas? Let us know: festival@mozilla.org. \n\t\t\n"
},
{
  "name": "fennec-search",
  "files": {
    "/": [
      ".gitignore",
      "Gruntfile.js",
      "LICENSE.md",
      "README.md",
      "app",
      "branding",
      "build.gradle",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "manifests",
      "package.json",
      "settings.gradle",
      "strings"
    ]
  },
  "makefile": null,
  "readme": "**This repo is no longer used for active development. To contribute to the Firefox search activity, please follow the normal [Firefox for Android development process](https://wiki.mozilla.org/Mobile/Fennec/Android).**\n\n# Fennec Search Activity\n\nThis is a stand-alone version of the search activity that is built with Firefox for Android. The main source code lives in [mozilla-central](http://hg.mozilla.org/mozilla-central/), but this repo is a tool to make development easier.\n\nDevelopment happens in the Firefox for Android::Search Activity [bugzilla component](https://bugzilla.mozilla.org/buglist.cgi?quicksearch=prod%3Aandroid%20component%3Asearch&list_id=10980886).\n\n## grunt\n\nThe Search Activity repository uses [grunt](http://gruntjs.com/) tasks to\nintegrate with a *Mozilla source tree* such as `fx-team` or `mozilla-inbound`.\nAll grunt tasks take such a *source tree* specified either via a command line\nargument like `--tree=PATH` or via the environment variable `MC`.\n\n### Getting started with grunt\n\nFirst, install [node.js](http://nodejs.org/) and [npm](http://npmjs.org) using\nyour OS-level package manager or similar.  Then, in the Search Activity\nrepository root directory, execute\n\n    $ npm install\n\nYou can check that grunt is working and the local dependencies are installed by\nexecuting `grunt --help`.  You should see a list of available tasks, including a\n`default` task.\n\n### Grunt tasks\n\n#### preprocess (default)\n\nThe default task, executed when you run `grunt`, is to preprocess the Android\nmanifest and Android string resources.  The inputs have the suffix `.in` and the\noutputs are written into the source tree.  (This is so that gradle and Android\nStudio can find them without having additional paths specified.  We might change\nthis in future.)\n\n#### clean\n\nDelete all of the preprocessed outputs created by the *preprocess* task.\n\n#### export\n\nCopy the current Java source code, Android resources, string definitions, and\nAndroid manifest snippets to the Mozilla source tree provided.  **Does not copy\nany preprocessed outputs.** (Preprocessed outputs must be created by the Mozilla\nsource tree's build system at Fennec build time.)  Use this to update your\nMozilla source tree with the changes you've made in your local Search Activity\nrepository.\n"
},
{
  "name": "osmdroid",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      "LICENSE",
      "Makefile",
      "README.md",
      "docs",
      "pom.xml",
      "src"
    ],
    "/docs": [
      "Android_SDK_Manager.png"
    ]
  },
  "makefile": "MAVEN_REPO=${HOME}/.mozosmdroid/repository\nMVN=/usr/local/bin/mvn -Dmaven.repo.local=${MAVEN_REPO}\n\nall: jar\n\n# This builds target/osmdroid-android-4.3-SNAPSHOT.jar\njar: build\n\t${MVN} package\n\nbuild:\n\t${MVN} compile\n\nandroid_sdk:\n\trm -rf ${MAVEN_REPO}\n\trm -rf maven-android-sdk-deployer\n\tgit clone https://github.com/mosabua/maven-android-sdk-deployer.git\n\tcd $(shell pwd)/maven-android-sdk-deployer && ${MVN} install -P 4.4\n\ntest:\n\t${MVN} test\n\nclean:\n\t${MVN} clean\n",
  "readme": "### This code is deprecated and has been merged into Mozilla Stumbler http://github.com/mozilla/mozstumbler\n\nThis is a fork of the osmdroid project from: https://github.com/osmdroid/osmdroid\n\n# osmdroid\n\nosmdroid is a (almost) full/free replacement for Android's MapView (v1\nAPI) class. It also includes a modular tile provider system with\nsupport for numerous online and offline tile sources and overlay\nsupport with built-in overlays for plotting icons, tracking location,\nand drawing shapes.\n\n\nBuild instructions are here: https://github.com/mozilla/osmdroid/wiki/Mozilla-OSMDroid\n"
},
{
  "name": "mofo-campaignurl",
  "files": {
    "/": [
      ".DS_Store",
      ".editorconfig",
      ".gitattributes",
      ".gitignore",
      ".jshintrc",
      ".yo-rc.json",
      "Gruntfile.js",
      "LICENSE",
      "README.md",
      "app",
      "bower.json",
      "dist",
      "package.json",
      "test"
    ]
  },
  "makefile": null,
  "readme": "campaignurl\n===========\n\nCheck URLs for marketing campaigns\n\nInit\n```\n$ npm install\n$ bower install\n```\n\nDevelop\n```\n$ grunt serve\n```\n\nDeploy\n```\n$ grunt build\n$ git add --all\n$ git commit -m '...'\n$ git subtree push --prefix dist mozilla gh-pages\n$ git push mozilla master\n```\n"
},
{
  "name": "mozbadging",
  "files": {
    "/": [
      ".gitignore",
      "LICENSE",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "Mozilla Badging Project\n==============\n\nwhere we track progress on mozilla-wide badging infrastructure; connecting badgekit, django-badger, webmaker badges, mozillian badges, and more.\n\nSee [the roadmap](https://github.com/mozilla/mozbadging/wiki/Summer-Roadmap).\n"
},
{
  "name": "mediawiki-bugzilla",
  "files": {
    "/": [
      ".gitignore",
      "Bugzilla.class.php",
      "Bugzilla.i18n.php",
      "Bugzilla.php",
      "BugzillaOutput.class.php",
      "BugzillaQuery.class.php",
      "LICENSE",
      "Makefile",
      "README.md",
      "Utils.php",
      "composer.json",
      "pchart",
      "phpcs.xml",
      "templates",
      "tests",
      "web"
    ]
  },
  "makefile": "\n\ninstall-dev:\n\tcomposer install\n\ntest: lint cs\n\nlint: phplint\n\ncs: phpcs\n\nphplint:\n\t./vendor/bin/parallel-lint --exclude vendor/ .\n\nphpcs:\n\t./vendor/bin/phpcs -p --colors .\n\nphpcsfix:\n\t./vendor/bin/phpcbf .\n\n.PHONY: test lint cs phpcs phplint phpcsfix\n",
  "readme": "MediaWiki extension for Bugzilla\n================================\n\nThis is a MediaWiki extension that provides read-only access to the \n[Bugzilla REST API](https://wiki.mozilla.org/Bugzilla:REST_API) \n\n__Please note that there are still big outstanding bugs!__\n\nRequirements\n================================\n\n* MediaWiki 1.17 or above.\n* For charting, requires <a href=\"http://libgd.bitbucket.org/\">gd</a>\n\nInstallation\n================================\n\n*These directions assume your MediaWiki installation is at /var/lib/mediawiki.\nPlease substitute your installation path if it is different*\n\n1. Install the requirements above\n2. Check the project out into `/path/to/your/mediawiki/extensions/Bugzilla`\n3. Edit `/path/to/your/mediawiki/LocalSettings.php` and add\n   `require_once(\"$IP/extensions/Bugzilla/Bugzilla.php\");`\n   and change/override any configuration variables.\n   Current configuration variables and their defaults can be found at the end of `Bugzilla.php`\n\nUsage\n================================\n\nYou use this extension in this way:\n\n    <bugzilla>\n        (JSON REST API query key/value pairs)\n    </bugzilla>\n\nBy default, it will output a colored table:\n\n![Example output](http://i.imgur.com/IM6xd.png)\n\nNote that the wiki tag name defaults to \"bugzilla\" but is \nconfigurable by the administrator.\n\nOptions\n================================\n\nValid bugzilla tag options are:\n\n* type: ``\"bug\"`` or ``\"count\"`` (defaults to bug)\n* For type bug:\n    * display: ``\"table\"`` or ``\"list\"`` or `\"count\"` (defaults to table)\n* For type count:\n    * display: ``\"bar\"`` or ``\"pie\"``\n    * size: ``\"small\"``, ``medium\"`` or ``\"large\"`` (defaults to large)\n* stats: ``\"show\"`` or ``\"hide\"`` (defaults to \"show\")\n\n\nExamples\n================================\n\nAll P1 bugs in the Bugzilla product:\n\n    <bugzilla>\n        {\n            \"product\": \"Bugzilla\",\n            \"priority\":\"P1\"\n        }\n    </bugzilla>\n\nAll new bugs flagged as uiwanted in the whiteboard:\n\n    <bugzilla>\n    \t{\n    \t    \"whiteboard\": \"uiwanted\",\n    \t    \"status\": \"NEW\"\n        }\n    </bugzilla>\n\nAll bugs in the bugzilla.org component that were resolved in 2011,\nwith the stats summary hidden:\t\n\n    <bugzilla stats=\"hide\">\n        {\n            \"component\": \"bugzilla.org\",\n            \"changed_after\": \"2011-01-01\",\n            \"changed_before\": \"2011-12-31\",\n            \"changed_field\": \"status\",\n            \"changed_field_to\": \"resolved\"\n        }\n    </bugzilla>\n\nSome commonly used query parameters are:\n\n* id\n* component\n* product\n* status\n* resolution\n* keywords\n* whiteboard\n* target_milestone\n* version\n* changed_after\n* changed_before\n\nFor more details on how to query in various ways, see the documentation for\nthe [Bugzilla REST API](https://wiki.mozilla.org/Bugzilla:REST_API)\n\n\nConfigurable fields/columns\n================================\n\nSpecify fields in the \"include_fields\" setting of BZ REST API options as you \nnormally would. Mediawiki-bugzilla will then a) only fetch those fields \nand b) display those columns.\n\n    <bugzilla>\n    {\n        \"whiteboard\": \"[mediawiki-bugzilla]\",\n        \"include_fields\": [\"id\", \"summary\", \"whiteboard\", \"status\", \"resolution\"]\n    }\n    </bugzilla>\n\n![Screenshot of the above](http://i.imgur.com/p3u7r.png \"Screenshot of the above\")\n\n\nCharting\n================================\n\nThere is also _some_ support for charting:\n\n    <bugzilla type=\"count\" display=\"bar\">\n        {\n            \"whiteboard\": \"[snappy:p1]\",\n            \"x_axis_field\": \"status\"\n        }\n    </bugzilla>\n\nScreenshot of the above:\n\n![Screenshot of the above](http://i.imgur.com/tDUZ1.png \"Screenshot of the above\")\n\n    <bugzilla type=\"count\" display=\"pie\">\n    {\n        \"whiteboard\": \"[mediawiki-bugzilla]\",\n        \"x_axis_field\": \"status\"\n    }\n    </bugzilla>\n    <bugzilla type=\"count\" display=\"pie\" size=\"medium\">\n    {\n        \"whiteboard\": \"[mediawiki-bugzilla]\",\n        \"x_axis_field\": \"status\"\n    }\n    </bugzilla>\n    <bugzilla type=\"count\" display=\"pie\" size=\"small\">\n    {\n        \"whiteboard\": \"[mediawiki-bugzilla]\",\n        \"x_axis_field\": \"status\"\n    }\n    </bugzilla>\n\nScreenshot of the above:\n\n![Screenshot of the above](http://i.imgur.com/mobHA.png \"Screenshot of the above\")\n\n\nLimitations\n================================\n\n* This extension (by design) is read-only\n* This extension currently queries as a public (not logged in) user\n* Charts are fairly hardcoded and don't work in many cases\n\nKnown Issues\n================================\n* The __size__ attribute only works on pie charts\n* Rendering a page with an uncached query can take a bit\n* Large queries may exceed the allocated memory causing a blank page to be displayed. In this case you can recover by editing the page as follows:\nIf your wiki page has the URL \n    https://wiki.mozilla.org/PagePath/PageTitle\nThe URL to edit your page is \n    https://wiki.mozilla.org/index.php?title=PagePath/PageTitle&action=edit\n\nTODO\n================================\n* Add more/smarter field display templates\n"
},
{
  "name": "PyBrowserID",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "CHANGES.txt",
      "CODE_OF_CONDUCT.md",
      "MANIFEST.in",
      "README.rst",
      "browserid",
      "requirements",
      "setup.py",
      "tox.ini"
    ]
  },
  "makefile": null,
  "readme": "========================================================\nPyBrowserID: a python library for the BrowserID Protocol\n========================================================\n\nThis is a python client library for the BrowserID protocol that underlies\nMozilla Persona:\n\n    https://login.persona.org/\n\nFor the vast majority of deployments, you will simply want to call the module-\nlevel \"verify\" functon to verify a given assertion::\n\n    >>> data = browserid.verify(BROWSERIDASSERTION, \"http://mysite.com\")\n    >>> print data[\"email\"]\n    \"test@example.com\"\n\nThe precise implementation of this function will change depending on the\ncurrent recommendations of the BrowserID team.  Currently it POSTs the\nassertion to the remote verifier service on persona.org.\n\nNote that you *must* specify your site's root URL as the second argument\nto that function.  This is the \"expected audience\" and is a key security\nfeature of BrowserID.\n\nIf you are not able to determine the precise hostname by which your site\nis being accessed (e.g. due to virtual hosting) then you may specify one or\nmore wildcard patterns like so::\n\n    >>> data = browserid.verify(BROWSERIDASSERTION, [\"http://*.mysite.com\"])\n    >>> print data[\"email\"]\n    \"test@example.com\"\n\nFor finer control over the verification process, you can create an instance of\na \"Verifier\" class and avoid having to specify the audience patterns over\nand over again::\n\n    >>> verifier = browserid.RemoteVerifier([\"*.mysite.com\"])\n    >>> data = verifier.verify(BROWSERIDASSERTION)\n    >>> print data[\"email\"]\n    \"test@example.com\"\n\nFor improved performance, or if you just want to live on the bleeding edge,\nyou can explicitly perform verification locally by using the LocalVerifier\nclass like so::\n\n    >>> verifier = browserid.LocalVerifier([\"*.mysite.com\"])\n    >>> data = verifier.verify(BROWSERIDASSERTION)\n    >>> print data[\"email\"]\n    \"test@example.com\"\n\nNote that the details of the BrowserID Protocol are still in flux, so\nlocal verification might break due to incompatible changes.  As things \nstabilise this will become the default implementation.\n"
},
{
  "name": "winchan",
  "files": {
    "/": [
      ".gitignore",
      "CODE_OF_CONDUCT.md",
      "README.md",
      "TESTING.md",
      "complex_example",
      "example",
      "relay.html",
      "scripts",
      "test",
      "winchan.js"
    ]
  },
  "makefile": null,
  "readme": "## An abstraction for opening browser windows cross domain\n\nHere's the scenario:  You want to build a secure means of some untrusted site\nopening a window, which loads content at a trusted site.  Then you want the \nuntrusted dude to be able to pass in parameters.  Then you want the trusted\ncode to do any amount of stuff, and return a response.\n\nThis kinda thing is what lots of services on the web do, services\nlike [BrowserID][].\n\n  [BrowserID]: https://browserid.org\n\nTrouble is that this is stupidly hard:\n\n  * Mobile Firefox doesn't like it when you open windows with window options\n  * IE 8 & 9 don't even allow postMessage between opener and window\n  * iOS 5 has some interesting optimizations that can bite you if not careful\n  * you should tightly check origins to avoid classes of attacks\n  * you probably will have to add stuff in the DOM, you should make sure you\n    can clean this up and avoid introducing fragile code\n\nWinChan is an abstraction to solve these problems and make it easy to open\nwindows which take and return parameters and load content cross domain.\n\n## Browser Support\n\nWinChan is expected to work on:\n\n  * winxp - win7 on IE8 and IE9\n  * windows, linux, osx - Chrome, Firefox, Opera, and Safari\n  * Android's \"native\" browser - 2.1, 2.2, 2.3.4, 3.2 (and presumably newer)\n  * Fennec on Android\n\n## Usage\n\nFor the site spawning the window, the \"untrusted\" or \"client\" code:\n\n    WinChan.open({\n      url: \"http://trusted.host/dialog.html\",\n      relay_url: \"http://trusted.host/relay.html\",\n      window_features: \"menubar=0,location=0,resizable=0,scrollbars=0,status=0,dialog=1,width=700,height=375\",\n      params: {\n        these: \"things\",\n        are: \"input parameters\"\n      }\n    }, function(err, r) {\n      // err is a string on failure, otherwise r is the response object\n    });\n\nFor the site providing the window, the \"trusted\" code:\n\n    WinChan.onOpen(function(origin, args, cb) {\n      // origin is the scheme+host+port that cause window invocation,\n      // it can be trusted\n\n      // args are the untrusted arguments provided by the calling site\n\n      // and cb you can call within the function, or synchronously later.\n      // calling it indicated the window is done and can be closed.\n      cb({\n        \"these things\": \"are the response\"\n      });\n    });\n\nFinally, you'll notice that the trusted code needs to host 'relay.html' somewhere (required\nfor IE support).\n\n## Running Examples\n\nthere's a little tiny webserver in-tree to let you run the examples.  You'll need node.js and\nnpm installed.  Once you have these, just:\n\n    $ scripts/setup_dev_deps.sh\n    $ scripts/run_example.js\n\nNow load `http://127.0.0.1:8100/example` (or the more complicated example which demonstrates\nnavigation away and back in window at `http://127.0.0.1:8100/complex_example`\n\n## Running Unit Tests\n\nnode.js and npm are required to run the unit tests. Once installed\n\n    $ scripts/setup_dev_deps.sh\n    $ scripts/run_example.js\n\nAnd open `http://127.0.0.1:8100/test` in your favorite web browser.\n\n**NOTE:** You'll need to disable popup blocking for localhost to run tests!\n\n## Testing over the network\n\nthe `run_example.js` script will bind whatever IP is in the `IP_ADDRESS` env var.\nSo to test over the network:\n\n    $ scripts/setup_dev_deps.sh\n    $ IP_ADDRESS=<my external IP> scripts/run_example.js\n\n(repace `<my external IP>` with *your* IP address)\n\nthen hit `http://<my external IP>:8100/test`\n\n## LICENSE\n\n    Copyright (c) 2012 Lloyd Hilaiel\n    \n    Permission is hereby granted, free of charge, to any person obtaining a copy \n    of this software and associated documentation files (the \"Software\"), to deal in \n    the Software without restriction, including without limitation the rights to use, \n    copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the \n    Software, and to permit persons to whom the Software is furnished to do so, \n    subject to the following conditions:\n    \n    The above copyright notice and this permission notice shall be included in all \n    copies or substantial portions of the Software.\n    \n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \n    INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR \n    PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE\n    FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE,\n    ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n"
},
{
  "name": "nunjucks-docs",
  "files": {
    "/": [
      ".gitignore",
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "\nThe nunjucks documentation now exists [within the project](https://github.com/mozilla/nunjucks/tree/master/docs), so please contribute there!"
},
{
  "name": "badges.mozillafestival.org",
  "files": {
    "/": [
      ".htaccess",
      "README.md",
      "index.html",
      "index.php",
      "media"
    ]
  },
  "makefile": null,
  "readme": "badges.mozillafestival.org\n==========================\n\nMozilla Festival Badges claim page\n"
},
{
  "name": "personas-plus",
  "files": {
    "/": [
      ".eslintignore",
      ".eslintrc.json",
      ".gitignore",
      "LICENSE",
      "README.md",
      "_locales",
      "background.js",
      "custom",
      "icons",
      "manifest.json",
      "notification",
      "popup"
    ]
  },
  "makefile": null,
  "readme": "Personas Plus\n-------------\n\n**Personas Plus is retiring on November 30th, 2018**. After many years of helping users create and access themes, and following the modernization of theming technology for Firefox, it\u2019s time to say goodbye to Personas Plus and to say hello to [Firefox Color](https://color.firefox.com), Mozilla\u2019s new tool for creating custom themes.\n\nPersonas, also known as lightweight themes, is a way of creating Firefox themes,\nnow deprecated. To learn about its replacement, check out the\n[documentation on themes](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/Themes/Theme_concepts)\non MDN.\n"
},
{
  "name": "mozilla-presentation-templates",
  "files": {
    "/": [
      ".editorconfig",
      "README.md",
      "html5",
      "mozilla-sandstone-1280-1.key",
      "mozilla-sandstone-1280.key",
      "mozilla-sandstone-1280.key.zip",
      "mozilla-sandstone-1280.ppt.zip"
    ]
  },
  "makefile": null,
  "readme": "Mozilla presentation templates\n==============================\n\nThis is the repository with Mozilla styled presentation templates in PPT, Keynote and HTML format. \n\nThere is a screencast showing how to use the Mozilla Evangelism Reps HTML5 slidedeck on YouTube: https://www.youtube.com/watch?v=M_9hf9joTcQ\n\nChanges in this version:\n\n* Firefox OS pick and mix overview deck (firefox-os-pick-and-mix.html)\n* Fullscreen button (as requested by Stormy Peters)\n* Overall cleanup of the CSS (different list styles, live CSS code)\n* Pressing \"N\" in fullscreen mode toggles note display\n* In list mode, each slide has a \"x\" button to turn it on and off. This does not persist though, so when you reload your slides, it will make them all active again (TODO I guess)\n* Support for multilingual slides, with URLs that update. Toggle the menu option to get the text in your desired language. (only in HTML5 slidedeck)\n\nNext steps: \n\n* Persist Toggle slides feature\n"
},
{
  "name": "django-session-csrf",
  "files": {
    "/": [
      ".gitignore",
      ".travis.yml",
      "LICENSE",
      "MANIFEST.in",
      "README.rst",
      "contribute.json",
      "requirements.txt",
      "runtests.sh",
      "session_csrf",
      "setup.cfg",
      "setup.py"
    ]
  },
  "makefile": null,
  "readme": "What is this?\n-------------\n\n``django-session-csrf`` is an alternative implementation of Django's CSRF\nprotection that does not use cookies. Instead, it maintains the CSRF token on\nthe server using Django's session backend. The csrf token must still be\nincluded in all POST requests (either with `csrfmiddlewaretoken` in the form or\nwith the `X-CSRFTOKEN` header).\n\n\nInstallation\n------------\n\nFrom PyPI::\n\n    pip install django-session-csrf\n\nFrom github::\n\n    git clone git://github.com/mozilla/django-session-csrf.git\n\nReplace ``django.core.context_processors.csrf`` with\n``session_csrf.context_processor`` in your ``TEMPLATE_CONTEXT_PROCESSORS``::\n\n    TEMPLATE_CONTEXT_PROCESSORS = (\n        ...\n        'session_csrf.context_processor',\n        ...\n    )\n\nReplace ``django.middleware.csrf.CsrfViewMiddleware`` with\n``session_csrf.CsrfMiddleware`` in your ``MIDDLEWARE_CLASSES``\nand make sure it is listed after the AuthenticationMiddleware::\n\n    MIDDLEWARE_CLASSES = (\n        ...\n        'django.contrib.auth.middleware.AuthenticationMiddleware',\n        ...\n        'session_csrf.CsrfMiddleware',\n        ...\n    )\n\nThen we have to monkeypatch Django to fix the ``@csrf_protect`` decorator::\n\n    import session_csrf\n    session_csrf.monkeypatch()\n\nMake sure that's in something like your root ``urls.py`` so the patch gets\napplied before your views are imported.\n\n\nDifferences from Django\n-----------------------\n\n``django-session-csrf`` does not assign CSRF tokens to anonymous users because\nwe don't want to support a session for every anonymous user. Instead, views\nthat need anonymous forms can be decorated with ``@anonymous_csrf``::\n\n    from session_csrf import anonymous_csrf\n\n    @anonymous_csrf\n    def login(request):\n        ...\n\n``anonymous_csrf`` uses the cache to give anonymous users a lightweight\nsession. It sends a cookie to uniquely identify the user and stores the CSRF\ntoken in the cache.  It can be controlled through these settings:\n\n    ``ANON_COOKIE``\n        the name used for the anonymous user's cookie\n\n        Default: ``anoncsrf``\n\n    ``ANON_TIMEOUT``\n        the cache timeout (in seconds) to use for the anonymous CSRF tokens\n\n        Default: ``60 * 60 * 2  # 2 hours``\n\nNote that by default Django uses local-memory caching, which will not\nwork with anonymous CSRF if there is more than one web server thread.\nTo use anonymous CSRF, you must configure a cache that's shared\nbetween web server instances, such as Memcached.  See the `Django cache\ndocumentation <https://docs.djangoproject.com/en/dev/topics/cache/>`_\nfor more information.\n\n\nIf you only want a view to have CSRF protection for logged-in users, you can\nuse the ``anonymous_csrf_exempt`` decorator. This could be useful if the\nanonymous view is protected through a CAPTCHA, for example.\n\n::\n\n    from session_csrf import anonymous_csrf_exempt\n\n    @anonymous_csrf_exempt\n    def protected_in_another_way(request):\n        ...\n\n\nIf you want all views to have CSRF protection for anonymous users as Django\ndoes, use the following setting:\n\n    ``ANON_ALWAYS``\n        always provide CSRF protection for anonymous users\n\n        Default: False\n\n\nWhy do I want this?\n-------------------\n\n1. Your site is on a subdomain with other sites that are not under your\n   control, so cookies could come from anywhere.\n2. You're worried about attackers using Flash to forge HTTP headers.\n3. You're tired of requiring a Referer header.\n\n\nWhy don't I want this?\n----------------------\n\n1. Storing tokens in sessions means you have to hit your session store more\n   often.\n2. It's a little bit more work to CSRF-protect forms for anonymous users.\n"
},
{
  "name": "task.js",
  "files": {
    "/": [
      "COPYING",
      "README.md",
      "examples",
      "lib"
    ]
  },
  "makefile": null,
  "readme": "# task.js\n\n* *What?*  Cooperative concurrency for [ES6](http://wiki.ecmascript.org/doku.php?id=harmony:proposals)\n* *Why?*   Who says JavaScript I/O has to be ugly?\n* *Where?* [http://taskjs.org](http://taskjs.org)\n* *When?*  As soon as your JS engine supports [generators](http://wiki.ecmascript.org/doku.php?id=harmony:generators)!\n* *How?*   [http://taskjs.org](http://taskjs.org)\n\ntask.js provides an **automatic task scheduler** along with a library of first-class, synchronizable\nevents, making it easy to do **I/O without callbacks**.\n\nWith task.js you can write non-blocking I/O in a synchronous style, even with error handling:\n\n``` javascript\nspawn(function*() {\n    try {\n        var [foo, bar] = yield join(read(\"foo.json\"),\n                                    read(\"bar.json\")).timeout(1000);\n        render(foo);\n        render(bar);\n    } catch (e) {\n        console.log(\"read failed: \" + e);\n    }\n});\n```\n\nCompared with callbacks:\n\n``` javascript\nvar foo, bar;\nvar tid = setTimeout(function() { failure(new Error(\"timed out\")) }, 1000);\n\nvar xhr1 = makeXHR(\"foo.json\",\n                   function(txt) { foo = txt; success() },\n                   function(err) { failure() });\nvar xhr2 = makeXHR(\"bar.json\",\n                   function(txt) { bar = txt; success() },\n                   function(e) { failure(e) });\n\nfunction success() {\n    if (typeof foo === \"string\" && typeof bar === \"string\") {\n        cancelTimeout(tid);\n        xhr1 = xhr2 = null;\n        render(foo);\n        render(bar);\n    }\n}\n    \nfunction failure(e) {\n    xhr1 && xhr1.abort();\n    xhr1 = null;\n    xhr2 && xhr2.abort();\n    xhr2 = null;\n    console.log(\"read failed: \" + e);\n}\n```\n\n...tasks can be a lot simpler and cleaner. And unlike pre-emptive\nthreads, `yield` always makes it clear where tasks block.\n\n# Contributing\n\nCurrently the best way to contribute is to **hang out on IRC**: the\nchannel is `#task.js` on [irc.mozilla.org](http://irc.mozilla.org). Or\nyou can always send me email (my Github nick at mozilla.com). And I'm\nalways happy to accept pull requests!\n\nIf you're looking for interesting things to work on, check out the\n**[issue tracker](task.js/issues)**.\n"
},
{
  "name": "Jisort",
  "files": {
    "/": [
      ".gitattributes",
      ".gitignore",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "LICENSE",
      "README.md",
      "app",
      "artifacts.gradle",
      "build.gradle",
      "docs",
      "gradle.properties",
      "gradle",
      "gradlew",
      "gradlew.bat",
      "keystore",
      "settings.gradle",
      "third_party",
      "versioning.gradle"
    ],
    "/docs": [
      "CUSTOM.md",
      "img"
    ]
  },
  "makefile": null,
  "readme": "# Jisort! [![Build Status](https://travis-ci.org/mozilla/Jisort.svg?branch=master)](https://travis-ci.org/mozilla/Jisort) [![Uses Mofo Standards](https://MozillaFoundation.github.io/mofo-standards/badge.svg)](https://github.com/MozillaFoundation/mofo-standards)\n\nJisort! is an informational prototype app for the Digital Skills Observatory project. It offers fun and visually interesting ways to increase basic smart phone awareness and skills, diagnose problems, and encourage exploration.\n\nThe Digital Skills Observatory project looks at the impact of digital skills on the usage of Digital Financial Services. Testing various teaching and delivery methods, the study aims to understand the skills people need to develop confidence and agency with their smart phones. [Learn more](http://mozillafoundation.github.io/digital-skills-observatory/).\n\n## Features\n\n  - Compatible with Android 4.0 (API 14) and above\n  - Simple and intuitive user interface\n  - Suggest content to users based on system events like low battery, or wifi status\n  - Customizable: create your own content using HTML tags and JSON\n  - Offline analytics collection system\n\n## Install\n\nJisort! is not available on app marketplaces such as Google Play. Follow the instructions below to manually install the app.\n\n  1. Startup the internet browser on the Android device you'd like to install Jisort! on\n  2. Enter the following website address into your browser's address bar: ```https://mzl.la/jisort```\n  3. Look for the version with the green \"latest release\" tag, and download the file ending with the extension name: ```apk```\n  4. Navigate to your downloads folder and verify that the downloaded file looks similar to the following: ```Jisort-qualityassurance-<version>-SNAPSHOT.apk```\n  5. Tap on the downloaded file to begin the installation process\n  6. If prompted with a pop-up asking if you'd like to install the application, select the appropriate answers (yes)\n  7. Once the application has been successfully installed, you will be notified and can find it in your app drawer\n\n## Contribute\n\nDid you know that the Digital Skills Observatory project is mostly run by community members just like you? Community contributions have played an important role in both the Digital Skills Observatory project, and the development of Jisort!\n\nIf you're interested in contributing, check out the [contributing guide](docs/CONTRIBUTE.md) and come say hello in our [chat room](https://chat.mozillafoundation.org).\n\n## License\n\nJisort! is licensed under [Mozilla Public License 2.0](LICENSE.md)\n\nHave questions about Mozilla Public License 2.0? Check out the [FAQ](https://www.mozilla.org/en-US/MPL/2.0/FAQ/)."
},
{
  "name": "brackets",
  "files": {
    "/": [
      ".appveyor.yml",
      ".brackets.json",
      ".eslintrc.json",
      ".gitattributes",
      ".gitignore",
      ".gitmodules",
      ".npmrc",
      ".travis.yml",
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "Gruntfile.js",
      "ISSUE_TEMPLATE.md",
      "LICENSE",
      "NOTICE",
      "README.md",
      "build.json",
      "env.dist",
      "invalidate.js",
      "locales",
      "package.json",
      "samples",
      "scripts",
      "src",
      "tasks",
      "templates",
      "test",
      "tools"
    ]
  },
  "makefile": null,
  "readme": "# Bramble is based on Brackets\n\nBrackets is a modern open-source code editor for HTML, CSS\nand JavaScript that is  *built* in HTML, CSS and JavaScript.\n\nBrackets is at 1.0 and we're not stopping there. We have many feature ideas on our\n[trello board](http://bit.ly/BracketsTrelloBoard) that we're anxious to add and other\ninnovative web development workflows that we're planning to build into Brackets.\nSo take Brackets out for a spin and let us know how we can make it your favorite editor.\n\nYou can see some\n[screenshots of Brackets](https://github.com/adobe/brackets/wiki/Brackets-Screenshots)\non the wiki, [intro videos](http://www.youtube.com/user/CodeBrackets) on YouTube, and news on the [Brackets blog](http://blog.brackets.io/).\n\nThe text editor inside Brackets is based on\n[CodeMirror](http://github.com/codemirror/CodeMirror)&mdash;thanks to Marijn for\ntaking our pull requests, implementing feature requests and fixing bugs! See\n[Notes on CodeMirror](https://github.com/adobe/brackets/wiki/Notes-on-CodeMirror)\nfor info on how we're using CodeMirror.\n\n# How to setup Bramble (Brackets) in your local machine\n\nStep 1: Make sure you fork and clone [Brackets](https://github.com/mozilla/brackets).\n\n```\n$ git clone https://github.com/[yourusername]/brackets --recursive\n```\n\nStep 2: Install its dependencies\n\nNavigate to the root of the directory you cloned and run:\n\n```\n$ npm install\n```\n\nNOTE: if you are running on Windows, and experience a build error with the `iltorb` package,\nconsider adding the `--no-optional` flag to have npm skip installing `iltorb`, which is optional\nand requires python, gyp and a working c++ build environment.\nSee comment in https://github.com/mozilla/brackets/pull/588#issuecomment-280438175\n\nStep 3: Run Bramble:\n\nThe easiest way to run Bramble is to simply use:\n\n```\n$ npm start\n```\n\nThis will generate the strings needed for localization in your `src/nls` folder and allow you to access Bramble on `localhost:8000` (NOTE: you need npm version 5 for the cleanup step to run properly; if it doesn't, use `npm run unlocalize` to restore the files in `src/nls/**/*`). It will also build the Bramble iframe API in `dist/` if necessary. You can terminate the server with `Ctrl+C` which will also clean up the strings that were generated in your `src/nls` folder. If any changes are made in the `src` directory, just refresh the page hosting Bramble in your browser to reflect those changes.\n\nIf you want to simply run the server without the localized strings, run:\n\n```\n$ npm run server\n```\n\nHowever, if you wish to run your own static server, there are several options available:\n* [Apache Webserver](http://www.apache.org/)\n* Host on [github pages](https://help.github.com/articles/what-are-github-pages)\n* [Python WebServer](https://docs.python.org/2/library/simplehttpserver.html)\n\nAssuming you have Bramble running on port `8000`. Now you can visit [http://localhost:8000/src](http://localhost:8000/src).\n\n**NOTE 1:** Bramble expects to be run in an iframe, which hosts its filesystem. For local\ndevelopment, use `src/hosted.html` instead of `src/index.html`.  To see how the remote end\nshould host Bramble's iframe, see `src/hosted.js`.\n\n**NOTE 2:** Using `npm run build` will overwrite contents in the `src/nls` folder. These changes are necessary if you access Bramble using [http://localhost:8000/src](http://localhost:8000/src). After using Bramble, you can undo the changes by running `npm run unlocalize`.\n\n**NOTE 3:** To use Bramble in a production setting locally, you can run `npm run production` and access Bramble at [http://localhost:8000/dist](http://localhost:8000/dist)\n\n# Extension Loading\n\nBramble loads a set of extensions defined in `src/extensions/bramble-extensions.json`. You can\nalter which extensions Bramble loads by adding or removing items from this list.  You can also\ntemporarily disable extensions by using `?disableExtensions`. For example: to disable QuickView\nand CSSCodeHints, load Bramble with `?disableExtensions=QuickView,CSSCodeHints` on the URL.\n\n--------------\n\n## After installation\n\nAfter you have everything setup, you can now run the server you chose in the root of your local Bramble directory and see it in action by visiting [http://localhost:8000/src](http://localhost:8000/src).\n\n# Bramble IFrame API\n\nBramble is designed to be run in an iframe, and the hosting web app to communicate with it\nvia `postMessage` and `MessageChannel`.  In order to simplify this, a convenience API exists\nfor creating and managing the iframe, as well as providing JavaScript functions for interacting\nwith the editor, preview, etc.\n\n## Loading the API\n\nThe hosting app must include the Bramble IFrame API (i.e., `dist/bramble.js`).  Note: in\ndevelopment you can use `src/hosted.html`, which does this.  This script can either be used as\nan AMD module, or as a browser global:\n\n```html\n<script src=\"bramble.js\"></script>\n<script>\n  // Option 1: AMD loading, assumes requirejs is loaded already\n  require([\"bramble\"], function(Bramble) {\n    ...\n  });\n\n  // Option 2: Browser global\n  var Bramble = window.Bramble;\n</script>\n```\n\n## Bramble\n\nThe Bramble module has a number of methods, properties, and events. During its lifetime,\nBramble goes through a number of states, including:\n\n* `Bramble.ERROR` - Bramble is in an error state\n* `Bramble.NOT_LOADED` - Initial state, `Bramble.load()` has not been called\n* `Bramble.LOADING` - `Bramble.load()` has been called, loading resources has begun\n* `Bramble.MOUNTABLE` - Loading is done and `Bramble.mount()` can be begin, or is safe to start\n* `Bramble.MOUNTING` - `Bramble.mount()` is being called, mounting is in process\n* `Bramble.READY` - `Bramble.mount()` has finished, Bramble is fully ready\n\nThe current state of Bramble can be obtained by calling `Bramble.getReadyState()`.  There are\nalso a number of events you can listen for (i.e., `Bramble` is an [`EventEmitter`](https://github.com/Wolfy87/EventEmitter/)):\n\n```js\nBramble.once(\"ready\", function(bramble) {\n  // bramble is the Bramble proxy instance, see below.\n});\n\nBramble.on(\"error\", function(err) {\n  // Bramble is in an error state, and `err` is the error.\n})\n\nBramble.on(\"readyStateChange\", function(previous, current) {\n  // Bramble's readyState changed from `previous` to `current`\n});\n```\n\nNOTE: in some browsers (e.g., Firefox) when the user is in \"Private Browsing\"\nmode, the filesystem (i.e., IndexedDB) will be inaccessible, and an error\nwill be sent via the `error` event (i.e., `err.code === \"EFILESYSTEMERROR\"`).  This\nis the same error that occurs when the filesystem is corrupt (see `autoRecoverFileSystem` below).\n\n## Bramble Offline Support\n\nThe Bramble code is offline capable, and will indicate, via events, when it is ready to be used offline, as well as\nwhen there are updates available for existing offline cached resources. These events are triggered on `Bramble` vs.\nthe `bramble` instance.  The offline related events include:\n\n* `\"offlineReady\"` - triggered when Bramble has been fully cached for offline use.  Users can safely work without network.\n* `\"updatesAvailable\"` - triggered when new or updated Bramble resources have been cached and are available for use. You might use this to indicate to the user that they should refresh the page to begin using the updates.\n\n## Bramble.getFileSystem()\n\nThe FileSystem is owned by the hosting application, and can be obtained at any time by calling:\n\n```js\nvar fs = Bramble.getFileSystem();\n```\n\nThis `fs` instance can be used to setup the filesystem for the Bramble editor prior to\nloading.  You can access things like `Path` and `Buffer` via `Bramble.Filer.*`.\n\n## Bramble.formatFileSystem(callback)\n\nWARNING: this **will** destroy data, and is meant to be used in the case that\nthe filesystem is corrupted (`err.code === \"EFILESYSTEMERROR\"`), or for when an\napp wants to allow a user to wipe their disk.\n\n```js\nBramble.on(\"error\", function(err) {\n  if(err.code === \"EFILESYSTEMERROR\") {\n    Bramble.formatFileSystem(function(err) {\n      if(err) {\n        // Unable to create filesystem, fatal (and highly unlikely) error\n      } else {\n        // filesystem is now clean and empty, use Bramble.getFileSystem() to obtain instance\n      }\n    });\n  }\n});\n```\n\nNOTE: you can turn this recovery behaviour on automatically by passing `autoRecoverFileSystem: true`\nin the options to `Bramble.load()`.\n\n## Bramble.load(elem[, options])\n\nOnce you have a reference to the `Bramble` object, you use it to starting loading the editor:\n\n```js\n// Start loading Bramble\nBramble.load(\"#webmaker-bramble\");\n\nBramble.once(\"error\", function(err) {\n  console.error(\"Bramble error\", err);\n});\n```\n\nThe `elem` argument specifies which element in the DOM should be used to hold the iframe.\nThis element's contents will be replaced by the iframe.  You can pass a selector, a reference\nto an actual DOM element, or leave it blank, and `document.body` will be used.\n\nThe `options` object allows you to configure Bramble:\n\n * `url`: `<String>` a URL to use when loading the Bramble iframe (defaults to prod)\n * `locale`: `<String>` the locale Brackets should use\n * `useLocationSearch`: `<Boolean>` whether to copy the window's location.search string to the iframe's url\n * `extensions:` `<Object>` with the following optional properties\n     * `enable`: `<Array(String)>` a list of extensions to enable\n     * `disable`: `<Array(String)>` a list of extensions to disable\n * `hideUntilReady`: `<Boolean>` whether to hide Bramble until it's fully loaded.\n * `disableUIState`: `<Boolean>` by default, UI state is kept between sessions.  This disables it (and clears old values), and uses the defaults from Bramble.\n * `autoRecoverFileSystem`: `<Boolean>` whether to try and autorecover the filesystem on failure (see `Bramble.formatFileSystem` above).\n * `debug`: `<Boolean>` whether to log debug info.\n * `zipFilenamePrefix`: `<String>` the prefix name to use for project zip files, or `\"thimble-project\"` by default.\n * `capacity`: `<Number>` the number of bytes of disk space to allow for this project.  Defaults to 5MB if not set.\n\n## Bramble.mount(root[, filename])\n\nAfter calling `Bramble.load()`, you can tell Bramble which project root directory\nto open, and which file to load into the editor.  NOTE: the optional `filename` argument,\nif provided, should be a relative path within the project root.  Bramble will use this information\nwhen it is ready to mount the filesystem.  Use the `\"ready\"` event to get access to the\n`bramble` instance:\n\n```js\n// Setup the filesystem while Bramble is loading\nvar fs = Bramble.getFileSystem();\n\nBramble.once(\"ready\", function(bramble) {\n  // The bramble instance is now usable, see below.\n});\n\nfs.mkdir(\"/project\", function(err) {\n  // If we run this multiple times, the dir will already exist\n  if (err && err.code !== \"EEXIST\") {\n    throw err;\n  }\n\n  var html = \"\"                    +\n    \"<html>\\n\"                     +\n    \"  <head>\\n\"                   +\n    \"    <title>Bramble</title>\\n\" +\n    \"  </head>\\n\"                  +\n    \"  <body>\\n\"                   +\n    \"    <p>Hello World</p>\\n\"     +\n    \"  </body>\\n\"                  +\n    \"</html>\";\n\n  fs.writeFile(\"/project/index.html\", html, function(err) {\n    if (err) {\n      throw err;\n    }\n\n    // Now that fs is setup, tell Bramble which root dir to mount\n    // and which file within that root to open on startup.\n    Bramble.mount(\"/project\", \"index.html\");\n  });\n});\n```\n\n## Bramble Instance Getters\n\nOnce the Bramble instance is created (e.g., via `ready` event or `Bramble.mount()` callback),\na number of read-only getters are available in order to access state information in the Bramble editor:\n\n* `getID()` - returns the iframe element's `id` in the DOM\n* `getIFrame()` - returns a reference to the iframe that hosts Bramble\n* `getFullPath()` - returns the absolute path of the file currently being edited\n* `getFilename()` - returns the filename portion (i.e., no dir info) of the file currently being edited\n* `getPreviewMode()` - returns one of `\"mobile\"` or `\"desktop\"`, depending on current preview mode\n* `getSidebarVisible()` - returns `true` or `false` depending on whether the sidebar (file tree) is visible\n* `getLayout()` - returns an `Object` with three integer properties: `sidebarWidth`, `firstPaneWidth`, `secondPaneWidth`.  The `firstPaneWidth` refers to the editor, where `secondPaneWidth` is the preview.\n* `getRootDir()` - returns the project root directory to which Bramble is mounted\n* `getTheme()` - returns the name of the current theme.\n* `getFontSize()` - returns the current font size as a string (e.g., `\"12px\"`).\n* `getWordWrap()` - returns the current word wrap setting as a `Boolean` (i.e., enabled or disabled).\n* `getAllowJavaScript()` - returns the current allow javascript setting as a `Boolean` (i.e., enabled or disabled).\n* `getAutocomplete()` - returns the current autocomplete settings as a `Boolean` (i.e., enabled or disabled).\n* `getAutoCloseTags()` - returns the current close tags setting as an `Object` with three properties: `whenOpening` a boolean that determines whether opening tags are closed upon typing \">\", `whenClosing` a boolean that determines whether closing tags are closed upon typing \"/\", and an array of tags `indentTags`, that when opened, has a blank line. These values default to, respectively: `true`, `true`, and an empty array.\n* `getTutorialExists()` - returns `true` or `false` depending on whether or not there is a tutorial in the project (i.e., if `tutorial.html` is present)\n* `getTutorialVisible()` - returns `true` or `false` depending on whether or not the preview browser is showing a tutorial or not.\n* `getAutoUpdate()` - returns `true` or `false` depending on whether or not the auto update preference is enabled or not.\n* `getTotalProjectSize()` - returns the current project size in bytes.\n* `hasIndexFile()` - returns `true` or `false` depending on whether or not there is an `\"index.html\"` file.\n* `getFileCount()` - returns total file count.\n\n**NOTE**: calling these getters before the `ready()` callback on the bramble instance\nwon't do what you want.\n\n## Bramble Instance Methods\n\nThe Bramble instance has a number of methods you can call in order to interact with the\nBramble editor and preview, all of which take an optional `callback` argument if you want\nto be notified when the action completes:\n\n* `undo([callback])` - undo the last operation in the editor (waits for focus)\n* `redo([callback])` - redo the last operation that was undone in the editor (waits for focus)\n* `increaseFontSize([callback])` - increases the editor's font size\n* `decreaseFontSize([callback])` - decreases the editor's font size\n* `restoreFontSize([callback])` - restores the editor's font size to normal\n* `save([callback])` - saves the current document\n* `saveAll([callback])` - saves all \"dirty\" documents\n* `useHorizontalSplitView([callback])` - splits the editor and preview horizontally\n* `useVerticalSplitView([callback])` - splits the editor and preview vertically (default)\n* `find([callback])` - opens the Find dialog to search within the current document\n* `findInFiles([callback])` - opens the Find in Files dialog to search in all project files\n* `replace([callback])` - opens the Replace dialog to replace text in the current document\n* `replaceInFiles([callback])` - opens the Replace in Files dialog to replace text in all project files\n* `useLightTheme([callback])` - sets the editor to use the light theme (default)\n* `useDarkTheme([callback])` - sets the editor to use the dark theme\n* `showSidebar([callback])` - opens the file tree sidebar\n* `hideSidebar([callback])` - hides the file tree sidebar\n* `showStatusbar([callback])` - enables and shows the statusbar\n* `hideStatusbar([callback])` - disables and hides the statusbar\n* `showPreview([callback])` - opens the preview pane\n* `hidePreview([callback])` - hides the preview pane\n* `refreshPreview([callback])` - reloads the preview with the latest content in the editor and filesystem\n* `useMobilePreview([callback])` - uses a Mobile view in the preview, as it would look on a smartphone\n* `useDesktopPreview([callback])` - uses a Desktop view in the preview, as it would look on a desktop computer (default)\n* `enableFullscreenPreview([callback])` - shows a fullscreen preview of the current file\n* `disableFullscreenPreview([callback])` - turns off the fullscreen preview of the current file\n* `enableAutoUpdate([callback])` - turns on auto-update for the preview (default)\n* `disableAutoUpdate([callback])` - turns off auto-update for the preview (manual reloads still work)\n* `enableJavaScript([callback])` - turns on JavaScript execution for the preview (default)\n* `disableJavaScript([callback])` - turns off JavaScript execution for the preview\n* `enableInspector([callback])` - turns on the preview inspector (shows code for hovered/clicked element)\n* `disableInspector([callback])` - turns off the preview inspector (default)\n* `enableWordWrap([callback])` - turns on word wrap for the editor (default)\n* `disableWordWrap([callback])` - turns off word wrap for the editor\n* `configureAutoCloseTags(options, [callback])` - enables/disables close tags for the editor using the provided options which consists of an `Object` that includes three properties: `whenOpening` a boolean, `whenClosing` a boolean, and an array `indentTags`.\n* `showTutorial([callback])` - shows tutorial (i.e., tutorial.html) vs editor contents in preview\n* `hideTutorial([callback])` - stops showing tutorial (i.e., tutorial.html) and uses editor contents in preview\n* `showUploadFilesDialog([callback])` - shows the Upload Files dialog, allowing users to drag-and-drop, upload a file, or take a selfie.\n* `addNewFile([options, callback])` - adds a new text file, using the provided options, which can include: `filename` a `String` with the complete filename to use; `contents` a `String` with the new text file's data; `ext` a `String` with the new file's extension; `basenamePrefix` a `String` with the basename to use when generating a new filename.  NOTE: if you provide `filename`, `basenamePrefix` and `ext` are ignored.\n* `addNewFolder([callback])` - adds a new folder.\n* `export([callback])` - creates an archive `.zip` file of the entire project's filesystem, and downloads it to the browser.\n* `addCodeSnippet(snippet, [callback])` - adds a new code `snippet` to the editor (if it is in focus) at the current cursor position. One required parameter (`snippet`) needs to be passed in which needs to be a `String`.\n* `openSVGasXML([callback])` - treats `.svg` files as XML and shows them in the text editor.\n* `openSVGasImage([callback])` - treats `.svg` files as Images and shows them in image viewer.\n\n## Bramble Instance Events\n\nThe Bramble instance is also an [`EventEmitter`](https://github.com/Wolfy87/EventEmitter/) and raises\nthe following events:\n\n* `\"layout\"` - triggered whenever the sidebar, editor, or preview panes are changed. It includes an `Object` that returns the same information as the `getLayout()` getter: `sidebarWidth`, `firstPaneWidth`, `secondPathWidth`\n* `\"activeEditorChange\"` - triggered whenever the editor changes from one file to another. It includes an `Object` with the current file's `fullPath` and `filename`.\n* `\"previewModeChange\"` - triggered whenever the preview mode is changed. It includes an `Object` with the new `mode`\n* `\"sidebarChange\"` - triggered whenever the sidebar is hidden or shown. It includes an `Object` with a `visible` property set to `true` or `false`\n* `\"themeChange\"` - triggered whenever the theme changes. It includes an `Object` with a `theme` property that indicates the new theme\n* `\"fontSizeChange\"` - triggered whenever the font size changes. It includes an `Object` with a `fontSize` property that indicates the new size (e.g., `\"12px\"`).\n* `\"wordWrapChange\"` - triggered whenever the word wrap value changes. It includes an `Object` with a `wordWrap` property that indicates the new value (e.g., `true` or `false`).\n* `\"allowJavaScriptChange\"` - triggered whenever the allow javascript value changes. It includes an `Object` with a `allowJavaScript` property that indicates the new value (e.g., `true` or `false`).\n* `\"autoCloseTagsChange\"` - triggered whenever the close tag value changes. It includes an `Object` with a `autoCloseTags` property that indicates the new value\n* `\"tutorialAdded\"` - triggered when a new tutorial is added to the project\n* `\"tutorialRemoved\"` - triggered when an existing tutorial for the project is removed\n* `\"tutorialVisibilityChange\"` - triggered when the tutorial preview is turned on or off. It includes an `Object` with a `visibility` property that indicates whether the tutorial is visible.\n* `\"inspectorChange\"` - triggered whenever the inspector changes from enabled to disabled, or vice versa. It includes an `Object` with an `enabled` property set to `true` or `false`.\n* `\"autoUpdateChange\"` - triggered whenever the auto update preference changes from enabled to disabled, or vice versa. It includes an `Object` with a `autoUpdate` property set to `true` or `false`\n* `\"projectDirty\"` - triggered when one of the files in the project has been edited and those changes haven't been saved yet. It includes an `Object` with the `path` to the current file.\n* `\"projectSaved\"` - triggered whenever the changes are saved to the filesystem in the browser are completed.\n* `\"dialogOpened\"` - triggered whenever a modal dialog opens, like when a user is deleting a file.\n* `\"dialogClosed\"` - triggered whenever a modal dialog closes.\n* `\"capacityExceeded\"` - triggered whenever the project's files reach or exceed the maximum allowed disk capacity. Some operations will be disallowed until sufficient space has been recovered (e.g., user deletes files). A second argument, `size`, indicates the number of bytes the project is over capacity.\n* `\"capacityRestored\"` - triggered after a `\"capacityExceeded\"` event when sufficient space has been recovered to continue normal disk activity.\n* `\"projectSizeChange\"` - triggered when the project's size on disk changes. The event includes two arguments: `size`, which is the new size of the project in bytes, and `percentUsed` which is a percentage of disk space used out of the total available capacity.\n\nThere are also high-level events for changes to files:\n\n* `\"fileChange\"` - triggered whenever a file is created or updated within the project root.  It includes the `filename` of the file that changed.\n* `\"fileDelete\"` - triggered whenever a file is deleted within the project root.  It includes the `filename` of the file that was deleted.\n* `\"fileRename\"` - triggered whenever a file is renamed within the project root.  It includes the `oldFilename` and the `newFilename` of the file that was renamed.\n* `\"folderRename\"` - triggered whenever a folder is renamed within the project root. It includes an object that looks something like this:\n```js\n{\n  oldPath: \"/path/before/rename\",\n  newPath: \"/path/after/rename\",\n  // Paths to all files contained inside the folder being renamed\n  children: [ \"relativeFilePath1\",  \"relativeFilePath2\", ... ]\n}\n```\n\nNOTE: if you want to receive generic events for file system events, especially events across windows using the same file system, use [fs.watch()](https://github.com/filerjs/filer#watch) instead.\n\n# Troubleshooting\n\nIf you forgot to add the `--recursive` flag while cloning this repository, you might run into a similar error:\n\n```bash\nTracing dependencies for: main\nError: ENOENT: no such file or directory, open '[..]/brackets/src/thirdparty/text/text.js'\nIn module tree:\n    brackets\n      language/LanguageManager\n        file/FileUtils\n          utils/Global\n```\n\nTo fix it, run `git submodule update --init --recursive` in the main directory."
},
{
  "name": "mozillaclubs",
  "files": {
    "/": [
      "CODE_OF_CONDUCT.md",
      "CONTRIBUTING.md",
      "DOTClubs",
      "LICENSE.md",
      "README.md",
      "Teaching_activities",
      "_config.yml",
      "club_guides",
      "designresources",
      "images",
      "orientation_mozilla_clubs",
      "readinglist.md"
    ]
  },
  "makefile": null,
  "readme": "![Imgur](http://i.imgur.com/61WeiVu.png)\n\n<details><summary><sub><b>Check Content</b></sub></summary>\n\nTable of Contents (ToC)\n========================\n\n* [<sub>About Mozilla Clubs</sub>](#about-mozilla-clubs)\n* [<sub>Connect with Mozilla Clubs</sub>](#connect-with-mozilla-clubs)\n* [<sub>Participate in the Repo</sub>](#participate-in-the-repo)\n\n#### About Mozilla Clubs\n\n<sub>A Mozilla Club brings people together locally to explore, participate in and create the open web in an engaging and collaborative way. They are groups that meet regularly and host events or meet-ups where creativity, collaboration, teaching, and learning come together with one objective: developing digital skills.</sub>\n\n<sub>There are currently 400+ registered Clubs that span 50+ countries. [Mozilla Clubs](http://learning.mozilla.org/) Captains have the opportunity to join several digital channels that help them connect to each other and share experiences.</sub>\n\n---\n\n#### Connect with Mozilla Clubs \n\n<sub>Mozilla Clubs also has a [Facebook group](https://www.facebook.com/groups/mozillaclubs/) with 1000+ participants, a [learning forum](https://forum.learning.mozilla.org/c/mozilla-clubs) with 80+ topic threads, a monthly Club Leaders Call with featured guests and an [event gallery](http://mozilla.github.io/clubs-events/).</sub>\n\n---\n\n#### Participate in the Repo\n\n<sub>In [this Repository](https://github.com/mozilla/mozillaclubs) Clubs leaders can find, share, comment and remix projects around Design resources and Teaching activities for Mozilla Clubs.</sub>\n\n<sub>These are the themes we're already working on. Feel free to add an issue in case you want to start a new one!</sub>\n\n* [<sub>Desing resources.</sub>](https://github.com/mozilla/mozillaclubs/tree/master/designresources) \n<sub>Images.  Find design resources created by community members which you can use, remix and share.</sub>\n* [<sub>Club Guides.</sub>](https://github.com/mozilla/mozillaclubs/tree/master/club_guides) <sub>This folder contains Mozilla Clubs guides & resources developed in collaboration with community members.</sub>\n* [<sub>Orientation for Mozilla Clubs.</sub>](https://github.com/mozilla/mozillaclubs/tree/master/orientation_mozilla_clubs) <sub>This is the Orientation that Mozilla Club Leaders take at mozilla.teachable.com,</sub> \n* [<sub>Teaching activities.</sub>](https://github.com/mozilla/mozillaclubs/tree/master/Teaching_activities) <sub>This folder is intended to hold curriculum created by communitiy members. You can and and share any activities you have used/developed for your club.</sub> \n\n---\n## Thank you\n\nThank you so much for visiting and partcipating in Mozilla Clubs. We hope you find this repository helpful and use it in ways that help move your work forward. \n\n## Glossary\n\n* **README file**: a document that introduces an open project to the public and any potential contributors\n* **repository** or **repo**: a collection of documents related to your project, in which you create and save new code or content\n* **Roadmap**: a document outlining the schedule of work to be done on a project\n* **Milestone**: an event or state marking a specific stage in development on the project\n* **Issue**: the GitHub term for tasks, enhancements, and bugs for your projects\n\n<img src=\"https://pbs.twimg.com/profile_images/821735271049768960/jJZXlJwZ.jpg\" width=\"50\"></img> \n<img src=\"https://orig00.deviantart.net/5b95/f/2016/070/3/b/mit_license_logo_by_excaliburzero-d9ur2lg.png\" width=\"70\"></img> \n</details>\n"
},
{
  "name": "doctorjs",
  "files": {
    "/": [
      ".gitignore",
      ".gitmodules",
      "LICENSE",
      "Makefile",
      "README.md",
      "bin",
      "example.js",
      "html",
      "js",
      "lib",
      "narcissus",
      "package.json",
      "serve.js",
      "test"
    ]
  },
  "makefile": "PREFIX=/usr/local\nINSTALL=install\nNODE=node\nNODE_PATH=`pwd`/lib/jsctags\nPROFILE=~/.profile\n\nBIN_SRC=$(addprefix bin/,jsctags.js)\nLIB_SRC=$(addprefix lib/jsctags/,getopt.js log.js paperboy.js traits.js \\\n\tunderscore.js)\nLIB_CTAGS_SRC=$(addprefix lib/jsctags/ctags/,index.js interp.js nativefn.js \\\n\treader.js writer.js)\nLIB_CFA2_SRC=$(addprefix lib/cfa2/,index.js jscfa.js)\nLIB_NARCISSUS_SRC=$(addprefix narcissus/lib/,../main.js decompiler.js \\\n\tdefinitions.js desugaring.js jsbrowser.js jsdecomp.js jsdefs.js \\\n\tjsdesugar.js jsexec.js jslex.js jsparse.js jsresolve.js \\\n\tlexer.js parser.js)\n\ninstall:\n\t$(INSTALL) -d $(PREFIX)/bin\n\t$(INSTALL) $(BIN_SRC) $(BIN_SRC:%.js=$(PREFIX)/%)\n\t$(INSTALL) -d $(PREFIX)/lib/jsctags\n\t$(INSTALL) $(LIB_SRC) $(PREFIX)/lib/jsctags\n\t$(INSTALL) -d $(PREFIX)/lib/jsctags/ctags\n\t$(INSTALL) $(LIB_CTAGS_SRC) $(PREFIX)/lib/jsctags/ctags\n\t$(INSTALL) -d $(PREFIX)/lib/cfa2\n\t$(INSTALL) $(LIB_CFA2_SRC) $(PREFIX)/lib/cfa2\n\t$(INSTALL) -d $(PREFIX)/narcissus\n\t$(INSTALL) -d $(PREFIX)/narcissus/lib\n\t$(INSTALL) $(LIB_NARCISSUS_SRC) $(PREFIX)/narcissus/lib\n\techo \"export NODE_PATH=$(PREFIX)/lib/jsctags/:\\$$NODE_PATH\" >> $(PROFILE)\n\t@echo \"\\nIf you want to use jsctags right here, right now,\\n\\\n\tplease type this in your terminal:\\n\\n\\\n\t    . $(PROFILE)\\n\"\n\t@echo \"\\nIf you want jsctags to work anywhere in the terminal,\\n\\\n\tadd this to your ~/.bashrc (Linux) or ~/.bash_profile (OSX):\\n\\n\\\n        NODE_PATH='$(PREFIX)/lib/jsctags:\\$${NODE_PATH}'\\n\"\n\nuninstall:\n\trm -rf $(BIN_SRC:%.js=$(PREFIX)/%) $(PREFIX)/lib/jsctags \\\n\t  $(PREFIX)/lib/cfa2 $(PREFIX)/narcissus\n\tcp $(PROFILE) $(PROFILE).bak\n\tsed -e \"s,^export NODE_PATH=$(PREFIX)/lib/jsctags/:\\$$NODE_PATH$$,,g\" \\\n\t  < $(PROFILE) > $(PROFILE)-tmp\n\tmv $(PROFILE)-tmp $(PROFILE)\n\nserve:\n\t$(NODE) serve.js\n\ntags:\n\tNODE_PATH=$(NODE_PATH) $(NODE) bin/jsctags.js js lib/jsctags\n\n.PHONY:\tall install uninstall serve tags\n\n",
  "readme": "Overview\n--------\njsctags is a [ctags] [1]-compatible code indexing solution for JavaScript. Its\ninterface and output are essentially identical to [Exuberant Ctags] [2], but,\ninstead of simply parsing the JavaScript, jsctags uses a simple form of\nabstract interpretation to determine which symbols are exported. This allows\njsctags to achieve much better results than Exuberant Ctags. Popular libraries\nsuch as jQuery and CommonJS modules can now be meaningfully indexed.\n\nYou can use jsctags to create `tags` files that are usable in many editors,\nfrom Vim to TextMate (via the [CodeBrowser] [3] plugin). jsctags is slated to\nbecome a key component of the [Bespin] [4] IDE, where it will be used to\nprovide code completion.\n\njsctags is written entirely in JavaScript, using CommonJS modules, the\n[node.js] [5] framework, and the [Narcissus] [6] engine.\n\nLicense\n-------\njsctags is tri-licensed under the Mozilla Public License 1.1, the GNU General\nPublic License 2.0, and the GNU Lesser General Public License 2.1.\n\nRequirements\n------------\n* node.js\n* `make`\n\nBuilding\n--------\nTo install:\n\n* `make install`\n\nTo uninstall:\n\n* `make uninstall`\n\nTo play with Narcissus' parser:\n\n* `make serve`\n* Navigate to [`http://localhost:8080/html/parser.html`] [parser].\n\nUsage\n-----\nSimply go to your project root and invoke `jsctags lib` (replacing `lib` with\nthe directory in which your JavaScript source files are stored). The `tags`\nfile will be placed in the current directory.\n\nFor more options, try `jsctags -h`.\n\nDirectory structure\n-------------------\nThe directory structure mostly follows the CommonJS packaging scheme:\n\n* `bin/`: tools runnable from node.js (should be directly executable in Unix)\n* `html/`: in-browser demo files\n* `js/`: support files for the HTML demos\n* `lib/`: CommonJS-compliant library files\n* `lib/ctags/`: the core jsctags code\n* `narcissus/`: the Narcissus engine\n* `test/`: test cases for the indexer\n\n[1]: http://en.wikipedia.org/wiki/Ctags\n[2]: http://ctags.sourceforge.net/\n[3]: http://www.cocoabits.com/TmCodeBrowser/\n[4]: http://mozillalabs.com/bespin/\n[5]: http://nodejs.org/\n[6]: http://mxr.mozilla.org/mozilla/source/js/narcissus/\n\n[parser]: http://localhost:8080/html/parser.html\n\n"
},
{
  "name": "mozilla-central",
  "files": {
    "/": [
      "README.md"
    ]
  },
  "makefile": null,
  "readme": "This repository is retired\n==========================\n\nYou probably want to use https://github.com/mozilla/gecko-dev and https://github.com/mozilla/gecko-projects now.  Please see this discussion on why this happened: http://bit.ly/17U3eMC\n\nTo migrate your repo to those:\n\n * http://blog.monotonous.org/2013/11/26/changing-gecko-git-mirrors/ is a quick way to migrate a single branch over.\n * https://bugzilla.mozilla.org/show_bug.cgi?id=929338 has a rebase script that can help you migrate.\n\nQuestions? Need help?  Ask in #git or ping ehsan at mozilla.\n\n"
},
]
